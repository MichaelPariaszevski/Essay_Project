{"title": "Harnessing AI for precision tonsillitis diagnosis: a revolutionary approach in endoscopic analysis.", "abstract": "Diagnosing and treating tonsillitis pose no significant challenge for otolaryngologists; however, it can increase the infection risk for healthcare professionals amidst the coronavirus pandemic. In recent years, with the advancement of artificial intelligence (AI), its application in medical imaging has also thrived. This research is to identify the optimal convolutional neural network (CNN) algorithm for accurate diagnosis of tonsillitis and early precision treatment.\nSemi-supervised learning with pseudo-labels used for self-training was adopted to train our CNN, with the algorithm including UNet, PSPNet, and FPN. A total of 485 pharyngoscopic images from 485 participants were included, comprising healthy individuals (133 cases), patients with the common cold (295 cases), and patients with tonsillitis (57 cases). Both color and texture features from 485 images are extracted for analysis.\nUNet outperformed PSPNet and FPN in accurately segmenting oropharyngeal anatomy automatically, with average Dice coefficient of 97.74% and a pixel accuracy of 98.12%, making it suitable for enhancing the diagnosis of tonsillitis. The normal tonsils generally have more uniform and smooth textures and have pinkish color, similar to the surrounding mucosal tissues, while tonsillitis, particularly the antibiotic-required type, shows white or yellowish pus-filled spots or patches, and shows more granular or lumpy texture in contrast, indicating inflammation and changes in tissue structure. After training with 485 cases, our algorithm with UNet achieved accuracy rates of 93.75%, 97.1%, and 91.67% in differentiating the three tonsil groups, demonstrating excellent results.\nOur research highlights the potential of using UNet for fully automated semantic segmentation of oropharyngeal structures, which aids in subsequent feature extraction, machine learning, and enables accurate AI diagnosis of tonsillitis. This innovation shows promise for enhancing both the accuracy and speed of tonsillitis assessments.", "journal": "European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the German Society for Oto-Rhino-Laryngology - Head and Neck Surgery", "date": "2024-09-04", "authors": ["Po-HsuanJeng", "Chien-YiYang", "Tien-RuHuang", "Chung-FengKuo", "Shao-ChengLiu"], "doi": "10.1007/s00405-024-08938-w\n10.2174/187152612801319230\n10.1136/jrnms-101-69\n10.1017/s0950268800050172\n10.1002/jmri.28381\n10.1109/jbhi.2022.3182722\n10.1371/journal.pdig.0000058\n10.1016/j.media.2022.102487\n10.1007/978-3-030-33128-3_1\n10.1016/j.compbiomed.2020.103980\n10.1155/2022/4189781"}
{"title": "Semi-supervised CT image segmentation via contrastive learning based on entropy constraints.", "abstract": "Deep learning-based methods for fast target segmentation of computed tomography (CT) imaging have become increasingly popular. The success of current deep learning methods usually depends on a large amount of labeled data. Labeling medical data is a time-consuming and laborious task. Therefore, this paper aims to enhance the segmentation of CT images by using a semi-supervised learning method. In order to utilize the valid information in unlabeled data, we design a semi-supervised network model for contrastive learning based on entropy constraints. We use CNN and Transformer to capture the image's local and global feature information, respectively. In addition, the pseudo-labels generated by the teacher networks are unreliable and will lead to degradation of the model performance if they are directly added to the training. Therefore, unreliable samples with high entropy values are discarded to avoid the model extracting the wrong features. In the student network, we also introduce the residual squeeze and excitation module to learn the connection between different channels of each layer feature to obtain better segmentation performance. We demonstrate the effectiveness of the proposed method on the COVID-19 CT public dataset. We mainly considered three evaluation metrics: DSC, HD", "journal": "Biomedical engineering letters", "date": "2024-09-02", "authors": ["ZhiyongXiao", "HaoSun", "FeiLiu"], "doi": "10.1007/s13534-024-00387-y\n10.1148/radiol.2020200432\n10.1016/j.bspc.2021.102897\n10.1016/j.bspc.2023.104825\n10.1016/j.media.2020.101836\n10.1109/TMI.2020.3001810\n10.1002/mp.14676\n10.48550/arXiv.1706.03762\n10.1016/j.jfoodeng.2023.111833\n10.1016/j.cmpb.2022.107099\n10.1016/j.neuroimage.2024.120608\n10.1016/j.knosys.2021.108021\n10.1016/j.eswa.2023.120836\n10.1016/j.compbiomed.2023.107398\n10.1109/TNNLS.2021.3066850\n10.5555/2976040.2976107\n10.5555/3157096.3157227\n10.48550/arXiv.1703.01780\n10.1016/j.media.2023.102792\n10.1109/TMI.2020.2996645\n10.1016/j.media.2022.102517"}
{"title": "Deep Learning-Based System Combining Chest X-Ray and Computerized Tomography Images for COVID-19 Diagnosis.", "abstract": "", "journal": "British journal of hospital medicine (London, England : 2005)", "date": "2024-08-31", "authors": ["HuiDing", "LingyanFan", "JingfengZhang", "GuoshengGao"], "doi": "10.12968/hmed.2024.0244"}
{"title": "A deep convolutional neural network approach using medical image classification.", "abstract": "The epidemic diseases such as COVID-19 are rapidly spreading all around the world. The diagnosis of epidemic at initial stage is of high importance to provide medical care to and recovery of infected people as well as protecting the uninfected population. In this paper, an automatic COVID-19 detection model using respiratory sound and medical image based on internet of health things (IoHT) is proposed. In this model, primarily to screen those people having suspected Coronavirus disease, the sound of coughing used to detect healthy people and those suffering from COVID-19, which finally obtained an accuracy of 94.999%. This approach not only expedites diagnosis and enhances accuracy but also facilitates swift screening in public places using simple equipment. Then, in the second step, in order to help radiologists to interpret medical images as best as possible, we use three pre-trained convolutional neural network models InceptionResNetV2, InceptionV3 and EfficientNetB4 and two data sets of chest radiology medical images, and CT Scan in a three-class classification. Utilizing transfer learning and pre-existing knowledge in these models leads to notable improvements in disease diagnosis and identification compared to traditional techniques. Finally, the best result obtained for CT-Scan images belonging to InceptionResNetV2 architecture with 99.414% accuracy and for radiology images related to InceptionV3 and EfficientNetB4 architectures with the accuracy is 96.943%. Therefore, the proposed model can help radiology specialists to confirm the initial assessments of the COVID-19 disease.", "journal": "BMC medical informatics and decision making", "date": "2024-08-31", "authors": ["MohammadMousavi", "SoodehHosseini"], "doi": "10.1186/s12911-024-02646-5\n10.1016/j.aej.2021.06.024\n10.1016/j.bspc.2021.102920\n10.1016/j.compbiomed.2021.105020\n10.3390/jpm11010028\n10.1109/TMI.2020.2995508\n10.1016/j.asoc.2020.106691\n10.1109/ACCESS.2020.3018028\n10.1109/JIOT.2020.3034074\n10.1109/JBHI.2024.3383591\n10.1109/TFUZZ.2023.3338565\n10.1109/TETC.2023.3239949\n10.1016/j.csbj.2024.06.032\n10.1016/j.snb.2019.127098\n10.1109/COMST.2019.2914094"}
{"title": "Machine-Learning-Enabled Diagnostics with Improved Visualization of Disease Lesions in Chest X-ray Images.", "abstract": "The class activation map (CAM) represents the neural-network-derived region of interest, which can help clarify the mechanism of the convolutional neural network's determination of any class of interest. In medical imaging, it can help medical practitioners diagnose diseases like COVID-19 or pneumonia by highlighting the suspicious regions in Computational Tomography (CT) or chest X-ray (CXR) film. Many contemporary deep learning techniques only focus on COVID-19 classification tasks using CXRs, while few attempt to make it explainable with a saliency map. To fill this research gap, we first propose a VGG-16-architecture-based deep learning approach in combination with image enhancement, segmentation-based region of interest (ROI) cropping, and data augmentation steps to enhance classification accuracy. Later, a multi-layer Gradient CAM (ML-Grad-CAM) algorithm is integrated to generate a class-specific saliency map for improved visualization in CXR images. We also define and calculate a Severity Assessment Index (SAI) from the saliency map to quantitatively measure infection severity. The trained model achieved an accuracy score of 96.44% for the three-class CXR classification task, i.e., COVID-19, pneumonia, and normal (healthy patients), outperforming many existing techniques in the literature. The saliency maps generated from the proposed ML-GRAD-CAM algorithm are compared with the original Gran-CAM algorithm.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2024-08-31", "authors": ["Md FashiarRahman", "Tzu-Liang BillTseng", "MichaelPokojovy", "PeterMcCaffrey", "EricWalser", "ScottMoen", "AlexVo", "Johnny CHo"], "doi": "10.3390/diagnostics14161699\n10.1111/tmi.13383\n10.1503/cmaj.1095949\n10.1016/S0140-6736(21)02758-6\n10.1126/scitranslmed.abc1931\n10.1016/j.cmpb.2020.105581\n10.1016/j.ejrad.2020.108961\n10.1016/j.compeleceng.2019.08.004\n10.1183/09031936.00047908\n10.1016/j.compmedimag.2007.02.002\n10.1038/s41591-018-0178-4\n10.2196/27370\n10.1053/j.semnuclmed.2011.06.004\n10.3390/electronics9111921\n10.1016/j.measurement.2021.110276\n10.3390/app10020559\n10.1016/j.measurement.2020.108046\n10.1109/TPAMI.2023.3330825\n10.1371/journal.pone.0242535\n10.1007/s10489-020-01829-7\n10.3390/electronics9091439\n10.1016/j.media.2020.101794\n10.1016/j.eswa.2020.114054\n10.1016/j.bspc.2020.102365\n10.1007/s13755-020-00119-3\n10.1016/j.compbiomed.2022.105213\n10.3390/app10093233\n10.1016/j.chaos.2020.110495\n10.1016/j.chaos.2020.110190\n10.3390/s21175813\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1007/s10489-020-02055-x\n10.17531/ein.2020.1.8\n10.1016/j.compmedimag.2019.02.003\n10.1007/s10462-021-10033-z\n10.1007/s11042-020-09054-7"}
{"title": "Enhancing Adult Asthma Management: A Review on the Utility of Remote Home Spirometry and Mobile Applications.", "abstract": "Asthma is a prevalent chronic disease, contributing significantly to the global burden of disease and economic costs. Despite advances in treatment, inadequate disease management and reliance on reliever medications lead to preventable deaths. Telemedicine, defined as the use of information and communication technology to improve healthcare access, has gained global attention, especially during the COVID-19 pandemic. This systematic review examines the effectiveness of home monitoring systems in managing severe asthma. A systematic literature search was conducted in PubMed, Web of Science, Scopus, and Cochrane Library, focusing on studies from 2014 to 2024. Fourteen studies involving 9093 patients were analyzed. The results indicate that telemedicine, through tools such as mobile applications and portable spirometers, positively impacts asthma control, self-management, and quality of life. Home spirometry, in particular, shows strong agreement with clinic spirometry, offering a feasible alternative for continuous monitoring. Digital coaching and machine learning-based telemedicine applications also demonstrate significant potential in improving asthma outcomes. However, challenges such as technology accessibility, data privacy, and the need for standardized protocols remain. This review highlights the promise of telemedicine in asthma management and calls for further research to optimize its implementation and address existing barriers.", "journal": "Journal of personalized medicine", "date": "2024-08-31", "authors": ["NorbertWellmann", "Monica StelutaMarc", "Emil RobertStoicescu", "Camelia CorinaPescaru", "Ana AdrianaTrusculescu", "Flavia GabrielaMartis", "IoanaCiortea", "Alexandru FlorianCrisan", "Madalina AlexandraBalica", "Diana RalucaVelescu", "OvidiuFira-Mladinescu"], "doi": "10.3390/jpm14080852\n10.1513/AnnalsATS.201703-259OC\n10.1007/s12098-018-2705-1\n10.1016/S0140-6736(17)33311-1\n10.1097/MCP.0000000000000870\n10.5694/mja17.00684\n10.1016/j.jaip.2014.10.003\n10.1002/14651858.CD011715.pub2\n10.1097/01.NAJ.0000520244.60138.1c\n10.1371/journal.pone.0071238\n10.1097/MCP.0000000000000965\n10.1007/s11882-022-01030-5\n10.1016/j.jaip.2020.10.005\n10.1001/jamadermatol.2013.606\n10.1164/rccm.201510-1966ST\n10.7224/1537-2073.2012-012\n10.1089/tmj.2012.0197\n10.1111/crj.12359\n10.1183/13993003.01721-2015\n10.1136/bmj.n71\n10.1136/bmjopen-2016-013935\n10.1080/02770903.2017.1362430\n10.1038/nbt.3826\n10.1080/02770903.2018.1493599\n10.2196/13551\n10.1055/s-0039-1697597\n10.1136/bmjhci-2019-100007\n10.1080/02770903.2019.1709864\n10.1111/cts.12901\n10.3390/jcm11040960\n10.1002/clt2.12088\n10.1080/02770903.2022.2160345\n10.18332/pne/174605\n10.1111/cts.13615\n10.1016/j.chest.2023.06.029\n10.28920/dhm53.4.327-332\n10.1038/s41533-023-00343-w\n10.1093/ije/dyab187\n10.1007/s11096-017-0495-6\n10.1183/13993003.01126-2017\n10.3390/tomography8040162\n10.2196/mhealth.3118\n10.1164/rccm.9120-11ST\n10.1016/j.rmed.2014.02.005\n10.3389/fmed.2023.1210329\n10.1089/tmj.2019.0158\n10.1016/j.wneu.2016.01.075\n10.1016/j.hlpt.2018.04.003\n10.1016/j.ijmedinf.2018.01.012\n10.1371/journal.pone.0192789\n10.1186/s12931-020-01341-z\n10.1111/nyas.13218\n10.2196/10956\n10.1177/1357633X19870025\n10.1016/j.bbe.2022.11.001\n10.2196/19006\n10.1016/j.jaip.2019.08.034\n10.23822/EurAnnACI.1764-1489.182"}
{"title": "Novel Deep CNNs Explore Regions, Boundaries, and Residual Learning for COVID-19 Infection Analysis in Lung CT.", "abstract": "COVID-19 poses a global health crisis, necessitating precise diagnostic methods for timely containment. However, accurately delineating COVID-19-affected regions in lung CT scans is challenging due to contrast variations and significant texture diversity. In this regard, this study introduces a novel two-stage classification and segmentation CNN approach for COVID-19 lung radiological pattern analysis. A novel Residual-BRNet is developed to integrate boundary and regional operations with residual learning, capturing key COVID-19 radiological homogeneous regions, texture variations, and structural contrast patterns in the classification stage. Subsequently, infectious CT images undergo lesion segmentation using the newly proposed RESeg segmentation CNN in the second stage. The RESeg leverages both average and max-pooling implementations to simultaneously learn region homogeneity and boundary-related patterns. Furthermore, novel pixel attention (PA) blocks are integrated into RESeg to effectively address mildly COVID-19-infected regions. The evaluation of the proposed Residual-BRNet CNN in the classification stage demonstrates promising performance metrics, achieving an accuracy of 97.97%, F1-score of 98.01%, sensitivity of 98.42%, and MCC of 96.81%. Meanwhile, PA-RESeg in the segmentation phase achieves an optimal segmentation performance with an IoU score of 98.43% and a dice similarity score of 95.96% of the lesion region. The framework's effectiveness in detecting and segmenting COVID-19 lesions highlights its potential for clinical applications.", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2024-08-28", "authors": ["Bader KhalidAlshemaimri"], "doi": "10.3390/tomography10080091\n10.1002/bjs.11627\n10.1038/s41591-020-0822-7\n10.1007/s11427-020-1637-5\n10.1016/S0140-6736(20)30183-5\n10.1016/j.compbiomed.2021.104293\n10.3390/diagnostics12020267\n10.1148/radiol.2020200432\n10.1155/2021/8869372\n10.2214/AJR.20.23034\n10.1016/j.ejrad.2020.108961\n10.1002/int.22449\n10.1007/s00330-019-06532-x\n10.1080/0952813X.2023.2165724\n10.1016/j.media.2021.102205\n10.1093/jmicro/dfac051\n10.1155/2020/9756518\n10.1016/j.esmoop.2020.100005\n10.1002/int.22504\n10.1155/2021/5528144\n10.1007/s10044-021-00984-y\n10.1007/s00330-021-07715-1\n10.1038/s41598-020-76550-z\n10.1016/j.patrec.2020.09.010\n10.1016/j.pdpdt.2021.102473\n10.1016/j.sigpro.2024.109448\n10.1109/TIP.2021.3058783\n10.1016/j.media.2020.101836\n10.1109/ACCESS.2022.3204876\n10.1007/s10462-020-09825-6\n10.1186/s40537-016-0043-6\n10.1007/s10462-020-09854-1\n10.1007/s10462-018-9641-3\n10.1109/TPAMI.2016.2644615\n10.1109/TPAMI.2017.2699184\n10.1002/mp.14676\n10.1038/s41598-023-49218-7\n10.1007/s40747-024-01406-2\n10.1038/s41467-020-17971-2\n10.1109/ACCESS.2020.3005510\n10.1016/j.compbiomed.2020.104037\n10.1109/TMI.2020.2996645"}
{"title": "Deep Learning for Pneumonia Detection in Chest X-ray Images: A Comprehensive Survey.", "abstract": "This paper addresses the significant problem of identifying the relevant background and contextual literature related to deep learning (DL) as an evolving technology in order to provide a comprehensive analysis of the application of DL to the specific problem of pneumonia detection via chest X-ray (CXR) imaging, which is the most common and cost-effective imaging technique available worldwide for pneumonia diagnosis. This paper in particular addresses the key period associated with COVID-19, 2020-2023, to explain, analyze, and systematically evaluate the limitations of approaches and determine their relative levels of effectiveness. The context in which DL is applied as both an aid to and an automated substitute for existing expert radiography professionals, who often have limited availability, is elaborated in detail. The rationale for the undertaken research is provided, along with a justification of the resources adopted and their relevance. This explanatory text and the subsequent analyses are intended to provide sufficient detail of the problem being addressed, existing solutions, and the limitations of these, ranging in detail from the specific to the more general. Indeed, our analysis and evaluation agree with the generally held view that the use of transformers, specifically, vision transformers (ViTs), is the most promising technique for obtaining further effective results in the area of pneumonia detection using CXR images. However, ViTs require extensive further research to address several limitations, specifically the following: biased CXR datasets, data and code availability, the ease with which a model can be explained, systematic methods of accurate model comparison, the notion of class imbalance in CXR datasets, and the possibility of adversarial attacks, the latter of which remains an area of fundamental research.", "journal": "Journal of imaging", "date": "2024-08-28", "authors": ["RaheelSiddiqi", "SameenaJavaid"], "doi": "10.3390/jimaging10080176\n10.1016/j.cell.2018.02.010\n10.1007/s12559-020-09787-5\n10.1016/S0140-6736(10)61459-6\n10.1109/ACCESS.2021.3069937\n10.1016/j.neucom.2022.01.055\n10.1016/j.bsheal.2020.03.004\n10.1038/s41467-021-22214-z\n10.1109/ACCESS.2022.3182498\n10.1016/j.ajem.2012.08.041\n10.1097/MD.0000000000004153\n10.1038/s41598-019-44145-y\n10.1007/s42979-020-00361-2\n10.1148/radiol.2481071451\n10.1016/j.media.2021.102125\n10.1038/s41598-020-76550-z\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1016/j.clinimag.2020.04.001\n10.3390/app11094233\n10.3390/diagnostics12061442\n10.1371/journal.pone.0256630\n10.1016/j.slast.2021.10.011\n10.3390/app11031242\n10.1038/nature14539\n10.1007/s42979-021-00815-1\n10.1109/JPROC.2021.3054390\n10.1007/s13244-018-0639-9\n10.1609/aaai.v33i01.3301590\n10.1038/s41597-019-0322-0\n10.1016/j.media.2020.101797\n10.1109/JBHI.2020.3037127\n10.1016/j.imu.2021.100779\n10.1007/s11263-015-0816-y\n10.1016/j.procs.2013.05.444\n10.1007/s10462-017-9605-z\n10.1016/j.asoc.2022.109319\n10.1109/ACCESS.2021.3054484\n10.1186/s40537-019-0197-0\n10.1109/ACCESS.2021.3079716\n10.1109/ACCESS.2021.3061058\n10.1016/j.eswa.2022.118650\n10.1109/ACCESS.2019.2896409\n10.1609/aaai.v34i07.7000\n10.1007/s42452-021-04917-6\n10.1111/1754-9485.13261\n10.1109/JAS.2017.7510583\n10.1109/TKDE.2021.3130191\n10.1016/j.cmpb.2022.107262\n10.1038/s41598-022-23692-x\n10.3390/sym12040651\n10.1145/3463475\n10.1016/j.compbiomed.2022.105350\n10.1016/j.displa.2023.102485\n10.1016/j.heliyon.2023.e14453\n10.1016/j.jvcir.2022.103521\n10.1016/j.measurement.2021.109953\n10.1016/j.compbiomed.2022.105466\n10.1016/j.asoc.2020.106744\n10.1016/j.eswa.2021.115695\n10.1109/ACCESS.2020.3025164\n10.1109/ACCESS.2021.3087583\n10.1109/ACCESS.2021.3075555\n10.1109/ACCESS.2021.3064927\n10.1109/ACCESS.2021.3064838\n10.1016/j.eswa.2020.114054\n10.1016/j.compbiomed.2020.103792\n10.1016/j.artmed.2022.102427\n10.1016/j.bbe.2022.08.001\n10.1016/j.radi.2022.09.011\n10.1007/s12652-021-02917-3\n10.1007/s13369-022-06841-2\n10.1007/s00521-020-05410-8\n10.1016/j.compbiomed.2022.105233\n10.1016/j.cmpb.2022.106833\n10.1109/TKDE.2009.191\n10.1016/j.asoc.2022.109588\n10.1109/ACCESS.2021.3061621\n10.1016/j.asoc.2020.106912\n10.1007/s00521-022-06918-x\n10.1007/s12559-021-09955-1\n10.1007/s10522-021-09946-7\n10.1038/s41598-022-06802-7\n10.1007/s12652-021-03686-9\n10.1007/s13369-021-05958-0\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.compbiomed.2022.105251\n10.1109/ACCESS.2021.3125324\n10.1016/j.compbiomed.2021.104803\n10.1016/j.compeleceng.2022.108402\n10.1016/j.sintl.2022.100167\n10.1016/j.compbiomed.2021.104252\n10.1007/s42600-021-00181-0\n10.1109/ACCESS.2021.3077592\n10.1016/j.bspc.2023.105472\n10.1016/j.health.2023.100176\n10.1016/j.compbiomed.2022.105979\n10.1016/j.asoc.2023.110500\n10.1016/j.eswa.2023.121724\n10.1016/j.bspc.2022.104392\n10.1016/j.cmpbup.2023.100116\n10.3390/diagnostics12020237\n10.1109/TNNLS.2020.3027314\n10.1109/ACCESS.2018.2870052\n10.1109/TAI.2022.3153754\n10.1109/ACCESS.2021.3090215\n10.1109/ACCESS.2021.3065456\n10.1016/j.media.2020.101794\n10.1016/j.asoc.2020.106691\n10.1109/TMI.2020.2993291\n10.1007/s13246-020-00888-x\n10.1007/s12559-020-09775-9\n10.1016/S2589-7500(21)00208-9\n10.1007/s13246-022-01169-5\n10.1016/j.measurement.2021.110491\n10.1007/s11042-022-13071-z\n10.1007/s11042-023-14930-z\n10.1016/j.future.2021.04.007\n10.1007/s00521-022-08099-z\n10.1016/j.engappai.2023.106416\n10.1007/s11042-022-13844-6\n10.1007/s00530-021-00794-6\n10.1007/s00521-021-06737-6\n10.1016/j.asoc.2022.108867\n10.1007/s10278-022-00706-8\n10.1109/JBHI.2023.3267057\n10.1016/j.media.2022.102664\n10.1007/s11042-022-13843-7\n10.1007/s00521-023-08200-0\n10.1109/ACCESS.2023.3302180\n10.1007/s11042-023-15995-6\n10.1148/ryai.220062\n10.1109/ACCESS.2023.3255403\n10.3390/v15061327\n10.1109/TMI.2021.3134270\n10.1016/j.asoc.2021.108190\n10.1016/j.eswa.2023.120528\n10.1016/j.bspc.2022.104197\n10.1016/j.bspc.2022.104445\n10.1007/s10140-021-01954-x\n10.59275/j.melba.2020-48g7\n10.1007/s13755-020-00135-3\n10.1016/j.bbe.2023.01.005\n10.1007/s00500-023-08874-7\n10.1007/s11517-022-02746-2\n10.1109/JBHI.2021.3111415\n10.1007/s11042-022-12156-z\n10.1016/j.compbiomed.2020.103869\n10.1016/j.asoc.2021.107522\n10.1007/s13721-023-00413-6\n10.1007/s12553-021-00630-x\n10.1002/ima.22770\n10.3390/s23010480\n10.1007/s10489-022-04446-8\n10.1155/2022/1678000\n10.1186/s12880-022-00904-4\n10.1007/s11042-022-14247-3\n10.1038/s41598-022-15013-z\n10.1186/s13634-021-00755-1\n10.1109/ACCESS.2022.3153059\n10.1109/ACCESS.2021.3123782\n10.1007/s00521-021-06102-7\n10.1089/big.2022.0261\n10.1016/j.aej.2022.10.053\n10.1002/ima.22812\n10.1016/j.jare.2022.08.021\n10.1007/s10586-022-03664-6\n10.1016/j.bbe.2023.08.004\n10.1007/s13246-023-01347-z\n10.1109/JBHI.2023.3247949\n10.48550/arXiv.1409.1556\n10.1007/s11263-019-01228-7\n10.1016/j.media.2021.102225\n10.1109/JBHI.2024.3372999\n10.3390/s23146585\n10.1038/s41598-023-30174-1\n10.1016/j.patrec.2020.09.010\n10.1016/j.cmpb.2020.105532\n10.1016/j.ejro.2022.100438\n10.1016/j.crad.2022.11.006\n10.1016/j.patcog.2020.107613\n10.1016/j.asoc.2020.106859\n10.1016/j.cmpb.2020.105608\n10.1016/j.compbiomed.2023.106668\n10.1007/s11042-022-13710-5\n10.1145/3236009\n10.1109/MCG.2021.3094858\n10.1016/j.asoc.2020.106580\n10.1145/3576898\n10.1109/ACCESS.2020.3003810\n10.48550/arXiv.2004.03747\n10.1016/j.asoc.2024.111806\n10.1504/IJESDF.2023.130665\n10.1016/j.eswa.2022.116815\n10.1109/TMI.2022.3156268\n10.1007/s11280-022-01066-7\n10.1007/s11042-022-11913-4\n10.1016/j.patcog.2020.107332\n10.1186/s12880-020-00530-y\n10.1109/CVPR.2017.17\n10.1016/j.compbiomed.2023.107251\n10.1007/s10278-023-00919-5"}
{"title": "COVID-19 severity detection using chest X-ray segmentation and deep learning.", "abstract": "COVID-19 has resulted in a significant global impact on health, the economy, education, and daily life. The disease can range from mild to severe, with individuals over 65 or those with underlying medical conditions being more susceptible to severe illness. Early testing and isolation are vital due to the virus's variable incubation period. Chest radiographs (CXR) have gained importance as a diagnostic tool due to their efficiency and reduced radiation exposure compared to CT scans. However, the sensitivity of CXR in detecting COVID-19 may be lower. This paper introduces a deep learning framework for accurate COVID-19 classification and severity prediction using CXR images. U-Net is used for lung segmentation, achieving a precision of 0.9924. Classification is performed using a Convulation-capsule network, with high true positive rates of 86% for COVID-19, 93% for pneumonia, and 85% for normal cases. Severity assessment employs ResNet50, VGG-16, and DenseNet201, with DenseNet201 showing superior accuracy. Empirical results, validated with 95% confidence intervals, confirm the framework's reliability and robustness. This integration of advanced deep learning techniques with radiological imaging enhances early detection and severity assessment, improving patient management and resource allocation in clinical settings.", "journal": "Scientific reports", "date": "2024-08-28", "authors": ["TinkuSingh", "SuryanshiMishra", "RiyaKalra", "NoneSatakshi", "ManishKumar", "TaehongKim"], "doi": "10.1038/s41598-024-70801-z\n10.1007/s12065-022-00739-6\n10.2196/27468\n10.1155/2020/9756518\n10.1016/j.asoc.2021.107238\n10.1016/j.imu.2020.100412\n10.1016/j.patrec.2020.09.010\n10.1007/s11547-020-01200-3\n10.1109/ACCESS.2021.3056516\n10.1109/ACCESS.2021.3131216\n10.1109/ACCESS.2019.2896961\n10.1155/2022/7954333\n10.3390/healthcare9091099\n10.1186/s43055-021-00524-y\n10.1016/j.eswa.2023.120477\n10.3389/frai.2021.598932\n10.1007/s10489-021-02731-6\n10.1136/bmjopen-2020-042946\n10.1371/journal.pone.0265949\n10.1038/s41598-019-42294-8\n10.1155/2021/6677314\n10.3390/diagnostics12020267\n10.1016/j.pdpdt.2021.102473\n10.1016/j.compbiomed.2021.104816\n10.1038/s41598-023-49218-7\n10.3390/life11111281\n10.1109/TIP.2020.3043128\n10.1101/2020.11.24.20235887\n10.1007/s42979-021-00695-5\n10.1038/s41467-020-18786-x\n10.1109/TMI.2020.3040950\n10.1016/j.ejro.2020.100239\n10.1148/radiol.2020200370\n10.5152/dir.2020.20205\n10.1016/j.bspc.2023.104857\n10.1186/s12880-024-01194-8\n10.1109/TMI.2013.2290491\n10.1109/TPAMI.2010.147\n10.1109/ACCESS.2020.3010287\n10.2214/ajr.174.1.1740071"}
{"title": "A non-enhanced CT-based deep learning diagnostic system for COVID-19 infection at high risk among lung cancer patients.", "abstract": "Pneumonia and lung cancer have a mutually reinforcing relationship. Lung cancer patients are prone to contracting COVID-19, with poorer prognoses. Additionally, COVID-19 infection can impact anticancer treatments for lung cancer patients. Developing an early diagnostic system for COVID-19 pneumonia can help improve the prognosis of lung cancer patients with COVID-19 infection.\nThis study proposes a neural network for COVID-19 diagnosis based on non-enhanced CT scans, consisting of two 3D convolutional neural networks (CNN) connected in series to form two diagnostic modules. The first diagnostic module classifies COVID-19 pneumonia patients from other pneumonia patients, while the second diagnostic module distinguishes severe COVID-19 patients from ordinary COVID-19 patients. We also analyzed the correlation between the deep learning features of the two diagnostic modules and various laboratory parameters, including KL-6.\nThe first diagnostic module achieved an accuracy of 0.9669 on the training set and 0.8884 on the test set, while the second diagnostic module achieved an accuracy of 0.9722 on the training set and 0.9184 on the test set. Strong correlation was observed between the deep learning parameters of the second diagnostic module and KL-6.\nOur neural network can differentiate between COVID-19 pneumonia and other pneumonias on CT images, while also distinguishing between ordinary COVID-19 patients and those with white lung. Patients with white lung in COVID-19 have greater alveolar damage compared to ordinary COVID-19 patients, and our deep learning features can serve as an imaging biomarker.", "journal": "Frontiers in medicine", "date": "2024-08-27", "authors": ["TianmingDu", "YihaoSun", "XinghaoWang", "TaoJiang", "NingXu", "ZeydBoukhers", "MarcinGrzegorzek", "HongzanSun", "ChenLi"], "doi": "10.3389/fmed.2024.1444708\n10.1016/S0140-6736(21)00312-3\n10.1513/pats.201009-061WR\n10.1016/j.lfs.2022.120374\n10.1093/carcin/bgac047\n10.1016/j.heliyon.2023.e23926\n10.1126/scitranslmed.abo5070\n10.1016/j.mehy.2020.110074\n10.1158/2159-8290.CD-20-0422\n10.2478/jtim-2020-0003\n10.1016/j.ctrv.2020.102041\n10.3389/fimmu.2023.1125246\n10.18632/aging.203832\n10.1002/jgm.3303\n10.1002/cbf.3799\n10.1016/j.radi.2020.09.010\n10.1016/j.compbiomed.2021.105182\n10.7705/biomedica.5927\n10.1038/s41598-023-29364-8\n10.3390/tomography8010041\n10.1016/j.apmr.2021.03.028\n10.1016/j.cmpb.2021.106406\n10.1007/s13246-020-00865-4\n10.3233/XST-200715\n10.1016/j.crad.2022.11.006"}
{"title": "Enhancing Pulmonary Embolism Detection in COVID-19 Patients Through Advanced Deep Learning Techniques.", "abstract": "The intersection of COVID-19 and pulmonary embolism (PE) has posed unprecedented challenges in medical diagnostics. The critical nature of PE and its increased incidence during the pandemic underline the need for improved detection methods. This study evaluates the effectiveness of advanced deep learning techniques in enhancing PE detection in post-COVID-19 patients through Computed Tomography Pulmonary Angiography (CTPA) scans. Using a dataset of 746 anonymized CTPA images from 25 patients, we fine-tuned the state-of-the-art Ultralytics YOLOv8 object detection model, which was trained on 676 images with 1,517 annotated bounding boxes and validated on 70 images with 108 bounding boxes. After 200 epochs of training, which lasted approximately 1.021 hours, the YOLOv8 model demonstrated significant diagnostic proficiency, achieving a mean Average Precision (mAP) of 0.683 at an IoU threshold of 0.50 and a mAP of 0.246 at the IoU range of 0.50:0.95 in the validation dataset. Notably, the model reached a maximum precision of 0.85949 and a maximum recall of 0.81481, though these metrics were observed in separate epochs. These findings emphasize the model's potential for high diagnostic accuracy and offer a promising direction for deploying AI tools in clinical settings, significantly contributing to healthcare innovation and patient care post-pandemic.", "journal": "Studies in health technology and informatics", "date": "2024-08-23", "authors": ["GeorgiosFeretzakis", "KonstantinosDalamarinis", "DimitrisKalles", "ChairiKiourt", "GeorgiosPantos", "IoannisPapadopoulos", "SpyrosKouris", "Vassilios SVerykios", "GeorgeIoannakis", "EvangelosLoupelis", "AikateriniSakagianni"], "doi": "10.3233/SHTI240622"}
{"title": "Improving Fairness of Automated Chest Radiograph Diagnosis by Contrastive Learning.", "abstract": "", "journal": "Radiology. Artificial intelligence", "date": "2024-08-21", "authors": ["MingquanLin", "TianhaoLi", "ZhaoyiSun", "GregoryHolste", "YingDing", "FeiWang", "GeorgeShih", "YifanPeng"], "doi": "10.1148/ryai.230342"}
{"title": "Effects of post-acute COVID-19 syndrome on cerebral white matter and emotional health among non-hospitalized individuals.", "abstract": "Post-acute COVID syndrome (PACS) is a growing concern, given its impact on mental health and quality of life. However, its effects on cerebral white matter remain poorly understood, particularly in non-hospitalized cohorts. The goals of this cross-sectional, observational study were to examine (1) whether PACS was associated with distinct alterations in white matter microstructure, compared to symptom-matched non-COVID viral infection; and (2) whether microstructural alterations correlated with indices of post-COVID emotional health.\nData were collected for 54 symptomatic individuals who tested positive for COVID-19 (mean age 41\u2009\u00b1\u200912\u2009yrs., 36 female) and 14 controls who tested negative for COVID-19 (mean age 41\u2009\u00b1\u200914\u2009yrs., 8 female), with both groups assessed an average of 4-5\u2009months after COVID testing. Diffusion magnetic resonance imaging data were collected, and emotional health was assessed via the NIH emotion toolbox, with summary scores indexing social satisfaction, well-being and negative affect.\nDespite similar symptoms, the COVID-19 group had reduced mean and axial diffusivity, along with increased mean kurtosis and neurite dispersion, in deep white matter. After adjusting for social satisfaction, higher levels of negative affect in the COVID-19 group were also correlated with increased mean kurtosis and reduced free water in white matter.\nThese results provide preliminary evidence that indices of white matter microstructure distinguish PACS from symptomatic non-COVID infection. Moreover, white matter effects seen in PACS correlate with the severity of emotional sequelae, providing novel insights into this highly prevalent disorder.", "journal": "Frontiers in neurology", "date": "2024-08-21", "authors": ["Nathan WChurchill", "EugenieRoudaia", "J JeanChen", "AllisonSekuler", "FuqiangGao", "MarioMasellis", "BenjaminLam", "IvyCheng", "ChrisHeyn", "Sandra EBlack", "Bradley JMacIntosh", "Simon JGraham", "Tom ASchweizer"], "doi": "10.3389/fneur.2024.1432450\n10.1056/NEJMoa2001017\n10.1016/S0140-6736(21)00847-3\n10.1038/s41591-021-01283-z\n10.1016/j.cmi.2022.01.014\n10.1021/acschemneuro.0c00122\n10.1016/j.ejim.2021.06.009\n10.1148/rg.2016160031\n10.1196/annals.1444.017\n10.1002/jmri.10424\n10.3389/fnins.2013.00031\n10.2214/AJR.13.11365\n10.1016/j.neuroimage.2012.03.072\n10.1038/s41586-022-04569-5\n10.1093/brain/awab435\n10.1016/j.eclinm.2020.100484\n10.1177/17562864221111995\n10.1172/JCI147329\n10.1515/nipt-2022-0016\n10.1038/nrn2297\n10.1016/S0166-2236(00)02088-9\n10.1016/S1474-4422(09)70335-7\n10.1093/brain/awac384\n10.9778/cmajo.20210023\n10.1212/WNL.0b013e3182872e11\n10.1080/03610926.2011.625486\n10.1016/j.neuroimage.2015.12.033\n10.1016/B978-0-12-374709-9.00010-9\n10.1097/00001756-200312190-00035\n10.1016/j.neuroimage.2003.07.005\n10.1016/j.neuroimage.2005.01.028\n10.1017/S0033291720000999\n10.1038/s41398-020-01039-2\n10.1016/j.nurpra.2021.05.003\n10.1016/j.bbi.2020.07.037\n10.3390/ijerph17239097\n10.1111/famp.12618\n10.12659/MSM.923549\n10.1002/acn3.445\n10.1007/s00234-019-02249-2\n10.1002/hbm.24500\n10.1152/physrev.00027.2007\n10.1093/schbul/sbaa134\n10.1016/j.schres.2015.05.034\n10.1111/eci.13706\n10.1056/NEJMp2025631\n10.1093/infdis/jiad309\n10.1212/WNL.0000000000207309\n10.1002/jmri.28555\n10.1002/mrm.26575"}
{"title": "Confidence-Aware Severity Assessment of Lung Disease from Chest X-Rays Using Deep Neural Network on a Multi-Reader Dataset.", "abstract": "In this study, we present a method based on Monte Carlo Dropout (MCD)\u00a0as\u00a0Bayesian neural network (BNN) approximation\u00a0for confidence-aware severity classification of lung diseases in COVID-19 patients using chest X-rays (CXRs). Trained and tested on 1208 CXRs from Hospital 1 in the USA, the model categorizes severity into four levels (i.e., normal, mild, moderate, and severe) based on lung consolidation and opacity. Severity labels, determined by the median consensus of five radiologists, serve as the reference standard. The model's performance is internally validated against evaluations from an additional radiologist and two residents that were excluded from the median. The performance of the model is further evaluated on additional internal and external datasets comprising 2200 CXRs from the same hospital and 1300 CXRs from Hospital 2 in South Korea. The\u00a0model achieves an average area under the curve (AUC) of 0.94\u2009\u00b1\u20090.01 across all classes in the primary dataset, surpassing human readers in each severity class and achieves a higher Kendall correlation coefficient (KCC) of 0.80\u2009\u00b1\u20090.03. The performance of the model is consistent across varied datasets, highlighting its generalization. A key aspect of the model is its predictive uncertainty (PU), which is inversely related to the level of agreement among radiologists, particularly in mild and moderate cases. The study concludes that the model outperforms human readers in severity assessment and maintains consistent accuracy across diverse datasets. Its ability to provide confidence measures in predictions is pivotal for potential clinical use, underscoring the BNN's role in enhancing diagnostic precision in lung disease analysis through CXR.", "journal": "Journal of imaging informatics in medicine", "date": "2024-08-21", "authors": ["MohammadrezaZandehshahvar", "Marlyvan Assen", "EunKim", "YasharKiarashi", "VikranthKeerthipati", "GiovanniTessarin", "EmanueleMuscogiuri", "Arthur EStillman", "PeterFilev", "Amir HDavarpanah", "Eugene ABerkowitz", "StefanTigges", "Scott JLee", "Brianna LVey", "CarloDe Cecco", "AliAdibi"], "doi": "10.1007/s10278-024-01151-5\n10.1016/j.media.2017.07.005"}
{"title": "COVID-19's Radiologic, Functional, and Serologic Consequences at 6-Month and 18-Month Follow-up: A Prospective Cohort Study.", "abstract": "We evaluated the radiologic, pulmonary functional, and antibody statuses of coronavirus disease 2019 (COVID-19) patients 6 and 18 months after discharge, comparing changes in status and focusing on risk factors for residual computed tomography (CT) abnormalities.\nThis prospective cohort study was conducted on COVID-19 patients discharged between April 2020 and January 2021. Chest CT, pulmonary function testing (PFT), and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) immunoglobulin G (IgG) measurements were performed 6 and 18 months after discharge. We evaluated factors associated with residual CT abnormalities and the correlation between lesion volume in CT (lesion\nThis study included 68 and 42 participants evaluated 6 and 18 months, respectively, after hospitalizations for COVID-19. CT abnormalities were noted in 22 participants (32.4%) at 6 months and 13 participants (31.0%) at 18 months. Lesion\nAt 18-month follow-up assessments, 31.0% of participants exhibited residual CT abnormalities. Age and higher SARS-CoV-2 IgG levels were significant predictors, and FVC was related to abnormal CT findings at 18 months. Lesion\nClinical Research Information Service Identifier: KCT0008573.", "journal": "Journal of Korean medical science", "date": "2024-08-21", "authors": ["CherryKim", "HyeriSeok", "JooyunKim", "Dae WonPark", "Marlyvan Assen", "Carlo NDe Cecco", "HangseokChoi", "ChoheeKim", "Sung HoHwang", "Hwan SeokYong", "Yu-WhanOh", "Won SukChoi"], "doi": "10.3346/jkms.2024.39.e228"}
{"title": "Multiscale heterogeneity of white matter morphometry in psychiatric disorders.", "abstract": "Inter-individual variability in neurobiological and clinical characteristics in mental illness is often overlooked by classical group-mean case-control studies. Studies using normative modelling to infer person-specific deviations of grey matter volume have indicated that group means are not representative of most individuals. The extent to which this variability is present in white matter morphometry, which is integral to brain function, remains unclear.\nWe applied Warped Bayesian Linear Regression normative models to T1-weighted magnetic resonance imaging data and mapped inter-individual variability in person-specific white matter volume deviations in 1,294 cases (58% male) diagnosed with one of six disorders (attention-deficit/hyperactivity, autism, bipolar, major depressive, obsessive-compulsive and schizophrenia) and 1,465 matched controls (54% male) recruited across 25 scan sites. We developed a framework to characterize deviation heterogeneity at multiple spatial scales, from individual voxels, through inter-regional connections, specific brain regions, and spatially extended brain networks.\nThe specific locations of white matter volume deviations were highly heterogeneous across participants, affecting the same voxel in fewer than 8% of individuals with the same diagnosis. For autism and schizophrenia, negative deviations (i.e., areas where volume is lower than normative expectations) aggregated into common tracts, regions and large-scale networks in up to 35% of individuals.\nThe prevalence of white matter volume deviations was lower than previously observed in grey matter, and the specific location of these deviations was highly heterogeneous when considering voxel-wise spatial resolution. Evidence of aggregation within common pathways and networks was apparent in schizophrenia and autism but not other disorders.", "journal": "bioRxiv : the preprint server for biology", "date": "2024-08-16", "authors": ["AshleaSegal", "Robert ESmith", "SidhantChopra", "StuartOldham", "LindenParkes", "KevinAquino", "Seyed MostafaKia", "ThomasWolfers", "BarbaraFranke", "MartineHoogman", "Christian FBeckmann", "Lars TWestlye", "Ole AAndreassen", "AndrewZalesky", "Ben JHarrison", "Christopher GDavey", "CarlesSoriano-Mas", "Narc\u00edsCardoner", "JegganTiego", "MuratY\u00fccel", "LeahBraganza", "ChaoSuo", "MichaelBerk", "SueCotton", "Mark ABellgrove", "Andre FMarquand", "AlexFornito"], "doi": "10.1101/2024.08.04.606523\n10.3389/fninf.2014.00030\n10.1176/appi.books.9780890425596\n10.1038/s41398-018-0108-8\n10.1017/S0033291721004323\n10.18112/openneuro.ds000212.v1.0.0\n10.18112/openneuro.ds002748.v1.0.5\n10.18112/openneuro.ds002522.v1.0.0"}
{"title": "FMD-UNet: fine-grained feature squeeze and multiscale cascade dilated semantic aggregation dual-decoder UNet for COVID-19 lung infection segmentation from CT images.", "abstract": "With the advancement of computer-aided diagnosis, the automatic segmentation of COVID-19 infection areas holds great promise for assisting in the timely diagnosis and recovery of patients in clinical practice. Currently, methods relying on U-Net face challenges in effectively utilizing fine-grained semantic information from input images and bridging the semantic gap between the encoder and decoder. To address these issues, we propose an FMD-UNet dual-decoder U-Net network for COVID-19 infection segmentation, which integrates a Fine-grained Feature Squeezing (FGFS) decoder and a Multi-scale Dilated Semantic Aggregation (MDSA) decoder. The FGFS decoder produces fine feature maps through the compression of fine-grained features and a weighted attention mechanism, guiding the model to capture detailed semantic information. The MDSA decoder consists of three hierarchical MDSA modules designed for different stages of input information. These modules progressively fuse different scales of dilated convolutions to process the shallow and deep semantic information from the encoder, and use the extracted feature information to bridge the semantic gaps at various stages, this design captures extensive contextual information while decoding and predicting segmentation, thereby suppressing the increase in model parameters. To better validate the robustness and generalizability of the FMD-UNet, we conducted comprehensive performance evaluations and ablation experiments on three public datasets, and achieved leading Dice Similarity Coefficient (DSC) scores of 84.76, 78.56 and 61.99% in COVID-19 infection segmentation, respectively. Compared to previous methods, the FMD-UNet has fewer parameters and shorter inference time, which also demonstrates its competitiveness.", "journal": "Biomedical physics & engineering express", "date": "2024-08-15", "authors": ["WenfengWang", "QiMao", "YiTian", "YanZhang", "ZhenwuXiang", "LijiaRen"], "doi": "10.1088/2057-1976/ad6f12"}
{"title": "Adaptive Mish activation and ranger optimizer-based SEA-ResNet50 model with explainable AI for multiclass classification of COVID-19 chest X-ray images.", "abstract": "A recent global health crisis, COVID-19 is a significant global health crisis that has profoundly affected lifestyles. The detection of such diseases from similar thoracic anomalies using medical images is a challenging task. Thus, the requirement of an end-to-end automated system is vastly necessary in clinical treatments. In this way, the work proposes a Squeeze-and-Excitation Attention-based ResNet50 (SEA-ResNet50) model for detecting COVID-19 utilizing chest X-ray data. Here, the idea lies in improving the residual units of ResNet50 using the squeeze-and-excitation attention mechanism. For further enhancement, the Ranger optimizer and adaptive Mish activation function are employed to improve the feature learning of the SEA-ResNet50 model. For evaluation, two publicly available COVID-19 radiographic datasets are utilized. The chest X-ray input images are augmented during experimentation for robust evaluation against four output classes namely normal, pneumonia, lung opacity, and COVID-19. Then a comparative study is done for the SEA-ResNet50 model against VGG-16, Xception, ResNet18, ResNet50, and DenseNet121 architectures. The proposed framework of SEA-ResNet50 together with the Ranger optimizer and adaptive Mish activation provided maximum classification accuracies of 98.38% (multiclass) and 99.29% (binary classification) as compared with the existing CNN architectures. The proposed method achieved the highest Kappa validation scores of 0.975 (multiclass) and 0.98 (binary classification) over others. Furthermore, the visualization of the saliency maps of the abnormal regions is represented using the explainable artificial intelligence (XAI) model, thereby enhancing interpretability in disease diagnosis.", "journal": "BMC medical imaging", "date": "2024-08-10", "authors": ["S RSannasi Chakravarthy", "NBharanidharan", "CVinothini", "VenkatesanVinoth Kumar", "T RMahesh", "SureshGuluwadi"], "doi": "10.1186/s12880-024-01394-2\n10.3389/fcimb.2021.596201\n10.1186/s12985-020-01452-5\n10.1016/j.jcv.2020.104455\n10.1136/bmjopen-2020-042946\n10.1109/JBHI.2021.3074893\n10.1016/j.bspc.2020.102365\n10.1007/s10044-021-00970-4\n10.1016/j.compbiomed.2021.104375\n10.1016/j.chaos.2021.110749\n10.1016/j.patcog.2021.108255\n10.1152/physiolgenomics.00029.2020\n10.1007/s11548-021-02317-0\n10.1038/s41598-020-76550-z\n10.1016/j.media.2020.101794\n10.1016/j.mehy.2020.109761\n10.3390/diagnostics10060358\n10.1016/j.cmpb.2020.105581\n10.1016/j.cmpb.2020.105532\n10.1186/s12938-020-00831-x\n10.1016/j.compbiomed.2022.106324\n10.1007/s11042-023-15995-6\n10.1016/j.compbiomed.2021.104306\n10.7717/peerj-cs.555\n10.3390/bioengineering10070850\n10.1038/s41598-023-27697-y\n10.1016/j.bspc.2023.104857\n10.1016/j.bspc.2022.103860\n10.1148/radiol.231319\n10.1002/ima.22493\n10.1016/j.irbm.2020.12.004\n10.1080/03772063.2022.2028584\n10.1016/j.irbm.2022.100749\n10.1186/s12911-024-02419-0\n10.1080/13467581.2023.2238038\n10.1016/j.compbiolchem.2024.108092\n10.1109/ACCESS.2020.2971225\n10.1002/ima.22364\n10.1007/s00500-022-07273-8\n10.1016/j.inffus.2019.12.012\n10.1016/j.bspc.2021.103408\n10.1016/j.compbiomed.2020.103869\n10.1016/j.compbiomed.2020.103792\n10.3389/frai.2021.598932\n10.1016/j.eng.2020.04.010\n10.1007/s10489-020-01943-6\n10.1016/j.bspc.2022.103977\n10.1007/s12652-022-03775-3\n10.1016/j.bbe.2023.01.005"}
{"title": "Development and External Validation of an Artificial Intelligence-Based Method for Scalable Chest Radiograph Diagnosis: A Multi-Country Cross-Sectional Study.", "abstract": "", "journal": "Research (Washington, D.C.)", "date": "2024-08-07", "authors": ["ZeyeLiu", "JingXu", "ChengliangYin", "GuojingHan", "YueChe", "GeFan", "XiaofeiLi", "LixinXie", "LeiBao", "ZiminPeng", "JinduoWang", "YanChen", "FengwenZhang", "WenbinOuyang", "ShouzhengWang", "JunweiGuo", "YanqiuMa", "XiangzhiMeng", "TaibingFan", "AihuaZhi", "NoneDawaciren", "KangYi", "TaoYou", "YuejinYang", "JueLiu", "YiShi", "YuanHuang", "XiangbinPan"], "doi": "10.34133/research.0426\n10.48550/arXiv.2304.02643"}
{"title": "Phenotyping COVID-19 respiratory failure in spontaneously breathing patients with AI on lung CT-scan.", "abstract": "Automated analysis of lung computed tomography (CT) scans may help characterize subphenotypes of acute respiratory illness. We integrated lung CT features measured via deep learning with clinical and laboratory data in spontaneously breathing subjects to enhance the identification of COVID-19 subphenotypes.\nThis is a multicenter observational cohort study in spontaneously breathing patients with COVID-19 respiratory failure exposed to early lung CT within 7\u00a0days of admission. We explored lung CT images using deep learning approaches to quantitative and qualitative analyses; latent class analysis (LCA) by using clinical, laboratory and lung CT variables; regional differences between subphenotypes following 3D spatial trajectories.\nComplete datasets were available in 559 patients. LCA identified two subphenotypes (subphenotype 1 and 2). As compared with subphenotype 2 (n\u2009=\u2009403), subphenotype 1 patients (n\u2009=\u2009156) were older, had higher inflammatory biomarkers, and were more hypoxemic. Lungs in subphenotype 1 had a higher density gravitational gradient with a greater proportion of consolidated lungs as compared with subphenotype 2. In contrast, subphenotype 2 had a higher density submantellar-hilar gradient with a greater proportion of ground glass opacities as compared with subphenotype 1. Subphenotype 1 showed higher prevalence of comorbidities associated with endothelial dysfunction and higher 90-day mortality than subphenotype 2, even after adjustment for clinically meaningful variables.\nIntegrating lung-CT data in a LCA allowed us to identify two subphenotypes of COVID-19, with different clinical trajectories. These exploratory findings suggest a role of automated imaging characterization guided by machine learning in subphenotyping patients with respiratory failure.\nClinicalTrials.gov Identifier: NCT04395482. Registration date: 19/05/2020.", "journal": "Critical care (London, England)", "date": "2024-08-06", "authors": ["EmanueleRezoagli", "YiXin", "DavideSignori", "WenliSun", "SarahGerard", "Kevin LDelucchi", "AuroraMagliocca", "GiovanniVitale", "MatteoGiacomini", "LindaMussoni", "JonathanMontomoli", "MatteoSubert", "AlessandraPonti", "SavinoSpadaro", "GiancarlaPoli", "FrancescoCasola", "JacobHerrmann", "GiuseppeFoti", "Carolyn SCalfee", "JohnLaffey", "GiacomoBellani", "MaurizioCereda", "NoneNone"], "doi": "10.1186/s13054-024-05046-3\n10.1001/jama.2019.5791\n10.1164/rccm.202108-1908ST\n10.1186/s13054-022-04121-x\n10.1055/s-0039-1684049\n10.1164/rccm.201603-0645OC\n10.1007/s00134-018-5378-3\n10.1016/S2213-2600(21)00461-6\n10.1016/S2213-2600(18)30177-2\n10.1016/j.chest.2016.03.016\n10.1016/S2213-2600(19)30138-9\n10.1148/radiol.222998\n10.1038/s41598-020-80936-4\n10.1016/j.ymeth.2022.07.007\n10.1186/s13054-021-03685-4\n10.1016/j.neuroimage.2006.01.015\n10.1164/ajrccm/136.3.730\n10.1007/s00330-021-07937-3\n10.1097/CCM.0000000000004710\n10.1016/S2213-2600(14)70097-9\n10.1016/0031-3203(94)00125-6\n10.1016/j.anclin.2021.02.003\n10.1513/AnnalsATS.202008-1080OC\n10.1186/s13054-022-04065-2\n10.1016/S2213-2600(21)00157-0\n10.1016/S2213-2600(19)30369-8\n10.1183/13993003.03498-2020\n10.1001/jama.2021.1545\n10.1016/S2213-2600(20)30366-0\n10.1016/S2213-2600(21)00365-9\n10.1016/j.eclinm.2021.100829\n10.1016/j.resp.2020.103455\n10.1186/s13054-022-04251-2\n10.1089/jamp.2022.0058\n10.1016/S2213-2600(20)30370-2\n10.1183/13993003.00609-2019\n10.1186/s13613-022-01015-7\n10.1056/NEJMra2212850\n10.1186/s13054-021-03610-9\n10.1186/s13054-022-04200-z"}
{"title": "The Role of Artificial Intelligence in Combatting Respiratory Tract Infections.", "abstract": "Respiratory tract infections (RTIs) such as pneumonia, bronchitis, and COVID-19 are significant global health concerns due to their high morbidity and mortality rates. The advent of artificial intelligence (AI) offers innovative solutions across various aspects of RTI management, including diagnosis, prediction, treatment, and prevention. AI algorithms enhance diagnostic accuracy by analyzing extensive data from electronic health records and imaging studies, often surpassing human radiologists in identifying diseases such as pneumonia. For instance, AI-based image recognition tools have demonstrated remarkable precision in detecting pneumonia from chest X-rays. Additionally, AI models can predict disease outbreaks and optimize public health responses, as exemplified during the COVID-19 pandemic where AI predicted infection hotspots and evaluated the effectiveness of containment measures. In personalized medicine, AI tailors treatments based on individual patient profiles, thereby improving therapeutic outcomes and accelerating drug discovery. Wearable AI devices facilitate early detection and prevention of RTIs through continuous health monitoring. Despite its transformative potential, AI implementation in healthcare faces challenges, including data privacy, algorithm transparency, and ethical concerns. Addressing these issues necessitates collaboration among technologists, healthcare providers, and policymakers to ensure responsible and equitable integration of AI technologies. This editorial underscores the transformative potential of AI in managing RTIs and calls for robust frameworks to harness AI's benefits while safeguarding patient rights.", "journal": "Cureus", "date": "2024-08-02", "authors": ["Vasiliki EGeorgakopoulou"], "doi": "10.7759/cureus.63635"}
{"title": "Study on lung CT image segmentation algorithm based on threshold-gradient combination and improved convex hull method.", "abstract": "Lung images often have the characteristics of strong noise, uneven grayscale distribution, and complex pathological structures, which makes lung image segmentation a challenging task. To solve this problems, this paper proposes an initial lung mask extraction algorithm that combines threshold and gradient. The gradient used in the algorithm is obtained by the time series feature extraction method based on differential memory (TFDM), which is obtained by the grayscale threshold and image grayscale features. At the same time, we also proposed a lung contour repair algorithm based on the improved convex hull method to solve the contour loss caused by solid nodules and other lesions. Experimental results show that on the COVID-19 CT segmentation dataset, the advanced lung segmentation algorithm proposed in this article achieves better segmentation results and greatly improves the consistency and accuracy of lung segmentation. Our method can obtain more lung information, resulting in ideal segmentation effects with improved accuracy and robustness.", "journal": "Scientific reports", "date": "2024-08-01", "authors": ["JunbaoZheng", "LixianWang", "JiangshengGui", "Abdulla HamadYussuf"], "doi": "10.1038/s41598-024-68409-4\n10.3322/caac.21492\n10.3390/diagnostics11040732\n10.1049/ipr2.12744\n10.1080/13682199.2022.2159291\n10.1016/j.procs.2023.01.144\n10.1186/s12859-021-04234-0\n10.1016/j.cmpb.2020.105864\n10.1007/s11042-021-11787-y\n10.1007/s11042-022-13660-y\n10.2478/acss-2021-0023\n10.1007/s11517-022-02667-0\n10.1016/j.bspc.2021.102666\n10.1016/j.neucom.2022.12.003\n10.1166/jmihi.2021.3422\n10.3390/s21010268\n10.3389/fphys.2022.981463\n10.1007/s13534-018-0086-z\n10.1109/TIP.2018.2836306\n10.1016/j.compbiomed.2021.104781\n10.1016/j.bspc.2022.104486\n10.1371/journal.pone.0282107\n10.1109/TMI.2018.2890510\n10.1007/s10515-021-00304-y\n10.3390/app8050832\n10.1049/iet-ipr.2019.1054\n10.1007/s11220-020-00300-8\n10.1109/ACCESS.2021.3086229\n10.1002/acm2.13392\n10.2214/AJR.19.22381\n10.1109/TMI.2020.2996645\n10.1080/03772063.2018.1494519\n10.1186/s12938-018-0619-9\n10.1109/JBHI.2021.3053023\n10.1016/j.neunet.2019.08.025\n10.1016/j.bspc.2022.103933\n10.1016/j.bspc.2023.104905\n10.1371/journal.pone.0263916"}
{"title": "Shape prior-constrained deep learning network for medical image segmentation.", "abstract": "We propose a shape prior representation-constrained multi-scale features fusion segmentation network for medical image segmentation, including training and testing stages. The novelty of our training framework lies in two modules comprised of the shape prior constraint and the multi-scale features fusion. The shape prior learning model is embedded into a segmentation neural network to solve the problems of low contrast and neighboring organs with intensities similar to the target organ. The latter can provide both local and global contexts to address the issues of large variations in patient postures as well as organ's shape. In the testing stage, we propose a circular collaboration framework strategy which combines a shape generator auto-encoder network model with a segmentation network model, allowing the two models to collaborate with each other, resulting in a cooperative effect that leads to accurate segmentations. Our proposed method is evaluated and demonstrated on the ACDC MICCAI'17 Challenge Dataset, CT scans datasets, namely, in COVID-19 CT lung, and LiTS2017 liver from three different datasets, and its results are compared with the recent state of the art in these areas. Our method ranked 1st on the ACDC Dataset in terms of Dice score and achieved very competitive performance on COVID-19 CT lung and LiTS2017 liver segmentation.", "journal": "Computers in biology and medicine", "date": "2024-07-31", "authors": ["PengfeiZhang", "YuanzhiCheng", "ShinichiTamura"], "doi": "10.1016/j.compbiomed.2024.108932"}
{"title": "A medical image classification method based on self-regularized adversarial learning.", "abstract": "Deep learning (DL) techniques have been extensively applied in medical image classification. The unique characteristics of medical imaging data present challenges, including small labeled datasets, severely imbalanced class distribution, and significant variations in imaging quality. Recently, generative adversarial network (GAN)-based classification methods have gained attention for their ability to enhance classification accuracy by incorporating realistic GAN-generated images as data augmentation. However, the performance of these GAN-based methods often relies on high-quality generated images, while large amounts of training data are required to train GAN models to achieve optimal\u00a0performance.\nIn this study, we propose an adversarial learning-based classification framework to achieve better classification performance. Innovatively, GAN models are employed as supplementary regularization terms to support classification, aiming to address the challenges described\u00a0above.\nThe proposed classification framework, GAN-DL, consists of a feature extraction network (F-Net), a classifier, and two adversarial networks, specifically a reconstruction network (R-Net) and a discriminator network (D-Net). The F-Net extracts features from input images, and the classifier uses these features for classification tasks. R-Net and D-Net have been designed following the GAN architecture. R-Net employs the extracted feature to reconstruct the original images, while D-Net is tasked with the discrimination between the reconstructed image and the original images. An iterative adversarial learning strategy is designed to guide model training by incorporating multiple network-specific loss functions. These loss functions, serving as supplementary regularization, are automatically derived during the reconstruction process and require no additional data\u00a0annotation.\nTo verify the model's effectiveness, we performed experiments on two datasets, including a COVID-19 dataset with 13\u00a0958 chest x-ray images and an oropharyngeal squamous cell carcinoma (OPSCC) dataset with 3255 positron emission tomography images. Thirteen classic DL-based classification methods were implemented on the same datasets for comparison. Performance metrics included precision, sensitivity, specificity, and \nOur adversarial-based classification framework leverages GAN-based adversarial networks and an iterative adversarial learning strategy to harness supplementary regularization during training. This design significantly enhances classification accuracy and mitigates overfitting issues in medical image datasets. Moreover, its modular design not only demonstrates flexibility but also indicates its potential applicability to various clinical contexts and medical imaging applications.", "journal": "Medical physics", "date": "2024-07-30", "authors": ["ZongFan", "XiaohuiZhang", "SuRuan", "WadeThorstad", "HiramGay", "PengfeiSong", "XiaoweiWang", "HuaLi"], "doi": "10.1002/mp.17320"}
{"title": "Closing the Gaps in Care of Dyslipidemia: Revolutionizing Management with Digital Health and Innovative Care Models.", "abstract": "Although great progress has been made in the diagnostic and treatment options for dyslipidemias, unawareness, underdiagnosis and undertreatment of these disorders remain a significant global health concern. Growth in digital applications and newer models of care provide novel tools to improve the management of chronic conditions such as dyslipidemia. In this review, we discuss the evolving landscape of lipid management in the 21st century, current treatment gaps and possible solutions through digital health and new models of care. Our discussion begins with the history and development of value-based care and the national establishment of quality metrics for various chronic conditions. These concepts on the level of healthcare policy not only inform reimbursements but also define the standard of care. Next, we consider the advances in atherosclerotic cardiovascular disease risk score calculators as well as evolving imaging modalities. The impact and growth of digital health, ranging from telehealth visits to online platforms and mobile applications, will also be explored. We then evaluate the ways in which machine learning and artificial intelligence-driven algorithms are being utilized to address gaps in lipid management. From an organizational perspective, we trace the redesign of medical practices to incorporate a multidisciplinary team model of care, recognizing that atherosclerotic cardiovascular disease risk is multifaceted and requires a comprehensive approach. Finally, we anticipate the future of dyslipidemia management, assessing the many ways in which atherosclerotic cardiovascular disease burden can be reduced on a population-wide scale.", "journal": "Reviews in cardiovascular medicine", "date": "2024-07-30", "authors": ["Samuel JApple", "RachelClark", "JonathanDaich", "Macarena LopezGonzalez", "Robert JOstfeld", "Peter PToth", "VeraBittner", "Seth SMartin", "Jamal SRana", "KhurramNasir", "Michael DShapiro", "Salim SVirani", "LeandroSlipczuk"], "doi": "10.31083/j.rcm2412350"}
{"title": "A Novel Hybrid Machine Learning-Based System Using Deep Learning Techniques and Meta-Heuristic Algorithms for Various Medical Datatypes Classification.", "abstract": "Medicine is one of the fields where the advancement of computer science is making significant progress. Some diseases require an immediate diagnosis in order to improve patient outcomes. The usage of computers in medicine improves precision and accelerates data processing and diagnosis. In order to categorize biological images, hybrid machine learning, a combination of various deep learning approaches, was utilized, and a meta-heuristic algorithm was provided in this research. In addition, two different medical datasets were introduced, one covering the magnetic resonance imaging (MRI) of brain tumors and the other dealing with chest X-rays (CXRs) of COVID-19. These datasets were introduced to the combination network that contained deep learning techniques, which were based on a convolutional neural network (CNN) or autoencoder, to extract features and combine them with the next step of the meta-heuristic algorithm in order to select optimal features using the particle swarm optimization (PSO) algorithm. This combination sought to reduce the dimensionality of the datasets while maintaining the original performance of the data. This is considered an innovative method and ensures highly accurate classification results across various medical datasets. Several classifiers were employed to predict the diseases. The COVID-19 dataset found that the highest accuracy was 99.76% using the combination of CNN-PSO-SVM. In comparison, the brain tumor dataset obtained 99.51% accuracy, the highest accuracy derived using the combination method of autoencoder-PSO-KNN.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2024-07-27", "authors": ["Yezi AliKadhim", "Mehmet SerdarGuzel", "AlokMishra"], "doi": "10.3390/diagnostics14141469\n10.2528/PIER12061410\n10.1016/j.cca.2020.03.022\n10.1016/j.bbi.2020.03.031\n10.1007/s12016-020-08792-8\n10.1016/j.chaos.2020.110059\n10.1016/j.compbiomed.2020.103795\n10.1016/j.jhin.2020.03.001\n10.1109/MCI.2006.329691\n10.13005/bpj/1511\n10.3390/s21062222\n10.1109/ACCESS.2019.2904145\n10.1007/s11042-015-2649-7\n10.1007/s11571-021-09712-y\n10.1016/j.eswa.2021.115452\n10.1016/j.knosys.2019.04.022\n10.1109/ACCESS.2020.3028012\n10.1016/j.compbiomed.2019.03.017\n10.1016/j.mehy.2020.109696\n10.3390/s19091992\n10.1016/j.compmedimag.2019.05.001\n10.1016/j.bbe.2020.06.001\n10.3390/diagnostics10080565\n10.1016/j.irbm.2020.05.003\n10.1371/journal.pone.0140381\n10.33889/IJMEMS.2020.5.4.052\n10.1016/j.compbiomed.2019.103345\n10.3390/sym11020157\n10.1016/j.bspc.2019.101678\n10.1109/ACCESS.2021.3076756\n10.1109/ACCESS.2021.3051723\n10.3390/ijerph17124204\n10.1016/j.chaos.2020.110210\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103792\n10.1016/j.neunet.2024.106183\n10.1016/j.neunet.2023.08.035\n10.1007/s12539-023-00571-1\n10.32604/biocell.2023.025905\n10.1109/ACCESS.2023.3236812\n10.1016/j.bspc.2022.103949\n10.1007/s11760-023-02567-2\n10.1016/j.bspc.2023.105419\n10.1186/s41256-020-00135-6\n10.1101/2020.08.31.20175828\n10.1371/journal.pone.0157112\n10.1016/S0893-6080(05)80056-5\n10.1016/S0042-6989(97)00169-7\n10.1016/j.ejor.2016.09.055\n10.1007/978-3-642-27737-5_530-5\n10.1016/j.swevo.2020.100663\n10.1016/j.scs.2020.102686\n10.22146/ijeis.34713\n10.1007/s10489-020-02002-w\n10.1016/j.jocs.2018.12.003\n10.1007/s10278-013-9600-0\n10.1016/j.imu.2020.100330\n10.1016/j.patrec.2020.09.010\n10.5120/908-1286\n10.3390/s22228999"}
{"title": "Machine Learning Analysis in Diffusion Kurtosis Imaging for Discriminating Pediatric Posterior Fossa Tumors: A Repeatability and Accuracy Pilot Study.", "abstract": "", "journal": "Cancers", "date": "2024-07-27", "authors": ["Ioan PaulVoicu", "FrancescoDotta", "AntonioNapolitano", "MassimoCaulo", "EleonoraPiccirilli", "ClaudiaD'Orazio", "AndreaCarai", "EvelinaMiele", "MariaVinci", "SabrinaRossi", "AntonellaCacchione", "SabinaVennarini", "GiadaDel Baldo", "AngelaMastronuzzi", "PaoloTom\u00e0", "Giovanna StefaniaColafati"], "doi": "10.3390/cancers16142578\n10.1093/neuonc/nou327\n10.1056/NEJM199412013312207\n10.1016/j.nic.2016.08.001\n10.3389/fcell.2022.1082947\n10.1002/jmri.21185\n10.1002/jmri.22722\n10.1016/j.neuroimage.2006.01.015\n10.1109/EMBC.2016.7591443\n10.1093/neuros/nyab311\n10.3174/ajnr.A7200\n10.1093/neuonc/noab272\n10.1038/s41598-021-82214-3\n10.1007/s00234-021-02819-3\n10.1016/S0006-3495(94)80775-1\n10.1148/radiology.201.3.8939209\n10.1002/mrm.22655\n10.1148/radiol.2017171315\n10.1016/j.ejrad.2019.108690\n10.1097/00004728-198811000-00021\n10.3390/cancers14194778\n10.1093/neuonc/noab106\n10.1002/mrm.20508\n10.1002/mrm.22501\n10.1002/nbm.3271\n10.1002/nbm.3269\n10.1016/S0140-6736(86)90837-8\n10.1613/jair.1.11192\n10.18637/jss.v039.i05\n10.2214/ajr.177.2.1770449\n10.3174/ajnr.A2155\n10.3174/ajnr.A7270\n10.3389/fimmu.2023.1180908\n10.3174/ajnr.A5899"}
{"title": "High throughput spatial immune mapping reveals an innate immune scar in post-COVID-19 brains.", "abstract": "The underlying pathogenesis of neurological sequelae in post-COVID-19 patients remains unclear. Here, we used multidimensional spatial immune phenotyping and machine learning methods on brains from initial COVID-19 survivors to identify the biological correlate associated with previous SARS-CoV-2 challenge. Compared to healthy controls, individuals with post-COVID-19 revealed a high percentage of TMEM119", "journal": "Acta neuropathologica", "date": "2024-07-27", "authors": ["MariusSchwabenland", "DilaraHasavci", "SibylleFrase", "KatharinaWolf", "NikolausDeigendesch", "Joerg MBuescher", "Kirsten DMertz", "BenjaminOndruschka", "HermannAltmeppen", "JakobMatschke", "MarkusGlatzel", "StephanFrank", "RobertThimme", "JuergenBeck", "Jonas AHosp", "ThomasBlank", "BertramBengsch", "MarcoPrinz"], "doi": "10.1007/s00401-024-02770-6\n10.1016/j.jhep.2022.03.040\n10.1038/s41591-023-02521-2\n10.1126/science.adg7942\n10.1016/j.eclinm.2021.101019\n10.1038/s41586-022-04569-5\n10.1021/acs.analchem.1c05224\n10.1016/j.cmet.2021.10.010\n10.1212/NXI.0000000000001164\n10.1093/brain/awab009\n10.1038/s41586-023-06651-y\n10.1016/S1474-4422(20)30308-2\n10.1136/bmj-2022-072529\n10.1016/S1474-4422(22)00027-8\n10.1038/s43587-021-00141-4\n10.1016/s0165-5728(99)00141-1\n10.1038/s41590-021-01113-x\n10.1038/s41593-024-01573-y\n10.1021/acs.analchem.2c04396\n10.1007/s00401-021-02370-8\n10.1038/s41467-023-38373-0\n10.1016/j.immuni.2021.06.002\n10.1016/S1473-3099(21)00703-9\n10.1038/s41591-021-01292-y\n10.1016/S2215-0366(21)00084-5\n10.1038/s41591-023-02525-y\n10.1038/s41596-023-00881-0\n10.1038/s41586-021-03710-0\n10.1038/s41590-023-01724-6\n10.1038/s41591-022-02116-3"}
{"title": "Prediction of early-phase cytomegalovirus pneumonia in post-stem cell transplantation using a deep learning model.", "abstract": "Diagnostic challenges exist for CMV pneumonia in post-hematopoietic stem cell transplantation (post-HSCT) patients, despite early-phase radiographic changes.\nThe study aims to employ a deep learning model distinguishing CMV pneumonia from COVID-19 pneumonia, community-acquired pneumonia, and normal lungs post-HSCT.\nInitially, 6 neural network models were pre-trained with COVID-19 pneumonia, community-acquired pneumonia, and normal lung CT images from Kaggle's COVID multiclass dataset (Dataset A), then Dataset A was combined with the CMV pneumonia images from our center, forming Dataset B. We use a few-shot transfer learning strategy to fine-tune the pre-trained models and evaluate model performance in Dataset B.\n34 cases of CMV pneumonia were found between January 2018 and December 2022 post-HSCT. Dataset A contained 1681 images of each subgroup from Kaggle. Combined with Dataset A, Dataset B was initially formed by 98 images of CMV pneumonia and normal lung. The optimal model (Xception) achieved an accuracy of 0.9034. Precision, recall, and F1-score all reached 0.9091, with an AUC of 0.9668 in the test set of Dataset B.\nThis framework demonstrates the deep learning model's ability to distinguish rare pneumonia types utilizing a small volume of CT images, facilitating early detection of CMV pneumonia post-HSCT.", "journal": "Technology and health care : official journal of the European Society for Engineering and Medicine", "date": "2024-07-26", "authors": ["YanhuaZheng", "RuilinRen", "TengZuo", "XuanChen", "HanxuanLi", "ChengXie", "MeilingWeng", "ChunxiaoHe", "MinXu", "LiliWang", "NainongLi", "XiaofanLi"], "doi": "10.3233/THC-240597"}
{"title": "Knowledge fused latent representation from lung ultrasound examination for COVID-19 pneumonia severity assessment.", "abstract": "COVID-19 pneumonia severity assessment is of great clinical importance, and lung ultrasound (LUS) plays a crucial role in aiding the severity assessment of COVID-19 pneumonia due to its safety and portability. However, its reliance on qualitative and subjective observations by clinicians is a limitation. Moreover, LUS images often exhibit significant heterogeneity, emphasizing the need for more quantitative assessment methods. In this paper, we propose a knowledge fused latent representation framework tailored for COVID-19 pneumonia severity assessment using LUS examinations. The framework transforms the LUS examination into latent representation and extracts knowledge from regions labeled by clinicians to improve accuracy. To fuse the knowledge into the latent representation, we employ a knowledge fusion with latent representation (KFLR) model. This model significantly reduces errors compared to approaches that lack prior knowledge integration. Experimental results demonstrate the effectiveness of our method, achieving high accuracy of 96.4\u00a0% and 87.4\u00a0% for binary-level and four-level COVID-19 pneumonia severity assessments, respectively. It is worth noting that only a limited number of studies have reported accuracy for clinically valuable exam level assessments, and our method surpass existing methods in this context. These findings highlight the potential of the proposed framework for monitoring disease progression and patient stratification in COVID-19 pneumonia cases.", "journal": "Ultrasonics", "date": "2024-07-26", "authors": ["ZhiqiangLi", "XuepingYang", "HengrongLan", "MixueWang", "LijieHuang", "XingyueWei", "GangqiaoXie", "RuiWang", "JingYu", "QiongHe", "YaoZhang", "JianwenLuo"], "doi": "10.1016/j.ultras.2024.107409"}
{"title": "Augmenting Radiological Diagnostics with AI for Tuberculosis and COVID-19 Disease Detection: Deep Learning Detection of Chest Radiographs.", "abstract": "In this research, we introduce a network that can identify pneumonia, COVID-19, and tuberculosis using X-ray images of patients' chests. The study emphasizes tuberculosis, COVID-19, and healthy lung conditions, discussing how advanced neural networks, like VGG16 and ResNet50, can improve the detection of lung issues from images. To prepare the images for the model's input requirements, we enhanced them through data augmentation techniques for training purposes. We evaluated the model's performance by analyzing the precision, recall, and F1 scores across training, validation, and testing datasets. The results show that the ResNet50 model outperformed VGG16 with accuracy and resilience. It displayed superior ROC AUC values in both validation and test scenarios. Particularly impressive were ResNet50's precision and recall rates, nearing 0.99 for all conditions in the test set. On the hand, VGG16 also performed well during testing-detecting tuberculosis with a precision of 0.99 and a recall of 0.93. Our study highlights the performance of our deep learning method by showcasing the effectiveness of ResNet50 over traditional approaches like VGG16. This progress utilizes methods to enhance classification accuracy by augmenting data and balancing them. This positions our approach as an advancement in using state-of-the-art deep learning applications in imaging. By enhancing the accuracy and reliability of diagnosing ailments such as COVID-19 and tuberculosis, our models have the potential to transform care and treatment strategies, highlighting their role in clinical diagnostics.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2024-07-13", "authors": ["ManjurKolhar", "Ahmed MAl Rajeh", "Raisa Nazir AhmedKazi"], "doi": "10.3390/diagnostics14131334\n10.1016/S1473-3099(24)00007-0\n10.1016/j.ijmmb.2022.10.008\n10.1183/13993003.00499-2023\n10.3390/ijms24065202\n10.3174/ajnr.A8252\n10.1080/10408363.2023.2259466\n10.3390/diagnostics13111954\n10.3390/cancers15225417\n10.1109/ACCESS.2021.3054735\n10.1007/s00530-021-00878-3\n10.3390/s22124426\n10.1016/j.compbiomed.2021.104348\n10.1016/j.compbiomed.2021.104887\n10.1007/s10278-024-01005-0\n10.1016/j.aej.2022.10.053\n10.1007/s11042-022-13843-7\n10.3390/s21227528\n10.3390/jpm10040224\n10.1016/j.compmedimag.2022.102123\n10.1002/widm.1264\n10.1016/j.neucom.2016.12.038\n10.1016/j.compbiomed.2020.104115\n10.1155/2021/5528144"}
{"title": "Optimization of vision transformer-based detection of lung diseases from chest X-ray images.", "abstract": "Recent advances in Vision Transformer (ViT)-based deep learning have significantly improved the accuracy of lung disease prediction from chest X-ray images. However, limited research exists on comparing the effectiveness of different optimizers for lung disease prediction within ViT models. This study aims to systematically evaluate and compare the performance of various optimization methods for ViT-based models in predicting lung diseases from chest X-ray images.\nThis study utilized a chest X-ray image dataset comprising 19,003 images containing both normal cases and six lung diseases: COVID-19, Viral Pneumonia, Bacterial Pneumonia, Middle East Respiratory Syndrome (MERS), Severe Acute Respiratory Syndrome (SARS), and Tuberculosis. Each ViT model (ViT, FastViT, and CrossViT) was individually trained with each optimization method (Adam, AdamW, NAdam, RAdam, SGDW, and Momentum) to assess their performance in lung disease prediction.\nWhen tested with ViT on the dataset with balanced-sample sized classes, RAdam demonstrated superior accuracy compared to other optimizers, achieving 95.87%. In the dataset with imbalanced sample size, FastViT with NAdam achieved the best performance with an accuracy of 97.63%.\nWe provide comprehensive optimization strategies for developing ViT-based model architectures, which can enhance the performance of these models for lung disease prediction from chest X-ray images.", "journal": "BMC medical informatics and decision making", "date": "2024-07-09", "authors": ["JinsolKo", "SoyeonPark", "Hyun GooWoo"], "doi": "10.1186/s12911-024-02591-3\n10.1016/j.cmpb.2020.105581\n10.1038/s41598-020-76550-z\n10.1109/JPROC.2021.3054390\n10.1109/JTEHM.2021.3134096\n10.3390/jcm11113013\n10.3390/ijerph182111086\n10.3390/math11112466\n10.3390/app12042080\n10.1016/j.compbiomed.2023.106646\n10.3390/jpm12020310\n10.1016/j.icte.2020.04.010\n10.1016/j.cmpb.2020.105532\n10.1186/s40537-019-0192-5\n10.1109/72.286891\n10.3390/brainsci10070427\n10.3390/app13010454\n10.1016/j.cmpb.2022.107141"}
{"title": "COVID-19 and Pneumonia detection and web deployment from CT scan and X-ray images using deep learning.", "abstract": "During the COVID-19 pandemic, pneumonia was the leading cause of respiratory failure and death. In addition to SARS-COV-2, it can be caused by several other bacterial and viral agents. Even today, variants of SARS-COV-2 are endemic and COVID-19 cases are common in many places. The symptoms of COVID-19 are highly diverse and robust, ranging from invisible to severe respiratory failure. Current detection methods for the disease are time-consuming and expensive with low accuracy and precision. To address such situations, we have designed a framework for COVID-19 and Pneumonia detection using multiple deep learning algorithms further accompanied by a deployment scheme. In this study, we have utilized four prominent deep learning models, which are VGG-19, ResNet-50, Inception V3 and Xception, on two separate datasets of CT scan and X-ray images (COVID/Non-COVID) to identify the best models for the detection of COVID-19. We achieved accuracies ranging from 86% to 99% depending on the model and dataset. To further validate our findings, we have applied the four distinct models on two more supplementary datasets of X-ray images of bacterial pneumonia and viral pneumonia. Additionally, we have implemented a flask app to visualize the outcome of our framework to show the identified COVID and Non-COVID images. The findings of this study will be helpful to develop an AI-driven automated tool for the cost effective and faster detection and better management of COVID-19 patients.", "journal": "PloS one", "date": "2024-07-08", "authors": ["NahidIslam", "Abu S MMohsin", "Shadab HafizChoudhury", "Tazwar ProdhanShaer", "Md AdnanIslam", "OmarSadat", "Nahid HossainTaz"], "doi": "10.1371/journal.pone.0302413\n10.1016/j.jinf.2020.03.007\n10.1186/s12890-023-02369-9\n10.1016/j.bbe.2021.05.013\n10.1007/s10489-020-01902-1\n10.1016/j.celrep.2021.109017\n10.1002/JPER.21-0335"}
{"title": "Assessing the prognostic utility of clinical and radiomic features for COVID-19 patients admitted to ICU: challenges and lessons learned.", "abstract": "Severe cases of COVID-19 often necessitate escalation to the Intensive Care Unit (ICU), where patients may face grave outcomes, including mortality. Chest X-rays play a crucial role in the diagnostic process for evaluating COVID-19 patients. Our collaborative efforts with Michigan Medicine in monitoring patient outcomes within the ICU have motivated us to investigate the potential advantages of incorporating clinical information and chest X-ray images for predicting patient outcomes. We propose an analytical workflow to address challenges such as the absence of standardized approaches for image pre-processing and data utilization. We then propose an ensemble learning approach designed to maximize the information derived from multiple prediction algorithms. This entails optimizing the weights within the ensemble and considering the common variability present in individual risk scores. Our simulations demonstrate the superior performance of this weighted ensemble averaging approach across various scenarios. We apply this refined ensemble methodology to analyze post-ICU COVID-19 mortality, an occurrence observed in 21% of COVID-19 patients admitted to the ICU at Michigan Medicine. Our findings reveal substantial performance improvement when incorporating imaging data compared to models trained solely on clinical risk factors. Furthermore, the addition of radiomic features yields even larger enhancements, particularly among older and more medically compromised patients. These results may carry implications for enhancing patient outcomes in similar clinical contexts.", "journal": "Harvard data science review", "date": "2024-07-08", "authors": ["YumingSun", "StephenSalerno", "ZiyangPan", "EileenYang", "ChinakornSujimongkol", "JiyeonSong", "XinanWang", "PeisongHan", "DonglinZeng", "JianKang", "David CChristiani", "YiLi"], "doi": "10.1162/99608f92.9d86a749\n10.1001/jamaoncol.2020.5403\n10.48550/arXiv.2205.02948"}
{"title": "Adaptability of prognostic prediction models for patients with acute coronary syndrome during the COVID-19 pandemic.", "abstract": "The detrimental repercussions of the COVID-19 pandemic on the quality of care and clinical outcomes for patients with acute coronary syndrome (ACS) necessitate a rigorous re-evaluation of prognostic prediction models in the context of the pandemic environment. This study aimed to elucidate the adaptability of prediction models for 30-day mortality in patients with ACS during the pandemic periods.\nA total of 2041 consecutive patients with ACS were included from 32 institutions between December 2020 and April 2023. The dataset comprised patients who were admitted for ACS and underwent coronary angiography for the diagnosis during hospitalisation. The prediction accuracy of the Global Registry of Acute Coronary Events (GRACE) and a machine learning model, KOTOMI, was evaluated for 30-day mortality in patients with ST-elevation acute myocardial infarction (STEMI) and non-ST-elevation acute coronary syndrome (NSTE-ACS).\nThe area under the receiver operating characteristics curve (AUROC) was 0.85 (95% CI 0.81 to 0.89) in the GRACE and 0.87 (95% CI 0.82 to 0.91) in the KOTOMI for STEMI. The difference of 0.020 (95% CI -0.098-0.13) was not significant. For NSTE-ACS, the respective AUROCs were 0.82 (95% CI 0.73 to 0.91) in the GRACE and 0.83 (95% CI 0.74 to 0.91) in the KOTOMI, also demonstrating insignificant difference of 0.010 (95% CI -0.023 to 0.25). The prediction accuracy of both models had consistency in patients with STEMI and insignificant variation in patients with NSTE-ACS between the pandemic periods.\nThe prediction models maintained high accuracy for 30-day mortality of patients with ACS even in the pandemic periods, despite marginal variation observed.", "journal": "BMJ health & care informatics", "date": "2024-07-03", "authors": ["MasahiroNishi", "TakeshiNakamura", "KenjiYanishi", "SatoakiMatoba", "NoneNone"], "doi": "10.1136/bmjhci-2024-101074\n10.3390/jcm11092323\n10.1007/s11936-023-00988-3\n10.1093/eurheartj/ehab621\n10.1016/j.ijcha.2022.101151\n10.1056/NEJMc2010418\n10.1001/jamacardio.2020.6210\n10.1016/S2468-2667(20)30117-1\n10.1016/j.ahj.2010.06.053\n10.1016/j.jjcc.2017.09.006\n10.5114/amsad/152107\n10.1097/MCA.0000000000001162\n10.1371/journal.pone.0277260\n10.1253/circj.cj-09-0774\n10.1001/archinte.163.19.2345\n10.1001/jama.291.22.2727\n10.1093/eurheartj/ehm004\n10.1001/jama.284.7.835\n10.1016/S0140-6736(20)32519-8\n10.1253/circj.CJ-19-0133\n10.1093/eurheartj/ehad191\n10.1161/CIR.0000000000001030\n10.1016/j.jacc.2005.12.003\n10.1016/j.jcin.2008.04.010"}
{"title": "Machine learning-aided algorithm design for prediction of severity from clinical, demographic, biochemical and immunological parameters: Our COVID-19 experience from the pandemic.", "abstract": "The severity of laboratory and imaging finding was found to be inconsistent with clinical symptoms in COVID-19 patients, thereby increasing casualties. As compared to conventional biomarkers, machine learning algorithms can learn nonlinear and complex interactions and thus improve prediction accuracy. This study aimed at evaluating role of biochemical and immunological parameters-based machine learning algorithms for severity indexing in COVID-19.\nLaboratory biochemical results of 5715 COVID-19 patients were mined from electronic records including 509 admitted in COVID-19 ICU. Random Forest Classifier (RFC), Support Vector Machine (SVM), Naive Bayesian Classifier (NBC) and K-Nearest Neighbours (KNN) classifier models were used. Lasso regression helped in identifying the most influential parameter. A decision tree was made for subdivided data set, based on randomization.\nAccuracy of SVM was highest with 94.18% and RFC with 94.04%. SVM had highest PPV (1.00), and NBC had highest NPV (0.95). QUEST modelling ignored age, urea and total protein, and only C-reactive protein and lactate dehydrogenase were considered to be a part of decision-tree algorithm. The overall percentage of correct classification was 78.31% in the overall algorithm with a sensitivity of 87.95% and an AUC of 0.747.\nC-reactive protein and lactate dehydrogenase being routinely performed tests in clinical laboratories in peripheral setups, this algorithm could be an effective predictive tool. SVM and RFC models showed significant accuracy in predicting COVID-19 severity and could be useful for future pandemics.", "journal": "Journal of family medicine and primary care", "date": "2024-07-01", "authors": ["SuchitraKumari", "SwagataTripathy", "SauravNayak", "Aishvarya SRajasimman"], "doi": "10.4103/jfmpc.jfmpc_1752_23"}
{"title": "Lung Ultrasonography in the Evaluation of Late Sequelae of COVID-19 Pneumonia-A Comparison with Chest Computed Tomography: A Prospective Study.", "abstract": "The onset of the COVID-19 pandemic allowed physicians to gain experience in lung ultrasound (LUS) during the acute phase of the disease. However, limited data are available on LUS findings during the recovery phase. The aim of this study was to evaluate the utility of LUS to assess lung involvement in patients with post-COVID-19 syndrome. This study prospectively enrolled 72 patients who underwent paired LUS and chest CT scans (112 pairs including follow-up). The most frequent CT findings were ground glass opacities (83.3%), subpleural lines (72.2%), traction bronchiectasis (37.5%), and consolidations (31.9%). LUS revealed irregular pleural lines as a common abnormality initially (56.9%), along with subpleural consolidation >2.5 mm \u226410 mm (26.5%) and B-lines (26.5%). A strong correlation was found between LUS score, calculated by artificial intelligence percentage involvement in ground glass opacities described in CT (r = 0.702, ", "journal": "Viruses", "date": "2024-06-27", "authors": ["KatarzynaZimna", "Ma\u0142gorzataSobiecka", "JacekWakuli\u0144ski", "DorotaWyrostkiewicz", "EwaJankowska", "MonikaSzturmowicz", "Witold ZTomkowski"], "doi": "10.3390/v16060905\n10.1007/s00117-020-00747-6\n10.1007/s00134-020-06096-1\n10.1016/j.clinimag.2020.10.035\n10.3906/sag-2106-238\n10.1016/j.cmi.2021.02.019\n10.1183/13993003.03690-2020\n10.1111/imr.12977\n10.1111/ijcp.13746\n10.1152/ajplung.00238.2020\n10.1155/2020/6175964\n10.1080/17476348.2021.1916472\n10.12659/MSM.928996\n10.1016/S1473-3099(21)00703-9\n10.4103/lungindia.lungindia_818_20\n10.4329/wjr.v14.i4.104\n10.1136/bmj.n1648\n10.1016/j.eclinm.2021.100731\n10.1136/thoraxjnl-2020-215314\n10.1007/s00330-021-08317-7\n10.7759/cureus.22770\n10.1146/annurev-med-043021-030635\n10.1016/j.dsx.2021.04.007\n10.1148/radiol.2021203153\n10.3390/s21020455\n10.1016/j.compbiomed.2021.104425\n10.1155/2021/5528144\n10.1007/s12539-020-00408-1\n10.1007/s10439-022-02958-5\n10.1016/j.crad.2020.04.003\n10.1111/anae.15082\n10.21037/apm-21-1731\n10.1007/s11739-022-03084-9\n10.3389/fmed.2021.815732\n10.1016/j.rmed.2021.106384\n10.3390/diagnostics10080597\n10.1183/13993003.01519-2020\n10.1002/jum.16088\n10.1007/s00134-019-05725-8\n10.1007/s40477-020-00501-7\n10.1159/000503585\n10.1002/jum.15285\n10.1148/ryai.2020200048\n10.1111/echo.15152\n10.1002/jum.15425\n10.1016/j.ejrad.2022.110156\n10.1111/resp.14311\n10.1093/rheumatology/keab801\n10.1016/j.jbspin.2022.105407\n10.1016/j.hrtlng.2022.09.011\n10.1186/s13075-017-1409-7\n10.4103/1817-1737.128856\n10.1002/jum.14297\n10.1002/jum.14406\n10.3390/jcm10163567"}
{"title": "A retrospective study of deep learning generalization across two centers and multiple models of X-ray devices using COVID-19 chest-X rays.", "abstract": "Generalization of deep learning (DL) algorithms is critical for the secure implementation of computer-aided diagnosis systems in clinical practice. However, broad generalization remains to be a challenge in machine learning. This research aims to identify and study potential factors that can affect the internal validation and generalization of DL networks, namely the institution where the images come from, the image processing applied by the X-ray device, and the type of response function of the X-ray device. For these purposes, a pre-trained convolutional neural network (CNN) (VGG16) was trained three times for classifying COVID-19 and control chest radiographs with the same hyperparameters, but using different combinations of data acquired in two institutions by three different X-ray device manufacturers. Regarding internal validation, the addition of images from an external institution to the training set did not modify the algorithm's internal performance, however, the inclusion of images acquired by a device from a different manufacturer decreased the performance up to 8% (p\u2009<\u20090.05). In contrast, generalization across institutions and X-ray devices with the same type of response function was achieved. Nonetheless, generalization was not observed across devices with different types of response function. This factor was the key impediment to achieving broad generalization in our research, followed by the device's image-processing and the inter-institutional differences, which both reduced generalization performance to 18.9% (p\u2009<\u20090.05), and 9.8% (p\u2009<\u20090.05), respectively. Finally, clustering analysis with features extracted by the CNN was performed, revealing a substantial dependence of feature values extracted by the pre-trained CNN on the X-ray device which acquired the images.", "journal": "Scientific reports", "date": "2024-06-26", "authors": ["Pablo Men\u00e9ndezFern\u00e1ndez-Miranda", "Enrique Marqu\u00e9sFraguela", "Marta \u00c1lvarezde Linera-Alperi", "MiriamCobo", "Amaia P\u00e9rezDel Barrio", "David Rodr\u00edguezGonz\u00e1lez", "Jos\u00e9 AVega", "Lara LloretIglesias"], "doi": "10.1038/s41598-024-64941-5\n10.1007/s11547-020-01200-3\n10.1007/s10140-008-0763-9\n10.1148/radiol.2019191225\n10.1038/s41597-019-0322-0\n10.1038/s41598-020-76550-z\n10.1016/j.inffus.2021.04.008\n10.1007/s10278-019-00180-9\n10.1016/j.heliyon.2020.e04614\n10.1371/journal.pmed.1002683\n10.1148/ryai.2021210097\n10.1001/jama.2013.281053\n10.1007/s11587-015-0246-8\n10.1097/00004691-200203000-00005\n10.21105/joss.03021\n10.1118/1.3611983\n10.1038/s41592-019-0686-2"}
{"title": "Saliency-driven explainable deep learning in medical imaging: bridging visual explainability and statistical quantitative analysis.", "abstract": "Deep learning shows great promise for medical image analysis but often lacks explainability, hindering its adoption in healthcare. Attribution techniques that explain model reasoning can potentially increase trust in deep learning among clinical stakeholders. In the literature, much of the research on attribution in medical imaging focuses on visual inspection rather than statistical quantitative analysis.In this paper, we proposed an image-based saliency framework to enhance the explainability of deep learning models in medical image analysis. We use adaptive path-based gradient integration, gradient-free techniques, and class activation mapping along with its derivatives to attribute predictions from brain tumor MRI and COVID-19 chest X-ray datasets made by recent deep convolutional neural network models.The proposed framework integrates qualitative and statistical quantitative assessments, employing Accuracy Information Curves (AICs) and Softmax Information Curves (SICs) to measure the effectiveness of saliency methods in retaining critical image information and their correlation with model predictions. Visual inspections indicate that methods such as ScoreCAM, XRAI, GradCAM, and GradCAM++ consistently produce focused and clinically interpretable attribution maps. These methods highlighted possible biomarkers, exposed model biases, and offered insights into the links between input features and predictions, demonstrating their ability to elucidate model reasoning on these datasets. Empirical evaluations reveal that ScoreCAM and XRAI are particularly effective in retaining relevant image regions, as reflected in their higher AUC values. However, SICs highlight variability, with instances of random saliency masks outperforming established methods, emphasizing the need for combining visual and empirical metrics for a comprehensive evaluation.The results underscore the importance of selecting appropriate saliency methods for specific medical imaging tasks and suggest that combining qualitative and quantitative approaches can enhance the transparency, trustworthiness, and clinical adoption of deep learning models in healthcare. This study advances model explainability to increase trust in deep learning among healthcare stakeholders by revealing the rationale behind predictions. Future research should refine empirical metrics for stability and reliability, include more diverse imaging modalities, and focus on improving model explainability to support clinical decision-making.", "journal": "BioData mining", "date": "2024-06-23", "authors": ["YusufBrima", "MarcellinAtemkeng"], "doi": "10.1186/s13040-024-00370-4\n10.1109/RBME.2022.3185953\n10.1016/j.media.2017.07.005\n10.1038/323533a0\n10.1007/s10462-019-09716-5\n10.1148/ryai.2020190043\n10.1016/j.eswa.2019.01.048\n10.1148/radiol.2018180887\n10.3390/diagnostics11081480\n10.3389/fncom.2020.00006\n10.3389/fnagi.2019.00194\n10.3390/cancers13061291\n10.1016/j.cell.2018.02.010\n10.1109/JBHI.2021.3074893\n10.3390/info15040182\n10.1109/ACCESS.2020.3010287"}
{"title": "Attention Feature Fusion Network via Knowledge Propagation for Automated Respiratory Sound Classification.", "abstract": "", "journal": "IEEE open journal of engineering in medicine and biology", "date": "2024-06-20", "authors": ["Ida A P ACrisdayanti", "Sung WooNam", "Seong KwanJung", "Seong-EunKim"], "doi": "10.1109/OJEMB.2024.3402139"}
{"title": "A high-accuracy lightweight network model for X-ray image diagnosis: A case study of COVID detection.", "abstract": "The Coronavirus Disease 2019(COVID-19) has caused widespread and significant harm globally. In order to address the urgent demand for a rapid and reliable diagnostic approach to mitigate transmission, the application of deep learning stands as a viable solution. The impracticality of many existing models is attributed to excessively large parameters, significantly limiting their utility. Additionally, the classification accuracy of the model with few parameters falls short of desirable levels. Motivated by this observation, the present study employs the lightweight network MobileNetV3 as the underlying architecture. This paper incorporates the dense block to capture intricate spatial information in images, as well as the transition layer designed to reduce the size and channel number of the feature map. Furthermore, this paper employs label smoothing loss to address the inter-class similarity effects and uses class weighting to tackle the problem of data imbalance. Additionally, this study applies the pruning technique to eliminate unnecessary structures and further reduce the number of parameters. As a result, this improved model achieves an impressive 98.71% accuracy on an openly accessible database, while utilizing only 5.94 million parameters. Compared to the previous method, this maximum improvement reaches 5.41%. Moreover, this research successfully reduces the parameter count by up to 24 times, showcasing the efficacy of our approach. This demonstrates the significant benefits in regions with limited availability of medical resources.", "journal": "PloS one", "date": "2024-06-18", "authors": ["ShujuanWang", "JialinRen", "XiaoliGuo"], "doi": "10.1371/journal.pone.0303049\n10.1016/j.heliyon.2024.e24653\n10.1007/s11042-022-12156-z\n10.1001/jama.2020.3786\n10.1016/j.neucom.2024.127317\n10.1007/s00247-022-05522-4\n10.1016/j.eswa.2023.119900\n10.1007/s11517-022-02758-y\n10.1007/s13246-020-00865-4\n10.1016/j.bspc.2021.103182\n10.1016/j.compbiomed.2022.105350\n10.1145/3065386\n10.1016/j.patcog.2020.107700\n10.1016/j.compbiomed.2021.105014\n10.1016/j.asoc.2020.106859\n10.1038/s41598-020-76550-z\n10.1016/j.chaos.2020.110495\n10.1007/s10489-020-01867-1\n10.1016/j.eswa.2023.121724\n10.1016/j.asoc.2023.110511\n10.1016/j.asoc.2021.107645\n10.1109/JPROC.2020.3004555\n10.1016/j.ijmedinf.2020.104284\n10.1109/ACCESS.2022.3182659\n10.1016/j.cmpb.2020.105581"}
{"title": "Prediction of short-term progression of COVID-19 pneumonia based on chest CT artificial intelligence: during the Omicron epidemic.", "abstract": "The persistent progression of pneumonia is a critical determinant of adverse outcomes in patients afflicted with COVID-19. This study aimed to predict personalized COVID-19 pneumonia progression between the duration of two weeks and 1 month after admission by integrating radiological and clinical features.\nA retrospective analysis, approved by the Institutional Review Board, encompassed patients diagnosed with COVID-19 pneumonia between December 2022 and February 2023. The cohort was divided into training and validation groups in a 7:3 ratio. A trained multi-task U-Net network was deployed to segment COVID-19 pneumonia and lung regions in CT images, from which quantitative features were extracted. The eXtreme Gradient Boosting (XGBoost) algorithm was employed to construct a radiological model. A clinical model was constructed by LASSO method and stepwise regression analysis, followed by the subsequent construction of the combined model. Model performance was assessed using ROC and decision curve analysis (DCA), while Shapley's Additive interpretation (SHAP) illustrated the importance of CT features.\nA total of 214 patients were recruited in our study. Four clinical characteristics and four CT features were identified as pivotal components for constructing the clinical and radiological models. The final four clinical characteristics were incorporated as well as the RS_radiological model to construct the combined prediction model. SHAP analysis revealed that CT score difference exerted the most significant influence on the predictive performance of the radiological model. The training group's radiological, clinical, and combined models exhibited AUC values of 0.89, 0.72, and 0.92, respectively. Correspondingly, in the validation group, these values were observed to be 0.75, 0.72, and 0.81. The DCA curve showed that the combined model exhibited greater clinical utility than the clinical or radiological models.\nOur novel combined model, fusing quantitative CT features with clinical characteristics, demonstrated effective prediction of COVID-19 pneumonia progression from 2 weeks to 1 month after admission. This comprehensive model can potentially serve as a valuable tool for clinicians to develop personalized treatment strategies and improve patient outcomes.", "journal": "BMC infectious diseases", "date": "2024-06-18", "authors": ["XinjingLou", "ChenGao", "LinyuWu", "TingWu", "LinyangHe", "JiahaoShen", "MeiqiHua", "MaoshengXu"], "doi": "10.1186/s12879-024-09504-9\n10.15585/mmwr.mm7104e4\n10.1016/S0140-6736(20)30211-7\n10.1148/radiol.222888\n10.1016/S2213-2600(21)00174-0\n10.1148/radiol.2020200642\n10.1148/radiol.2020200823\n10.1007/s00330-020-07347-x\n10.1016/j.cell.2020.04.045\n10.1007/s00330-021-08432-5\n10.1148/ryct.2020200047\n10.1007/s00330-021-08435-2\n10.1001/jama.2020.2648\n10.1097/CM9.0000000000000819\n10.1148/radiol.2020200370\n10.1016/j.immuni.2022.03.016\n10.1097/RLI.0000000000000672\n10.1186/s40001-022-00634-x\n10.1016/j.jmii.2014.11.005\n10.1016/j.ccm.2004.10.011\n10.3389/fcimb.2022.819267\n10.1038/s41579-020-00459-7\n10.1038/s41598-020-66895-w\n10.1002/jmv.27060\n10.1007/s11606-009-1182-7\n10.1016/j.jinf.2020.04.021\n10.1148/radiol.222600\n10.1016/j.biopha.2020.110493\n10.1007/s00134-020-06033-2\n10.1007/s12016-021-08908-8\n10.1002/ejhf.2095\n10.1080/10408363.2020.1770685\n10.1007/s00330-020-06879-6\n10.1007/s00330-020-07201-0"}
{"title": "Removing Adversarial Noise in X-ray Images via Total Variation Minimization\u00a0and Patch-Based Regularization for Robust Deep\u00a0Learning-based Diagnosis.", "abstract": "Deep learning has significantly advanced the field of radiology-based disease diagnosis, offering enhanced accuracy and efficiency in detecting various medical conditions through the analysis of complex medical images such as X-rays. This technology's ability to discern subtle patterns and anomalies has proven invaluable for swift and accurate disease identification. The relevance of deep learning in radiology has been particularly highlighted during the COVID-19 pandemic, where rapid and accurate diagnosis is crucial for effective treatment and containment. However, recent research has uncovered vulnerabilities in deep learning models when exposed to adversarial attacks, leading to incorrect predictions. In response to this critical challenge, we introduce a novel approach that leverages total variation minimization to combat adversarial noise within X-ray images effectively. Our focus narrows to COVID-19 diagnosis as a case study, where we initially construct a classification model through transfer learning designed to accurately classify lung X-ray images encompassing no pneumonia, COVID-19 pneumonia, and non-COVID pneumonia cases. Subsequently, we extensively evaluated the model's susceptibility to targeted and un-targeted\u00a0adversarial attacks\u00a0by employing the fast gradient sign gradient (FGSM) method. Our findings reveal a substantial reduction in the model's performance, with the average accuracy plummeting from 95.56 to 19.83% under adversarial conditions. However, the experimental results demonstrate the exceptional efficacy of the proposed denoising approach in enhancing the performance of diagnosis models when applied to adversarial examples. Post-denoising, the model exhibits a remarkable accuracy improvement, surging from 19.83 to 88.23% on adversarial images. These promising outcomes underscore the potential of denoising techniques to fortify the resilience and reliability of AI-based COVID-19 diagnostic systems, laying the foundation for their successful deployment in clinical settings.", "journal": "Journal of imaging informatics in medicine", "date": "2024-06-18", "authors": ["Burhan Ul HaqueSheikh", "AasimZafar"], "doi": "10.1007/s10278-023-00919-5\n10.3348/kjr.2020.0146\n10.1016/j.eswa.2015.10.015\n10.1016/j.compmedimag.2016.07.004\n10.1109/ACCESS.2017.2789324\n10.1109/ACCESS.2020.3016780\n10.1007/s13246-020-00865-4\n10.1016/j.asoc.2021.107329\n10.1007/s11548-022-02813-x\n10.1007/s11548-020-02305-w\n10.1007/s10916-020-01597-4\n10.3390/app11094233"}
{"title": "A versatile automated pipeline for quantifying virus infectivity by label-free light microscopy and artificial intelligence.", "abstract": "Virus infectivity is traditionally determined by endpoint titration in cell cultures, and requires complex processing steps and human annotation. Here we developed an artificial intelligence (AI)-powered automated framework for ready detection of virus-induced cytopathic effect (DVICE). DVICE uses the convolutional neural network EfficientNet-B0 and transmitted light microscopy images of infected cell cultures, including coronavirus, influenza virus, rhinovirus, herpes simplex virus, vaccinia virus, and adenovirus. DVICE robustly measures virus-induced cytopathic effects (CPE), as shown by class activation mapping. Leave-one-out cross-validation in different cell types demonstrates high accuracy for different viruses, including SARS-CoV-2 in human saliva. Strikingly, DVICE exhibits virus class specificity, as shown with adenovirus, herpesvirus, rhinovirus, vaccinia virus, and SARS-CoV-2. In sum, DVICE provides unbiased infectivity scores of infectious agents causing CPE, and can be adapted to laboratory diagnostics, drug screening, serum neutralization or clinical samples.", "journal": "Nature communications", "date": "2024-06-16", "authors": ["AnthonyPetkidis", "VardanAndriasyan", "LucaMurer", "RomainVolle", "Urs FGreber"], "doi": "10.1038/s41467-024-49444-1\n10.1146/annurev-virology-012420-022445\n10.1016/j.cell.2014.02.032\n10.1016/S0065-3527(07)70004-0\n10.1128/CMR.00002-06\n10.1073/pnas.38.8.747\n10.1093/oxfordjournals.aje.a118408\n10.1016/j.jviromet.2013.05.015\n10.1016/0887-2333(89)90039-8\n10.1128/JVI.01102-12\n10.1016/j.tibtech.2018.09.007\n10.1038/s41592-019-0403-1\n10.1016/j.cell.2018.03.040\n10.1038/s41592-018-0111-2\n10.1038/s41598-019-54961-x\n10.1016/j.jviromet.2021.114318\n10.1371/journal.pcbi.1007883\n10.1016/j.crviro.2022.100019\n10.1038/s41467-022-33632-y\n10.1371/journal.pcbi.1009480\n10.1038/s41592-021-01284-3\n10.1038/s41597-022-01733-4\n10.2307/2288384\n10.1056/NEJMc2016359\n10.1038/s41591-022-01816-0\n10.1016/S0140-6736(04)17102-X\n10.1128/JVI.02323-13\n10.1128/JCM.01570-10\n10.1038/s41597-020-00604-0\n10.1016/j.isci.2021.102543\n10.1371/journal.pone.0138760\n10.1038/s41587-022-01213-5\n10.1016/j.coviro.2018.12.006\n10.1016/j.coviro.2011.09.002\n10.1016/j.virusres.2004.11.003\n10.1128/JVI.01060-21\n10.1038/s41591-021-01377-8\n10.1146/annurev-immunol-032712-095916\n10.1109/TKDE.2009.191\n10.1128/JVI.01503-09\n10.1038/s41467-018-04379-2\n10.1109/TPAMI.2019.2913372\n10.7717/peerj.453\n10.1038/s41592-019-0582-9"}
{"title": "An emerging network for COVID-19 CT-scan classification using an ensemble deep transfer learning model.", "abstract": "Over the past few years, the widespread outbreak of COVID-19 has caused the death of millions of people worldwide. Early diagnosis of the virus is essential to control its spread and provide timely treatment. Artificial intelligence methods are often used as powerful tools to reach a COVID-19 diagnosis via computed tomography (CT) samples. In this paper, artificial intelligence-based methods are introduced to diagnose COVID-19. At first, a network called CT6-CNN is designed, and then two ensemble deep transfer learning models are developed based on Xception, ResNet-101, DenseNet-169, and CT6-CNN to reach a COVID-19 diagnosis by CT samples. The publicly available SARS-CoV-2 CT dataset is utilized for our implementation, including 2481 CT scans. The dataset is separated into 2108, 248, and 125 images for training, validation, and testing, respectively. Based on experimental results, the CT6-CNN model achieved 94.66% accuracy, 94.67% precision, 94.67% sensitivity, and 94.65% F1-score rate. Moreover, the ensemble learning models reached 99.2% accuracy. Experimental results affirm the effectiveness of designed models, especially the ensemble deep learning models, to reach a diagnosis of COVID-19.", "journal": "Acta tropica", "date": "2024-06-16", "authors": ["KolsoumYousefpanah", "M JEbadi", "SinaSabzekar", "Nor HidayatiZakaria", "Nurul AidaOsman", "AliAhmadian"], "doi": "10.1016/j.actatropica.2024.107277"}
{"title": "The STOIC2021 COVID-19 AI challenge: Applying reusable training methodologies to private data.", "abstract": "Challenges drive the state-of-the-art of automated medical image analysis. The quantity of public training data that they provide can limit the performance of their solutions. Public access to the training methodology for these solutions remains absent. This study implements the Type Three (T3) challenge format, which allows for training solutions on private data and guarantees reusable training methodologies. With T3, challenge organizers train a codebase provided by the participants on sequestered training data. T3 was implemented in the STOIC2021 challenge, with the goal of predicting from a computed tomography (CT) scan whether subjects had a severe COVID-19 infection, defined as intubation or death within one month. STOIC2021 consisted of a Qualification phase, where participants developed challenge solutions using 2000 publicly available CT scans, and a Final phase, where participants submitted their training methodologies with which solutions were trained on CT scans of 9724 subjects. The organizers successfully trained six of the eight Final phase submissions. The submitted codebases for training and running inference were released publicly. The winning solution obtained an area under the receiver operating characteristic curve for discerning between severe and non-severe COVID-19 of 0.815. The Final phase solutions of all finalists improved upon their Qualification phase solutions.", "journal": "Medical image analysis", "date": "2024-06-14", "authors": ["Luuk HBoulogne", "JulianLorenz", "DanielKienzle", "RobinSch\u00f6n", "KatjaLudwig", "RainerLienhart", "SimonJ\u00e9gou", "GuangLi", "CongChen", "QiWang", "DerikShi", "MayugManiparambil", "DominikM\u00fcller", "SilvanMertes", "NiklasSchr\u00f6ter", "FabioHellmann", "MiriamElia", "IneDirks", "Mat\u00edas Nicol\u00e1sBossa", "Abel D\u00edazBerenguer", "TanmoyMukherjee", "JefVandemeulebroucke", "HichemSahli", "NikosDeligiannis", "PanagiotisGonidakis", "Ngoc DungHuynh", "ImranRazzak", "RedaBouadjenek", "MarioVerdicchio", "PasqualeBorrelli", "MarcoAiello", "James AMeakin", "AlexanderLemm", "ChristophRuss", "RazvanIonasec", "NikosParagios", "Bramvan Ginneken", "Marie-PierreRevel"], "doi": "10.1016/j.media.2024.103230"}
{"title": "Learning From International Comparators of National Medical Imaging Initiatives for AI Development: Multiphase Qualitative Study.", "abstract": "The COVID-19 pandemic drove investment and research into medical imaging platforms to provide data to create artificial intelligence (AI) algorithms for the management of patients with COVID-19. Building on the success of England's National COVID-19 Chest Imaging Database, the national digital policy body (NHSX) sought to create a generalized national medical imaging platform for the development, validation, and deployment of algorithms.\nThis study aims to understand international use cases of medical imaging platforms for the development and implementation of algorithms to inform the creation of England's national imaging platform.\nThe National Health Service (NHS) AI Lab Policy and Strategy Team adopted a multiphased approach: (1) identification and prioritization of national AI imaging platforms; (2) Political, Economic, Social, Technological, Legal, and Environmental (PESTLE) factor analysis deep dive into national AI imaging platforms; (3) semistructured interviews with key stakeholders; (4) workshop on emerging themes and insights with the internal NHSX team; and (5) formulation of policy recommendations.\nInternational use cases of national AI imaging platforms (n=7) were prioritized for PESTLE factor analysis. Stakeholders (n=13) from the international use cases were interviewed. Themes (n=8) from the semistructured interviews, including interview quotes, were analyzed with workshop participants (n=5). The outputs of the deep dives, interviews, and workshop were synthesized thematically into 8 categories with 17 subcategories. On the basis of the insights from the international use cases, policy recommendations (n=12) were developed to support the NHS AI Lab in the design and development of the English national medical imaging platform.\nThe creation of AI algorithms supporting technology and infrastructure such as platforms often occurs in isolation within countries, let alone between countries. This novel policy research project sought to bridge the gap by learning from the challenges, successes, and experience of England's international counterparts. Policy recommendations based on international learnings focused on the demonstrable benefits of the platform to secure sustainable funding, validation of algorithms and infrastructure to support in situ deployment, and creating wraparound tools for nontechnical participants such as clinicians to engage with algorithm creation. As health care organizations increasingly adopt technological solutions, policy makers have a responsibility to ensure that initiatives are informed by learnings from both national and international initiatives as well as disseminating the outcomes of their work.", "journal": "JMIR AI", "date": "2024-06-14", "authors": ["KassandraKarpathakis", "EmmaPencheon", "DominicCushnan"], "doi": "10.2196/51168\n10.1177/20552076211048654\n10.1177/20552076211048654\n10.1177/1609406919899220\n10.1177/1609406919899220\n10.1007/s13218-020-00689-0\n10.1007/s13218-020-00689-0\n10.1136/bmj.l2349\n10.2196/31623\n10.2196/15816\n10.1177/2055207618770325?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.1177/2055207618770325\n10.1016/j.joclim.2021.100056"}
{"title": "Evaluating ChatGPT-4V in chest CT diagnostics: a critical image interpretation assessment.", "abstract": "To assess the diagnostic accuracy of ChatGPT-4V in interpreting a set of four chest CT slices for each case of COVID-19, non-small cell lung cancer (NSCLC), and control cases, thereby evaluating its potential as an AI tool in radiological diagnostics.\nIn this retrospective study, 60 CT scans from The Cancer Imaging Archive, covering COVID-19, NSCLC, and control cases were analyzed using ChatGPT-4V. A radiologist selected four CT slices from each scan for evaluation. ChatGPT-4V's interpretations were compared against the gold standard diagnoses and assessed by two radiologists. Statistical analyses focused on accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), along with an examination of the impact of pathology location and lobe involvement.\nChatGPT-4V showed an overall diagnostic accuracy of 56.76%. For NSCLC, sensitivity was 27.27% and specificity was 60.47%. In COVID-19 detection, sensitivity was 13.64% and specificity of 64.29%. For control cases, the sensitivity was 31.82%, with a specificity of 95.24%. The highest sensitivity (83.33%) was observed in cases involving all lung lobes. The chi-squared statistical analysis indicated significant differences in Sensitivity across categories and in relation to the location and lobar involvement of pathologies.\nChatGPT-4V demonstrated variable diagnostic performance in chest CT interpretation, with notable proficiency in specific scenarios. This underscores the challenges of cross-modal AI models like ChatGPT-4V in radiology, pointing toward significant areas for improvement to ensure dependability. The study emphasizes the importance of enhancing these models for broader, more reliable medical use.", "journal": "Japanese journal of radiology", "date": "2024-06-13", "authors": ["RezaDehdab", "AndreasBrendlin", "SebastianWerner", "HaidaraAlmansour", "SebastianGassenmaier", "Jan MichaelBrendel", "KonstantinNikolaou", "SaifAfat"], "doi": "10.1007/s11604-024-01606-3\n10.1007/S11547-006-0060-1\n10.1016/J.CRAD.2014.09.017\n10.1371/JOURNAL.PMED.1002686\n10.1001/JAMANETWORKOPEN.2019.1095\n10.1148/RADIOL.2018180237\n10.1145/3233231\n10.1038/S41746-021-00453-0\n10.1148/RADIOL.230725\n10.1148/RADIOL.231362\n10.1016/S2589-7500(23)00048-1\n10.1148/RADIOL.230987\n10.1148/RADIOL.230163\n10.1007/S10278-013-9622-7\n10.1148/RYAI.2020200029\n10.1038/SDATA.2018.202\n10.1148/RADIOL.10091808/-/DC1\n10.1148/RYAI.2020190043\n10.2214/AJR.07.2302\n10.1148/RADIOGRAPHICS.19.5.G99SE181303\n10.1148/RADIOL.2341040589\n10.1148/RADIOL.2383050167\n10.1186/S12916-021-01928-3\n10.1007/S10916-019-1180-1\n10.1148/RADIOL.222268\n10.1148/RADIOL.2020200905\n10.1371/JOURNAL.PONE.0253205\n10.2196/53559"}
{"title": "Reproducibility Analysis of Radiomic Features on T2-weighted MR Images after Processing and Segmentation Alterations in Neuroblastoma Tumors.", "abstract": "Purpose To evaluate the reproducibility of radiomics features extracted from T2-weighted MR images in patients with neuroblastoma. Materials and Methods A retrospective study included 419 patients (mean age, 29 months \u00b1 34 [SD]; 220 male, 199 female) with neuroblastic tumors diagnosed between 2002 and 2023, within the scope of the PRedictive In-silico Multiscale Analytics to support cancer personalized diaGnosis and prognosis, Empowered by imaging biomarkers (ie, PRIMAGE) project, involving 746 T2/T2*-weighted MRI sequences at diagnosis and/or after initial chemotherapy. Images underwent processing steps (denoising, inhomogeneity bias field correction, normalization, and resampling). Tumors were automatically segmented, and 107 shape, first-order, and second-order radiomics features were extracted, considered as the reference standard. Subsequently, the previous image processing settings were modified, and volumetric masks were applied. New radiomics features were extracted and compared with the reference standard. Reproducibility was assessed using the concordance correlation coefficient (CCC); intrasubject repeatability was measured using the coefficient of variation (CoV). Results When normalization was omitted, only 5% of the radiomics features demonstrated high reproducibility. Statistical analysis revealed significant changes in the normalization and resampling processes (", "journal": "Radiology. Artificial intelligence", "date": "2024-06-12", "authors": ["DianaVeiga-Canuto", "Mat\u00edasFern\u00e1ndez-Pat\u00f3n", "LeonorCerd\u00e0 Alberich", "AnaJim\u00e9nez Pastor", "ArmandoGomis Maya", "Jose MiguelCarot Sierra", "CintaSang\u00fcesa Nebot", "BlancaMart\u00ednez de Las Heras", "UlrikeP\u00f6tschger", "SabineTaschner-Mandl", "EmanueleNeri", "AdelaCa\u00f1ete", "RuthLadenstein", "BarbaraHero", "\u00c1ngelAlberich-Bayarri", "LuisMart\u00ed-Bonmat\u00ed"], "doi": "10.1148/ryai.230208"}
{"title": "Zero-shot denoising of microscopy images recorded at high-resolution limits.", "abstract": "Conventional and electron microscopy visualize structures in the micrometer to nanometer range, and such visualizations contribute decisively to our understanding of biological processes. Due to different factors in recording processes, microscopy images are subject to noise. Especially at their respective resolution limits, a high degree of noise can negatively effect both image interpretation by experts and further automated processing. However, the deteriorating effects of strong noise can be alleviated to a large extend by image enhancement algorithms. Because of the inherent high noise, a requirement for such algorithms is their applicability directly to noisy images or, in the extreme case, to just a single noisy image without a priori noise level information (referred to as blind zero-shot setting). This work investigates blind zero-shot algorithms for microscopy image denoising. The strategies for denoising applied by the investigated approaches include: filtering methods, recent feed-forward neural networks which were amended to be trainable on noisy images, and recent probabilistic generative models. As datasets we consider transmission electron microscopy images including images of SARS-CoV-2 viruses and fluorescence microscopy images. A natural goal of denoising algorithms is to simultaneously reduce noise while preserving the original image features, e.g., the sharpness of structures. However, in practice, a tradeoff between both aspects often has to be found. Our performance evaluations, therefore, focus not only on noise removal but set noise removal in relation to a metric which is instructive about sharpness. For all considered approaches, we numerically investigate their performance, report their denoising/sharpness tradeoff on different images, and discuss future developments. We observe that, depending on the data, the different algorithms can provide significant advantages or disadvantages in terms of their noise removal vs. sharpness preservation capabilities, which may be very relevant for different virological applications, e.g., virological analysis or image segmentation.", "journal": "PLoS computational biology", "date": "2024-06-10", "authors": ["SebastianSalwig", "JakobDrefs", "J\u00f6rgL\u00fccke"], "doi": "10.1371/journal.pcbi.1012192\n10.1126/science.abc1669\n10.1128/JVI.03167-15\n10.1016/S0140-6736(20)32079-1\n10.1016/j.ultramic.2014.01.010\n10.1016/j.jsb.2009.02.012\n10.1109/MSP.2006.1628875\n10.1038/srep20640\n10.1186/s42492-019-0016-7\n10.1109/TSMC.1979.4310076\n10.1109/TIP.2007.901238\n10.1109/TIP.2010.2056693\n10.1109/LSP.2016.2580600\n10.1364/BOE.413181\n10.1109/TIP.2017.2662206\n10.1109/TPAMI.2018.2873610\n10.1016/j.neunet.2019.08.022\n10.1109/TCYB.2017.2713421\n10.5201/ipol.2018.242\n10.1186/s42649-020-00041-8\n10.1038/s41467-020-18952-1\n10.1038/s42256-022-00547-8\n10.1109/TIP.2015.2499698\n10.1109/TIP.2018.2866691\n10.1371/journal.pone.0136964\n10.1038/s41598-020-65261-0\n10.1371/journal.pcbi.1010271\n10.1109/TIP.2011.2160072\n10.1038/s41598-021-82852-7\n10.1146/annurev-statistics-031017-100325\n10.1109/TMI.2021.3067512\n10.1016/j.biocel.2021.106077\n10.1109/TIP.2003.819861\n10.1109/MSP.2008.930649\n10.1038/s41598-022-20378-2\n10.3390/app12126227\n10.5281/zenodo.3985103\n10.5281/zenodo.3985110\n10.5281/zenodo.3986580\n10.5281/zenodo.11188503\n10.3389/fcomp.2020.00005\n10.5281/zenodo.5156913\n10.5281/zenodo.5156937\n10.5281/zenodo.5156960\n10.1016/j.micron.2011.07.009\n10.1038/381607a0\n10.1109/TIP.2006.881969\n10.1109/TIP.2007.911828\n10.1371/journal.pcbi.1003062\n10.3390/e23050552\n10.1109/TIP.2017.2651365\n10.1109/TIP.2003.819861\n10.1016/j.isprsjprs.2016.09.010"}
{"title": "Improved Latin hypercube sampling initialization-based whale optimization algorithm for COVID-19 X-ray multi-threshold image segmentation.", "abstract": "Image segmentation techniques play a vital role in aiding COVID-19 diagnosis. Multi-threshold image segmentation methods are favored for their computational simplicity and operational efficiency. Existing threshold selection techniques\u00a0in multi-threshold image segmentation, such as Kapur based on exhaustive enumeration, often hamper efficiency and accuracy. The whale optimization algorithm (WOA) has shown promise in addressing this challenge, but issues persist, including poor stability, low efficiency, and accuracy in COVID-19 threshold image segmentation. To tackle these issues, we introduce a Latin hypercube sampling initialization-based multi-strategy enhanced WOA (CAGWOA). It incorporates a COS sampling initialization strategy (COSI), an adaptive global search approach (GS), and an all-dimensional neighborhood mechanism (ADN). COSI leverages probability density functions created from Latin hypercube sampling, ensuring even solution space coverage to improve the stability of the segmentation model. GS widens the exploration scope to combat stagnation during iterations and improve segmentation efficiency. ADN refines convergence accuracy around optimal individuals to improve segmentation accuracy. CAGWOA's performance is validated through experiments on various benchmark function test sets. Furthermore, we apply CAGWOA alongside similar methods in a multi-threshold image segmentation model for comparative experiments on lung X-ray images of infected patients. The results demonstrate CAGWOA's superiority, including better image detail preservation, clear segmentation boundaries, and adaptability across different threshold levels.", "journal": "Scientific reports", "date": "2024-06-10", "authors": ["ZhenWang", "DongZhao", "Ali AsgharHeidari", "YiChen", "HuilingChen", "GuoxiLiang"], "doi": "10.1038/s41598-024-63739-9\n10.1016/S0079-6123(06)55002-2\n10.1016/S0031-3203(00)00149-7\n10.1145/2184442.2184444\n10.1109/TMI.2018.2845918\n10.1007/s42979-021-00823-1\n10.1016/j.jiph.2021.11.013\n10.1016/j.dsx.2020.05.008\n10.1016/j.jvcir.2019.03.004\n10.1016/j.trsl.2017.10.010\n10.4304/jsw.7.5.1074-1082\n10.1023/A:1008202821328\n10.1109/MCI.2006.329691\n10.1016/j.knosys.2015.07.006\n10.1016/j.knosys.2015.12.022\n10.1007/s42235-021-0050-y\n10.1016/j.eswa.2021.114864\n10.1016/j.future.2019.02.028\n10.1016/j.chemolab.2015.08.020\n10.1016/j.advengsoft.2013.12.007\n10.1016/j.eswa.2021.115079\n10.1016/j.advengsoft.2016.01.008\n10.1016/j.apm.2020.01.002\n10.1016/j.cie.2020.107086\n10.3390/sym13010048\n10.1007/s13349-020-00456-7\n10.1016/j.engappai.2019.103323\n10.1007/s40747-021-00294-0\n10.1016/j.eswa.2019.113018\n10.1109/4235.585893\n10.1016/j.proeng.2013.02.063\n10.1016/j.asoc.2020.106193\n10.1111/itor.13237\n10.1016/j.ress.2015.12.002\n10.1016/j.jspi.2011.09.016\n10.1109/TRPMS.2018.2890359\n10.3389/frsip.2022.826967\n10.1016/S0146-664X(81)80015-9\n10.1080/13682199.2023.2178094\n10.1007/s11277-021-09410-2\n10.2478/jaiscr-2024-0005\n10.1007/s41095-019-0151-2\n10.1109/TMI.2018.2791721\n10.1016/j.procs.2016.09.407\n10.1038/s41592-020-01008-z\n10.1016/j.imu.2020.100297\n10.1007/s11517-022-02637-6\n10.1016/j.eswa.2022.119206\n10.1155/2022/5677961\n10.1016/j.bspc.2021.102764\n10.1109/TITS.2019.2900385\n10.3390/su13031224\n10.1016/j.eswa.2007.09.003\n10.1109/TIP.2008.920761\n10.1016/j.measurement.2010.03.013\n10.1186/s13640-018-0322-6\n10.1016/j.procs.2017.12.021\n10.1007/s11432-012-4556-0\n10.1016/S0031-3203(02)00057-2\n10.1016/0734-189X(90)90161-N\n10.1007/s11263-009-0234-0\n10.1109/91.995125\n10.1109/97.991134\n10.1007/s11063-010-9149-6\n10.1002/mrm.1910370320\n10.26483/ijarcs.v8i5.4036\n10.1155/2009/140492\n10.1359/JBMR.040705\n10.1016/j.knosys.2020.106510\n10.1007/s00521-020-04820-y\n10.18178/ijmlc.2019.9.3.800\n10.1016/0734-189X(85)90125-2\n10.1080/00401706.1987.10488205\n10.1016/j.ins.2017.04.007\n10.1016/j.ins.2009.12.010\n10.1016/j.swevo.2011.02.002\n10.1016/j.eswa.2018.01.019\n10.1109/TEC.2017.2669518\n10.1016/j.measurement.2020.107901\n10.1002/tee.23326\n10.1371/journal.pone.0173516\n10.1109/ACCESS.2020.3005711\n10.1016/j.enconman.2018.05.062\n10.1016/j.eswa.2018.08.027\n10.1109/ACCESS.2017.2695498\n10.1007/s00500-021-06623-2\n10.1016/j.apm.2019.02.004\n10.1016/j.apm.2019.03.046\n10.1007/s00521-019-04015-0\n10.1007/s10489-018-1334-8\n10.1007/s00500-016-2307-7\n10.1109/TEVC.2005.857610\n10.1016/j.asoc.2017.09.039\n10.1049/el:20080522\n10.1109/TIP.2003.819861\n10.1109/TIP.2011.2109730"}
{"title": "Multi-centre benchmarking of deep learning models for COVID-19 detection in chest x-rays.", "abstract": "This study is a retrospective evaluation of the performance of deep learning models that were developed for the detection of COVID-19 from chest x-rays, undertaken with the goal of assessing the suitability of such systems as clinical decision support tools.\nModels were trained on the National COVID-19 Chest Imaging Database (NCCID), a UK-wide multi-centre dataset from 26 different NHS hospitals and evaluated on independent multi-national clinical datasets. The evaluation considers clinical and technical contributors to model error and potential model bias. Model predictions are examined for spurious feature correlations using techniques for explainable prediction.\nModels performed adequately on NHS populations, with performance comparable to radiologists, but generalised poorly to international populations. Models performed better in males than females, and performance varied across age groups. Alarmingly, models routinely failed when applied to complex clinical cases with confounding pathologies and when applied to radiologist defined \"mild\" cases.\nThis comprehensive benchmarking study examines the pitfalls in current practices that have led to impractical model development. Key findings highlight the need for clinician involvement at all stages of model development, from data curation and label definition, to model evaluation, to ensure that all clinical factors and disease features are appropriately considered during model design. This is imperative to ensure automated approaches developed for disease detection are fit-for-purpose in a clinical setting.", "journal": "Frontiers in radiology", "date": "2024-06-05", "authors": ["RachaelHarkness", "Alejandro FFrangi", "KieranZucker", "NishantRavikumar"], "doi": "10.3389/fradi.2024.1386906\n10.1371/journal.pone.0251661\n10.1136/bmj.m1808\n10.1017/S0950268823000249\n10.1038/s42256-021-00307-0\n10.1038/s42256-021-00338-7\n10.1097/RTI.0000000000000516\n10.1609/aaai.v33i01.3301590\n10.1016/j.cmpb.2020.105581\n10.7717/peerj-cs.551\n10.1038/s41598-020-76550-z\n10.1109/ICMLA51294.2020.00211\n10.1109/JBHI.2021.3058293\n10.1016/j.inffus.2022.09.023\n10.1016/j.patrec.2020.09.010\n10.3390/electronics10161996\n10.1109/JTEHM.2021.3134096\n10.1016/j.compbiomed.2021.105002\n10.1016/j.ejrad.2020.109272\n10.1186/s13244-022-01250-3\n10.1136/thoraxjnl-2017-211280\n10.1038/s41598-021-81930-0\n10.1148/ryai.210217\n10.1148/radiol.2020202602\n10.3390/math11061489\n10.1148/radiol.2020201365"}
{"title": "Worldwide research landscape of artificial intelligence in lung disease: A scientometric study.", "abstract": "To perform a comprehensive bibliometric analysis of the application of artificial intelligence (AI) in lung disease to understand the current status and emerging trends of this field.\nAI-based lung disease research publications were selected from the Web of Science Core Collection. Citespace, VOS viewer and Excel were used to analyze and visualize co-authorship, co-citation, and co-occurrence analysis of authors, keywords, countries/regions, references and institutions in this field.\nOur study included a total of 5210 papers. The number of publications on AI in lung disease showed explosive growth since 2017. China and the United States lead in publication numbers. The most productive author were Li, Weimin and Qian Wei, with Shanghai Jiaotong University as the most productive institution. Radiology was the most co-cited journal. Lung cancer and COVID-19 emerged as the most studied diseases. Deep learning, convolutional neural network, lung cancer, radiomics will be the focus of future research.\nAI-based diagnosis and treatment of lung disease has become a research hotspot in recent years, yielding significant results. Future work should focus on establishing multimodal AI models that incorporate clinical, imaging and laboratory information. Enhanced visualization of deep learning, AI-driven differential diagnosis model for lung disease and the creation of international large-scale lung disease databases should also be considered.", "journal": "Heliyon", "date": "2024-06-03", "authors": ["MengZeng", "XianQiWang", "WeiChen"], "doi": "10.1016/j.heliyon.2024.e31129\n10.1016/S0140-6736(16)31012-1\n10.3322/caac.21660\n10.3322/caac.21492\n10.1016/S0140-6736(19)30308-3\n10.1016/S1473-3099(18)30310-4\n10.1016/S0140-6736(22)00470-6\n10.1016/j.semcancer.2023.01.006\n10.1515/cclm-2022-0291\n10.1164/rccm.202112-2684OC\n10.1097/RLI.0000000000000974\n10.1007/s00330-023-09833-4\n10.1007/s40037-021-00695-4\n10.1161/CIRCULATIONAHA.114.015080\n10.1186/s12967-022-03615-0\n10.1016/j.phrs.2021.105645\n10.1001/jamanetworkopen.2019.18007\n10.1097/CORR.0000000000000727\n10.1148/radiol.12111976\n10.1148/radiol.2018180547\n10.1007/s00330-023-09772-0\n10.1016/j.ejrad.2019.108774\n10.3390/jimaging6120131\n10.32322/jhsm.1294551\n10.2217/fon-2020-0987\n10.1097/RTI.0000000000000251\n10.3389/fpubh.2022.943435\n10.1371/journal.pone.0223994\n10.1118/1.3528204\n10.1056/NEJMoa1102873\n10.1148/radiol.2015151169\n10.1016/j.media.2013.12.001\n10.1007/s10278-014-9718-8\n10.1109/TMI.2016.2536809\n10.1016/j.media.2017.06.015\n10.1109/CVPR.2016.90\n10.1016/j.jtcvs.2011.07.022\n10.1001/jama.2021.24287\n10.1186/s40779-020-00270-8\n10.3390/cancers14225569\n10.1016/j.media.2022.102444\n10.1126/science.178.4060.471\n10.1023/A:1012798911792\n10.1001/jama.287.21.2847\n10.1109/TMI.2018.2876510\n10.1007/s00330-019-06533-w\n10.3390/s19173722\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2016.90\n10.1109/TPAMI.2016.2577031\n10.1371/journal.pmed.1002683\n10.1109/TMI.2021.3060634\n10.1038/s41746-023-00811-0\n10.3322/caac.21552\n10.1158/0008-5472.CAN-17-0122\n10.1016/j.ijrobp.2014.07.020\n10.1002/mp.13455\n10.1001/jamanetworkopen.2019.1095\n10.1016/S2213-2600(18)30286-8\n10.1164/rccm.201705-0860OC\n10.3390/jimaging6120131\n10.1038/s41591-019-0447-x"}
{"title": "A deep learning-based model for detecting Leishmania amastigotes in microscopic slides: a new approach to telemedicine.", "abstract": "Leishmaniasis, an illness caused by protozoa, accounts for a substantial number of human fatalities globally, thereby emerging as one of the most fatal parasitic diseases. The conventional methods employed for detecting the Leishmania parasite through microscopy are not only time-consuming but also susceptible to errors. Therefore, the main objective of this study is to develop a model based on deep learning, a subfield of artificial intelligence, that could facilitate automated diagnosis of leishmaniasis.\nIn this research, we introduce LeishFuNet, a deep learning framework designed for detecting Leishmania parasites in microscopic images. To enhance the performance of our model through same-domain transfer learning, we initially train four distinct models: VGG19, ResNet50, MobileNetV2, and DenseNet 169 on a dataset related to another infectious disease, COVID-19. These trained models are then utilized as new pre-trained models and fine-tuned on a set of 292 self-collected high-resolution microscopic images, consisting of 138 positive cases and 154 negative cases. The final prediction is generated through the fusion of information analyzed by these pre-trained models. Grad-CAM, an explainable artificial intelligence technique, is implemented to demonstrate the model's interpretability.\nThe final results of utilizing our model for detecting amastigotes in microscopic images are as follows: accuracy of 98.95 1.4%, specificity of 98 2.67%, sensitivity of 100%, precision of 97.91 2.77%, F1-score of 98.92 1.43%, and Area Under Receiver Operating Characteristic Curve of 99 1.33.\nThe newly devised system is precise, swift, user-friendly, and economical, thus indicating the potential of deep learning as a substitute for the prevailing leishmanial diagnostic techniques.", "journal": "BMC infectious diseases", "date": "2024-06-02", "authors": ["AlirezaSadeghi", "MahdiehSadeghi", "MahdiFakhar", "ZakariaZakariaei", "MohammadrezaSadeghi", "RezaBastani"], "doi": "10.1186/s12879-024-09428-4\n10.1016/S2214-109X(22)00065-1\n10.3390/pathogens8030119\n10.1016/j.scitotenv.2021.148960\n10.1371/journal.pntd.0006255\n10.1016/S0140-6736(18)33057-5\n10.1128/spectrum.00679-22\n10.3389/fcimb.2021.625001\n10.1007/s00253-020-10846-y\n10.1016/j.ejphar.2021.174436\n10.1017/S0031182016002523\n10.1089/tmj.2019.0158\n10.1109/JBHI.2020.3034863\n10.1186/s12859-021-04036-4\n10.30699/fhi.v10i1.278\n10.1002/jbio.202100141\n10.1186/s12879-022-07029-7\n10.3390/cancers15154007\n10.1016/j.engappai.2023.107627\n10.1016/j.media.2021.101985\n10.1016/j.media.2020.101693\n10.1109/JPROC.2020.3004555\n10.3390/math10050844\n10.1007/s11263-015-0816-y\n10.1186/s40537-023-00727-2\n10.1038/s41597-021-00900-3\n10.1038/s41598-023-50742-9"}
{"title": "DKCNN: Improving deep kernel convolutional neural network-based COVID-19 identification from CT images of the chest.", "abstract": "An efficient deep convolutional neural network (DeepCNN) is proposed in this article for the classification of Covid-19 disease.\nA novel structure known as the Pointwise-Temporal-pointwise convolution unit is developed incorporated with the varying kernel-based depth wise temporal convolution before and after the pointwise convolution operations.\nThe outcome is optimized by the Slap Swarm algorithm (SSA). The proposed Deep CNN is composed of depth wise temporal convolution and end-to-end automatic detection of disease. First, the datasets SARS-COV-2 Ct-Scan Dataset and CT scan COVID Prediction dataset are preprocessed using the min-max approach and the features are extracted for further processing.\nThe experimental analysis is conducted between the proposed and some state-of-art works and stated that the proposed work effectively classifies the disease than the other approaches.\nThe proposed structural unit is used to design the deep CNN with the increasing kernel sizes. The classification process is improved by the inclusion of depth wise temporal convolutions along with the kernel variation. The computational complexity is reduced by the introduction of stride convolutions are used in the residual linkage among the adjacent structural units.", "journal": "Journal of X-ray science and technology", "date": "2024-05-31", "authors": ["TVaikunta Pai", "KMaithili", "RavulaArun Kumar", "DNagaraju", "DAnuradha", "ShailendraKumar", "AnandaRavuri", "TSunilkumar Reddy", "MSivaram", "R GVidhya"], "doi": "10.3233/XST-230424"}
{"title": "Deep learning reveals lung shape differences on baseline chest CT between mild and severe COVID-19: A multi-site retrospective study.", "abstract": "Severe COVID-19 can lead to extensive lung disease causing lung architectural distortion. In this study we employed machine learning and statistical atlas-based approaches to explore possible changes in lung shape among COVID-19 patients and evaluated whether the extent of these changes was associated with COVID-19 severity. On a large multi-institutional dataset (N\u00a0=\u00a03443), three different populations were defined; a) healthy (no COVID-19), b) mild COVID-19 (no ventilator required), c) severe COVID-19 (ventilator required), and the presence of lung shape differences between them were explored using baseline chest CT. Significant lung shape differences were observed along mediastinal surfaces of the lungs across all severity of COVID-19 disease. Additionally, differences were seen on basal surfaces of the lung when compared between healthy and severe COVID-19 patients. Finally, an AI model (a 3D residual convolutional network) characterizing these shape differences coupled with lung infiltrates (ground-glass opacities and consolidation regions) was found to be associated with COVID-19 severity.", "journal": "Computers in biology and medicine", "date": "2024-05-31", "authors": ["AmoghHiremath", "Vidya SankarViswanathan", "KaustavBera", "RakeshShiradkar", "LeiYuan", "KeithArmitage", "RobertGilkeson", "MengyaoJi", "PingfuFu", "AmitGupta", "ChengLu", "AnantMadabhushi"], "doi": "10.1016/j.compbiomed.2024.108643\n10.1016/j.jiph.2021.07.015\n10.1161/circ.144.suppl_1.14084\n10.1002/(SICI)1097-0193(1998)6:5/6<348::AID-HBM4>3.0.CO;2-P\n10.1016/j.ebiom.2020.103104\n10.1007/s11547-021-01370-8\n10.1016/j.acra.2022.03.023\n10.1259/bjr.20200634\n10.1007/s10278-013-9622-7\n10.5281/zenodo.4323059\n10.1016/j.lfs.2021.119341\n10.48550/arXiv.2010.11929\n10.1016/j.mri.2012.05.001\n10.1038/s41598-017-13443-8\n10.1109/JBHI.2021.3103389\n10.1016/j.dsx.2021.102212\n10.1148/radiol.2020200241\n10.1109/TMI.2009.2035616\n10.1038/s41467-020-20657-4\n10.1038/s41746-020-00369-1\n10.1002/ccr3.4656\n10.1109/TMI.2020.2974574\n10.1148/radiol.211670\n10.1136/thoraxjnl-2021-217031\n10.1109/JBHI.2020.3034296\n10.7937/TCIA.HMQ8-J677\n10.1056/NEJMoa1102873\n10.1038/s41598-020-73117-w\n10.48550/arXiv.1912.01703\n10.1038/s41598-022-05325-5\n10.1109/JBHI.2021.3132157\n10.1148/radiol.2021210384\n10.7937/TCIA.BBAG-2923\n10.1126/scitranslmed.abo5070\n10.1016/j.neuroimage.2006.12.036\n10.1007/s11263-019-01228-7\n10.1016/j.compbiomed.2021.104306\n10.22037/aaem.v9i1.1058\n10.1148/radiol.10091808\n10.3390/jcm10050975\n10.1016/j.cell.2022.06.005\n10.2139/ssrn.3878078\n10.1158/00085472.CAN-17-0339\n10.12659/MSM.928996\n10.1109/JBHI.2021.3076086\n10.1016/j.neuroimage.2014.01.060\n10.1016/j.neuroimage.2008.10.055\n10.6026/97320630016293\n10.1038/s41467-021-26513-3\n10.18632/aging.102999"}
{"title": "Surface plasmons-phonons for mid-infrared hyperspectral imaging.", "abstract": "Surface plasmons have proven their ability to boost the sensitivity of mid-infrared hyperspectral imaging by enhancing light-matter interactions. Surface phonons, a counterpart technology to plasmons, present unclear contributions to hyperspectral imaging. Here, we investigate this by developing a plasmon-phonon hyperspectral imaging system that uses asymmetric cross-shaped nanoantennas composed of stacked plasmon-phonon materials. The phonon modes within this system, controlled by light polarization, capture molecular refractive index intensity and lineshape features, distinct from those observed with plasmons, enabling more precise and sensitive molecule identification. In a deep learning-assisted imaging demonstration of severe acute respiratory syndrome coronavirus (SARS-CoV), phonons exhibit enhanced identification capabilities (230,400 spectra/s), facilitating the de-overlapping and observation of the spatial distribution of two mixed SARS-CoV spike proteins. In addition, the plasmon-phonon system demonstrates increased identification accuracy (93%), heightened sensitivity, and enhanced detection limits (down to molecule monolayers). These findings extend phonon polaritonics to hyperspectral imaging, promising applications in imaging-guided molecule screening and pharmaceutical analysis.", "journal": "Science advances", "date": "2024-05-29", "authors": ["HongZhou", "DongxiaoLi", "ZhihaoRen", "ChengXu", "Lin-FaWang", "ChengkuoLee"], "doi": "10.1126/sciadv.ado3179\n10.1016/b978-012544415-6.50015-7\n10.2210/pdb8h16/pdb\n10.2210/pdb7vhh/pdb"}
{"title": "Altered serum concentrations of IL-8, IL-32 and IL-10 in patients with lung impairment 6\u00a0months after COVID-19.", "abstract": "Post-COVID symptoms are reported in 10-35\u00a0% of patients not requiring hospitalization, and in up to 80\u00a0% of hospitalized patients and patients with severe disease. The pathogenesis of post-COVID syndrome remains largely unknown. Some evidence suggests that prolonged inflammation has a key role in the pathogenesis of most post-COVID manifestations. We evaluated a panel of inflammatory and immune-mediated cytokines in individuals with altered HRCT features and in patients without any long-term COVID symptoms. Blood samples of 89 adult patients previously hospitalized with COVID-19 were collected and stratified as patients with and without HRCT evidence of fibrotic lung alterations. Serum analyte concentrations of IL-4, IL-2, CXCL10 (IP-10), IL-1\u03b2, TNF-\u03b1, CCL2 (MCP-1), IL-17A, IL-6, IL-10, IFN-\u03b3, IL-12p70 and TGF-\u03b21 (free active form) were quantified by bead-based multiplex assay. Clinical and functional data were recorded in a database. With the use of machine learning approach, IL-32, IL-8, and IL-10 proved to be associated with the development of HRCT evidence of lung sequelae at follow-up. Direct comparison of cytokine levels in the two groups showed increased levels of IL-32 and decreased levels of IL-8 in patients with lung impairment. After further stratification of patients by severity (severe versus mild/moderate) during hospitalization, IL-10 emerged as the only cytokine showing decreased levels in severe patients. These findings contribute to a better understanding of the immune response and potential prognostic markers in patients with lung sequelae after COVID-19.", "journal": "Immunobiology", "date": "2024-05-29", "authors": ["LauraBergantini", "SaraGangi", "Mirianad'Alessandro", "PaoloCameli", "BeatricePerea", "MartinaMeocci", "GaiaFabbri", "FrancescoBianchi", "ElenaBargagli"], "doi": "10.1016/j.imbio.2024.152813"}
{"title": "Fog-based deep learning framework for real-time pandemic screening in smart cities from multi-site tomographies.", "abstract": "The quick proliferation of pandemic diseases has been imposing many concerns on the international health infrastructure. To combat pandemic diseases in smart cities, Artificial Intelligence of Things (AIoT) technology, based on the integration of artificial intelligence (AI) with the Internet of Things (IoT), is commonly used to promote efficient control and diagnosis during the outbreak, thereby minimizing possible losses. However, the presence of multi-source institutional data remains one of the major challenges hindering the practical usage of AIoT solutions for pandemic disease diagnosis. This paper presents a novel framework that utilizes multi-site data fusion to boost the accurateness of pandemic disease diagnosis. In particular, we focus on a case study of COVID-19 lesion segmentation, a crucial task for understanding disease progression and optimizing treatment strategies. In this study, we propose a novel multi-decoder segmentation network for efficient segmentation of infections from cross-domain CT scans in smart cities. The multi-decoder segmentation network leverages data from heterogeneous domains and utilizes strong learning representations to accurately segment infections. Performance evaluation of the multi-decoder segmentation network was conducted on three publicly accessible datasets, demonstrating robust results with an average dice score of 89.9% and an average surface dice of 86.87%. To address scalability and latency issues associated with centralized cloud systems, fog computing (FC) emerges as a viable solution. FC brings resources closer to the operator, offering low latency and energy-efficient data management and processing. In this context, we propose a unique FC technique called PANDFOG to deploy the multi-decoder segmentation network on edge nodes for practical and clinical applications of automated COVID-19 pneumonia analysis. The results of this study highlight the efficacy of the multi-decoder segmentation network in accurately segmenting infections from cross-domain CT scans. Moreover, the proposed PANDFOG system demonstrates the practical deployment of the multi-decoder segmentation network on edge nodes, providing real-time access to COVID-19 segmentation findings for improved patient monitoring and clinical decision-making.", "journal": "BMC medical imaging", "date": "2024-05-27", "authors": ["IbrahimAlrashdi"], "doi": "10.1186/s12880-024-01302-8\n10.1111/exsy.12838\n10.3390/s23094200\n10.3390/s23031639\n10.3390/su15031862\n10.3390/electronics12092050\n10.1109/TWC.2019.2946140\n10.3390/electronics12112490\n10.3390/healthcare8010046\n10.1049/iet-smc.2020.0044\n10.1016/j.scs.2020.102582\n10.3934/mbe.2021418\n10.1016/j.jbi.2021.103802\n10.1038/s41746-022-00653-2\n10.5858/arpa.2020-0786-SA\n10.1038/s42256-021-00337-8\n10.1016/j.scs.2021.102897\n10.3390/s21113838\n10.1109/TSC.2020.3004627\n10.1109/ACCESS.2020.3030090\n10.1109/TMI.2019.2952939\n10.1109/TMI.2018.2867261\n10.1016/j.jss.2019.04.050\n10.1109/TMI.2018.2845918\n10.1109/JIOT.2019.2923068\n10.1016/j.ins.2018.06.066\n10.1016/j.jpdc.2018.07.003\n10.1109/ACCESS.2017.2702013\n10.1109/TNSE.2019.2941754\n10.1109/TMI.2019.2918096"}
{"title": "Bayesian Networks in the Management of Hospital Admissions: A Comparison between Explainable AI and Black Box AI during the Pandemic.", "abstract": "Artificial Intelligence (AI) and Machine Learning (ML) approaches that could learn from large data sources have been identified as useful tools to support clinicians in their decisional process; AI and ML implementations have had a rapid acceleration during the recent COVID-19 pandemic. However, many ML classifiers are \"black box\" to the final user, since their underlying reasoning process is often obscure. Additionally, the performance of such models suffers from poor generalization ability in the presence of dataset shifts. Here, we present a comparison between an explainable-by-design (\"white box\") model (Bayesian Network (BN)) versus a black box model (Random Forest), both studied with the aim of supporting clinicians of Policlinico San Matteo University Hospital in Pavia (Italy) during the triage of COVID-19 patients. Our aim is to evaluate whether the BN predictive performances are comparable with those of a widely used but less explainable ML model such as Random Forest and to test the generalization ability of the ML models across different waves of the pandemic.", "journal": "Journal of imaging", "date": "2024-05-24", "authors": ["GiovannaNicora", "MicheleCatalano", "ChandraBortolotto", "Marina FrancescaAchilli", "GaiaMessana", "AntonioLo Tito", "AlessioConsonni", "SaraCutti", "FedericoComotto", "Giulia MariaStella", "AngeloCorsico", "StefanoPerlini", "RiccardoBellazzi", "RaffaeleBruno", "LorenzoPreda"], "doi": "10.3390/jimaging10050117\n10.1007/s12652-021-03612-z\n10.1016/j.matpr.2021.04.241\n10.14288/1.0427394\n10.3390/ijerph19158979\n10.3390/bioengineering10050613\n10.1016/j.gie.2020.06.040\n10.1007/s10796-021-10131-x\n10.3390/bdcc7010011\n10.1186/s12916-019-1426-2\n10.1016/j.patcog.2011.06.019\n10.1186/s12916-023-03212-y\n10.1016/j.ijmedinf.2022.104930\n10.1136/bmjhci-2022-100643\n10.1145/3236009\n10.1016/j.cjca.2021.09.004\n10.1145/3544548.3580682\n10.2759/346720\n10.48550/arXiv.1705.07874\n10.1016/j.artmed.2022.102471\n10.1002/widm.1312\n10.1145/3375627.3375830\n10.48550/arXiv.2103.04244\n10.1016/j.artmed.2022.102438\n10.1038/s42256-019-0048-x\n10.1016/j.eswa.2022.118348\n10.1007/978-3-030-45093-9_35\n10.3390/metabo12010005\n10.1007/s10115-016-0987-z\n10.48550/arXiv.1312.1121\n10.1016/j.ejro.2023.100497\n10.1016/j.ijar.2019.10.003\n10.1021/ci034160g\n10.1080/01431160412331269698\n10.1016/j.imu.2019.100180\n10.1186/s12864-019-6413-7\n10.1162/089976698300017197\n10.2307/2531595\n10.1136/bmjopen-2021-057725\n10.1016/j.eswa.2022.116547\n10.1145/3610218\n10.1016/j.ijhcs.2022.102922"}
{"title": "Comparing Visual and Software-Based Quantitative Assessment Scores of Lungs' Parenchymal Involvement Quantification in COVID-19 Patients.", "abstract": "(1) Background: Computed tomography (CT) plays a paramount role in the characterization and follow-up of COVID-19. Several score systems have been implemented to properly assess the lung parenchyma involved in patients suffering from SARS-CoV-2 infection, such as the visual quantitative assessment score (VQAS) and software-based quantitative assessment score (SBQAS) to help in managing patients with SARS-CoV-2 infection. This study aims to investigate and compare the diagnostic accuracy of the VQAS and SBQAS with two different types of software based on artificial intelligence (AI) in patients affected by SARS-CoV-2. (2) Methods: This is a retrospective study; a total of 90 patients were enrolled with the following criteria: patients' age more than 18 years old, positive test for COVID-19 and unenhanced chest CT scan obtained between March and June 2021. The VQAS was independently assessed, and the SBQAS was performed with two different artificial intelligence-driven software programs (Icolung and CT-COPD). The Intraclass Correlation Coefficient (ICC) statistical index and Bland-Altman Plot were employed. (3) Results: The agreement scores between radiologists (R1 and R2) for the VQAS of the lung parenchyma involved in the CT images were good (ICC = 0.871). The agreement score between the two software types for the SBQAS was moderate (ICC = 0.584). The accordance between Icolung and the median of the visual evaluations (Median R1-R2) was good (ICC = 0.885). The correspondence between CT-COPD and the median of the VQAS (Median R1-R2) was moderate (ICC = 0.622). (4) Conclusions: This study showed moderate and good agreement upon the VQAS and the SBQAS; enhancing this approach as a valuable tool to manage COVID-19 patients and the combination of AI tools with physician expertise can lead to the most accurate diagnosis and treatment plans for patients.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2024-05-24", "authors": ["MarcoNicol\u00f2", "AltinAdraman", "CamillaRisoli", "AnnaMenta", "FrancescoRenda", "MicheleTadiello", "SaraPalmieri", "MarcoLechiara", "DavideColombi", "LuigiGrazioli", "Matteo PioNatale", "MatteoScardino", "AndreaDemeco", "RubenForesti", "AttilioMontanari", "LucaBarbato", "MirkoSantarelli", "ChiaraMartini"], "doi": "10.3390/diagnostics14100985\n10.1007/s00330-020-06865-y\n10.7759/cureus.36821\n10.1007/s11604-021-01120-w\n10.1016/j.jcct.2020.08.013\n10.1148/radiol.2462070712\n10.1148/radiol.2020203453\n10.3390/life13040992\n10.3390/diagnostics12061501\n10.1007/s00330-015-3810-4\n10.1016/j.ejrad.2016.09.001\n10.1097/RTI.0000000000000516\n10.1007/s00330-021-08432-5\n10.1016/j.ejrad.2020.109344\n10.1007/s11547-022-01458-9\n10.1007/s00330-020-06817-6\n10.3390/diagnostics12071608\n10.1148/radiol.2020200370\n10.3390/jpm12060955\n10.5152/dir.2020.20407\n10.1007/s11548-021-02317-0\n10.3390/diagnostics12061482\n10.3390/diagnostics11010041\n10.1007/s00330-021-08409-4\n10.1148/radiol.2020201473"}
{"title": "Big data analysis for Covid-19 in hospital information systems.", "abstract": "The COVID-19 pandemic has triggered a global public health crisis, affecting hundreds of countries. With the increasing number of infected cases, developing automated COVID-19 identification tools based on CT images can effectively assist clinical diagnosis and reduce the tedious workload of image interpretation. To expand the dataset for machine learning methods, it is necessary to aggregate cases from different medical systems to learn robust and generalizable models. This paper proposes a novel deep learning joint framework that can effectively handle heterogeneous datasets with distribution discrepancies for accurate COVID-19 identification. We address the cross-site domain shift by redesigning the COVID-Net's network architecture and learning strategy, and independent feature normalization in latent space to improve prediction accuracy and learning efficiency. Additionally, we propose using a contrastive training objective to enhance the domain invariance of semantic embeddings and boost classification performance on each dataset. We develop and evaluate our method with two large-scale public COVID-19 diagnosis datasets containing CT images. Extensive experiments show that our method consistently improves the performance both datasets, outperforming the original COVID-Net trained on each dataset by 13.27% and 15.15% in AUC respectively, also exceeding existing state-of-the-art multi-site learning methods.", "journal": "PloS one", "date": "2024-05-22", "authors": ["XinpaYing", "HaiyangPeng", "JunXie"], "doi": "10.1371/journal.pone.0294481\n10.1038/s41591-020-0931-3\n10.1109/TMI.2020.2996645\n10.1016/j.neunet.2022.03.031\n10.1016/j.media.2020.101910\n10.1038/s41598-020-76550-z\n10.1109/TMI.2020.2974574\n10.1371/journal.pmed.1002683\n10.1109/RBME.2020.2987975\n10.1016/j.eswa.2023.120477\n10.1016/j.patcog.2023.109319\n10.1007/s10489-020-01829-7\n10.1007/s10044-021-00984-y\n10.3390/life12111709\n10.1016/j.pdpdt.2021.102473\n10.3390/s22072726\n10.1016/j.compbiomed.2021.104816\n10.1080/0952813X.2023.2165724\n10.1016/j.patrec.2020.09.010\n10.1016/j.imu.2020.100360\n10.1109/TPAMI.2015.2496141\n10.1016/j.patcog.2021.107826\n10.1016/j.patcog.2021.107848\n10.1109/TMI.2019.2963882\n10.1109/JBHI.2020.3023246"}
{"title": "COVID\u201119 detection from chest X-ray images using transfer learning.", "abstract": "COVID-19 is a kind of coronavirus that appeared in China in the Province of Wuhan in December 2019. The most significant influence of this virus is its very highly contagious characteristic which may lead to death. The standard diagnosis of COVID-19 is based on swabs from the throat and nose, their sensitivity is not high enough and so they are prone to errors. Early diagnosis of COVID-19 disease is important to provide the chance of quick isolation of the suspected cases and to decrease the opportunity of infection in healthy people. In this research, a framework for chest X-ray image classification tasks based on deep learning is proposed to help in early diagnosis of COVID-19. The proposed framework contains two phases which are the pre-processing phase and classification phase which uses pre-trained convolution neural network models based on transfer learning. In the pre-processing phase, different image enhancements have been applied to full and segmented X-ray images to improve the classification performance of the CNN models. Two CNN pre-trained models have been used for classification which are VGG19 and EfficientNetB0. From experimental results, the best model achieved a sensitivity of 0.96, specificity of 0.94, precision of 0.9412, F1 score of 0.9505 and accuracy of 0.95 using enhanced full X-ray images for binary classification of chest X-ray images into COVID-19 or normal with VGG19. The proposed framework is promising and achieved a classification accuracy of 0.935 for 4-class classification.", "journal": "Scientific reports", "date": "2024-05-22", "authors": ["Enas M FEl Houby"], "doi": "10.1038/s41598-024-61693-0\n10.1148/radiol.2020200527\n10.1016/j.eswa.2023.121300\n10.1016/j.bspc.2023.104724\n10.1016/j.bspc.2023.105380\n10.1016/j.bspc.2021.103076\n10.1148/radiol.2020201160\n10.1109/TPAMI.2016.2644615\n10.1007/s11263-015-0816-y\n10.1016/j.cell.2018.02.010\n10.1038/s41598-020-74539-2\n10.1016/j.media.2020.101794\n10.1016/j.bspc.2022.103977\n10.1016/j.chaos.2020.109944\n10.1016/j.eswa.2021.114883\n10.1007/s42979-022-01545-8\n10.1016/j.ijleo.2019.02.054\n10.1109/42.14513"}
{"title": "Corrigendum: Intelligent diagnosis of the severity of disease conditions in COVID-19 patients based on the LASSO method.", "abstract": "[This corrects the article DOI: 10.3389/fpubh.2024.1302256.].", "journal": "Frontiers in public health", "date": "2024-05-21", "authors": ["ZhuoJiang", "AixiangYang", "HaoChen", "YiqiuShi", "XiaojingLi"], "doi": "10.3389/fpubh.2024.1421217"}
{"title": "D-TrAttUnet: Toward hybrid CNN-transformer architecture for generic and subtle segmentation in medical images.", "abstract": "Over the past two decades, machine analysis of medical imaging has advanced rapidly, opening up significant potential for several important medical applications. As complicated diseases increase and the number of cases rises, the role of machine-based imaging analysis has become indispensable. It serves as both a tool and an assistant to medical experts, providing valuable insights and guidance. A particularly challenging task in this area is lesion segmentation, a task that is challenging even for experienced radiologists. The complexity of this task highlights the urgent need for robust machine learning approaches to support medical staff. In response, we present our novel solution: the D-TrAttUnet architecture. This framework is based on the observation that different diseases often target specific organs. Our architecture includes an encoder-decoder structure with a composite Transformer-CNN encoder and dual decoders. The encoder includes two paths: the Transformer path and the Encoders Fusion Module path. The Dual-Decoder configuration uses two identical decoders, each with attention gates. This allows the model to simultaneously segment lesions and organs and integrate their segmentation losses. To validate our approach, we performed evaluations on the Covid-19 and Bone Metastasis segmentation tasks. We also investigated the adaptability of the model by testing it without the second decoder in the segmentation of glands and nuclei. The results confirmed the superiority of our approach, especially in Covid-19 infections and the segmentation of bone metastases. In addition, the hybrid encoder showed exceptional performance in the segmentation of glands and nuclei, solidifying its role in modern medical image analysis.", "journal": "Computers in biology and medicine", "date": "2024-05-20", "authors": ["FaresBougourzi", "FadiDornaika", "CosimoDistante", "AbdelmalikTaleb-Ahmed"], "doi": "10.1016/j.compbiomed.2024.108590"}
{"title": "A retrospective prognostic evaluation using unsupervised learning in the treatment of COVID-19 patients with hypertension treated with ACEI/ARB drugs.", "abstract": "This study aimed to evaluate the prognosis of patients with COVID-19 and hypertension who were treated with angiotensin-converting enzyme inhibitor (ACEI)/angiotensin receptor B (ARB) drugs and to identify key features affecting patient prognosis using an unsupervised learning method.\nA large-scale clinical dataset, including patient information, medical history, and laboratory test results, was collected. Two hundred patients with COVID-19 and hypertension were included. After cluster analysis, patients were divided into good and poor prognosis groups. The unsupervised learning method was used to evaluate clinical characteristics and prognosis, and patients were divided into different prognosis groups. The improved wild dog optimization algorithm (IDOA) was used for feature selection and cluster analysis, followed by the IDOA-k-means algorithm. The impact of ACEI/ARB drugs on patient prognosis and key characteristics affecting patient prognosis were also analysed.\nKey features related to prognosis included baseline information and laboratory test results, while clinical symptoms and imaging results had low predictive power. The top six important features were age, hypertension grade, MuLBSTA, ACEI/ARB, NT-proBNP, and high-sensitivity troponin I. These features were consistent with the results of the unsupervised prediction model. A visualization system was developed based on these key features.\nUsing unsupervised learning and the improved k-means algorithm, this study accurately analysed the prognosis of patients with COVID-19 and hypertension. The use of ACEI/ARB drugs was found to be a protective factor for poor clinical prognosis. Unsupervised learning methods can be used to differentiate patient populations and assess treatment effects. This study identified important features affecting patient prognosis and developed a visualization system with clinical significance for prognosis assessment and treatment decision-making.", "journal": "PeerJ", "date": "2024-05-17", "authors": ["LiyeGe", "YongjunMeng", "WeinaMa", "JunyuMu"], "doi": "10.7717/peerj.17340\n10.1016/j.ejim.2022.06.015\n10.1186/s12874-021-01400-z\n10.1016/j.jhep.2020.09.031\n10.1016/S2213-2600(21)00214-9\n10.1093/ndt/gfv346\n10.1016/j.cardfail.2020.04.013\n10.1055/a-1661-0240\n10.1111/j.1742-1241.2009.02038.x\n10.1007/s00134-022-06808-9\n10.1007/s00059-006-2829-3\n10.1016/S2213-2600(20)30116-8\n10.1007/s40292-022-00506-9\n10.1016/j.clim.2020.108651\n10.1039/c1an15369b\n10.1109/TIP.2022.3211476\n10.1016/j.jiac.2020.10.013\n10.1016/j.jiac.2020.10.013\n10.1016/j.neunet.2022.11.019\n10.1038/s41440-020-0455-8\n10.1186/s12967-020-02520-8\n10.1002/clc.23836\n10.1109/TCBB.2021.3103777\n10.1024/0301-1526/a000971\n10.3390/genes14020421\n10.3758/s13428-020-01407-2\n10.1109/TNNLS.2022.3190420\n10.3389/fcvm.2021.577398\n10.1016/j.pec.2023.108073\n10.1109/TPAMI.2021.3126713\n10.3389/fimmu.2022.812940\n10.1016/j.ijsu.2020.04.001\n10.1007/s11886-020-01291-4\n10.1093/ajh/hpaa057\n10.1002/ehf2.13644\n10.1080/20013078.2018.1535750\n10.1186/s12929-020-00703-5\n10.1177/1470320320981321\n10.1016/j.yexmp.2019.104350\n10.1186/1471-2164-9-S2-S27\n10.1097/MD.0000000000035853\n10.1111/jcmm.17051"}
{"title": "Enhancing multi-class lung disease classification in chest x-ray images: A hybrid manta-ray foraging volcano eruption algorithm boosted multilayer perceptron neural network approach.", "abstract": "One of the most used diagnostic imaging techniques for identifying a variety of lung and bone-related conditions is the chest X-ray. Recent developments in deep learning have demonstrated several successful cases of illness diagnosis from chest X-rays. However, issues of stability and class imbalance still need to be resolved. Hence in this manuscript, multi-class lung disease classification in chest x-ray images using a hybrid manta-ray foraging volcano eruption algorithm boosted multilayer perceptron neural network approach is proposed (MPNN-Hyb-MRF-VEA). Initially, the input chest X-ray images are taken from the Covid-Chest X-ray dataset. Anisotropic diffusion Kuwahara filtering (ADKF) is used to enhance the quality of these images and lower noise. To capture significant discriminative features, the Term frequency-inverse document frequency (TF-IDF) based feature extraction method is utilized in this case. The Multilayer Perceptron Neural Network (MPNN) serves as the classification model for multi-class lung disorders classification as COVID-19, pneumonia, tuberculosis (TB), and normal. A Hybrid Manta-Ray Foraging and Volcano Eruption Algorithm (Hyb-MRF-VEA) is introduced to further optimize and fine-tune the MPNN's parameters. The Python platform is used to accurately evaluate the proposed methodology. The performance of the proposed method provides 23.21%, 12.09%, and 5.66% higher accuracy compared with existing methods like NFM, SVM, and CNN respectively.", "journal": "Network (Bristol, England)", "date": "2024-05-16", "authors": ["RajendranThavasimuthu", "SudheerHanumanthakari", "SridharSekar", "SakthivelKirubakaran"], "doi": "10.1080/0954898X.2024.2350579"}
{"title": "Investigating the impact of data heterogeneity on the performance of federated learning algorithm using medical imaging.", "abstract": "In recent years, Federated Learning (FL) has gained traction as a privacy-centric approach in medical imaging. This study explores the challenges posed by data heterogeneity on FL algorithms, using the COVIDx CXR-3 dataset as a case study. We contrast the performance of the Federated Averaging (FedAvg) algorithm on non-identically and independently distributed (non-IID) data against identically and independently distributed (IID) data. Our findings reveal a notable performance decline with increased data heterogeneity, emphasizing the need for innovative strategies to enhance FL in diverse environments. This research contributes to the practical implementation of FL, extending beyond theoretical concepts and addressing the nuances in medical imaging applications. This research uncovers the inherent challenges in FL due to data diversity. It sets the stage for future advancements in FL strategies to effectively manage data heterogeneity, especially in sensitive fields like healthcare.", "journal": "PloS one", "date": "2024-05-15", "authors": ["MuhammadBabar", "BasitQureshi", "AnisKoubaa"], "doi": "10.1371/journal.pone.0302539\n10.1109/JIOT.2023.3299573\n10.1145/3570953\n10.1016/j.jksuci.2022.07.006\n10.1016/j.jksuci.2021.05.016\n10.1145/3460427\n10.1016/j.inffus.2022.09.011\n10.1016/j.ins.2022.11.031\n10.1109/TPDS.2021.3098467\n10.1002/int.22777\n10.1111/exsy.13173\n10.1016/j.ymssp.2020.107077\n10.1186/s13174-018-0087-2\n10.1109/TWC.2019.2961673\n10.3390/su15065457\n10.1109/ACCESS.2023.3280422\n10.1109/ACCESS.2023.3281859\n10.1016/j.future.2023.02.021\n10.1007/s41666-020-00082-4\n10.1016/j.rineng.2022.100637\n10.1016/j.rineng.2022.100847\n10.1038/s41746-020-00323-1\n10.3390/app112311191\n10.1109/ACCESS.2022.3201876\n10.1038/s41591-021-01506-3\n10.1145/3412357\n10.1145/3533708\n10.1109/ACCESS.2022.3141913\n10.1145/3501813\n10.1016/j.neucom.2021.07.098"}
{"title": "Cardiac function in a large animal model of myocardial infarction at 7\u00a0T: deep learning based automatic segmentation increases reproducibility.", "abstract": "Cardiac magnetic resonance (CMR) imaging allows precise non-invasive quantification of cardiac function. It requires reliable image segmentation for myocardial tissue. Clinically used software usually offers automatic approaches for this step. These are, however, designed for segmentation of human images obtained at clinical field strengths. They reach their limits when applied to preclinical data and ultrahigh field strength (such as CMR of pigs at 7\u00a0T). In our study, eleven animals (seven with myocardial infarction) underwent four CMR scans each. Short-axis cine stacks were acquired and used for functional cardiac analysis. End-systolic and end-diastolic images were labelled manually by two observers and inter- and intra-observer variability were assessed. Aiming to make the functional analysis faster and more reproducible, an established deep learning (DL) model for myocardial segmentation in humans was re-trained using our preclinical 7\u00a0T data (n\u2009=\u2009772 images and labels). We then tested the model on n\u2009=\u2009288 images. Excellent agreement in parameters of cardiac function was found between manual and DL segmentation: For ejection fraction (EF) we achieved a Pearson's r of 0.95, an Intraclass correlation coefficient (ICC) of 0.97, and a Coefficient of variability (CoV) of 6.6%. Dice scores were 0.88 for the left ventricle and 0.84 for the myocardium.", "journal": "Scientific reports", "date": "2024-05-15", "authors": ["AlenaKollmann", "DavidLohr", "Markus JAnkenbrand", "MayaBille", "MaximTerekhov", "MichaelHock", "IbrahimElabyad", "SteffenBaltes", "TheresaReiter", "FlorianSchnitter", "Wolfgang RBauer", "UlrichHofmann", "Laura MSchreiber"], "doi": "10.1038/s41598-024-61417-4\n10.1161/CIR.0000000000001052\n10.1186/s12968-016-0225-6\n10.1093/eurheartj/ehw128\n10.1007/s11897-015-0261-9\n10.1161/CIRCULATIONAHA.108.811547\n10.1007/s00330-010-1888-2\n10.1371/journal.pone.0252797\n10.1007/s00330-010-1902-8\n10.1371/journal.pone.0148066\n10.1186/s12968-019-0532-9\n10.1038/s41598-022-07566-w\n10.1002/mrm.28822\n10.1038/s41598-020-59949-6\n10.1002/jmri.20125\n10.1186/s12968-020-00610-6\n10.3390/info11020108\n10.1186/s12968-018-0471-x\n10.1016/j.jcm.2016.02.012\n10.1371/journal.pone.0193746\n10.1186/1532-429X-14-43\n10.1007/s10554-009-9501-y\n10.1016/S0140-6736(86)90837-8\n10.1046/j.1469-7580.1998.19310105.x\n10.1007/s00330-005-2853-3\n10.1080/10976640500295516\n10.1002/jmri.1178\n10.1002/jmri.21662\n10.1038/s41598-021-90702-9\n10.1080/10976640701545073\n10.3390/app12083846\n10.1186/s13195-022-01043-2\n10.1161/JAHA.120.016612\n10.1186/s12968-020-00678-0\n10.1161/CIRCHEARTFAILURE.108.814459\n10.1177/0023677217705152"}
{"title": "Multi-strategy learning-based particle swarm optimization algorithm for COVID-19 threshold segmentation.", "abstract": "With advancements in science and technology, the depth of human research on COVID-19 is increasing, making the investigation of medical images a focal point. Image segmentation, a crucial step preceding image processing, holds significance in the realm of medical image analysis. Traditional threshold image segmentation proves to be less efficient, posing challenges in selecting an appropriate threshold value. In response to these issues, this paper introduces Inner-based multi-strategy particle swarm optimization (IPSOsono) for conducting numerical experiments and enhancing threshold image segmentation in COVID-19 medical images. A novel dynamic oscillatory weight, derived from the PSO variant for single-objective numerical optimization (PSOsono) is incorporated. Simultaneously, the historical optimal positions of individuals in the particle swarm undergo random updates, diminishing the likelihood of algorithm stagnation and local optima. Moreover, an inner selection learning mechanism is proposed in the update of optimal positions, dynamically refining the global optimal solution. In the CEC 2013 benchmark test, PSOsono demonstrates a certain advantage in optimization capability compared to algorithms proposed in recent years, proving the effectiveness and feasibility of PSOsono. In the Minimum Cross Entropy threshold segmentation experiments for COVID-19, PSOsono exhibits a more prominent segmentation capability compared to other algorithms, showing good generalization across 6\u00a0CT images and further validating the practicality of the algorithm.", "journal": "Computers in biology and medicine", "date": "2024-05-15", "authors": ["DonglinZhu", "JiayingShen", "YangyangZheng", "RuiLi", "ChangjunZhou", "ShiCheng", "YilinYao"], "doi": "10.1016/j.compbiomed.2024.108498"}
{"title": "Persistent fatigue in post-acute COVID syndrome is associated with altered T1 MRI texture in subcortical structures: a preliminary investigation.", "abstract": "Post-acute COVID syndrome (PACS) is a global health concern and is often associated with debilitating symptoms. Post-COVID fatigue is a particularly frequent and troubling issue, and its underlying mechanisms remain incompletely understood. One potential contributor is micropathological injury of subcortical and brainstem structures, as has been identified in other patient populations. Texture-based analysis (TA) may be used to measure such changes in anatomical MRI data. The present study develops a methodology of voxel-wise TA mapping in subcortical and brainstem regions, which is then applied to T1-weighted MRI data from a cohort of 48 individuals who had PACS (32 with and 16 without ongoing fatigue symptoms) and 15 controls who had cold and flu-like symptoms but tested negative for COVID-19. Both groups were assessed an average of 4-5 months post-infection. There were no significant differences between PACS and control groups, but significant differences were observed within the PACS groups, between those with and without fatigue symptoms. This included reduced texture energy and increased entropy, along with reduced texture correlation, cluster shade and profile in the putamen, pallidum, thalamus and brainstem. These findings provide new insights into the neurophysiological mechanisms that underlie PACS, with altered tissue texture as a potential biomarker of this debilitating condition.", "journal": "Behavioural brain research", "date": "2024-05-12", "authors": ["Nathan WChurchill", "EugenieRoudaia", "J JeanChen", "AllisonSekuler", "FuqiangGao", "MarioMasellis", "BenjaminLam", "IvyCheng", "ChrisHeyn", "Sandra EBlack", "Bradley JMacIntosh", "Simon JGraham", "Tom ASchweizer"], "doi": "10.1016/j.bbr.2024.115045"}
{"title": "Application of convolutional neural networks in medical images: a bibliometric analysis.", "abstract": "In the field of medical imaging, the rapid rise of convolutional neural networks (CNNs) has presented significant opportunities for conserving healthcare resources. However, with the wide spread application of CNNs, several challenges have emerged, such as enormous data annotation costs, difficulties in ensuring user privacy and security, weak model interpretability, and the consumption of substantial computational resources. The fundamental challenge lies in optimizing and seamlessly integrating CNN technology to enhance the precision and efficiency of medical diagnosis.\nThis study sought to provide a comprehensive bibliometric overview of current research on the application of CNNs in medical imaging. Initially, bibliometric methods were used to calculate the frequency statistics, and perform the cluster analysis and the co-citation analysis of countries, institutions, authors, keywords, and references. Subsequently, the latent Dirichlet allocation (LDA) method was employed for the topic modeling of the literature. Next, an in-depth analysis of the topics was conducted, and the topics in the medical field, technical aspects, and trends in topic evolution were summarized. Finally, by integrating the bibliometrics and LDA results, the developmental trajectory, milestones, and future directions in this field were outlined.\nA data set containing 6,310 articles in this field published from January 2013 to December 2023 was complied. With a total of 55,538 articles, the United States led in terms of the citation count, while in terms of the publication volume, China led with 2,385 articles. Harvard University emerged as the most influential institution, boasting an average of 69.92 citations per article. Within the realm of CNNs, residual neural network (ResNet) and U-Net stood out, receiving 1,602 and 1,419 citations, respectively, which highlights the significant attention these models have received. The impact of coronavirus disease 2019 (COVID-19) was unmistakable, as reflected by the publication of 597 articles, making it a focal point of research. Additionally, among various disease topics, with 290 articles, brain-related research was the most prevalent. Computed tomography (CT) imaging dominated the research landscape, representing 73% of the 30 different topics.\nOver the past 11 years, CNN-related research in medical imaging has grown exponentially. The findings of the present study provide insights into the field's status and research hotspots. In addition, this article meticulously chronicled the development of CNNs and highlighted key milestones, starting with LeNet in 1989, followed by a challenging 20-year exploration period, and culminating in the breakthrough moment with AlexNet in 2012. Finally, this article explored recent advancements in CNN technology, including semi-supervised learning, efficient learning, trustworthy artificial intelligence (AI), and federated learning methods, and also addressed challenges related to data annotation costs, diagnostic efficiency, model performance, and data privacy.", "journal": "Quantitative imaging in medicine and surgery", "date": "2024-05-09", "authors": ["HuixinJia", "JialiZhang", "KejunMa", "XiaoyanQiao", "LijieRen", "XinShi"], "doi": "10.21037/qims-23-1600\n10.1016/S1473-3099(14)70794-7\n10.1007/s10278-020-00320-6\n10.21037/qims-22-172\n10.1016/j.media.2018.12.006\n10.1007/s13246-020-00865-4\n10.1148/radiol.2018180958\n10.1186/s40537-021-00444-8\n10.3390/cancers13071590\n10.1001/jama.2016.17216\n10.1109/TMI.2016.2528162\n10.1109/TMI.2016.2538465\n10.1109/TMI.2020.2968472\n10.1016/j.compbiomed.2021.104319\n10.1016/j.media.2019.03.007\n10.1016/j.media.2021.102010\n10.1186/s40537-021-00492-0\n10.1016/j.media.2019.07.004\n10.1080/01612840.2019.1705944\n10.1186/s12859-017-1640-x\n10.1136/bmjopen-2016-011623\n10.3389/fimmu.2022.948314\n10.1007/s11192-009-0146-3\n10.1073/pnas.0307513100\n10.3389/fphar.2021.646626\n10.1186/s13677-022-00291-9\n10.3389/fonc.2021.687904\n10.1038/nature14539\n10.1016/j.media.2017.07.005\n10.1016/j.media.2016.10.004\n10.1007/978-3-319-10590-1_53\n10.1016/j.media.2016.05.004\n10.1109/TMI.2016.2535302\n10.1109/TPAMI.2016.2577031\n10.1038/s41598-018-22437-z\n10.1016/j.neuroimage.2019.116459\n10.1038/s41598-018-34817-6\n10.1016/j.media.2018.11.009\n10.1002/mp.13416\n10.1109/TMI.2016.2535865\n10.1016/j.cmpb.2020.105581\n10.21037/qims-21-1042\n10.1109/TMI.2019.2897538\n10.1155/2020/6265708\n10.1088/1361-6560/aa93d4\n10.1109/TPAMI.2018.2889052"}
{"title": "FACNN: fuzzy-based adaptive convolution neural network for classifying COVID-19 in noisy CXR images.", "abstract": "COVID-19 detection using chest X-rays (CXR) has evolved as a significant method for early diagnosis of the pandemic disease. Clinical trials and methods utilize X-ray images with computer and intelligent algorithms to improve detection and classification precision. This article thus proposes a fuzzy-based adaptive convolution neural network (FACNN) model to improve the detection precision by confining the false rates. The feature extraction process between the successive regions is validated using a fuzzy process that classifies labeled and unknown pixels. The membership functions are derived based on high precision features for detection and false rate suppression process. The convolution neural network process is responsible for increasing detection precision through recurrent training based on feature availability. This availability analysis is verified using fuzzy derivatives under local variances. Based on variance-reduced features, the appropriate regions with labeled and unknown features are used for normal or infected classification. Thus, the proposed FACNN improves accuracy, precision, and feature extraction by 14.36%, 8.74%, and 12.35%, respectively. This model reduces the false rate and extraction time by 10.35% and 10.66%, respectively.", "journal": "Medical & biological engineering & computing", "date": "2024-05-07", "authors": ["SuganyadeviS", "SeethalakshmiV"], "doi": "10.1007/s11517-024-03107-x\n10.1016/j.displa.2023.102370\n10.1016/j.jocs.2022.101763\n10.1109/ACCESS.2021.3061058\n10.1016/j.compbiomed.2022.106331\n10.1109/TFUZZ.2021.3097806\n10.1016/j.neucom.2022.01.055\n10.1016/j.eswa.2022.117812\n10.1007/s10278-022-00667-y\n10.1016/j.compbiomed.2022.106483\n10.1016/j.asoc.2020.106859\n10.1007/s42600-023-00302-x\n10.1007/s10723-022-09615-0\n10.1007/s12652-021-03306-6\n10.1007/s42979-022-01182-1\n10.1109/TCBB.2020.3009859\n10.1109/ACCESS.2020.3025010\n10.1109/JBHI.2022.3177854\n10.1016/j.compbiomed.2022.106065\n10.1016/j.bspc.2022.103530\n10.1016/j.compbiomed.2021.104585\n10.1016/j.neucom.2021.03.034\n10.1007/s13369-021-05958-0\n10.1007/s11042-022-13710-5\n10.1007/s11760-020-01820-2\n10.1007/s00521-021-06806-w\n10.1007/s00530-021-00826-1\n10.1007/s12652-021-03686-9\n10.1002/jemt.23713\n10.1002/cpe.7031\n10.3389/fpubh.2022.959667\n10.1016/j.engappai.2023.106030\n10.1109/ACCESS.2020.3041951\n10.1155/2022/2697303"}
{"title": "Contribution to pulmonary diseases diagnostic from X-ray images using innovative deep learning models.", "abstract": "Pulmonary disease identification and characterization are among the most intriguing research topics of recent years since they require an accurate and prompt diagnosis. Although pulmonary radiography has helped in lung disease diagnosis, the interpretation of the radiographic image has always been a major concern for doctors and radiologists to reduce diagnosis errors. Due to their success in image classification and segmentation tasks, cutting-edge artificial intelligence techniques like machine learning (ML) and deep learning (DL) are widely encouraged to be applied in the field of diagnosing lung disorders and identifying them using medical images, particularly radiographic ones. For this end, the researchers are concurring to build systems based on these techniques in particular deep learning ones. In this paper, we proposed three deep-learning models that were trained to identify the presence of certain lung diseases using thoracic radiography. The first model, named \"CovCXR-Net\", identifies the COVID-19 disease (two cases: COVID-19 or normal). The second model, named \"MDCXR3-Net\", identifies the COVID-19 and pneumonia diseases (three cases: COVID-19, pneumonia, or normal), and the last model, named \"MDCXR4-Net\", is destined to identify the COVID-19, pneumonia and the pulmonary opacity diseases (4 cases: COVID-19, pneumonia, pulmonary opacity or normal). These models have proven their superiority in comparison with the state-of-the-art models and reached an accuracy of 99,09\u00a0%, 97.74\u00a0%, and 90,37\u00a0% respectively with three benchmarks.", "journal": "Heliyon", "date": "2024-05-06", "authors": ["AkramBennour", "NajibBen Aoun", "Osamah IbrahimKhalaf", "FahadGhabban", "Wing-KeungWong", "SameerAlgburi"], "doi": "10.1016/j.heliyon.2024.e30308\n10.1136/thx.43.2.127\n10.1155/2022/8044887\n10.4018/IJSSMET.313175\n10.35940/ijitee.H9259.0610821\n10.1016/J.COMPBIOMED.2020.103792\n10.1038/s41598-020-76550-z\n10.1016/J.CMPB.2020.105581\n10.1016/j.compbiomed.2020.103869\n10.1016/J.CHAOS.2020.110245\n10.1109/TMI.2020.3040950\n10.1016/J.CHAOS.2020.110122\n10.1016/J.MEHY.2020.109761\n10.1371/journal.pone.0262052\n10.1007/s13246-020-00865-4\n10.3390/BDCC5040073\n10.2214/AJR.19.21512\n10.1016/J.PATCOG.2021.108243\n10.3390/s23010480\n10.1109/DASA53625.2021.9682248\n10.1109/ICARC54489.2022.9753811\n10.3991/ijoe.v18i07.30807\n10.1016/j.asoc.2022.109319\n10.1109/ICARC57651.2023.10145699\n10.1016/J.MEDIA.2020.101794\n10.1007/s42600-021-00151-6\n10.1016/j.cell.2018.02.010\n10.1016/j.cell.2018.02.010\n10.1016/J.COMPBIOMED.2021.104319\n10.1016/B978-0-12-824145-5.00005-8\n10.1016/j.compbiomed.2021.104348\n10.1007/s10489-020-02031-2\n10.1109/CVPR.2017.233"}
{"title": "Integration of Cine-cardiac Magnetic Resonance Radiomics and Machine Learning for Differentiating Ischemic and Dilated Cardiomyopathy.", "abstract": "This study aims to evaluate the capability of machine learning algorithms in utilizing radiomic features extracted from cine-cardiac magnetic resonance (CMR) sequences for differentiating between ischemic cardiomyopathy (ICM) and dilated cardiomyopathy (DCM).\nThis retrospective study included 115 cardiomyopathy patients subdivided into ICM (n\u00a0=\u00a064) and DCM cohorts (n\u00a0=\u00a051). We collected invasive clinical (IC), noninvasive clinical (NIC), and combined clinical (CC) feature subsets. Radiomic features were extracted from regions of interest (ROIs) in the left ventricle (LV), LV cavity (LVC), and myocardium (MYO). We tested 10 classical machine learning classifiers and validated them through fivefold cross-validation. We compared the efficacy of clinical feature-based models and radiomics-based models to identify the superior diagnostic approach.\nIn the validation set, the Gaussian naive Bayes (GNB) model outperformed the other models in all categories, with areas under the curve (AUCs) of 0.879 for IC_GNB, 0.906 for NIC_GNB, and 0.906 for CC_GNB. Among the radiomics models, the MYO_LASSOCV_MLP model demonstrated the highest AUC (0.919). In the test set, the MYO_RFECV_GNB radiomics model achieved the highest AUC (0.857), surpassing the performance of the three clinical feature models (IC_GNB: 0.732; NIC_GNB: 0.75; CC_GNB: 0.786).\nRadiomics models leveraging MYO images from cine-CMR exhibit promising potential for differentiating ICM from DCM, indicating the significant clinical application scope of such models.\nThe integration of radiomics models and machine learning methods utilizing cine-CMR sequences enhances the diagnostic capability to distinguish between ICM and DCM, minimizes examination risks for patients, and potentially reduces the duration of medical imaging procedures.", "journal": "Academic radiology", "date": "2024-05-05", "authors": ["JiaDeng", "LangtaoZhou", "YueyanLi", "YingYu", "JingjingZhang", "BihongLiao", "GuanghuaLuo", "JinweiTian", "HongZhou", "HuifangTang"], "doi": "10.1016/j.acra.2024.03.032"}
{"title": "ELRL-MD: a deep learning approach for myocarditis diagnosis using cardiac magnetic resonance images with ensemble and reinforcement learning integration.", "abstract": "", "journal": "Physiological measurement", "date": "2024-05-03", "authors": ["AdeleMirzaee Moghaddam Kasmaee", "AlirezaAtaei", "Seyed VahidMoravvej", "RoohallahAlizadehsani", "Juan MGorriz", "Yu-DongZhang", "Ru-SanTan", "U RajendraAcharya"], "doi": "10.1088/1361-6579/ad46e2"}
{"title": "COVID-19 Hierarchical Classification Using a Deep Learning Multi-Modal.", "abstract": "Coronavirus disease 2019 (COVID-19), originating in China, has rapidly spread worldwide. Physicians must examine infected patients and make timely decisions to isolate them. However, completing these processes is difficult due to limited time and availability of expert radiologists, as well as limitations of the reverse-transcription polymerase chain reaction (RT-PCR) method. Deep learning, a sophisticated machine learning technique, leverages radiological imaging modalities for disease diagnosis and image classification tasks. Previous research on COVID-19 classification has encountered several limitations, including binary classification methods, single-feature modalities, small public datasets, and reliance on CT diagnostic processes. Additionally, studies have often utilized a flat structure, disregarding the hierarchical structure of pneumonia classification. This study aims to overcome these limitations by identifying pneumonia caused by COVID-19, distinguishing it from other types of pneumonia and healthy lungs using chest X-ray (CXR) images and related tabular medical data, and demonstrate the value of incorporating tabular medical data in achieving more accurate diagnoses. Resnet-based and VGG-based pre-trained convolutional neural network (CNN) models were employed to extract features, which were then combined using early fusion for the classification of eight distinct classes. We leveraged the hierarchal structure of pneumonia classification within our approach to achieve improved classification outcomes. Since an imbalanced dataset is common in this field, a variety of versions of generative adversarial networks (GANs) were used to generate synthetic data. The proposed approach tested in our private datasets of 4523 patients achieved a macro-avg F1-score of 95.9% and an F1-score of 87.5% for COVID-19 identification using a Resnet-based structure. In conclusion, in this study, we were able to create an accurate deep learning multi-modal to diagnose COVID-19 and differentiate it from other kinds of pneumonia and normal lungs, which will enhance the radiological diagnostic process.", "journal": "Sensors (Basel, Switzerland)", "date": "2024-04-27", "authors": ["Albatoul SAlthenayan", "Shada AAlSalamah", "SherinAly", "ThamerNouh", "BassamMahboub", "LailaSalameh", "MetabAlkubeyyer", "AbdulrahmanMirza"], "doi": "10.3390/s24082641\n10.7759/cureus.7423\n10.1109/ACCESS.2020.3010287\n10.31729/jnma.5383\n10.1016/j.chaos.2020.109947\n10.3390/diagnostics10030165\n10.3390/jcm9041225\n10.1007/s11548-019-01917-1\n10.1016/j.clinimag.2020.04.001\n10.1007/s00330-019-06327-0\n10.1038/s42256-021-00307-0\n10.3390/app122010535\n10.1162/neco_a_01273\n10.1016/j.cmpb.2020.105532\n10.1016/j.jinf.2020.08.047\n10.1308/003588406X130615\n10.1111/j.1468-0394.1996.tb00182.x\n10.1016/j.artmed.2023.102587\n10.4103/0971-3026.95396\n10.1016/j.compbiomed.2021.104319\n10.1007/s11042-022-13486-8\n10.1016/j.cmpb.2015.12.006\n10.1109/COMPSAC.2016.144\n10.3390/app12147075\n10.4108/eetpht.9.4212\n10.32629/jai.v7i2.928\n10.1007/s10618-010-0175-9\n10.5121/ijdkp.2015.5201\n10.1080/08839514.2022.2055398\n10.1007/s00330-022-08588-8\n10.3390/sym12040651\n10.1371/journal.pone.0242301"}
{"title": "Predicting oxygen needs in COVID-19 patients using chest radiography multi-region radiomics.", "abstract": "The objective is to evaluate the performance of blood test results, radiomics, and a combination of the two data types on the prediction of the 24-h oxygenation support need for the Coronavirus disease 2019 (COVID-19) patients. In this retrospective cohort study, COVID-19 patients with confirmed real-time reverse transcription-polymerase chain reaction assay (RT-PCR) test results between February 2020 and August 2021 were investigated. Initial blood cell counts, chest radiograph, and the status of oxygenation support used within 24\u00a0h were collected (n\u2009=\u2009290; mean age, 45\u2009\u00b1\u200919\u00a0years; 125 men). Radiomics features from six lung zones were extracted. Logistic regression and random forest models were developed using the clinical-only, radiomics-only, and combined data. Ten repeats of fivefold cross-validation with bootstrapping were used to identify the input features and models with the highest area under the receiver operating characteristic curve (AUC). Higher AUCs were achieved when using only radiomics features compared to using only clinical features (0.94\u2009\u00b1\u20090.03 vs. 0.88\u2009\u00b1\u20090.04). The best combined model using both radiomics and clinical features achieved highest in the cross-validation (0.95\u2009\u00b1\u20090.02) and test sets (0.96\u2009\u00b1\u20090.02). In comparison, the best clinical-only model yielded AUCs of 0.88\u2009\u00b1\u20090.04 in cross-validation and 0.89\u2009\u00b1\u20090.03 in test set. Both radiomics and clinical data can be used to predict 24-h oxygenation support need for COVID-19 patients with AUC\u2009>\u20090.88. Moreover, the combination of both data types further improved the performance.", "journal": "Radiological physics and technology", "date": "2024-04-26", "authors": ["Sa-AngtipNetprasert", "SararasKhongwirotphan", "RoongpraiSeangsawang", "SupanuchPatipipittana", "WatsamonJantarabenjakul", "ThanyaweePuthanakit", "WariyaChintanapakdee", "SiraSriswasdi", "YothinRakvongthai"], "doi": "10.1007/s12194-024-00803-z\n10.1016/S2213-2600(20)30076-X\n10.1148/radiol.2020201365\n10.1016/j.mri.2012.06.010\n10.1038/nrclinonc.2017.141\n10.1148/radiol.2015151169\n10.1016/j.ejca.2011.11.036\n10.2967/jnumed.118.222893\n10.1001/jamaoncol.2016.2631\n10.1016/j.chemolab.2022.104750\n10.1002/mp.15582\n10.1016/j.compbiomed.2022.105467\n10.1111/crj.13604\n10.1038/s41598-023-34559-0\n10.1088/1361-6560/abf717\n10.1148/radiol.2020201754\n10.1056/NEJMoa2001191\n10.1016/S0140-6736(20)30633-4\n10.1007/s11547-020-01200-3\n10.1016/j.mri.2012.05.001\n10.1158/0008-5472.CAN-17-0339\n10.1038/ncomms5006\n10.1148/rg.2021210037\n10.3390/diagnostics11101812\n10.2147/JMDH.S322431"}
{"title": "Lung volume measurement using chest CT in COVID-19 patients: a cohort study in Japan.", "abstract": "This study aimed to investigate the utility of CT quantification of lung volume for predicting critical outcomes in COVID-19 patients.\nThis retrospective cohort study included 1200 hospitalised patients with COVID-19 from 4 hospitals. Lung fields were extracted using artificial intelligence-based segmentation, and the percentage of the predicted (%pred) total lung volume (TLC (%pred)) was calculated. The incidence of critical outcomes and posthospitalisation complications was compared between patients with low and high CT lung volumes classified based on the median percentage of predicted TLC\nThe incidence of critical outcomes was higher in the low TLC\nLower CT lung volume was associated with critical outcomes, posthospitalisation complications and slower improvement of clinical conditions in COVID-19 patients.", "journal": "BMJ open respiratory research", "date": "2024-04-26", "authors": ["ShiroOtake", "YusukeShiraishi", "ShotaroChubachi", "NaoyaTanabe", "TomokiMaetani", "TakanoriAsakura", "HoNamkoong", "TakashiShimada", "ShuheiAzekawa", "KensukeNakagawara", "HiromuTanaka", "TakahiroFukushima", "MayukoWatase", "HidekiTerai", "MamoruSasaki", "SoichiroUeda", "YukariKato", "NorihiroHarada", "ShojiSuzuki", "ShuichiYoshida", "HirokiTateno", "YoshitakeYamada", "MasahiroJinzaki", "ToyohiroHirai", "YukinoriOkada", "RyujiKoike", "MakotoIshii", "NaokiHasegawa", "AkinoriKimura", "SeiyaImoto", "SatoruMiyano", "SeishiOgawa", "TakanoriKanai", "KoichiFukunaga"], "doi": "10.1136/bmjresp-2023-002234\n10.1016/j.ijid.2021.11.009\n10.1056/NEJMoa2007764\n10.1056/NEJMoa2113017\n10.1016/j.ijid.2021.09.070\n10.1080/07853890.2022.2031274\n10.1016/j.cell.2020.04.045\n10.1007/s00330-020-07033-y\n10.1038/s41598-022-24157-x\n10.1183/16000617.0031-2021\n10.1159/000444418\n10.1007/s00330-021-08338-2\n10.1007/s00330-021-08482-9\n10.1016/j.rmed.2020.106239\n10.1155/2020/6175964\n10.5152/dir.2020.20451\n10.1007/s00330-020-07271-0\n10.1097/RTI.0000000000000572\n10.3389/fmed.2021.643917\n10.1152/ajpcell.00375.2021\n10.1148/radiol.220019\n10.1016/S2213-2600(21)00174-0\n10.1016/j.chest.2021.02.062\n10.1016/j.ejrad.2021.109676\n10.1186/s12931-023-02530-2\n10.1183/09041950.005s1693\n10.1136/bmj.m1985\n10.1186/s13054-020-02902-w\n10.1093/ehjqcco/qcab029\n10.1016/j.ijid.2022.07.014\n10.1186/s12879-022-07927-w\n10.1007/s00330-021-08432-5\n10.1148/radiol.2020200823\n10.1016/j.chest.2020.06.065\n10.1111/eci.13362\n10.1093/cvr/cvaa284\n10.1038/s41387-022-00217-z\n10.1136/bmjopen-2021-052777\n10.1053/j.ajkd.2021.07.003\n10.1007/s15010-021-01666-x\n10.1378/chest.129.3.558\n10.3389/fmed.2021.711435\n10.1118/1.4866889"}
{"title": "Artificial intelligence in the healthcare sector: comparison of deep learning networks using chest X-ray images.", "abstract": "Artificial intelligence has led to significant developments in the healthcare sector, as in other sectors and fields. In light of its significance, the present study delves into exploring deep learning, a branch of artificial intelligence.\nIn the study, deep learning networks ResNet101, AlexNet, GoogLeNet, and Xception were considered, and it was aimed to determine the success of these networks in disease diagnosis. For this purpose, a dataset of 1,680 chest X-ray images was utilized, consisting of cases of COVID-19, viral pneumonia, and individuals without these diseases. These images were obtained by employing a rotation method to generate replicated data, wherein a split of 70 and 30% was adopted for training and validation, respectively.\nThe analysis findings revealed that the deep learning networks were successful in classifying COVID-19, Viral Pneumonia, and Normal (disease-free) images. Moreover, an examination of the success levels revealed that the ResNet101 deep learning network was more successful than the others with a 96.32% success rate.\nIn the study, it was seen that deep learning can be used in disease diagnosis and can help experts in the relevant field, ultimately contributing to healthcare organizations and the practices of country managers.", "journal": "Frontiers in public health", "date": "2024-04-25", "authors": ["M AkifYenikaya", "G\u00f6khanKerse", "OnurOktaysoy"], "doi": "10.3389/fpubh.2024.1386110\n10.3389/fpubh.2024.1345808\n10.1177/1468018120966657\n10.26633/RPSP.2021.800\n10.14746/sr.2020.4.2.05\n10.26650/acin.850857\n10.1259/bjr.20190840\n10.1002/jmv.28462\n10.1016/j.techsoc.2019.101198\n10.1109/ACCESS.2021.3123472\n10.3389/frai.2020.578983\n10.23937/2378-3516/1410016\n10.1155/2009/675753\n10.1155/2021/5528144\n10.1007/s13755-020-00135-3\n10.3390/app8060981\n10.3390/math11061279\n10.5815/ijigsp.2023.01.04\n10.1007/s13246-020-00865-4\n10.3389/fpubh.2022.948205\n10.3390/biology10111174\n10.1007/s10799-019-00304-1\n10.1007/s10799-021-00336-6\n10.1038/s41524-022-00734-6\n10.31590/ejosat.573248\n10.3390/app122211457\n10.1007/s42979-021-00815-1\n10.1109/ACCESS.2022.3182659\n10.14569/IJACSA.2022.0130729\n10.1109/ACCESS.2021.3057654\n10.31590/ejosat.1011806\n10.32604/cmc.2021.016736\n10.31590/ejosat.780705\n10.1007/s00500-021-06143-z\n10.3390/sym12040651\n10.3390/ijms22147721\n10.16984/saufenbilder.903886\n10.3390/ijerph18084069\n10.3389/fradi.2022.810731\n10.3389/fonc.2023.1189370\n10.1016/j.gaceta.2020.12.019\n10.21037/jhmhp-20-126"}
{"title": "[Identifying Novel Coronavirus Pneumonia With CT Images: A Deep Learning Approach With Detail Upsampling and Attention Guidance].", "abstract": "To construct a deep learning-based target detection method to help radiologists perform rapid diagnosis of lesions in the CT images of patients with novel coronavirus pneumonia (NCP) by restoring detailed information and mining local information.\nWe present a deep learning approach that integrates detail upsampling and attention guidance. A linear upsampling algorithm based on bicubic interpolation algorithm was adopted to improve the restoration of detailed information within feature maps during the upsampling phase. Additionally, a visual attention mechanism based on vertical and horizontal spatial dimensions embedded in the feature extraction module to enhance the capability of the object detection algorithm to represent key information related to NCP lesions.\nExperimental results on the NCP dataset showed that the detection method based on the detail upsampling algorithm improved the recall rate by 1.07% compared with the baseline model, with the AP50 reaching 85.14%. After embedding the attention mechanism in the feature extraction module, 86.13% AP50, 73.92% recall, and 90.37% accuracy were achieved, which were better than those of the popular object detection models.\nThe feature information mining of CT images based on deep learning can further improve the lesion detection ability. The proposed approach helps radiologists rapidly identify NCP lesions on CT images and provides an important clinical basis for early intervention and high-intensity monitoring of NCP patients.\n\u901a\u8fc7\u5bf9\u7ec6\u8282\u4fe1\u606f\u7684\u6062\u590d\u5e76\u7ed3\u5408\u5c40\u90e8\u4fe1\u606f\u7684\u6316\u6398\uff0c\u6784\u5efa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u5e2e\u52a9\u653e\u5c04\u79d1\u533b\u751f\u5feb\u901f\u8bca\u65ad\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u611f\u67d3\u5bfc\u81f4\u7684\u80ba\u708e\uff08novel coronavirus pneumonia, NCP\uff09\u60a3\u8005CT\u56fe\u50cf\u4e2d\u7684\u75c5\u7076\u3002\n\u63d0\u51fa\u4e00\u79cd\u7ec6\u8282\u4e0a\u91c7\u6837\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e00\u79cd\u57fa\u4e8e\u4e09\u53cc\u7ebf\u63d2\u503c\u7684\u7ebf\u6027\u4e0a\u91c7\u6837\u7b97\u6cd5\u6765\u589e\u5f3a\u7279\u5f81\u56fe\u5728\u4e0a\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7ec6\u8282\u4fe1\u606f\u7684\u6062\u590d\u80fd\u529b\uff0c\u5e76\u5728\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u4e2d\u5d4c\u5165\u57fa\u4e8e\u5782\u76f4\u548c\u6c34\u5e73\u7a7a\u95f4\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u589e\u5f3a\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u5bf9NCP\u75c5\u7076\u5173\u952e\u4fe1\u606f\u7684\u8868\u5f81\u80fd\u529b\u3002\n\u5728NCP\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u57fa\u4e8e\u7ec6\u8282\u4e0a\u91c7\u6837\u7b97\u6cd5\u7684\u68c0\u6d4b\u65b9\u6cd5\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\u63d0\u5347\u4e861.07%\u7684\u53ec\u56de\u7387\uff0c\u8fbe\u5230\u4e8685.14%\u7684AP50\u3002\u5728\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u4e2d\u5d4c\u5165\u6ce8\u610f\u529b\u673a\u5236\u540e\u53d6\u5f97\u4e8686.13%\u7684AP50\u548c73.92 %\u7684\u53ec\u56de\u7387\u4ee5\u53ca90.37%\u7684\u7cbe\u786e\u7387\uff0c\u4f18\u4e8e\u6d41\u884c\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002\nCT\u56fe\u50cf\u4e2d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81\u4fe1\u606f\u6316\u6398\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u5347\u75c5\u7076\u68c0\u6d4b\u80fd\u529b\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u653e\u5c04\u79d1\u533b\u751f\u5feb\u901f\u68c0\u6d4bCT\u56fe\u50cf\u4e2d\u7684NCP\u75c5\u7076\uff0c\u4e3aNCP\u60a3\u8005\u7684\u65e9\u671f\u5e72\u9884\u548c\u9ad8\u5f3a\u5ea6\u76d1\u6d4b\u63d0\u4f9b\u91cd\u8981\u7684\u4e34\u5e8a\u4f9d\u636e\u3002", "journal": "Sichuan da xue xue bao. Yi xue ban = Journal of Sichuan University. Medical science edition", "date": "2024-04-22", "authors": ["JunrenChen", "RuiChen", "JiajunQiu", "JinYin", "LeiZhang"], "doi": "10.12182/20240360605\n10.1056/NEJMoa2001017\n10.1001/jamainternmed.2020.2033\n10.1002/ccr3.5684\n10.1016/j.cell.2020.04.045\n10.12182/20230960301\n10.12182/20230960301\n10.12182/20230960104\n10.12182/20230960104\n10.7507/1672-2531.202102036\n10.7507/1672-2531.202102036\n10.1109/TMI.2020.3000314\n10.1117/1.JMI.5.3.036501\n10.12182/20230960106\n10.12182/20230960106\n10.1109/TPAMI.2016.2577031\n10.1007/978-3-030-33128-3_4\n10.3390/diagnostics9030072\n10.1109/TMI.2020.3021254\n10.1109/TASSP.1981.1163711\n10.1007/S41095-022-0271-Y"}
{"title": "ERSegDiff: a diffusion-based model for edge reshaping in medical image segmentation.", "abstract": "Medical image segmentation is a crucial field of computer vision. Obtaining correct pathological areas can help clinicians analyze patient conditions more precisely. We have observed that both CNN-based and attention-based neural networks often produce rough segmentation results around the edges of the regions of interest. This significantly impacts the accuracy of obtaining the pathological areas. Without altering the original data and model architecture, further refining the initial segmentation outcomes can effectively address this issue and lead to more satisfactory results. Recently, diffusion models have demonstrated outstanding results in image generation, showcasing their powerful ability to model distributions. We believe that this ability can greatly enhance the accuracy of the reshaping results. This research proposes ERSegDiff, a neural network based on the diffusion model for reshaping segmentation borders. The diffusion model is trained to fit the distribution of the target edge area and is then used to modify the segmentation edge to produce more accurate segmentation results. By incorporating prior knowledge into the diffusion model, we can help it more accurately simulate the edge probability distribution of the samples. Moreover, we introduce the edge concern module, which leverages attention mechanisms to produce feature weights and further refine the segmentation outcomes. To validate our approach, we employed the COVID-19 and ISIC-2018 datasets for lung segmentation and skin cancer segmentation tasks, respectively. Compared with the baseline model, ERSegDiff improved the dice score by 3%-4% and 2%-4%, respectively, and achieved state-of-the-art scores compared to several mainstream neural networks, such as swinUNETR.", "journal": "Physics in medicine and biology", "date": "2024-04-19", "authors": ["BaijingChen", "JunxiaWang", "YuanjieZheng"], "doi": "10.1088/1361-6560/ad4080"}
{"title": "Stability of radiomic features from positron emission tomography images: a phantom study comparing advanced reconstruction algorithms and ordered subset expectation maximization.", "abstract": "In this study, we compared the repeatability and reproducibility of radiomic features obtained from positron emission tomography (PET) images according to the reconstruction algorithm used-advanced reconstruction algorithms, such as HYPER iterative (IT), HYPER deep learning reconstruction (DLR), and HYPER deep progressive reconstruction (DPR), or traditional Ordered Subset Expectation Maximization (OSEM)-to understand the potential variations and implications of using advanced reconstruction techniques in PET-based radiomics. We used a heterogeneous phantom with acrylic spherical beads (4- or 8-mm diameter) filled with ", "journal": "Physical and engineering sciences in medicine", "date": "2024-04-16", "authors": ["TakuroShiiba", "MasanoriWatanabe"], "doi": "10.1007/s13246-024-01416-x\n10.3390/cancers6041821\n10.1053/snuc.2000.7439\n10.1016/j.rcl.2004.09.007\n10.1136/jnnp.2003.028175\n10.1097/MNM.0000000000000320\n10.1007/s12149-021-01710-8\n10.1007/s00259-022-05746-4\n10.1109/TRPMS.2020.3014786\n10.1016/j.ejca.2011.11.036\n10.1148/radiol.2015151169\n10.18383/j.tom.2016.00208\n10.3390/tomography8020091\n10.1002/mp.13322\n10.2967/jnumed.111.099127\n10.1186/s13550-022-00945-4\n10.1186/s40658-023-00573-4\n10.1186/s40658-022-00431-9\n10.1088/1361-6560/abfb17\n10.1186/s40658-022-00508-5\n10.1186/s40658-022-00445-3\n10.1007/s00330-017-4859-z\n10.2466/pr0.1966.19.1.3\n10.1016/j.jcm.2016.02.012\n10.2307/2532051\n10.1186/s41824-022-00153-2\n10.1002/acm2.13967"}
{"title": "Multitask Adversarial Networks Based on Extensive Nonlinear Spiking Neuron Models.", "abstract": "Deep learning technology has been successfully used in Chest X-ray (CXR) images of COVID-19 patients. However, due to the characteristics of COVID-19 pneumonia and X-ray imaging, the deep learning methods still face many challenges, such as lower imaging quality, fewer training samples, complex radiological features and irregular shapes. To address these challenges, this study first introduces an extensive NSNP-like neuron model, and then proposes a multitask adversarial network architecture based on ENSNP-like neurons for chest X-ray images of COVID-19, called MAE-Net. The MAE-Net serves two tasks: (i) converting low-quality CXR images to high-quality images; (ii) classifying CXR images of COVID-19. The adversarial architecture of MAE-Net uses two generators and two discriminators, and two new loss functions have been introduced to guide the optimization of the network. The MAE-Net is tested on four benchmark COVID-19 CXR image datasets and compared them with eight deep learning models. The experimental results show that the proposed MAE-Net can enhance the conversion quality and the accuracy of image classification results.", "journal": "International journal of neural systems", "date": "2024-04-16", "authors": ["JunFu", "HongPeng", "BingLi", "ZhicaiLiu", "RikongLugu", "JunWang", "AntonioRam\u00edrez-de-Arellano"], "doi": "10.1142/S0129065724500321"}
{"title": "Beyond the Clinic Walls: Examining Radiology Technicians' Experiences in Home-Based Radiography.", "abstract": "In recent years, the landscape of diagnostic imaging has undergone a significant transformation with the emergence of home radiology, challenging the traditional paradigm. This shift, bringing diagnostic imaging directly to patients, has gained momentum and has been further accelerated by the global COVID-19 pandemic, highlighting the increasing importance and convenience of decentralized healthcare services. This study aims to offer a nuanced understanding of the attitudes and experiences influencing the integration of in-home radiography into contemporary healthcare practices. The research methodology involves a survey administered through Computer-Aided Web Interviewing (CAWI) tools, enabling real-time engagement with a diverse cohort of medical radiology technicians in the health domain. A second CAWI tool is submitted to experts to assess their feedback on the methodology. The survey explores key themes, including perceived advantages and challenges associated with domiciliary imaging, its impact on patient care, and the technological intricacies specific to conducting radiologic procedures outside the conventional clinical environment. Findings from a sample of 26 medical radiology technicians (drawn from a larger pool of 186 respondents) highlight a spectrum of opinions and constructive feedback. Enthusiasm is evident for the potential of domiciliary imaging to enhance patient convenience and provide a more patient-centric approach to healthcare. Simultaneously, this study suggests areas of intervention to improve the diffusion of home-based radiology. The methodology based on CAWI tools proves instrumental in the efficiency and depth of data collection, as evaluated by 16 experts from diverse professional backgrounds. The dynamic and responsive nature of this approach allows for a more allocated exploration of technicians' opinions, contributing to a comprehensive understanding of the evolving landscape of medical imaging services. Emphasis is placed on the need for national and international initiatives in the field, supported by scientific societies, to further explore the evolving landscape of teleradiology and the integration of artificial intelligence in radiology. This study encourages expansion involving other key figures in this practice, including, naturally, medical radiologists, general practitioners, medical physicists, and other stakeholders.", "journal": "Healthcare (Basel, Switzerland)", "date": "2024-04-13", "authors": ["GrazianoLepri", "FrancescoOddi", "Rosario AlfioGulino", "DanieleGiansanti"], "doi": "10.3390/healthcare12070732\n10.1186/s12913-020-05564-0\n10.1016/j.radi.2022.01.002\n10.1111/jep.13058\n10.1016/j.radi.2023.02.027\n10.1186/s12877-022-03212-2\n10.1016/S0009-9260(05)82966-6\n10.1136/bmjopen-2015-008050\n10.1186/s12913-017-2173-8\n10.1186/s13104-017-2420-4\n10.4274/dir.2022.221713\n10.1016/j.heliyon.2022.e12603\n10.1186/s12875-022-01799-4\n10.1089/tmj.2020.0108\n10.1016/j.ejrad.2020.109285\n10.1007/s11604-020-00941-5\n10.1016/j.ejrad.2019.108742\n10.1007/s11547-016-0640-7\n10.1186/s12875-016-0418-y\n10.1055/s-0033-1342936\n10.4082/kjfm.2011.32.6.341\n10.1007/s13244-011-0099-y\n10.1089/tmj.2010.0138\n10.1258/jtt.2010.090512\n10.3390/healthcare10030509\n10.3390/healthcare10010153\n10.3390/healthcare9030331\n10.2196/24221\n10.1177/14604582211011215\n10.1007/s00330-019-06486-0\n10.1093/jamia/ocaa292\n10.1007/s12553-021-00583-1\n10.1016/j.radi.2021.07.007\n10.1016/j.acra.2020.09.014\n10.3390/healthcare9070834\n10.1186/s13244-019-0798-3\n10.1186/1472-6947-12-44"}
{"title": "DDA-SSNets: Dual decoder attention-based semantic segmentation networks for COVID-19 infection segmentation and classification using chest X-Ray images.", "abstract": "COVID-19 needs to be diagnosed and staged to be treated accurately. However, prior studies' diagnostic and staging abilities for COVID-19 infection needed to be improved. Therefore, new deep learning-based approaches are required to aid radiologists in detecting and quantifying COVID-19-related lung infections.\nTo develop deep learning-based models to classify and quantify COVID-19-related lung infections.\nInitially, Dual Decoder Attention-based Semantic Segmentation Networks (DDA-SSNets) such as Dual Decoder Attention-UNet (DDA-UNet) and Dual Decoder Attention-SegNet (DDA-SegNet) are proposed to facilitate the dual segmentation tasks such as lung lobes and infection segmentation in chest X-ray (CXR) images. The lung lobe and infection segmentations are mapped to grade the severity of COVID-19 infection in both the lungs of CXRs. Later, a Genetic algorithm-based Deep Convolutional Neural Network classifier with the optimum number of layers, namely GADCNet, is proposed to classify the extracted regions of interest (ROI) from the CXR lung lobes into COVID-19 and non-COVID-19.\nThe DDA-SegNet shows better segmentation with an average BCSSDC of 99.53% and 99.97% for lung lobes and infection segmentations, respectively, compared with DDA-UNet with an average BCSSDC of 99.14% and 99.92%. The proposed DDA-SegNet with GADCNet classifier offered excellent classification results with an average BCCAC of 99.98%, followed by the GADCNet with DDA-UNet with an average BCCAC of 99.92% after extensive testing and analysis.\nThe results show that the proposed DDA-SegNet has superior performance in the segmentation of lung lobes and COVID-19-infected regions in CXRs, along with improved severity grading compared to the DDA-UNet and improved accuracy of the GADCNet classifier in classifying the CXRs into COVID-19, and non-COVID-19.", "journal": "Journal of X-ray science and technology", "date": "2024-04-12", "authors": ["AnandbabuGopatoti", "RamyaJayakumar", "PoornaiahBilla", "VijayalakshmiPatteeswaran"], "doi": "10.3233/XST-230421"}
{"title": "Intelligent diagnosis of the severity of disease conditions in COVID-19 patients based on the LASSO method.", "abstract": "The purpose of this study is to develop an intelligent diagnosis model based on the LASSO method to predict the severity of COVID-19 patients.\nThe study uses the clinical data of 500 COVID-19 patients from a designated hospital in Guangzhou, China, and selects eight features, including age, sex, dyspnea, comorbidity, complication, lymphocytes (LYM), CRP, and lung injury score, as the most important predictors of COVID-19 severity. The study applies the LASSO method to perform feature selection and regularization, and compares the LASSO method with other machine learning methods, such as ridge regression, support vector machine, and random forest.\nThe study finds that the ridge regression model has the best performance among the four models, with an AUROC of 0.92 in the internal validation and 0.91 in the external validation.\nThe study provides a simple, robust, and interpretable model for the intelligent diagnosis of COVID-19 severity, and a convenient and practical tool for the public and the health care workers to assess COVID-19 severity. However, the study also has some limitations and directions for future research, such as the need for more data from different sources and settings, and from prospective, longitudinal, multi-class classification models. The study hopes to contribute to the prevention and control of COVID-19, and to the improvement of the diagnosis and treatment of COVID-19 patients.", "journal": "Frontiers in public health", "date": "2024-04-12", "authors": ["ZhuoJiang", "AixiangYang", "HaoChen", "YiqiuShi", "XiaojingLi"], "doi": "10.3389/fpubh.2024.1302256\n10.1056/NEJMoa2001017\n10.1016/S0140-6736(20)30183-5\n10.1111/j.2517-6161.1996.tb02080.x\n10.1016/j.retram.2020.01.002\n10.1111/j.1467-9868.2005.00503.x\n10.1080/00401706.1970.10488634\n10.1007/BF00994018\n10.1023/A:1010933404324\n10.1001/jama.2020.2648\n10.1016/j.jinf.2020.03.005\n10.3389/fpubh.2020.00152\n10.1136/bmj.m1091\n10.1183/13993003.00547-2020\n10.1016/S0140-6736(20)30566-3\n10.1038/s41392-020-0148-4\n10.1016/j.jcv.2020.104370\n10.1016/j.jinf.2020.04.002\n10.1016/j.jaci.2020.05.008\n10.1148/ryct.2020200047\n10.1148/radiol.2020200843\n10.1016/j.jco.2009.01.002"}
{"title": "Segmentation and classification of lungs CT-scan for detecting COVID-19 abnormalities by deep learning technique: U-Net model.", "abstract": "Artificial intelligence (AI) techniques have been ascertained useful in the analysis and description of infectious areas in radiological images promptly. Our aim in this study was to design a web-based application for detecting and labeling infected tissues on CT (computed tomography) lung images of patients based on the deep learning (DL) method as a type of AI.\nThe U-Net architecture, one of the DL networks, is used as a hybrid model with pre-trained densely connected convolutional network 121 (DenseNet121) architecture for the segmentation process. The proposed model was constructed on 1031 persons' CT-scan images from Ibn Sina Hospital of Iran in 2021 and some publicly available datasets. The network was trained using 6000 slices, validated on 1000 slices images, and tested against the 150 slices. Accuracy, sensitivity, specificity, and area under the receiver operating characteristics (ROC) curve (AUC) were calculated to evaluate model performance.\nThe results indicate the acceptable ability of the U-Net-DenseNet121 model in detecting COVID-19 abnormality (accuracy = 0.88 and AUC = 0.96 for thresholds of 0.13 and accuracy = 0.88 and AUC = 0.90 for thresholds of 0.2). Based on this model, we developed the \"Imaging-Tech\" web-based application for use at hospitals and clinics to make our project's output more practical and attractive in the market.\nWe designed a DL-based model for the segmentation of COVID-19 CT scan images and, based on this model, constructed a web-based application that, according to the results, is a reliable detector for infected tissue in lung CT-scans. The availability of such tools would aid in automating, prioritizing, fastening, and broadening the treatment of COVID-19 patients globally.", "journal": "Journal of family medicine and primary care", "date": "2024-04-12", "authors": ["Abdoulreza SMoosavi", "AshrafMahboobi", "FarzinArabzadeh", "NazaninRamezani", "Helia SMoosavi", "GolbargMehrpoor"], "doi": "10.4103/jfmpc.jfmpc_695_23"}
{"title": "An adult and pediatric size-based contrast administration reduction phantom study for single and dual-energy CT through preservation of contrast-to-noise ratio.", "abstract": "Global shortages of iodinated contrast media (ICM) during COVID-19 pandemic forced the imaging community to use ICM more strategically in CT exams.\nThe purpose of this work is to provide a quantitative framework for preserving iodine CNR while reducing ICM dosage by either lowering kV in single-energy CT (SECT) or using lower energy virtual monochromatic images (VMI) from dual-energy CT (DECT) in a phantom study.\nIn SECT study, phantoms with effective diameters of 9.7, 15.9, 21.1, and 28.5\u00a0cm were scanned on SECT scanners of two different manufacturers at a range of tube voltages. Statistical based iterative reconstruction and deep learning reconstruction were used. In DECT study, phantoms with effective diameters of 20, 29.5, 34.6, and 39.7\u00a0cm were scanned on DECT scanners from three different manufacturers. VMIs were created from 40 to 140\u00a0keV. ICM reduction by lowering kV levels for SECT or switching from SECT to DECT was calculated based on the linear relationship between iodine CNR and its concentration under different scanning conditions.\nOn SECT scanner A, while matching CNR at 120\u00a0kV, ICM reductions of 21%, 58%, and 72% were achieved at 100, 80, and 70\u00a0kV, respectively. On SECT scanner B, 27% and 80% ICM reduction was obtained at 80 and 100\u00a0kV. On the Fast-kV switch DECT, with CNR matched at 120\u00a0kV, ICM reductions were 35%, 30%, 23%, and 15% with VMIs at 40, 50, 60, and 68\u00a0keV, respectively. On the dual-source DECT, ICM reductions were 52%, 48%, 42%, 33%, and 22% with VMIs at 40, 50, 60, 70, and 80\u00a0keV. On the dual-layer DECT, ICM reductions were 74%, 62%, 45%, and 22% with VMIs at 40, 50, 60, and 70\u00a0keV.\nOur work provided a quantitative baseline for other institutions to further optimize their scanning protocols to reduce the use of ICM.", "journal": "Journal of applied clinical medical physics", "date": "2024-04-12", "authors": ["JiaWang", "XinhuiDuan", "UsmanMahmood", "Sarah EvaMcKenney", "Samuel LorenBrady"], "doi": "10.1002/acm2.14340\n10.1016/j.jvir.2022.05.011"}
{"title": "COVID-19 diagnostic prediction on chest CT scan images using hybrid quantum-classical convolutional neural network.", "abstract": "Notwithstanding the extensive research efforts directed towards devising a dependable approach for the diagnosis of coronavirus disease 2019 (COVID-19), the inherent complexity and capriciousness of the virus continue to pose a formidable challenge to the precise identification of affected individuals. In light of this predicament, it is essential to devise a model for COVID-19 prediction utilizing chest computed tomography (CT) scans. To this end, we present a hybrid quantum-classical convolutional neural network (HQCNN) model, which is founded on stochastic quantum circuits that can discern COVID-19 patients from chest CT images. Two publicly available chest CT image datasets were employed to evaluate the performance of our model. The experimental outcomes evinced diagnostic accuracies of 99.39% and 97.91%, along with precisions of 99.19% and 98.52%, respectively. These findings are indicative of the fact that the proposed model surpasses recently published works in terms of performance, thus providing a superior ability to precisely predict COVID-19 positive instances.Communicated by Ramaswamy H. Sarma.", "journal": "Journal of biomolecular structure & dynamics", "date": "2024-04-11", "authors": ["HaorongZhao", "XingDeng", "HaijianShao", "YingtaoJiang"], "doi": "10.1080/07391102.2023.2226215"}
{"title": "CovMediScanX: A medical imaging solution for COVID-19 diagnosis from chest X-ray images.", "abstract": "Radiologists have extensively employed the interpretation of chest X-rays (CXR) to identify visual markers indicative of COVID-19 infection, offering an alternative approach for the screening of infected individuals. This research article presents CovMediScanX, a deep learning-based framework designed for a rapid and automated diagnosis of COVID-19 from CXR scan images.\nThe proposed approach encompasses gathering and preprocessing CXR image datasets, training deep learning-based custom-made Convolutional Neural Network (CNN), pre-trained and hybrid transfer learning models, identifying the highest-performing model based on key evaluation metrics, and embedding this model into a web interface called CovMediScanX, designed for radiologists to detect the COVID-19 status in new CXR images.\nThe custom-made CNN model obtained a remarkable testing accuracy of 94.32% outperforming other models. CovMediScanX, employing the custom-made CNN underwent evaluation with an independent dataset also. The images in the independent dataset are sourced from a scanning machine that is entirely different from those used for the training dataset, highlighting a clear distinction of datasets in their origins. The evaluation outcome highlighted the framework's capability to accurately detect COVID-19 cases, showcasing encouraging results with a precision of 73% and a recall of 84% for positive cases. However, the model requires further enhancement, particularly in improving its detection of normal cases, as evidenced by lower precision and recall rates.\nThe research proposes CovMediScanX framework that demonstrates promising potential in automatically identifying COVID-19 cases from CXR images. While the model's overall performance on independent data needs improvement, it is evident that addressing bias through the inclusion of diverse data sources during training could further enhance accuracy and reliability.", "journal": "Journal of medical imaging and radiation sciences", "date": "2024-04-10", "authors": ["Smitha Sunil KumaranNair", "Leena RDavid", "AbdulwahidShariff", "Saqar AlMaskari", "Adhra AlMawali", "SammyWeis", "TahaFouad", "Dilber UzunOzsahin", "AishaAlshuweihi", "AbdulmunhemObaideen", "WiamElshami"], "doi": "10.1016/j.jmir.2024.03.046"}
{"title": "Patient-led skin cancer teledermatology without dermoscopy during the COVID-19 pandemic: important lessons for the development of future patient-facing teledermatology and artificial intelligence-assisted -self-diagnosis.", "abstract": "MySkinSelfie is a mobile phone application for skin self-monitoring, enabling secure sharing of patient-captured images with healthcare providers. This retrospective study assessed MySkinSelfie's role in remote skin cancer assessment at two centres for urgent (melanoma and squamous cell carcinoma) and nonurgent skin cancer referrals, investigating the feasibility of using patient-captured images without dermoscopy for remote diagnosis. The total number of lesions using MySkinSelfie was 814, with a mean patient age of 63\u2005years. Remote consultations reduced face-to-face appointments by 90% for basal cell carcinoma and by 63% for referrals on a 2-week waiting list. Diagnostic concordance (consultant vs. histological diagnosis) rates of 72% and 83% were observed for basal cell carcinoma (n = 107) and urgent skin cancers (n = 704), respectively. Challenges included image quality, workflow integration and lack of dermoscopy. Higher sensitivities were observed in recent artificial intelligence algorithms employing dermoscopy. While patient-captured images proved useful during the COVID-19 pandemic, further research is needed to explore the feasibility of widespread patient-led dermoscopy to enable direct patient-to-artificial intelligence diagnostic assessment.", "journal": "Clinical and experimental dermatology", "date": "2024-04-09", "authors": ["Omar M EAli", "BethWright", "CharlotteGoodhead", "Philip JHampton"], "doi": "10.1093/ced/llae126"}
{"title": "Lung pneumonia severity scoring in chest X-ray images using transformers.", "abstract": "To create robust and adaptable methods for lung pneumonia diagnosis and the assessment of its severity using chest X-rays (CXR), access to well-curated, extensive datasets is crucial. Many current severity quantification approaches require resource-intensive training for optimal results. Healthcare practitioners require efficient computational tools to swiftly identify COVID-19 cases and predict the severity of the condition. In this research, we introduce a novel image augmentation scheme as well as a neural network model founded on Vision Transformers (ViT) with a small number of trainable parameters for quantifying COVID-19 severity and other lung diseases. Our method, named Vision Transformer Regressor Infection Prediction (ViTReg-IP), leverages a ViT architecture and a regression head. To assess the model's adaptability, we evaluate its performance on diverse chest radiograph datasets from various open sources. We conduct a comparative analysis against several competing deep learning methods. Our results achieved a minimum Mean Absolute Error (MAE) of 0.569 and 0.512 and a maximum Pearson Correlation Coefficient (PC) of 0.923 and 0.855 for the geographic extent score and the lung opacity score, respectively, when the CXRs from the RALO dataset were used in training. The experimental results reveal that our model delivers exceptional performance in severity quantification while maintaining robust generalizability, all with relatively modest computational requirements. The source codes used in our work are publicly available at https://github.com/bouthainas/ViTReg-IP .", "journal": "Medical & biological engineering & computing", "date": "2024-04-09", "authors": ["BouthainaSlika", "FadiDornaika", "HamidMerdji", "KarimHammoudi"], "doi": "10.1007/s11517-024-03066-3\n10.3348/kjr.2020.0132\n10.1056/NEJMoa2002032\n10.1016/j.clinimag.2020.04.001\n10.1148/radiol.2020201365\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020201160\n10.1371/journal.pone.0235844\n10.1177/0846537120924606\n10.1016/j.crad.2020.03.008\n10.1109/JBHI.2022.3220813\n10.1016/j.knosys.2022.108246\n10.1109/TBME.2020.2993278\n10.1038/s41591-020-0824-5\n10.1109/TAI.2020.3020521\n10.1109/RBME.2020.2987975\n10.3390/jcm10091961\n10.1109/TMI.2020.2993291\n10.1007/s10916-021-01745-4\n10.1109/TBME.2021.3085576\n10.1109/JBHI.2022.3151171\n10.1109/JBHI.2022.3196489\n10.1049/ipr2.12153\n10.4269/ajtmh.20-0535\n10.1001/jamainternmed.2020.2033\n10.1038/s41467-020-17280-8\n10.1109/JBHI.2021.3069169\n10.1038/s41598-021-88538-4\n10.1148/radiol.2020192224\n10.3390/info11020125\n10.1109/TBME.2019.2933508\n10.1109/ACCESS.2020.3025372"}
{"title": "[Virtual training of practical competences in sonography].", "abstract": "During the COVID-19 pandemic, a\u00a0particular challenge in the transition to digital teaching was to teach practical skills such as sonography of the head and neck online. The aim of this study was to validate the digital sonography course for medical students established at the Freiburg University Hospital ENT Department.\nParticipants were 178\u00a0students of human medicine. The study group simulated the sonography examination at home with a\u00a0dummy transducer using the Peyton method under the guidance of a\u00a0tutor via video seminar. In a\u00a0subsequent learning success check, the results of the students in the online course were compared with those of the control group, who learned sonography in the classroom.\nStudents of the online course achieved comparable results to the classroom group.\nThis study shows that practical skills which require extensive equipment such as a\u00a0sonography machine can be taught to a\u00a0certain extent digitally or at least in a\u00a0hybrid form.\nZIELSETZUNG: W\u00e4hrend der COVID-19-Pandemie bestand eine besondere Herausforderung bei der Umstellung auf den digitalen Unterricht darin, praktische Fertigkeiten wie die Sonographie der Kopf- und Halsweichteile online zu vermitteln. Ziel dieser Studie war es, den an der Universit\u00e4ts-HNO-Klinik Freiburg etablierten digitalen Sonographie-Kurs f\u00fcr Studierende der Humanmedizin zu validieren.\nTeilnehmende waren 178 Studierende der Humanmedizin. Die Studiengruppe simulierte die Sonographie-Untersuchung zu Hause mit einer Schallkopfattrappe anhand der Peyton-Methode unter Anleitung eines Tutors per Videoseminar. In einer anschlie\u00dfenden Lernerfolgskontrolle wurden die Ergebnisse der Studierenden des Online-Kurses mit der Kontrollgruppe verglichen, welche die Sonographie im Pr\u00e4senzunterricht lernte.\nDie Studierenden des Online-Kurses konnten vergleichbare Ergebnisse zur Pr\u00e4senzgruppe erzielen.\nDie Studie zeigt, dass praktische Fertigkeiten, die eine umfangreiche Ausr\u00fcstung wie ein Sonographieger\u00e4t erfordern, bis zu einem gewissen Grad digital oder zumindest in einer hybriden Form vermittelt werden k\u00f6nnen.", "journal": "HNO", "date": "2024-04-08", "authors": ["FEverad", "LSeifert", "NMansour", "BHofauer", "AKnopf", "COffergeld"], "doi": "10.1007/s00106-024-01476-1\n10.1055/a-1232-4911\n10.1007/s00347-021-01344-1\n10.1007/s00106-021-01107-z\n10.1056/NEJMp1311944\n10.1007/s00063-019-0550-2\n10.3205/zma001045\n10.1007/s10984-012-9108-4\n10.1055/a-1900-8166\n10.1007/s00106-020-00829-w\n10.1055/s-0029-1245837\n10.1055/s-0034-1370944\n10.1007/s00106-020-00939-5\n10.1007/s00106-019-00745-8\n10.1007/s00103-017-2673-z\n10.3205/zma001401"}
{"title": "Thorax computed tomography (CTX) guided ground truth annotation of CHEST radiographs (CXR) for improved classification and detection of COVID-19.", "abstract": "Several data sets have been collected and various artificial intelligence models have been developed for COVID-19 classification and detection from both chest radiography (CXR) and thorax computed tomography (CTX) images. However, the pitfalls and shortcomings of these systems significantly limit their clinical use. In this respect, improving the weaknesses of advanced models can be very effective besides developing new ones. The inability to diagnose ground-glass opacities by conventional CXR has limited the use of this modality in the diagnostic work-up of COVID-19. In our study, we investigated whether we could increase the diagnostic efficiency by collecting a novel CXR data set, which contains pneumonic regions that are not visible to the experts and can only be annotated under CTX guidance. We develop an ensemble methodology of well-established deep CXR models for this new data set and develop a machine learning-based non-maximum suppression strategy to boost the performance for challenging CXR images. CTX and CXR images of 379 patients who applied to our hospital with suspected COVID-19 were evaluated with consensus by seven radiologists. Among these, CXR images of 161 patients who also have had a CTX examination on the same day or until the day before or after and whose CTX findings are compatible with COVID-19 pneumonia, are selected for annotating. CTX images are arranged in the main section passing through the anterior, middle, and posterior according to the sagittal plane with the reformed maximum intensity projection (MIP) method in the coronal plane. Based on the analysis of coronal MIP reconstructed CTX images, the regions corresponding to the pneumonia foci are annotated manually in CXR images. Radiologically classified posterior to anterior (PA) CXR of 218 patients with negative thorax CTX imaging were classified as COVID-19 pneumonia negative group. Accordingly, we have collected a new data set using anonymized CXR (JPEG) and CT (DICOM) images, where the PA CXRs contain pneumonic regions that are hidden or not easily recognized and annotated under CTX guidance. The reference finding was the presence of pneumonic infiltration consistent with COVID-19 on chest CTX examination. COVID-Net, a specially designed convolutional neural network, was used to detect cases of COVID-19 among CXRs. Diagnostic performances were evaluated by ROC analysis by applying six COVID-Net variants (COVIDNet-CXR3-A, -B, -C/COVIDNet-CXR4-A, -B, -C) to the defined data set and combining these models in various ways via ensemble strategies. Finally, a convex optimization strategy is carried out to find the outperforming weighted ensemble of individual models. The mean age of 161 patients with pneumonia was 49.31\u2009\u00b1\u200915.12, and the median age was 48\u2009years. The mean age of 218 patients without signs of pneumonia in thorax CTX examination was 40.04\u2009\u00b1\u200914.46, and the median was 38. When working with different combinations of COVID-Net's six variants, the area under the curve (AUC) using the ensemble COVID-Net CXR 4A-4B-3C was .78, sensitivity 67%, specificity 95%; COVID-Net CXR 4a-3b-3c was .79, sensitivity 69% and specificity 94%. When diverse and complementary COVID-Net models are used together through an ensemble, it has been determined that the AUC values are close to other studies, and the specificity is significantly higher than other studies in the literature.", "journal": "International journal for numerical methods in biomedical engineering", "date": "2024-04-08", "authors": ["\u015e\u00fckr\u00fc MehmetErt\u00fcrk", "Tu\u011f\u00e7eToprak", "Rana G\u00fcn\u00f6zC\u00f6mert", "CemreCandemir", "EdaCing\u00f6z", "Zeynep NurAkyol Sari", "Celal CanerErcan", "EsinD\u00fcvek", "BerkeErsoy", "EdanurKarapinar", "AtadanTunaci", "M AlperSelver"], "doi": "10.1002/cnm.3823"}
{"title": "Innovative applications of artificial intelligence during the COVID-19 pandemic.", "abstract": "The COVID-19 pandemic has created unprecedented challenges worldwide. Artificial intelligence (AI) technologies hold tremendous potential for tackling key aspects of pandemic management and response. In the present review, we discuss the tremendous possibilities of AI technology in addressing the global challenges posed by the COVID-19 pandemic. First, we outline the multiple impacts of the current pandemic on public health, the economy, and society. Next, we focus on the innovative applications of advanced AI technologies in key areas such as COVID-19 prediction, detection, control, and drug discovery for treatment. Specifically, AI-based predictive analytics models can use clinical, epidemiological, and omics data to forecast disease spread and patient outcomes. Additionally, deep neural networks enable rapid diagnosis through medical imaging. Intelligent systems can support risk assessment, decision-making, and social sensing, thereby improving epidemic control and public health policies. Furthermore, high-throughput virtual screening enables AI to accelerate the identification of therapeutic drug candidates and opportunities for drug repurposing. Finally, we discuss future research directions for AI technology in combating COVID-19, emphasizing the importance of interdisciplinary collaboration. Though promising, barriers related to model generalization, data quality, infrastructure readiness, and ethical risks must be addressed to fully translate these innovations into real-world impacts. Multidisciplinary collaboration engaging diverse expertise and stakeholders is imperative for developing robust, responsible, and human-centered AI solutions against COVID-19 and future public health emergencies.", "journal": "Infectious medicine", "date": "2024-04-08", "authors": ["ChenruiLv", "WenqiangGuo", "XinyiYin", "LiuLiu", "XinleiHuang", "ShiminLi", "LiZhang"], "doi": "10.1016/j.imj.2024.100095\n10.1038/s41392-020-00243-2\n10.1080/14760584.2023.2157817\n10.1109/ACCESS.2022.3181605\n10.2174/1573405617666210224115722\n10.1108/jstpm-08-2021-0122\n10.1108/ijpsm-07-2021-0172\n10.1016/j.tranpol.2021.05.013\n10.1038/s41591-021-01381-y\n10.1055/s-0040-1709715\n10.1186/s13054-019-2616-1\n10.1007/s10916-020-01596-5\n10.1007/978-3-030-87019-5_1\n10.3390/sym14010016\n10.1016/j.soh.2023.100045\n10.1016/j.soh.2023.100061\n10.4155/fmc-2021-0243\n10.1007/s10479-023-05462-8\n10.1007/978-981-16-8150-9_14\n10.3390/iot1020028\n10.1007/s40264-022-01156-5\n10.1063/5.0111115\n10.1126/science.aaa8415\n10.3390/ijerph17082749\n10.1016/j.dajour.2022.100071\n10.3390/biology11121732\n10.1007/978-981-10-5768-7_1\n10.1038/s41467-020-18684-2\n10.1049/htl2.12009\n10.1016/j.neucom.2016.06.014\n10.1109/TBDATA.2020.3032755\n10.1016/j.chaos.2020.110203\n10.11919/j.issn.1002-0829.215044\n10.1186/s12911-019-1004-8\n10.3390/electronics10161903\n10.1109/MCOM.2019.1800155\n10.1016/j.asoc.2020.106116\n10.2991/ijndc.k.201218.003\n10.1155/2021/8785636\n10.1186/s12859-021-04224-2\n10.1007/s00330-021-08049-8\n10.3348/kjr.2020.1104\n10.1038/s41746-021-00453-0\n10.1016/j.chaos.2020.110214\n10.1038/s41467-020-20657-4\n10.1016/j.media.2021.102096\n10.1136/bmjhci-2020-100235\n10.1136/bmjhci-2021-100389\n10.3389/fmed.2021.592336\n10.1038/s41598-021-82492-x\n10.1186/s12879-021-06478-w\n10.1007/s10489-021-02352-z\n10.1016/j.asoc.2022.109315\n10.1016/j.psep.2020.11.007\n10.1007/978-981-16-2164-2_18\n10.1109/TCYB.2020.2990162\n10.3390/biom11070993\n10.3892/etm.2022.11343\n10.3390/math10224267\n10.1016/j.jiph.2022.11.024\n10.1016/j.micpath.2020.104719\n10.3389/fgene.2022.858252\n10.1038/s41586-023-06617-0\n10.1016/j.chaos.2020.110170\n10.1016/j.patcog.2021.108255\n10.1007/s00264-020-04609-7\n10.1148/radiol.2020203511\n10.3390/math11061489\n10.1016/j.aej.2022.09.016\n10.1016/j.bbe.2022.11.003\n10.3389/fpubh.2021.744100\n10.1016/j.asoc.2021.107878\n10.1109/CCCI49893.2020.9256562\n10.1148/radiol.2020200847\n10.1016/j.bios.2022.114449\n10.1148/radiol.2020200905\n10.3390/jcm10102196\n10.1007/s10140-020-01784-3\n10.1002/cpe.7705\n10.1016/j.irbm.2021.01.004\n10.48550/arXiv.2004.03747\n10.1146/annurev-immunol-032712-095916\n10.1016/j.artmed.2022.102288\n10.1007/s11356-021-13823-8\n10.1109/JBHI.2020.3009314\n10.2196/19786\n10.3390/app10217514\n10.1016/j.glt.2020.11.002\n10.1016/j.amsu.2020.08.037\n10.1109/TCSS.2021.3075955\n10.1109/ACCESS.2021.3078080\n10.3390/ijerph192316023\n10.1016/j.scs.2022.103772\n10.1088/1742-6596/1864/1/012108\n10.1007/s11030-021-10217-3\n10.1371/journal.pone.0241543\n10.1016/j.medidd.2020.100077\n10.1038/s42256-020-00285-9\n10.1016/j.compbiolchem.2021.107536\n10.1016/j.csbj.2020.04.004\n10.3390/covid2020011\n10.1016/j.dsx.2020.06.068\n10.1016/j.drudis.2021.01.013\n10.1186/s12911-021-01617-4\n10.21203/rs.3.rs-114758/v1\n10.2139/ssrn.3561442\n10.1038/s41392-021-00568-6\n10.1089/omi.2022.0155\n10.1101/2020.08.02.233064\n10.1007/s12065-020-00540-3\n10.1016/j.tips.2019.07.013\n10.1109/TUFFC.2020.3002249\n10.1016/j.media.2021.102054\n10.4103/2229-3485.184782\n10.1002/rmv.2205\n10.1002/hbe2.237\n10.1007/s10489-020-01770-9\n10.1016/j.cmpb.2021.106288\n10.1016/j.chaos.2020.110337\n10.1186/s12992-021-00665-9\n10.1016/j.artmed.2021.102158\n10.1145/3351095.3372827\n10.1093/jamia/ocz192\n10.1016/j.artmed.2022.102423\n10.1038/s41746-020-0288-5\n10.1093/bioinformatics/btx160\n10.1021/ci500747n\n10.1016/j.psychres.2023.115466\n10.1016/j.jsis.2021.101683\n10.1145/3419764\n10.1016/j.mehy.2018.07.015\n10.1146/annurev-biodatasci-072018-021321"}
{"title": "Chromatin modifiers in human disease: from functional roles to regulatory mechanisms.", "abstract": "The field of transcriptional regulation has revealed the vital role of chromatin modifiers in human diseases from the beginning of functional exploration to the process of participating in many types of disease regulatory mechanisms. Chromatin modifiers are a class of enzymes that can catalyze the chemical conversion of pyrimidine residues or amino acid residues, including histone modifiers, DNA methyltransferases, and chromatin remodeling complexes. Chromatin modifiers assist in the formation of transcriptional regulatory circuits between transcription factors, enhancers, and promoters by regulating chromatin accessibility and the ability of transcription factors to acquire DNA. This is achieved by recruiting associated proteins and RNA polymerases. They modify the physical contact between cis-regulatory factor elements, transcription factors, and chromatin DNA to influence transcriptional regulatory processes. Then, abnormal chromatin perturbations can impair the homeostasis of organs, tissues, and cells, leading to diseases. The review offers a comprehensive elucidation on the function and regulatory mechanism of chromatin modifiers, thereby highlighting their indispensability in the development of diseases. Furthermore, this underscores the potential of chromatin modifiers as biomarkers, which may enable early disease diagnosis. With the aid of this paper, a deeper understanding of the role of chromatin modifiers in the pathogenesis of diseases can be gained, which could help in devising effective diagnostic and therapeutic interventions.", "journal": "Molecular biomedicine", "date": "2024-04-08", "authors": ["YaliNie", "ChaoSong", "HongHuang", "ShuqingMao", "KaiDing", "HuifangTang"], "doi": "10.1186/s43556-024-00175-1\n10.1038/s41573-020-0077-5\n10.1073/pnas.2310063120\n10.1161/CIRCRESAHA.110.224287\n10.1038/nrg.2016.59\n10.3390/genes10120955\n10.1093/nar/gkaa943\n10.1007/s10495-022-01777-2\n10.3389/fcell.2022.1070338\n10.1038/s41467-022-34577-y\n10.1016/j.devcel.2023.03.020\n10.1126/sciadv.adc9465\n10.1172/jci.insight.95625\n10.1155/2022/5249367\n10.1016/j.celrep.2022.111703\n10.1016/j.cmet.2008.06.013\n10.1016/j.celrep.2015.10.033\n10.3390/ijms232012536\n10.1073/pnas.1621425114\n10.1161/ATVBAHA.111.239897\n10.1161/CIRCRESAHA.121.319066\n10.1093/eurheartj/ehac097\n10.1172/jci.insight.147984\n10.1016/j.trsl.2022.04.001\n10.1016/j.biocel.2021.105974\n10.3724/abbs.2022093\n10.3389/fbioe.2021.695461\n10.7150/thno.70821\n10.1016/j.atherosclerosis.2022.06.1026\n10.1016/j.ebiom.2022.104139\n10.1126/scitranslmed.aaw9996\n10.1038/nm.3820\n10.1093/hmg/ddt614\n10.1172/jci.insight.155475\n10.7554/eLife.60311\n10.1016/j.apsb.2021.10.016\n10.1007/s12265-019-09882-5\n10.1016/j.yjmcc.2022.09.003\n10.1155/2022/4622520\n10.1016/j.ebiom.2022.103964\n10.1161/CIRCULATIONAHA.110.947531\n10.1016/j.jacbts.2022.06.004\n10.1002/jcp.27471\n10.3390/ijms21197010\n10.14814/phy2.14835\n10.1155/2020/5751768\n10.3390/antiox10040531\n10.1371/journal.pone.0169206\n10.3389/fnut.2022.857562\n10.3389/fphys.2021.688322\n10.1080/21655979.2021.2019869\n10.1159/000514311\n10.3390/ijms232214456\n10.1074/jbc.M211762200\n10.1016/j.yexcr.2012.11.015\n10.1111/jcmm.13961\n10.1016/j.lfs.2021.119552\n10.1016/j.yjmcc.2015.09.010\n10.1016/j.jnutbio.2022.109031\n10.1016/j.jphs.2021.07.001\n10.1016/j.bbadis.2016.05.006\n10.1038/s41419-019-2121-0\n10.1161/ATVBAHA.115.305230\n10.1016/j.phrs.2020.105104\n10.1016/j.yjmcc.2020.05.018\n10.1074/jbc.RA118.007006\n10.1038/s12276-018-0121-2\n10.1016/j.yexcr.2020.112059\n10.1038/ng.1068\n10.1093/nar/gkt896\n10.1038/nm.4179\n10.1016/j.yjmcc.2021.02.002\n10.1002/1873-3468.13515\n10.1172/JCI46277\n10.1038/s41467-018-07173-2\n10.1016/j.mce.2018.05.009\n10.1161/circulationaha.117.029430\n10.1038/nm.2961\n10.1007/s11596-018-1866-5\n10.1172/JCI61084\n10.1016/j.yjmcc.2017.02.003\n10.1038/s12276-022-00904-y\n10.1093/eurheartj/ehaa845\n10.1161/CIRCRESAHA.120.317104\n10.1016/j.lfs.2021.119769\n10.1155/2022/5009289\n10.1007/s00395-017-0608-3\n10.1096/fj.201903129RRR\n10.1002/ehf2.12789\n10.1002/pbc.30774\n10.1002/bies.202200239\n10.1080/15548627.2023.2249762\n10.1186/s13148-019-0645-x\n10.1172/jci168670\n10.1158/0008-5472.Can-23-2545\n10.1016/j.canlet.2017.10.027\n10.1186/s12885-018-4874-8\n10.1016/j.sbi.2011.11.006\n10.1016/j.phrs.2022.106244\n10.1038/cdd.2015.85\n10.1371/journal.pbio.3002354\n10.3390/genes14061179\n10.1186/s12864-022-09071-w\n10.1016/j.neulet.2023.137533\n10.1111/acel.13895\n10.3390/ijms24054805\n10.1007/s00018-023-04945-y\n10.1016/j.biopha.2023.115312\n10.2147/ijn.S380515\n10.3390/cells11213448\n10.1016/j.it.2019.10.006\n10.1073/pnas.2014562118\n10.4049/jimmunol.1700595\n10.4049/jimmunol.2200927\n10.1016/j.jaci.2018.02.032\n10.3389/fimmu.2020.550769\n10.1097/shk.0000000000001928\n10.1038/s41586-018-0761-3\n10.1016/j.jacbts.2018.06.004\n10.1016/j.bcp.2023.115693\n10.1016/j.isci.2022.104799\n10.1089/dna.2015.2993\n10.1161/CIRCRESAHA.116.303630\n10.1101/cshperspect.a018762\n10.3390/cells12071075\n10.1016/j.celrep.2023.113146\n10.1016/j.jbc.2023.105220\n10.1016/j.tcm.2012.12.006\n10.1620/tjem.219.17\n10.1074/jbc.M112.367847\n10.1038/s41598-019-55900-6\n10.1016/j.tranon.2023.101779\n10.1016/j.isci.2023.108517\n10.1016/j.ejmech.2023.115915\n10.1038/s43018-020-00171-8\n10.1016/j.ijrobp.2023.06.022\n10.1016/j.virol.2023.109901\n10.1021/acsomega.2c00984\n10.1016/j.gene.2022.146832\n10.1038/s41594-019-0298-7\n10.1016/j.biopha.2023.115456\n10.1161/CIRCULATIONAHA.122.059891\n10.1038/s41586-021-03994-2\n10.15252/embj.2023114558\n10.3390/cells12202425\n10.1182/blood.2022019419\n10.1016/j.celrep.2023.113329\n10.7150/ijbs.80323\n10.1038/s41392-022-01055-2\n10.1152/physrev.00048.2017\n10.1016/j.scitotenv.2023.161903\n10.1186/s10020-022-00551-z\n10.1002/cbin.11805\n10.1002/1878-0261.13032\n10.1038/ncomms16034\n10.1186/s12915-021-01109-x\n10.1073/pnas.2005222117\n10.1101/gad.277178.115\n10.1016/j.jbc.2023.105245\n10.1093/nar/gkad609\n10.1186/s12885-023-11321-3\n10.1080/15548627.2023.2200352\n10.1016/j.ydbio.2022.02.008\n10.3389/fgene.2023.1148430\n10.1172/jci.insight.155899\n10.1111/cas.16052\n10.1186/s12964-022-00958-5\n10.1073/pnas.1016071107\n10.1186/s13059-023-02883-3\n10.1016/j.molcel.2023.01.017\n10.1007/bf00916567\n10.1038/s41467-023-44578-0\n10.3389/fonc.2021.821495\n10.1158/1078-0432.Ccr-16-0534\n10.3390/ijms21238950\n10.1186/s12967-021-03211-8\n10.1002/biof.1704\n10.1038/s41419-020-2645-3\n10.3390/ijms19041174\n10.3390/cells11203211\n10.1016/j.tem.2018.04.007\n10.1038/s41582-022-00693-y\n10.1038/nrg2005\n10.1098/rsob.210350\n10.1016/s1097-2765(01)00404-x\n10.1016/s1097-2765(00)80256-7\n10.1002/pro.3359\n10.1038/nature02017\n10.1091/mbc.e05-09-0906\n10.1111/j.1365-313x.2009.03861.x\n10.1186/s12929-021-00721-x\n10.3390/pharmaceutics14071509\n10.1126/scitranslmed.aay7205\n10.1002/pul2.12323\n10.1021/acs.jmedchem.3c00977\n10.1038/s41598-023-36872-0\n10.1038/oncsis.2017.28\n10.3892/ijo.2018.4343\n10.1371/journal.pone.0187191\n10.1016/j.ejmech.2023.116015\n10.3390/nu14030580\n10.3390/nu13082608\n10.1253/circj.cj-10-1072\n10.1016/j.jnutbio.2019.108339\n10.1080/15592294.2017.1370173\n10.1111/jcmm.14162\n10.1111/jcmm.13419\n10.3390/nu12030761\n10.3390/nu9111201\n10.1007/s12017-016-8404-z\n10.1186/s13046-018-0926-9\n10.1016/j.phymed.2023.154855\n10.1002/ijc.31756\n10.2119/molmed.2011.00020\n10.1038/s41408-021-00445-z\n10.1038/s41401-021-00725-1\n10.2119/molmed.2011.00058\n10.1073/pnas.1320850111\n10.1096/fj.09-134700\n10.3389/fphar.2023.1275833\n10.3324/haematol.2009.012088\n10.1038/leu.2017.93\n10.1016/j.jhep.2004.10.020\n10.1038/s41419-017-0174-5\n10.1126/scitranslmed.aao0144\n10.1161/circulationaha.120.046462\n10.4049/jimmunol.1800885\n10.1158/2326-6066.Cir-13-0126\n10.3389/fonc.2020.581801\n10.1161/circep.118.007071\n10.1016/j.yjmcc.2018.01.006\n10.1111/jcmm.14188\n10.1016/j.jnutbio.2019.07.003\n10.1371/journal.pone.0261388\n10.1161/jaha.122.025857\n10.1161/hypertensionaha.119.14400\n10.1371/journal.pone.0213186\n10.33549/physiolres.934110\n10.31083/j.rcm2203113\n10.1097/fjc.0000000000001174\n10.7150/thno.55878\n10.1016/j.cell.2013.04.022\n10.1038/nrg3607\n10.1038/s41420-022-00903-y\n10.2337/db21-0706\n10.1016/j.yjmcc.2021.07.002"}
{"title": "Towards classification and comprehensive analysis of AI-based COVID-19 diagnostic techniques: A survey.", "abstract": "The unpredictable pandemic came to light at the end of December 2019, known as the novel coronavirus, also termed COVID-19, identified by the World Health Organization (WHO). The virus first originated in Wuhan (China) and rapidly affected most of the world's population. This outbreak's impact is experienced worldwide because it causes high mortality risk, many cases, and economic falls. Around the globe, the total number of cases and deaths reported till November 12, 2022, were >600 million and 6.6 million, respectively. During the period of COVID-19, several diverse diagnostic techniques have been proposed. This work presents a systematic review of COVID-19 diagnostic techniques in response to such acts. Initially, these techniques are classified into different categories based on their working principle and detection modalities, i.e. chest X-ray imaging, cough sound or respiratory patterns, RT-PCR, antigen testing, and antibody testing. After that, a comparative analysis is performed to evaluate these techniques' efficacy which may help to determine an optimum solution for a particular scenario. The findings of the proposed work show that Artificial Intelligence plays a vital role in developing COVID-19 diagnostic techniques which support the healthcare system. The related work can be a footprint for all the researchers, available under a single umbrella. Additionally, all the techniques are long-lasting and can be used for future pandemics.", "journal": "Artificial intelligence in medicine", "date": "2024-04-08", "authors": ["AmnaKosar", "MuhammadAsif", "Maaz BinAhmad", "WaseemAkram", "KhalidMahmood", "SaruKumari"], "doi": "10.1016/j.artmed.2024.102858"}
{"title": "Validated machine learning tools to distinguish immune checkpoint inhibitor, radiotherapy, COVID-19 and other infective pneumonitis.", "abstract": "Pneumonitis is a well-described, potentially disabling, or fatal adverse effect associated with both immune checkpoint inhibitors (ICI) and thoracic radiotherapy. Accurate differentiation between checkpoint inhibitor pneumonitis (CIP) radiation pneumonitis (RP), and infective pneumonitis (IP) is crucial for swift, appropriate, and tailored management to achieve optimal patient outcomes. However, correct diagnosis is often challenging, owing to overlapping clinical presentations and radiological patterns.\nIn this multi-centre study of 455 patients, we used machine learning with radiomic features extracted from chest CT imaging to develop and validate five models to distinguish CIP and RP from COVID-19, non-COVID-19 infective pneumonitis, and each other. Model performance was compared to that of two radiologists.\nModels to distinguish RP from COVID-19, CIP from COVID-19 and CIP from non-COVID-19 IP out-performed radiologists (test set AUCs of 0.92 vs 0.8 and 0.8; 0.68 vs 0.43 and 0.4; 0.71 vs 0.55 and 0.63 respectively). Models to distinguish RP from non-COVID-19 IP and CIP from RP were not superior to radiologists but demonstrated modest performance, with test set AUCs of 0.81 and 0.8 respectively. The CIP vs RP model performed less well on patients with prior exposure to both ICI and radiotherapy (AUC 0.54), though the radiologists also had difficulty distinguishing this test cohort (AUC values 0.6 and 0.6).\nOur results demonstrate the potential utility of such tools as a second or concurrent reader to support oncologists, radiologists, and chest physicians in cases of diagnostic uncertainty. Further research is required for patients with exposure to both ICI and thoracic radiotherapy.", "journal": "Radiotherapy and oncology : journal of the European Society for Therapeutic Radiology and Oncology", "date": "2024-04-07", "authors": ["SumeetHindocha", "BenjaminHunter", "KristoferLinton-Reid", "ThomasGeorge Charlton", "MitchellChen", "AndrewLogan", "MerinaAhmed", "ImogenLocke", "BhupinderSharma", "SimonDoran", "MatthewOrton", "CateyBunce", "DaniellePower", "ShahreenAhmad", "KarenChan", "PengNg", "RichardToshner", "BinnazYasar", "JohnConibear", "RavindhiMurphy", "TomNewsom-Davis", "PatrickGoodley", "MatthewEvison", "NadiaYousaf", "GeorgeBitar", "FionaMcDonald", "MatthewBlackledge", "EricAboagye", "RichardLee"], "doi": "10.1016/j.radonc.2024.110266"}
{"title": "MIDRC-MetricTree: a decision tree-based tool for recommending performance metrics in artificial intelligence-assisted medical image analysis.", "abstract": "The Medical Imaging and Data Resource Center (MIDRC) was created to facilitate medical imaging machine learning (ML) research for tasks including early detection, diagnosis, prognosis, and assessment of treatment response related to the coronavirus disease 2019 pandemic and beyond. The purpose of this work was to create a publicly available metrology resource to assist researchers in evaluating the performance of their medical image analysis ML algorithms.\nAn interactive decision tree, called MIDRC-MetricTree, has been developed, organized by the type of task that the ML algorithm was trained to perform. The criteria for this decision tree were that (1)\u00a0users can select information such as the type of task, the nature of the reference standard, and the type of the algorithm output and (2)\u00a0based on the user input, recommendations are provided regarding appropriate performance evaluation approaches and metrics, including literature references and, when possible, links to publicly available software/code as well as short tutorial videos.\nFive types of tasks were identified for the decision tree: (a)\u00a0classification, (b)\u00a0detection/localization, (c)\u00a0segmentation, (d)\u00a0time-to-event (TTE) analysis, and (e)\u00a0estimation. As an example, the classification branch of the decision tree includes two-class (binary) and multiclass classification tasks and provides suggestions for methods, metrics, software/code recommendations, and literature references for situations where the algorithm produces either binary or non-binary (e.g., continuous) output and for reference standards with negligible or non-negligible variability and unreliability.\nThe publicly available decision tree is a resource to assist researchers in conducting task-specific performance evaluations, including classification, detection/localization, segmentation, TTE, and estimation tasks.", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2024-04-05", "authors": ["KarenDrukker", "BerkmanSahiner", "TingtingHu", "Grace HyunKim", "Heather MWhitney", "NatalieBaughan", "Kyle JMyers", "Maryellen LGiger", "MichaelMcNitt-Gray"], "doi": "10.1117/1.JMI.11.2.024504\n10.1016/S0001-2998(78)80014-2\n10.1002/mp.16188\n10.1016/j.jacr.2009.06.001\n10.1016/j.acra.2013.03.001\n10.1117/12.955926\n10.1093/bioinformatics/btw570\n10.1016/j.cviu.2007.08.003\n10.1186/s12880-015-0068-x\n10.1109/TMI.2004.828354\n10.4103/0974-7788.76794\n10.1080/01621459.1958.10501452\n10.1111/j.2517-6161.1972.tb00899.x\n10.2307/2533667\n10.1016/S0197-2456(03)00072-2\n10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4\n10.1016/S0378-3758(97)00108-0\n10.1016/j.jbi.2020.103496\n10.1148/radiol.2015142202\n10.2196/jmir.5870\n10.1148/ryai.2020200029\n10.7326/M14-0698\n10.1002/mp.15170\n10.1136/bmjopen-2020-047709\n10.1148/radiol.220182\n10.1038/s41592-023-02151-z\n10.1117/1.JMI.10.6.061104\n10.1177/0962280214537390"}
{"title": "Anomaly Detection for Medical Images Using Heterogeneous Auto-Encoder.", "abstract": "Anomaly detection is an important task for medical image analysis, which can alleviate the reliance of supervised methods on large labelled datasets. Most existing methods use a pixel-wise self-reconstruction framework for anomaly detection. However, there are two challenges of these studies: 1) they tend to overfit learning an identity mapping between the input and output, which leads to failure in detecting abnormal samples; 2) the reconstruction considers the pixel-wise differences which may lead to an undesirable result. To mitigate the above problems, we propose a novel heterogeneous Auto-Encoder (Hetero-AE) for medical anomaly detection. Our model utilizes a convolutional neural network (CNN) as the encoder and a hybrid CNN-Transformer network as the decoder. The heterogeneous structure enables the model to learn the intrinsic information of normal data and enlarge the difference on abnormal samples. To fully exploit the effectiveness of Transformer in the hybrid network, a multi-scale sparse Transformer block is proposed to trade off modelling long-range feature dependencies and high computational costs. Moreover, the multi-stage feature comparison is introduced to reduce the noise of pixel-wise comparison. Extensive experiments on four public datasets (i.e., retinal OCT, chest X-ray, brain MRI, and COVID-19) verify the effectiveness of our method on different imaging modalities for anomaly detection. Additionally, our method can accurately detect tumors in brain MRI and lesions in retinal OCT with interpretable heatmaps to locate lesion areas, assisting clinicians in diagnosing abnormalities efficiently.", "journal": "IEEE transactions on image processing : a publication of the IEEE Signal Processing Society", "date": "2024-03-29", "authors": ["ShuaiLu", "WeihangZhang", "HeZhao", "HanruoLiu", "NingliWang", "HuiqiLi"], "doi": "10.1109/TIP.2024.3381435"}
{"title": "Hierarchical medical image report adversarial generation with hybrid discriminator.", "abstract": "Generating coherent reports from medical images is an important task for reducing doctors' workload. Unlike traditional image captioning tasks, the task of medical image report generation faces more challenges. Current models for generating reports from medical images often fail to characterize some abnormal findings, and some models generate reports with low quality. In this study, we propose a model to generate high-quality reports from medical images.\nIn this paper, we propose a model called Hybrid Discriminator Generative Adversarial Network (HDGAN), which combines Generative Adversarial Network (GAN) with Reinforcement Learning (RL). The HDGAN model consists of a generator, a one-sentence discriminator, and a one-word discriminator. Specifically, the RL reward signals are judged on the one-sentence discriminator and one-word discriminator separately. The one-sentence discriminator can better learn sentence-level structural information, while the one-word discriminator can learn word diversity information effectively.\nOur approach performs better on the IU-X-ray and COV-CTR datasets than the baseline models. For the ROUGE metric, our method outperforms the state-of-the-art model by 0.36 on the IU-X-ray, 0.06 on the MIMIC-CXR and 0.156 on the COV-CTR.\nThe compositional framework we proposed can generate more accurate medical image reports at different levels.", "journal": "Artificial intelligence in medicine", "date": "2024-03-29", "authors": ["JunsanZhang", "MingCheng", "QiaoqiaoCheng", "XiuxuanShen", "YaoWan", "JieZhu", "MengxuanLiu"], "doi": "10.1016/j.artmed.2024.102846"}
{"title": "Examining factors related to low performance of predicting remission in participants with major depressive disorder using neuroimaging data and other clinical features.", "abstract": "Major depressive disorder (MDD), a prevalent mental health issue, affects more than 8% of the US population, and almost 17% in the young group of 18-25 years old. Since Covid-19, its prevalence has become even more significant. However, the remission (being free of depression) rates of first-line antidepressant treatments on MDD are only about 30%. To improve treatment outcomes, researchers have built various predictive models for treatment responses and yet none of them have been adopted in clinical use. One reason is that most predictive models are based on data from subjective questionnaires, which are less reliable. Neuroimaging data are promising objective prognostic factors, but they are expensive to obtain and hence predictive models using neuroimaging data are limited and such studies were usually in small scale (N<100). In this paper, we proposed an advanced machine learning (ML) pipeline for small training dataset with large number of features. We implemented multiple imputation for missing data and repeated K-fold cross validation (CV) to robustly estimate predictive performances. Different feature selection methods and stacking methods using 6 general ML models including random forest, gradient boosting decision tree, XGBoost, penalized logistic regression, support vector machine (SVM), and neural network were examined to evaluate the model performances. All predictive models were compared using model performance metrics such as accuracy, balanced accuracy, area under ROC curve (AUC), sensitivity and specificity. Our proposed ML pipeline was applied to a training dataset and obtained an accuracy and AUC above 0.80. But such high performance failed while applying our ML pipeline using an external validation dataset from the EMBARC study which is a multi-center study. We further examined the possible reasons especially the site heterogeneity issue.", "journal": "PloS one", "date": "2024-03-28", "authors": ["JunyingWang", "David DWu", "ChristineDeLorenzo", "JieYang"], "doi": "10.1371/journal.pone.0299625\n10.1176/appi.books.9780890425596\n10.1007/s40273-021-01019-4\n10.1002/wps.21120\n10.1001/jamanetworkopen.2023.37011\n10.1016/S0140-6736(22)00187-8\n10.1186/s12888-023-05300-y\n10.2174/1570159X17666191001142934\n10.1016/j.comppsych.2014.09.007\n10.1080/14737175.2017.1307737\n10.1038/s41398-019-0524-4\n10.1176/appi.ajp.163.1.28\n10.1007/s11920-010-0160-4\n10.1176/appi.ajp.2011.10111645\n10.1016/S2215-0366(15)00471-X\n10.1038/s41398-021-01488-3\n10.1016/j.jad.2018.09.067\n10.1016/j.biopsych.2014.11.018\n10.1038/s41398-021-01286-x\n10.1001/jamapsychiatry.2022.1606\n10.1016/j.nicl.2021.102858\n10.1016/j.neuroimage.2012.01.021\n10.1001/jamapsychiatry.2013.143\n10.1176/appi.ajp.159.5.728\n10.1001/jama.287.14.1840\n10.1111/j.1755-5949.2010.00151.x\n10.1027/2151-2604/a000176\n10.1038/mp.2015.53\n10.1016/j.nicl.2023.103553\n10.1038/s41380-022-01730-4\n10.1016/j.neuri.2022.100110\n10.1038/s41746-023-00827-6\n10.1016/j.jpsychires.2016.03.001\n10.1001/jama.2015.15281\n10.1007/s11121-007-0070-9\n10.1002/hbm.24282\n10.1001/archpsyc.62.4.397\n10.1177/07067437211037141\n10.1016/j.jpsychires.2016.03.016\n10.1371/journal.pone.0253023\n10.1176/appi.ajp.2007.06111868\n10.4088/JCP.09m05650blu\n10.1097/YCO.0000000000000376\n10.1017/S0033291710000553\n10.1038/s41398-022-02152-0\n10.1176/appi.prcp.20220015\n10.4088/JCP.16m11385"}
{"title": "Algorithm-Driven Tele-otoscope for Remote Care for Patients With Otitis Media.", "abstract": "The COVID-19 pandemic has spurred a growing demand for telemedicine. Artificial intelligence and image processing systems with wireless transmission functionalities can facilitate remote care for otitis media (OM). Accordingly, this study developed and validated an algorithm-driven tele-otoscope system equipped with Wi-Fi transmission and a cloud-based automatic OM diagnostic algorithm.\nProspective, cross-sectional, diagnostic study.\nTertiary Academic Medical Center.\nWe designed a tele-otoscope (Otiscan, SyncVision Technology Corp) equipped with digital imaging and processing modules, Wi-Fi transmission capabilities, and an automatic OM diagnostic algorithm. A total of 1137 otoscopic images, comprising 987 images of normal cases and 150 images of cases of acute OM and OM with effusion, were used as the dataset for image classification. Two convolutional neural network models, trained using our dataset, were used for raw image segmentation and OM classification.\nThe tele-otoscope delivered images with a resolution of 1280\u2009\u00d7\u2009720 pixels. Our tele-otoscope effectively differentiated OM from normal images, achieving a classification accuracy rate of up to 94% (sensitivity, 80%; specificity, 96%).\nOur study demonstrated that the developed tele-otoscope has acceptable accuracy in diagnosing OM. This system can assist health care professionals in early detection and continuous remote monitoring, thus mitigating the consequences of OM.", "journal": "Otolaryngology--head and neck surgery : official journal of American Academy of Otolaryngology-Head and Neck Surgery", "date": "2024-03-28", "authors": ["Te-YungFang", "Tse-YuLin", "Chung-MinShen", "Su-YiHsu", "Shing-HueyLin", "Yu-JungKuo", "Ming-HsuChen", "Tan-KueiYin", "Chih-HsienLiu", "Men-TzungLo", "Pa-ChunWang"], "doi": "10.1002/ohn.738"}
{"title": "Enhancing COVID-19 Detection: An Xception-Based Model with Advanced Transfer Learning from X-ray Thorax Images.", "abstract": "Rapid and precise identification of Coronavirus Disease 2019 (COVID-19) is pivotal for effective patient care, comprehending the pandemic's trajectory, and enhancing long-term patient survival rates. Despite numerous recent endeavors in medical imaging, many convolutional neural network-based models grapple with the expressiveness problem and overfitting, and the training process of these models is always resource-intensive. This paper presents an innovative approach employing Xception, augmented with cutting-edge transfer learning techniques to forecast COVID-19 from X-ray thorax images. Our experimental findings demonstrate that the proposed model surpasses the predictive accuracy of established models in the domain, including Xception, VGG-16, and ResNet. This research marks a significant stride toward enhancing COVID-19 detection through a sophisticated and high-performing imaging model.", "journal": "Journal of imaging", "date": "2024-03-27", "authors": ["Reagan EMandiya", "Herv\u00e9 MKongo", "Selain KKasereka", "KyamakyaKyandoghere", "Petro MushidiTshakwanda", "Nathana\u00ebl MKasoro"], "doi": "10.3390/jimaging10030063\n10.1016/j.rinp.2021.104096\n10.3390/math11010253\n10.1016/j.cell.2018.02.010\n10.1148/radiol.2020200642\n10.1101/2020.02.11.20021493\n10.2214/AJR.20.23034\n10.3390/jimaging8120312\n10.1109/5.726791\n10.21203/rs.3.rs-2863523/v1"}
{"title": "Impact of AI-Based Post-Processing on Image Quality of Non-Contrast Computed Tomography of the Chest and Abdomen.", "abstract": "Non-contrast computed tomography (CT) is commonly used for the evaluation of various pathologies including pulmonary infections or urolithiasis but, especially in low-dose protocols, image quality is reduced. To improve this, deep learning-based post-processing approaches are being developed. Therefore, we aimed to compare the objective and subjective image quality of different reconstruction techniques and a deep learning-based software on non-contrast chest and low-dose abdominal CTs. In this retrospective study, non-contrast chest CTs of patients suspected of COVID-19 pneumonia and low-dose abdominal CTs suspected of urolithiasis were analysed. All images were reconstructed using filtered back-projection (FBP) and were post-processed using an artificial intelligence (AI)-based commercial software (PixelShine (PS)). Additional iterative reconstruction (IR) was performed for abdominal CTs. Objective and subjective image quality were evaluated. AI-based post-processing led to an overall significant noise reduction independent of the protocol (chest or abdomen) while maintaining image information (max. difference in SNR 2.59 \u00b1 2.9 and CNR 15.92 \u00b1 8.9, ", "journal": "Diagnostics (Basel, Switzerland)", "date": "2024-03-27", "authors": ["Marcel ADrews", "AydinDemircio\u011flu", "JuliaNeuhoff", "JohannesHaubold", "SebastianZensen", "Marcel KOpitz", "MichaelForsting", "KaiNassenstein", "DeniseBos"], "doi": "10.3390/diagnostics14060612\n10.1056/NEJMra072149\n10.1097/RLI.0000000000000601\n10.4103/0970-1591.156924\n10.3390/life13040992\n10.1016/j.icrp.2007.10.003\n10.1667/RR13829.1\n10.1055/a-1248-2556\n10.1097/RCT.0000000000000216\n10.1148/radiol.2015132766\n10.1016/j.ejrad.2018.10.025\n10.1007/s00330-020-07668-x\n10.1002/mp.13937\n10.3390/diagnostics12071627\n10.3390/tomography8040140\n10.1259/bjr.20200677\n10.1016/j.oooo.2020.11.018\n10.1097/RLI.0b013e31821690a1\n10.1148/radiol.2016152771\n10.1259/bjr.20181019\n10.1007/s11604-018-0798-0\n10.21037/qims-21-564\n10.3390/diagnostics12010225\n10.1007/s00330-020-07537-7\n10.3348/kjr.2020.0020\n10.3348/kjr.2021.0140\n10.1007/s00261-021-03111-x\n10.2214/AJR.19.21809\n10.1016/j.resinv.2021.10.004\n10.3348/kjr.2020.0116\n10.3348/kjr.2019.0413"}
{"title": "Reimagining Radiology: A Comprehensive Overview of Reviews at the Intersection of Mobile and Domiciliary Radiology over the Last Five Years.", "abstract": "(Background) Domiciliary radiology, which originated in pioneering studies in 1958, has transformed healthcare, particularly during the COVID-19 pandemic, through advancements such as miniaturization and digitization. This evolution, driven by the synergy of advanced technologies and robust data networks, reshapes the intersection of domiciliary radiology and mobile technology in healthcare delivery. (Objective) The objective of this study is to overview the reviews in this field with reference to the last five years to face the state of development and integration of this practice in the health domain. (Methods) A review was conducted on PubMed and Scopus, applying a standard checklist and a qualification process. The outcome detected 21 studies. (Key Content and Findings) The exploration of mobile and domiciliary radiology unveils a compelling and optimistic perspective. Notable strides in this dynamic field include the integration of Artificial Intelligence (AI), revolutionary applications in telemedicine, and the educational potential of mobile devices. Post-COVID-19, telemedicine advances and the influential role of AI in pediatric radiology signify significant progress. Mobile mammography units emerge as a solution for underserved women, highlighting the crucial importance of early breast cancer detection. The investigation into domiciliary radiology, especially with mobile X-ray equipment, points toward a promising frontier, prompting in-depth research for comprehensive insights into its potential benefits for diverse populations. The study also identifies limitations and suggests future exploration in various domains of mobile and domiciliary radiology. A key recommendation stresses the strategic prioritization of multi-domain technology assessment initiatives, with scientific societies' endorsement, emphasizing regulatory considerations for responsible and ethical technology integration in healthcare practices. The broader landscape of technology assessment should aim to be innovative, ethical, and aligned with societal needs and regulatory standards. (Conclusions) The dynamic state of the field is evident, with active exploration of new frontiers. This overview also provides a roadmap, urging scholars, industry players, and regulators to collectively contribute to the further integration of this technology in the health domain.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2024-03-27", "authors": ["GrazianoLepri", "FrancescoOddi", "Rosario AlfioGulino", "DanieleGiansanti"], "doi": "10.3390/bioengineering11030216\n10.1016/S0009-9260(05)82966-6\n10.1016/j.radi.2022.01.002\n10.1186/s41747-022-00273-1\n10.1016/j.ipemt.2022.100005\n10.3390/healthcare8020118\n10.1016/j.jmir.2022.06.007\n10.3390/jpm14010041\n10.1002/jmrs.755\n10.7759/cureus.35117\n10.1016/j.ultrasmedbio.2023.02.008\n10.3390/healthcare10102040\n10.7417/CT.2022.2467\n10.3390/children9071044\n10.3390/diagnostics12040902\n10.3390/healthcare10010154\n10.5055/ajdm.2022.0434\n10.1002/ase.2155\n10.48095/ccko2021374\n10.1186/s12913-020-05564-0\n10.1097/RLI.0000000000000675\n10.1097/RMR.0000000000000238\n10.1097/PTS.0000000000000709\n10.1007/s10278-019-00311-2\n10.1007/s10278-019-00313-0\n10.1053/j.tvir.2019.04.004\n10.1007/s10278-019-00248-6\n10.1067/j.cpradiol.2019.04.007\n10.1016/j.radi.2023.02.027\n10.1186/s12877-022-03212-2\n10.1111/jep.13058\n10.1136/bmjopen-2015-008050\n10.1186/s12913-017-2173-8\n10.1186/s13104-017-2420-4\n10.1016/j.jacr.2021.08.024\n10.1016/j.jacr.2023.11.011\n10.1007/978-3-030-42618-7_1\n10.1038/s41598-020-76647-5\n10.3390/healthcare10010153\n10.4274/dir.2022.221713\n10.1016/j.heliyon.2022.e12603\n10.1186/s12875-022-01799-4\n10.1089/tmj.2020.0108\n10.1016/j.ejrad.2020.109285\n10.1007/s11604-020-00941-5\n10.1016/j.ejrad.2019.108742\n10.1007/s11547-016-0640-7\n10.1186/s12875-016-0418-y\n10.1055/s-0033-1342936\n10.4082/kjfm.2011.32.6.341\n10.1007/s13244-011-0099-y\n10.1089/tmj.2010.0138"}
{"title": "Radiomics analysis for distinctive identification of COVID-19 pulmonary nodules from other benign and malignant counterparts.", "abstract": "This observational study investigated the potential of radiomics as a non-invasive adjunct to CT in distinguishing COVID-19 lung nodules from other benign and malignant lung nodules. Lesion segmentation, feature extraction, and machine learning algorithms, including decision tree, support vector machine, random forest, feed-forward neural network, and discriminant analysis, were employed in the radiomics workflow. Key features such as Idmn, skewness, and long-run low grey level emphasis were identified as crucial in differentiation. The model demonstrated an accuracy of 83% in distinguishing COVID-19 from other benign nodules and 88% from malignant nodules. This study concludes that radiomics, through machine learning, serves as a valuable tool for non-invasive discrimination between COVID-19 and other benign and malignant lung nodules. The findings suggest the potential complementary role of radiomics in patients with COVID-19 pneumonia exhibiting lung nodules and suspicion of concurrent lung pathologies. The clinical relevance lies in the utilization of radiomics analysis for feature extraction and classification, contributing to the enhanced differentiation of lung nodules, particularly in the context of COVID-19.", "journal": "Scientific reports", "date": "2024-03-26", "authors": ["MinminiSelvam", "AnupamaChandrasekharan", "AbjasreeSadanandan", "Vikas KAnand", "SidharthRamesh", "ArunanMurali", "GanapathyKrishnamurthi"], "doi": "10.1038/s41598-024-57899-x\n10.1021/acscentsci.0c00501\n10.1007/s00330-020-07347-x\n10.1001/jama.2020.1585\n10.1148/radiol.2020200370\n10.1016/S1473-3099(20)30086-4\n10.1016/S1473-3099(20)30134-1\n10.1148/ryct.2020200280\n10.1093/ejcts/ezaa222\n10.1148/radiol.2021211396\n10.1148/radiol.2020200463\n10.1007/s00330-020-06801-0\n10.1148/radiol.2015151169\n10.1186/s12879-021-06614-6\n10.1148/radiol.2017161659\n10.1016/j.neuroimage.2006.01.015\n10.1109/TSMC.1973.4309314\n10.1016/S0146-664X(75)80008-6\n10.1016/j.mri.2012.06.010\n10.1023/A:1012487302797\n10.1038/s41598-023-38350-z\n10.1038/s41598-023-46391-7"}
{"title": "Deep learning for real-time multi-class segmentation of artefacts in lung ultrasound.", "abstract": "Lung ultrasound (LUS) has emerged as a safe and cost-effective modality for assessing lung health, particularly during the COVID-19 pandemic. However, interpreting LUS images remains challenging due to its reliance on artefacts, leading to operator variability and limiting its practical uptake. To address this, we propose a deep learning pipeline for multi-class segmentation of objects (ribs, pleural line) and artefacts (A-lines, B-lines, B-line confluence) in ultrasound images of a lung training phantom. Lightweight models achieved a mean Dice Similarity Coefficient (DSC) of 0.74, requiring fewer than 500 training images. Applying this method in real-time, at up to 33.4 frames per second in inference, allows enhanced visualisation of these features in LUS images. This could be useful in providing LUS training and helping to address the skill gap. Moreover, the segmentation masks obtained from this model enable the development of explainable measures of disease severity, which have the potential to assist in the triage and management of patients. We suggest one such semi-quantitative measure called the B-line Artefact Score, which is related to the percentage of an intercostal space occupied by B-lines and in turn may be associated with the severity of a number of lung conditions. Moreover, we show how transfer learning could be used to train models for small datasets of clinical LUS images, identifying pathologies such as simple pleural effusions and lung consolidation with DSC values of 0.48 and 0.32 respectively. Finally, we demonstrate how such DL models could be translated into clinical practice, implementing the phantom model alongside a portable point-of-care ultrasound system, facilitating bedside assessment and improving the accessibility of LUS.", "journal": "Ultrasonics", "date": "2024-03-24", "authors": ["LewisHowell", "NicolaIngram", "RogerLapham", "AdamMorrell", "James RMcLaughlan"], "doi": "10.1016/j.ultras.2024.107251"}
{"title": "CD-Net: Cascaded 3D Dilated convolutional neural network for pneumonia lesion segmentation.", "abstract": "COVID-19 is a global pandemic that has caused significant global, social, and economic disruption. To effectively assist in screening and monitoring diagnosed cases, it is crucial to accurately segment lesions from Computer Tomography (CT) scans. Due to the lack of labeled data and the presence of redundant parameters in 3D CT, there are still significant challenges in diagnosing COVID-19 in related fields. To address the problem, we have developed a new model called the Cascaded 3D Dilated convolutional neural network (CD-Net) for directly processing CT volume data. To reduce memory consumption when cutting volume data into small patches, we initially design a cascade architecture in CD-Net to preserve global information. Then, we construct a Multi-scale Parallel Dilated Convolution (MPDC) block to aggregate features of different sizes and simultaneously reduce the parameters. Moreover, to alleviate the shortage of labeled data, we employ classical transfer learning, which requires only a small amount of data while achieving better performance. Experimental results conducted on the different public-available datasets verify that the proposed CD-Net has reduced the negative-positive ratio and outperformed other existing segmentation methods while requiring less data.", "journal": "Computers in biology and medicine", "date": "2024-03-22", "authors": ["JinliZhang", "ShaomengWang", "ZongliJiang", "ZhijieChen", "XiaoluBai"], "doi": "10.1016/j.compbiomed.2024.108311"}
{"title": "Enhancing explainable SARS-CoV-2 vaccine development leveraging bee colony optimised Bi-LSTM, Bi-GRU models and bioinformatic analysis.", "abstract": "The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is a single-stranded RNA virus that caused the outbreak of the coronavirus disease 2019 (COVID-19). The COVID-19 outbreak has led to millions of deaths and economic losses globally. Vaccination is the most practical solution, but finding epitopes (antigenic peptide regions) in the SARS-CoV-2 proteome is challenging, costly, and time-consuming. Here, we proposed a deep learning method based on standalone Recurrent Neural networks to predict epitopes from SARS-CoV-2 proteins easily. We optimised the standalone Bidirectional Long Short-Term Memory (Bi-LSTM) and Bidirectional Gated Recurrent Unit (Bi-GRU) with a bioinspired optimisation algorithm, namely, Bee Colony Optimization (BCO). The study shows that LSTM-based models, particularly BCO-Bi-LSTM, outperform all other models and achieve an accuracy of 0.92 and AUC of 0.944. To overcome the challenge of understanding the model predictions, explainable AI using the Shapely Additive Explanations (SHAP) method was employed to explain how Blackbox models make decisions. Finally, the predicted epitopes led to the development of a multi-epitope vaccine. The multi-epitope vaccine effectiveness evaluation is based on vaccine toxicity, allergic response risk, and antigenic and biochemical characteristics using bioinformatic tools. The developed multi-epitope vaccine is non-toxic and highly antigenic. Codon adaptation, cloning, gel electrophoresis assess genomic sequence, protein composition, expression and purification while docking and IMMSIM servers simulate interactions and immunological response, respectively. These investigations provide a conceptual framework for developing a SARS-CoV-2 vaccine.", "journal": "Scientific reports", "date": "2024-03-21", "authors": ["Dilber UzunOzsahin", "Zubaida SaidAmeen", "Abdurrahman ShuaibuHassan", "Auwalu SalehMubarak"], "doi": "10.1038/s41598-024-55762-7\n10.1038/s41586-020-2012-7\n10.1038/s41541-020-0170-0\n10.1007/s13337-022-00755-1\n10.1016/j.compbiomed.2020.104131\n10.1016/j.prrv.2020.06.010\n10.3906/biy-2006-1\n10.1016/j.virusres.2022.199024\n10.1016/j.intimp.2020.106738\n10.1016/j.micpath.2021.105171\n10.1080/14760584.2022.2021882\n10.3390/vaccines9091038\n10.1080/07391102.2021.1964600\n10.1007/s11274-021-03188-y\n10.1016/j.sajb.2021.04.014\n10.1080/01490451.2021.1992042\n10.1016/j.jim.2022.113325\n10.1038/s41598-017-09199-w\n10.1038/s41598-020-73371-y\n10.1038/s41598-020-80899-6\n10.1038/s41598-022-15956-3\n10.1016/j.imbio.2020.151949\n10.1128/JVI.01505-07\n10.1016/j.sajb.2022.04.043\n10.1007/s11224-022-02027-6\n10.1007/s40203-021-00098-7\n10.3389/fimmu.2020.01663\n10.3390/vaccines8020290\n10.3389/fimmu.2020.01784\n10.3390/pathogens11020146\n10.3390/diagnostics11111990\n10.1038/s41598-022-11731-6\n10.1111/exsy.12842\n10.1016/j.compbiomed.2023.107153\n10.1016/j.compbiomed.2023.107113\n10.1038/s41598-021-81749-9\n10.3390/pr11061829\n10.7717/peerj.13380\n10.1038/nrd2224\n10.1038/s41577-019-0243-3\n10.1016/j.neunet.2005.06.042\n10.1016/j.procs.2020.01.020\n10.1371/journal.pone.0073957\n10.1007/s00894-014-2278-5\n10.1186/1471-2105-8-4\n10.1093/nar/gkg563\n10.1093/bioinformatics/btw760\n10.1093/emboj/19.19.5241\n10.1093/protein/14.8.529\n10.1385/0-89603-402-X:491\n10.1093/bioinformatics/16.4.404\n10.1093/bioinformatics/btx345\n10.1038/nmeth.3213\n10.1093/nar/gkt458\n10.1038/nprot.2016.169\n10.1093/nar/gki376\n10.1371/journal.pone.0009862\n10.1038/s41577-020-0321-6"}
{"title": "Artificial intelligence (AI)-assisted chest computer tomography (CT) insights: a study on intensive care unit (ICU) admittance trends in 78 coronavirus disease 2019 (COVID-19) patients.", "abstract": "The global coronavirus disease 2019 (COVID-19) pandemic has posed substantial challenges for healthcare systems, notably the increased demand for chest computed tomography (CT) scans, which lack automated analysis. Our study addresses this by utilizing artificial intelligence-supported automated computer analysis to investigate lung involvement distribution and extent in COVID-19 patients. Additionally, we explore the association between lung involvement and intensive care unit (ICU) admission, while also comparing computer analysis performance with expert radiologists' assessments.\nA total of 81 patients from an open-source COVID database with confirmed COVID-19 infection were included in the study. Three patients were excluded. Lung involvement was assessed in 78 patients using CT scans, and the extent of infiltration and collapse was quantified across various lung lobes and regions. The associations between lung involvement and ICU admission were analysed. Additionally, the computer analysis of COVID-19 involvement was compared against a human rating provided by radiological experts.\nThe results showed a higher degree of infiltration and collapse in the lower lobes compared to the upper lobes (P<0.05). No significant difference was detected in the COVID-19-related involvement of the left and right lower lobes. The right middle lobe demonstrated lower involvement compared to the right lower lobes (P<0.05). When examining the regions, significantly more COVID-19 involvement was found when comparing the posterior \nThe findings suggest that the extent of lung involvement, particularly in the lower lobes, dorsal lungs, and lower half of the lungs, may be associated with the need for ICU admission in patients with COVID-19. Computer analysis showed a high correlation with expert rating, highlighting its potential utility in clinical settings for assessing lung involvement. This information may help guide clinical decision-making and resource allocation during ongoing or future pandemics. Further studies with larger sample sizes are warranted to validate these findings.", "journal": "Journal of thoracic disease", "date": "2024-03-20", "authors": ["RudolfBumm", "PaoloZaffino", "AndrasLasso", "Ra\u00fal San Jos\u00e9Est\u00e9par", "StevenPieper", "JakobWasserthal", "Maria FrancescaSpadea", "TsogyalLatshang", "NadineKawel-Boehm", "AdrianW\u00e4ckerlin", "RaphaelWerner", "GabrielaH\u00e4ssig", "MarkusFurrer", "RonKikinis"], "doi": "10.21037/jtd-23-1150\n10.1056/NEJMoa2001316\n10.1038/s41586-020-2008-3\n10.1016/S1473-3099(20)30120-1\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1148/radiol.2020200370\n10.3390/bioengineering8020026\n10.1148/ryai.230024\n10.3390/diagnostics12061501\n10.1038/s41592-020-01008-z\n10.1016/j.crad.2022.11.006\n10.1007/s00330-020-07042-x\n10.1016/S0140-6736(20)30566-3\n10.1056/NEJMoa2002032\n10.1016/S2213-2600(20)30079-5\n10.1073/pnas.2004911117\n10.1136/bmj.m1966\n10.21037/qims-22-718"}
{"title": "An AI-Based Low-Risk Lung Health Image Visualization Framework Using LR-ULDCT.", "abstract": "In this article, we propose\u00a0an AI-based low-risk visualization framework for lung health monitoring using low-resolution ultra-low-dose CT (LR-ULDCT). We present a novel deep cascade processing workflow to achieve diagnostic visualization on LR-ULDCT (<0.3 mSv) at par high-resolution CT (HRCT) of 100 mSV radiation technology. To this end, we build a low-risk and affordable deep cascade network comprising three sequential deep processes: restoration, super-resolution (SR), and segmentation. Given degraded LR-ULDCT, the first novel network unsupervisedly learns restoration function from augmenting patch-based dictionaries and residuals. The restored version is then super-resolved (SR) for target (sensor) resolution. Here, we combine perceptual and adversarial losses in novel GAN to establish the closeness between probability distributions of generated SR-ULDCT and restored LR-ULDCT. Thus SR-ULDCT is presented to the segmentation network that first separates the chest portion from SR-ULDCT followed by lobe-wise colorization. Finally, we extract five lobes to account for the presence of ground glass opacity (GGO) in the lung. Hence, our AI-based system provides low-risk visualization of input degraded LR-ULDCT to various stages, i.e., restored LR-ULDCT, restored SR-ULDCT, and segmented SR-ULDCT, and achieves diagnostic power of HRCT. We perform case studies by experimenting on real datasets of COVID-19, pneumonia, and pulmonary edema/congestion while comparing our results with state-of-the-art. Ablation experiments are conducted for better visualizing different operating pipelines. Finally, we present a verification report by fourteen (14) experienced radiologists and pulmonologists.", "journal": "Journal of imaging informatics in medicine", "date": "2024-03-16", "authors": ["SwatiRai", "Jignesh SBhatt", "Sarat KumarPatra"], "doi": "10.1007/s10278-024-01062-5\n10.1097/JTO.0b013e318189f526\n10.1109/TMI.2016.2528162\n10.1016/j.media.2017.07.005\n10.1016/j.media.2016.06.032\n10.1007/s10278-019-00274-4\n10.1007/s10278-023-00842-9\n10.1109/TMI.2018.2823338\n10.1109/TRPMS.2018.2877644\n10.1109/JSTSP.2020.2998413\n10.1109/TMI.2021.3085839\n10.1109/ACCESS.2021.3106707\n10.1007/s10278-018-0056-0\n10.1109/TCI.2016.2629284\n10.1109/TMI.2019.2922960\n10.1007/s10278-017-0033-z\n10.1109/TRPMS.2018.2890359\n10.1007/s10278-019-00223-1\n10.1109/TIP.2021.3058783\n10.1007/s10278-021-00556-w\n10.1109/TCBB.2021.3065361\n10.1109/TMI.2020.2995965\n10.1109/TMI.2020.2996645\n10.1007/s00330-020-07225-6\n10.1007/s00259-020-04953-1\n10.1007/s00330-020-07044-9\n10.1097/RTI.0000000000000613\n10.1109/TPAMI.2015.2439281\n10.4066/AMJ.2013.1856\n10.1038/s41598-022-12743-y\n10.5121/ijcsea.2012.2314\n10.1109/TMI.2014.2336860\n10.1109/TMI.2021.3066161\n10.1109/TMI.2020.3001810\n10.4236/jcc.2019.73002"}
{"title": "Impact of the COVID-19 pandemic and COVID vaccination campaign on imaging case volumes and medicolegal aspects.", "abstract": "The coronavirus pandemic (COVID-19) significantly impacted the global economy and health. Italy was one of the first and most affected countries. The objective of our study was to assess the impact of the pandemic and the vaccination campaign on the radiological examinations performed in a radiology department of a tertiary center in Southern Italy.\nWe analyzed weekly and retrospectively electronic medical records of case volumes performed at the Radiology Department of \"Mater Domini\" University Hospital of Catanzaro from March 2020 to March 2022, comparing them with the volumes in the same period of the year 2019. We considered the origin of patients (outpatient, inpatient) and the type of examinations carried out (x-ray, mammography, CT, MRI, and ultrasound). A non-parametric test (Wilcoxon Signed Rank test) was applied to evaluate the average volumes.\nTotal flows in the pandemic period from COVID-19 were lower than in the same pre-pandemic period with values of 552 (120) vs. 427 (149) median (IQR) (\nThe pandemic impacted the volume of radiological examinations performed, particularly with the reduction of tests in outpatients. The vaccination allowed the return to the pre-COVID period imaging case volumes.", "journal": "Frontiers in health services", "date": "2024-03-15", "authors": ["CaterinaBattaglia", "FrancescoManti", "DanielaMazzuca", "AntonioCutruzzol\u00e0", "Marcello DellaCorte", "FiorellaCaputo", "SantoGratteri", "DomenicoLagan\u00e0"], "doi": "10.3389/frhs.2024.1253905\n10.23750/abm.v91i1.9397\n10.3390/math10111929\n10.1016/j.jacr.2020.07.001\n10.1067/j.cpradiol.2021.03.017\n10.1016/j.jacr.2020.05.004\n10.3389/fmed.2023.1103083\n10.1007/s10787-021-00847-2\n10.1038/s41467-022-31394-1\n10.1515/dx-2017-0025\n10.1148/radiol.2020201495\n10.1016/j.acra.2020.06.024\n10.2147/IDR.S315727\n10.1016/j.ypmed.2021.106585\n10.1007/s00384-020-03635-6\n10.1177/0310057X1404200304\n10.1155/2013/219259\n10.1007/s11999-014-3847-8\n10.1177/2150132720985055\n10.3390/pathogens12010017\n10.3390/biomimetics8070552\n10.1007/s40846-023-00783-2\n10.1016/j.imu.2023.101235\n10.3390/ijerph19095614\n10.3390/medicina57121314\n10.1186/s13244-021-00964-0\n10.3390/ijerph17238834"}
{"title": "MIS-Net: A deep learning-based multi-class segmentation model for CT images.", "abstract": "The accuracy of traditional CT image segmentation algorithms is hindered by issues such as low contrast and high noise in the images. While numerous scholars have introduced deep learning-based CT image segmentation algorithms, they still face challenges, particularly in achieving high edge accuracy and addressing pixel classification errors. To tackle these issues, this study proposes the MIS-Net (Medical Images Segment Net) model, a deep learning-based approach. The MIS-Net model incorporates multi-scale atrous convolution into the encoding and decoding structure with symmetry, enabling the comprehensive extraction of multi-scale features from CT images. This enhancement aims to improve the accuracy of lung and liver edge segmentation. In the evaluation using the COVID-19 CT Lung and Infection Segmentation dataset, the left and right lung segmentation results demonstrate that MIS-Net achieves a Dice Similarity Coefficient (DSC) of 97.61. Similarly, in the Liver Tumor Segmentation Challenge 2017 public dataset, the DSC of MIS-Net reaches 98.78.", "journal": "PloS one", "date": "2024-03-13", "authors": ["HuaweiLi", "ChangyingWang"], "doi": "10.1371/journal.pone.0299970\n10.1148/rg.2015140232\n10.3389/fneur.2023.1148846\n10.2174/157340561101150423103441\n10.1109/ACCESS.2019.2929258\n10.1109/TPAMI.2017.2699184\n10.1109/TMI.2020.2975347\n10.1109/ACCESS.2019.2896961\n10.3389/fbioe.2020.00897\n10.1186/s12880-021-00728-8\n10.3390/bioengineering9110709\n10.1109/TPAMI.2015.2389824\n10.3390/rs14133109\n10.1002/mp.14676\n10.1186/s13244-022-01163-1\n10.1002/mp.16135\n10.1038/s41598-022-09978-0\n10.1007/s11845-022-03113-8"}
{"title": "COVID-Net L2C-ULTRA: An Explainable Linear-Convex Ultrasound Augmentation Learning Framework to Improve COVID-19 Assessment and Monitoring.", "abstract": "While no longer a public health emergency of international concern, COVID-19 remains an established and ongoing global health threat. As the global population continues to face significant negative impacts of the pandemic, there has been an increased usage of point-of-care ultrasound (POCUS) imaging as a low-cost, portable, and effective modality of choice in the COVID-19 clinical workflow. A major barrier to the widespread adoption of POCUS in the COVID-19 clinical workflow is the scarcity of expert clinicians who can interpret POCUS examinations, leading to considerable interest in artificial intelligence-driven clinical decision support systems to tackle this challenge. A major challenge to building deep neural networks for COVID-19 screening using POCUS is the heterogeneity in the types of probes used to capture ultrasound images (e.g., convex vs. linear probes), which can lead to very different visual appearances. In this study, we propose an analytic framework for COVID-19 assessment able to consume ultrasound images captured by linear and convex probes. We analyze the impact of leveraging extended linear-convex ultrasound augmentation learning on producing enhanced deep neural networks for COVID-19 assessment, where we conduct data augmentation on convex probe data alongside linear probe data that have been transformed to better resemble convex probe data. The proposed explainable framework, called COVID-Net L2C-ULTRA, employs an efficient deep columnar anti-aliased convolutional neural network designed via a machine-driven design exploration strategy. Our experimental results confirm that the proposed extended linear-convex ultrasound augmentation learning significantly increases performance, with a gain of 3.9% in test accuracy and 3.2% in AUC, 10.9% in recall, and 4.4% in precision. The proposed method also demonstrates a much more effective utilization of linear probe images through a 5.1% performance improvement in recall when such images are added to the training dataset, while all other methods show a decrease in recall when trained on the combined linear-convex dataset. We further verify the validity of the model by assessing what the network considers to be the critical regions of an image with our contribution clinician.", "journal": "Sensors (Basel, Switzerland)", "date": "2024-03-13", "authors": ["E ZhixuanZeng", "AshkanEbadi", "AdrianFlorea", "AlexanderWong"], "doi": "10.3390/s24051664\n10.1183/23120541.00196-2022\n10.1007/s00261-018-1517-0\n10.3390/s23052621\n10.1155/2018/7068349\n10.1109/MSP.2012.2211477\n10.1148/radiol.2020192224\n10.1016/j.ultrasmedbio.2019.09.018\n10.1109/TIP.2009.2024064\n10.1007/s11548-020-02203-1\n10.1016/j.compmedimag.2010.09.003\n10.1109/MSP.2017.2765202\n10.5455/aim.2011.19.168-171\n10.1016/S0301-5629(00)00204-0\n10.1109/TUFFC.2020.3020055\n10.1007/s10396-020-01074-y\n10.1007/s40477-021-00586-8\n10.1016/B978-0-7020-3131-1.00068-7\n10.1186/s13089-018-0109-0\n10.31083/j.fbl2707198\n10.3390/app11020672"}
{"title": "COVID-19 Infection Percentage Estimation from Computed Tomography Scans: Results and Insights from the International Per-COVID-19 Challenge.", "abstract": "COVID-19 analysis from medical imaging is an important task that has been intensively studied in the last years due to the spread of the COVID-19 pandemic. In fact, medical imaging has often been used as a complementary or main tool to recognize the infected persons. On the other hand, medical imaging has the ability to provide more details about COVID-19 infection, including its severity and spread, which makes it possible to evaluate the infection and follow-up the patient's state. CT scans are the most informative tool for COVID-19 infection, where the evaluation of COVID-19 infection is usually performed through infection segmentation. However, segmentation is a tedious task that requires much effort and time from expert radiologists. To deal with this limitation, an efficient framework for estimating COVID-19 infection as a regression task is proposed. The goal of the Per-COVID-19 challenge is to test the efficiency of modern deep learning methods on COVID-19 infection percentage estimation (CIPE) from CT scans. Participants had to develop an efficient deep learning approach that can learn from noisy data. In addition, participants had to cope with many challenges, including those related to COVID-19 infection complexity and crossdataset scenarios. This paper provides an overview of the COVID-19 infection percentage estimation challenge (Per-COVID-19) held at MIA-COVID-2022. Details of the competition data, challenges, and evaluation metrics are presented. The best performing approaches and their results are described and discussed.", "journal": "Sensors (Basel, Switzerland)", "date": "2024-03-13", "authors": ["FaresBougourzi", "CosimoDistante", "FadiDornaika", "AbdelmalikTaleb-Ahmed", "AbdenourHadid", "SumanChaudhary", "WantingYang", "YanQiang", "TalhaAnwar", "Mihaela ElenaBreaban", "Chih-ChungHsu", "Shen-ChiehTai", "Shao-NingChen", "DavideTricarico", "Hafiza Ayesha HoorChaudhry", "AttilioFiandrotti", "MarcoGrangetto", "Maria Ausilia NapoliSpatafora", "AlessandroOrtis", "SebastianoBattiato"], "doi": "10.3390/s24051557\n10.7326/M20-1495\n10.3390/s21051742\n10.1038/s41551-021-00704-1\n10.1016/j.media.2022.102605\n10.1016/j.media.2021.102054\n10.1007/978-3-031-13324-4_39\n10.1007/s00521-022-07055-1\n10.1038/s41591-020-0952-y\n10.1109/TMI.2020.3000314\n10.1080/20479700.2020.1810453\n10.1109/TMI.2020.2995965\n10.1016/j.media.2022.102722\n10.3390/healthcare11020213\n10.1002/mp.14676\n10.1109/TMI.2020.2996645\n10.1016/j.media.2021.101992\n10.3390/jimaging7090189\n10.1007/978-3-319-24574-4_28\n10.1016/j.knosys.2020.106647\n10.1007/s11263-015-0816-y\n10.1007/978-3-031-13324-4_44\n10.36227/techrxiv.19497467.v1\n10.1109/CVPRW56347.2022.00309\n10.1109/CVPR42600.2020.01155\n10.1007/978-3-031-13324-4_40\n10.1007/978-3-031-13324-4_45\n10.1007/978-3-031-13324-4_42\n10.1007/978-3-031-13324-4_43"}
{"title": "Quantitative CT Texture Analysis of COVID-19 Hospitalized Patients during 3-24-Month Follow-Up and Correlation with Functional Parameters.", "abstract": "To quantitatively evaluate CT lung abnormalities in COVID-19 survivors from the acute phase to 24-month follow-up. Quantitative CT features as predictors of abnormalities' persistence were investigated.\nPatients who survived COVID-19 were retrospectively enrolled and underwent a chest CT at baseline (T0) and 3 months (T3) after discharge, with pulmonary function tests (PFTs). Patients with residual CT abnormalities repeated the CT at 12 (T12) and 24 (T24) months after discharge. A machine-learning-based software, CALIPER, calculated the CT percentage of the whole lung of normal parenchyma, ground glass (GG), reticulation (Ret), and vascular-related structures (VRSs). Differences (\u0394) were calculated between time points. Receiver operating characteristic (ROC) curve analyses were performed to test the baseline parameters as predictors of functional impairment at T3 and of the persistence of CT abnormalities at T12.\nThe cohort included 128 patients at T0, 133 at T3, 61 at T12, and 34 at T24. The GG medians were 8.44%, 0.14%, 0.13% and 0.12% at T0, T3, T12 and T24. The Ret medians were 2.79% at T0 and 0.14% at the following time points. All \u0394 significantly differed from 0, except between T12 and T24. The GG and VRSs at T0 achieved AUCs of 0.73 as predictors of functional impairment, and area under the curves (AUCs) of 0.71 and 0.72 for the persistence of CT abnormalities at T12.\nCALIPER accurately quantified the CT changes up to the 24-month follow-up. Resolution mostly occurred at T3, and Ret persisting at T12 was almost unchanged at T24. The baseline parameters were good predictors of functional impairment at T3 and of abnormalities' persistence at T12.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2024-03-13", "authors": ["Salvatore ClaudioFanni", "FedericaVolpi", "LeonardoColligiani", "DavideChimera", "MicheleTonerini", "FrancescoPistelli", "RobertaPancani", "ChiaraAiroldi", "Brian JBartholmai", "DaniaCioni", "LauraCarrozzi", "EmanueleNeri", "AnnalisaDe Liperi", "ChiaraRomei", "NoneNone"], "doi": "10.3390/diagnostics14050550\n10.1056/NEJMoa2001017\n10.1038/s41579-020-00459-7\n10.1016/S0140-6736(20)30251-8\n10.1007/s00330-020-06975-7\n10.1183/13993003.00607-2020\n10.2214/AJR.20.23418\n10.1148/radiol.2020204267\n10.1148/radiol.2020200843\n10.1007/s00330-021-08317-7\n10.3390/diagnostics11081317\n10.1007/s15010-020-01474-9\n10.1111/imr.12977\n10.1148/radiol.2021211199\n10.3390/diagnostics13122020\n10.1186/s41747-023-00334-z\n10.3390/diagnostics13162623\n10.1148/ryai.220257\n10.1016/j.ejrad.2020.108852\n10.1183/09031936.00071812\n10.1007/s00330-021-08485-6\n10.1164/rccm.201908-1590ST\n10.1183/09031936.00080312\n10.1183/09041950.005s1693\n10.1177/2047487320932695\n10.1148/radiol.2363040958\n10.1378/chest.128.4.2247\n10.1148/rg.2020200159\n10.1148/radiol.2021211746\n10.1148/radiol.220019\n10.1111/resp.14311\n10.1148/radiol.2021210972\n10.1016/S0140-6736(16)00578-X\n10.1186/1465-9921-6-5\n10.1038/s41413-020-0084-5\n10.1016/j.pulmoe.2020.10.013\n10.1097/RTI.0000000000000220\n10.3390/diagnostics13213328\n10.1371/journal.pone.0257892"}
{"title": "Improving Respiratory Infection Diagnosis with Deep Learning and Combinatorial Fusion: A Two-Stage Approach Using Chest X-ray Imaging.", "abstract": "The challenges of respiratory infections persist as a global health crisis, placing substantial stress on healthcare infrastructures and necessitating ongoing investigation into efficacious treatment modalities. The persistent challenge of respiratory infections, including COVID-19, underscores the critical need for enhanced diagnostic methodologies to support early treatment interventions. This study introduces an innovative two-stage data analytics framework that leverages deep learning algorithms through a strategic combinatorial fusion technique, aimed at refining the accuracy of early-stage diagnosis of such infections. Utilizing a comprehensive dataset compiled from publicly available lung X-ray images, the research employs advanced pre-trained deep learning models to navigate the complexities of disease classification, addressing inherent data imbalances through methodical validation processes. The core contribution of this work lies in its novel application of combinatorial fusion, integrating select models to significantly elevate diagnostic precision. This approach not only showcases the adaptability and strength of deep learning in navigating the intricacies of medical imaging but also marks a significant step forward in the utilization of artificial intelligence to improve outcomes in healthcare diagnostics. The study's findings illuminate the path toward leveraging technological advancements in enhancing diagnostic accuracies, ultimately contributing to the timely and effective treatment of respiratory diseases.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2024-03-13", "authors": ["Cheng-TangPan", "RahulKumar", "Zhi-HongWen", "Chih-HsuanWang", "Chun-YungChang", "Yow-LingShiue"], "doi": "10.3390/diagnostics14050500\n10.3390/life11060561\n10.3390/diagnostics13233507\n10.1002/path.1196\n10.2214/AJR.20.23214\n10.1148/ryct.2020200213\n10.1038/s41598-019-42557-4\n10.1007/s42979-021-00823-1\n10.1016/j.eururo.2019.08.032\n10.1109/TIM.2021.3055802\n10.1109/ICDABI51230.2020.9325626\n10.1016/j.compbiomed.2020.103869\n10.1016/j.asoc.2020.106859\n10.3390/jpm11100993\n10.3390/diagnostics10030165\n10.1371/journal.pmed.1002686\n10.1148/radiol.2017162326\n10.1016/j.patrec.2020.09.010\n10.1007/s10489-020-01829-7\n10.1038/s41598-020-76550-z\n10.1016/j.mehy.2020.109761\n10.1016/j.cmpb.2020.105581\n10.1080/17474124.2021.1840351\n10.1109/ACCESS.2020.3000111\n10.1016/j.media.2021.102299\n10.1007/s11390-020-0679-8\n10.1016/j.cmpb.2020.105532\n10.1038/s41598-021-02731-z\n10.1016/j.neucom.2024.127257\n10.1080/24699322.2019.1649071\n10.31661/jbpe.v0i0.2008-1153\n10.1109/ACCESS.2018.2813079"}
{"title": "Multi-modal deep learning methods for classification of chest diseases using different medical imaging and cough sounds.", "abstract": "Chest disease refers to a wide range of conditions affecting the lungs, such as COVID-19, lung cancer (LC), consolidation lung (COL), and many more. When diagnosing chest disorders medical professionals may be thrown off by the overlapping symptoms (such as fever, cough, sore throat, etc.). Additionally, researchers and medical professionals make use of chest X-rays (CXR), cough sounds, and computed tomography (CT) scans to diagnose chest disorders. The present study aims to classify the nine different conditions of chest disorders, including COVID-19, LC, COL, atelectasis (ATE), tuberculosis (TB), pneumothorax (PNEUTH), edema (EDE), pneumonia (PNEU). Thus, we suggested four novel convolutional neural network (CNN) models that train distinct image-level representations for nine different chest disease classifications by extracting features from images. Furthermore, the proposed CNN employed several new approaches such as a max-pooling layer, batch normalization layers (BANL), dropout, rank-based average pooling (RBAP), and multiple-way data generation (MWDG). The scalogram method is utilized to transform the sounds of coughing into a visual representation. Before beginning to train the model that has been developed, the SMOTE approach is used to calibrate the CXR and CT scans as well as the cough sound images (CSI) of nine different chest disorders. The CXR, CT scan, and CSI used for training and evaluating the proposed model come from 24 publicly available benchmark chest illness datasets. The classification performance of the proposed model is compared with that of seven baseline models, namely Vgg-19, ResNet-101, ResNet-50, DenseNet-121, EfficientNetB0, DenseNet-201, and Inception-V3, in addition to state-of-the-art (SOTA) classifiers. The effectiveness of the proposed model is further demonstrated by the results of the ablation experiments. The proposed model was successful in achieving an accuracy of 99.01%, making it superior to both the baseline models and the SOTA classifiers. As a result, the proposed approach is capable of offering significant support to radiologists and other medical professionals.", "journal": "PloS one", "date": "2024-03-12", "authors": ["HassaanMalik", "TayyabaAnees"], "doi": "10.1371/journal.pone.0296352\n10.1056/NEJMoa2001017\n10.1371/journal.pone.0242958\n10.1056/NEJMp2015897\n10.1016/j.jcct.2020.08.013\n10.1016/j.soc.2016.02.003\n10.3390/biomedicines11020333\n10.1038/s41598-022-22196-y\n10.1016/j.compbiomed.2021.105175\n10.1002/mp.15328\n10.1016/j.bea.2022.100025\n10.1016/j.compbiomed.2021.104572\n10.3390/diagnostics12071527\n10.1016/j.jcct.2020.08.013\n10.1093/cid/ciab611\n10.3390/diagnostics12112823\n10.1007/s00530-021-00878-3\n10.1007/s10489-020-01902-1\n10.1371/journal.pone.0266462\n10.1016/j.cmpb.2019.06.023\n10.3390/s23020743\n10.1016/j.bspc.2022.104499\n10.3390/diagnostics13172772\n10.1007/s11760-023-02589-w\n10.1002/jmv.25902\n10.3390/bioengineering10020203\n10.1109/ACCESS.2021.3058537\n10.1016/j.compbiomed.2020.103869\n10.1016/j.asoc.2020.106691\n10.3390/diagnostics12081850\n10.1038/s41588-021-00962-4\n10.1038/s41576-021-00434-9\n10.1007/s11517-022-02632-x\n10.1155/2022/8549707\n10.1007/s11042-021-11158-7\n10.1016/j.ipm.2022.103025\n10.1109/ACCESS.2020.3025010\n10.1155/2021/8828404\n10.1155/2021/8890226\n10.1007/s00530-021-00826-1\n10.1038/s41591-020-0931-3\n10.1371/journal.pone.0250688\n10.1016/j.chaos.2020.110190\n10.1016/j.bea.2021.100003\n10.1371/journal.pone.0285121\n10.1007/s00530-023-01083-0\n10.1371/journal.pone.0288681\n10.1371/journal.pone.0287755\n10.3390/s21020455\n10.7717/peerj-cs.655\n10.1155/2022/7377502\n10.3389/fpubh.2022.948205\n10.3389/fpubh.2022.1046296\n10.1007/s12652-022-03732-0\n10.1016/j.compbiomed.2021.105020\n10.1016/j.compbiomed.2022.105405\n10.1007/s11517-023-02803-4\n10.3390/ijerph20032035\n10.1016/j.crad.2022.11.006\n10.1016/j.asoc.2022.109851\n10.1007/s11517-022-02632-x\n10.1155/2022/8549707\n10.1016/j.ipm.2022.103025\n10.1007/s11042-021-11158-7\n10.1016/j.compbiomed.2021.105020\n10.1155/2021/8828404\n10.1371/journal.pone.0250688\n10.1016/j.chaos.2020.110190\n10.17632/2FXZ4PX6D8.4\n10.17632/y6dvssx73b.1\n10.1001/jamanetworkopen.2019.1095\n10.1016/j.cell.2018.02.010\n10.1016/j.fcij.2017.12.001\n10.1109/ACCESS.2020.3031384\n10.1016/j.bspc.2021.102588\n10.1016/j.bspc.2021.102588\n10.17632/3y55vgckg6.1\n10.2196/27468\n10.1148/ryai.2021200254\n10.1016/j.compbiomed.2021.104572\n10.1016/j.compbiomed.2021.105153\n10.1088/1361-6579/ab03ea\n10.3390/s19040935\n10.1016/j.inffus.2020.01.001\n10.1016/j.inffus.2020.07.006\n10.1007/s40747-020-00218-4\n10.3390/diagnostics13172772\n10.1148/radiol.2020200905\n10.1016/j.clinimag.2020.04.001\n10.1111/1754-9485.13273"}
{"title": "COVID-19 detection in lung CT slices using Brownian-butterfly-algorithm optimized lightweight deep features.", "abstract": "Several deep-learning assisted disease assessment schemes (DAS) have been proposed to enhance accurate detection of COVID-19, a critical medical emergency, through the analysis of clinical data. Lung imaging, particularly from CT scans, plays a pivotal role in identifying and assessing the severity of COVID-19 infections. Existing automated methods leveraging deep learning contribute significantly to reducing the diagnostic burden associated with this process. This research aims in developing a simple DAS for COVID-19 detection using the pre-trained lightweight deep learning methods (LDMs) applied to lung CT slices. The use of LDMs contributes to a less complex yet highly accurate detection system. The key stages of the developed DAS include image collection and initial processing using Shannon's thresholding, deep-feature mining supported by LDMs, feature optimization utilizing the Brownian Butterfly Algorithm (BBA), and binary classification through three-fold cross-validation. The performance evaluation of the proposed scheme involves assessing individual, fused, and ensemble features. The investigation reveals that the developed DAS achieves a detection accuracy of 93.80% with individual features, 96% accuracy with fused features, and an impressive 99.10% accuracy with ensemble features. These outcomes affirm the effectiveness of the proposed scheme in significantly enhancing COVID-19 detection accuracy in the chosen lung CT database.", "journal": "Heliyon", "date": "2024-03-12", "authors": ["VenkatesanRajinikanth", "RoshimaBiju", "NitinMittal", "VikasMittal", "S SAskar", "MohamedAbouhawwash"], "doi": "10.1016/j.heliyon.2024.e27509"}
{"title": "Trends and Hotspots in Global Radiomics Research: A Bibliometric Analysis.", "abstract": "", "journal": "Technology in cancer research & treatment", "date": "2024-03-11", "authors": ["MinghuiZhang", "YanWang", "MutianLv", "LiSang", "XuemeiWang", "ZijunYu", "ZiyiYang", "ZhongqingWang", "LiangSang"], "doi": "10.1177/15330338241235769"}
{"title": "A hybridized feature extraction for COVID-19 multi-class classification on computed tomography images.", "abstract": "COVID-19 has killed more than 5 million individuals worldwide within a short time. It is caused by SARS-CoV-2 which continuously mutates and produces more transmissible new different strains. It is therefore of great significance to diagnose COVID-19 early to curb its spread and reduce the death rate. Owing to the COVID-19 pandemic, traditional diagnostic methods such as reverse-transcription polymerase chain reaction (RT-PCR) are ineffective for diagnosis. Medical imaging is among the most effective techniques of respiratory disorders detection through machine learning and deep learning. However, conventional machine learning methods depend on extracted and engineered features, whereby the optimum features influence the classifier's performance. In this study, Histogram of Oriented Gradient (HOG) and eight deep learning models were utilized for feature extraction while K-Nearest Neighbour (KNN) and Support Vector Machines (SVM) were used for classification. A combined feature of HOG and deep learning feature was proposed to improve the performance of the classifiers. VGG-16 + HOG achieved 99.4 overall accuracy with SVM. This indicates that our proposed concatenated feature can enhance the SVM classifier's performance in COVID-19 detection.", "journal": "Heliyon", "date": "2024-03-11", "authors": ["HassanaAbubakar", "FadiAl-Turjman", "Zubaida SAmeen", "Auwalu SMubarak", "ChadiAltrjman"], "doi": "10.1016/j.heliyon.2024.e26939\n10.1016/j.ajem.2020.03.036\n10.15212/bioi-2020-0038\n10.21037/jtd.2020.03.62\n10.1016/j.ijantimicag.2020.105955\n10.1021/acsnano.0c02624\n10.1016/j.media.2012.02.005\n10.3390/diagnostics10080565\n10.1515/bmt-2021-0310\n10.5220/0005266303950402\n10.1007/978-3-031-05744-1_25\n10.1371/journal.pone.0083554\n10.1007/s10044-020-00950-0\n10.1016/j.asoc.2021.107323\n10.1038/s41598-022-25539-x\n10.1016/j.chemolab.2022.104539\n10.1016/j.compbiomed.2022.105806\n10.1007/s00521-023-08604-y\n10.1007/s00530-021-00826-1\n10.1002/ima.22914\n10.1111/exsy.12842\n10.1016/j.imu.2022.101158\n10.1007/s00521-020-05437-x\n10.1109/ICICOS.2018.8621687\n10.1364/BOE.5.003568\n10.1109/ACCESS.2017.2784842\n10.48550/arXiv.2105.12564\n10.1007/978-1-4842-2845-6\n10.1007/s10278-016-9929-2\n10.1007/978-3-319-46976-8_20\n10.3390/info8030091\n10.1109/TMI.2016.2535302"}
{"title": "Automated Spontaneous Echo Contrast Detection Using a Multisequence Attention Convolutional Neural Network.", "abstract": "Spontaneous echo contrast (SEC) is a vascular ultrasound finding associated with increased thromboembolism risk. However, identification requires expert determination and clinician time to report. We developed a deep learning model that can automatically identify SEC. Our model can be applied retrospectively without deviating from routine clinical practice. The retrospective nature of our model means future works could scan archival data to opportunistically correlate SEC findings with documented clinical outcomes.\nWe curated a data set of 801 archival acquisitions along the femoral vein from 201 patients. We used a multisequence convolutional neural network (CNN) with ResNetv2 backbone and visualized keyframe importance using soft attention. We evaluated SEC prediction performance using an 80/20 train/test split. We report receiver operating characteristic area under the curve (ROC-AUC), along with the Youden threshold-associated sensitivity, specificity, F1 score, true negative, false negative, false positive and true positive.\nUsing soft attention, we can identify SEC with an AUC of 0.74, sensitivity of 0.73 and specificity of 0.68. Without soft attention, our model achieves an AUC of 0.69, sensitivity of 0.71 and specificity of 0.60. Additionally, we provide attention visualizations and note that our model assigns higher attention score to ultrasound frames containing more vessel lumen.\nOur multisequence CNN model can identify the presence of SEC from ultrasound keyframes with an AUC of 0.74, which could enable screening applications and enable more SEC data discovery. The model does not require the expert intervention or additional clinician reporting time that are currently significant barriers to SEC adoption. Model and processed data sets are publicly available at https://github.com/Ouwen/automatic-spontaneous-echo-contrast.", "journal": "Ultrasound in medicine & biology", "date": "2024-03-10", "authors": ["OuwenHuang", "ZeweiShi", "NaveenGarg", "CoreyJensen", "Mark LPalmeri"], "doi": "10.1016/j.ultrasmedbio.2024.01.016"}
{"title": "Evaluation of Effectiveness of Self-Supervised Learning in Chest X-Ray Imaging to Reduce Annotated Images.", "abstract": "A significant challenge in machine learning-based medical image analysis is the scarcity of medical images. Obtaining a large number of labeled medical images is difficult because annotating medical images is a time-consuming process that requires specialized knowledge. In addition, inappropriate annotation processes can increase model bias. Self-supervised learning (SSL) is a type of unsupervised learning method that extracts image representations. Thus, SSL can be an effective method to reduce the number of labeled images. In this study, we investigated the feasibility of reducing the number of labeled images in a limited set of unlabeled medical images. The unlabeled chest X-ray (CXR) images were pretrained using the SimCLR framework, and then the representations were fine-tuned as supervised learning for the target task. A total of 2000 task-specific CXR images were used to perform binary classification of coronavirus disease 2019 (COVID-19) and normal cases. The results demonstrate that the performance of pretraining on task-specific unlabeled CXR images can be maintained when the number of labeled CXR images is reduced by approximately 40%. In addition, the performance was significantly better than that obtained without pretraining. In contrast, a large number of pretrained unlabeled images are required to maintain performance regardless of task specificity among a small number of labeled CXR images. In summary, to reduce the number of labeled images using SimCLR, we must consider both the number of images and the task-specific characteristics of the target images.", "journal": "Journal of imaging informatics in medicine", "date": "2024-03-09", "authors": ["KunikiImagawa", "KoheiShiomoto"], "doi": "10.1007/s10278-024-00975-5\n10.7717/peerj-cs.1045\n10.1007/s11548-022-02813-x\n10.1016/j.compbiomed.2022.105251\n10.1609/aaai.v33i01.3301590"}
{"title": "Comparison of the Discrimination Performance of AI Scoring and the Brixia Score in Predicting COVID-19 Severity on Chest X-Ray Imaging: Diagnostic Accuracy Study.", "abstract": "The artificial intelligence (AI) analysis of chest x-rays can increase the precision of binary COVID-19 diagnosis. However, it is unknown if AI-based chest x-rays can predict who will develop severe COVID-19, especially in low- and middle-income countries.\nThe study aims to compare the performance of human radiologist Brixia scores versus 2 AI scoring systems in predicting the severity of COVID-19 pneumonia.\nWe performed a cross-sectional study of 300 patients suspected with and with confirmed COVID-19 infection in Jakarta, Indonesia. A total of 2 AI scores were generated using CAD4COVID x-ray software.\nThe AI probability score had slightly lower discrimination (area under the curve [AUC] 0.787, 95% CI 0.722-0.852). The AI score for the affected lung area (AUC 0.857, 95% CI 0.809-0.905) was almost as good as the human Brixia score (AUC 0.863, 95% CI 0.818-0.908).\nThe AI score for the affected lung area and the human radiologist Brixia score had similar and good discrimination performance in predicting COVID-19 severity. Our study demonstrated that using AI-based diagnostic tools is possible, even in low-resource settings. However, before it is widely adopted in daily practice, more studies with a larger scale and that are prospective in nature are needed to confirm our findings.", "journal": "JMIR formative research", "date": "2024-03-07", "authors": ["Eric DanielTenda", "Reyhan EddyYunus", "BennyZulkarnaen", "Muhammad ReynalziYugo", "Ceva WicaksonoPitoyo", "Moses MazmurAsaf", "Tiara NurIslamiyati", "AriertaPujitresnani", "AndrySetiadharma", "JoshuaHenrina", "Cleopas MartinRumende", "VallyWulani", "KuntjoroHarimurti", "AidaLydia", "HamzahShatri", "PradanaSoewondo", "Prasandhya AstagiriYusuf"], "doi": "10.2196/46817\n10.1148/radiol.2020200432\n10.1128/JCM.00297-20\n10.1148/radiol.2020200463\n10.1007/s11547-020-01200-3\n10.32007/jfacmedbagdad.6421875\n10.1016/S2589-7500(21)00116-3\n10.1038/s41568-018-0016-5\n10.1016/j.compbiomed.2021.105002\n10.1148/radiol.2020201874\n10.1148/radiol.2020203173\n10.1111/eci.13706\n10.1136/bmjgast-2020-000590\n10.2337/dc22-S002\n10.1001/jama.2013.284427\n10.1164/rccm.202009-3533SO\n10.1183/13993003.02730-2021\n10.1001/jama.2012.5669\n10.12659/MSM.931277\n10.1186/s41747-020-00195-w\n10.1259/bjr.20211028\n10.1080/14737159.2020.1757437\n10.1016/j.bbrc.2020.10.069\n10.11613/BM.2020.030401\n10.1016/j.cmi.2020.06.019\n10.1371/journal.pone.0236311\n10.3390/microorganisms8071064\n10.5455/jpma.08\n10.1007/s00330-021-07715-1\n10.1186/s12879-022-07617-7\n10.1186/s12879-022-07617-7\n10.18280/ts.370313\n10.18280/ts.370313\n10.1007/s00354-022-00172-4\n10.1371/journal.pone.0252440\n10.1371/journal.pone.0252440\n10.3389/fmed.2022.930055\n10.5114/pjr.2021.108172\n10.5114/pjr.2021.108172"}
{"title": "CODENET: A deep learning model for COVID-19 detection.", "abstract": "Conventional COVID-19 testing methods have some flaws: they are expensive and time-consuming. Chest X-ray (CXR) diagnostic approaches can alleviate these flaws to some extent. However, there is no accurate and practical automatic diagnostic framework with good interpretability. The application of artificial intelligence (AI) technology to medical radiography can help to accurately detect the disease, reduce the burden on healthcare organizations, and provide good interpretability. Therefore, this study proposes a new deep neural network (CNN) based on CXR for COVID-19 diagnosis - CodeNet. This method uses contrastive learning to make full use of latent image data to enhance the model's ability to extract features and generalize across different data domains. On the evaluation dataset, the proposed method achieves an accuracy as high as 94.20%, outperforming several other existing methods used for comparison. Ablation studies validate the efficacy of the proposed method, while interpretability analysis shows that the method can effectively guide clinical professionals. This work demonstrates the superior detection performance of a CNN using contrastive learning techniques on CXR images, paving the way for computer vision and artificial intelligence technologies to leverage massive medical data for disease diagnosis.", "journal": "Computers in biology and medicine", "date": "2024-03-07", "authors": ["HongJu", "YanyanCui", "QiaosenSu", "LiranJuan", "BalachandranManavalan"], "doi": "10.1016/j.compbiomed.2024.108229"}
{"title": "Detection of COVID-19, pneumonia, and tuberculosis from radiographs using AI-driven knowledge distillation.", "abstract": "Chest radiography is an essential diagnostic tool for respiratory diseases such as COVID-19, pneumonia, and tuberculosis because it accurately depicts the structures of the chest. However, accurate detection of these diseases from radiographs is a complex task that requires the availability of medical imaging equipment and trained personnel. Conventional deep learning models offer a viable automated solution for this task. However, the high complexity of these models often poses a significant obstacle to their practical deployment within automated medical applications, including mobile apps, web apps, and cloud-based platforms. This study addresses and resolves this dilemma by reducing the complexity of neural networks using knowledge distillation techniques (KDT). The proposed technique trains a neural network on an extensive collection of chest X-ray images and propagates the knowledge to a smaller network capable of real-time detection. To create a comprehensive dataset, we have integrated three popular chest radiograph datasets with chest radiographs for COVID-19, pneumonia, and tuberculosis. Our experiments show that this knowledge distillation approach outperforms conventional deep learning methods in terms of computational complexity and performance for real-time respiratory disease detection. Specifically, our system achieves an impressive average accuracy of 0.97, precision of 0.94, and recall of 0.97.", "journal": "Heliyon", "date": "2024-03-06", "authors": ["Md MohsinKabir", "M FMridha", "AshifurRahman", "Md AbdulHamid", "Muhammad MostafaMonowar"], "doi": "10.1016/j.heliyon.2024.e26801"}
{"title": "Neuroimaging data repositories and AI-driven healthcare-Global aspirations vs. ethical considerations in machine learning models of neurological disease.", "abstract": "Neuroimaging data repositories are data-rich resources comprising brain imaging with clinical and biomarker data. The potential for such repositories to transform healthcare is tremendous, especially in their capacity to support machine learning (ML) and artificial intelligence (AI) tools. Current discussions about the generalizability of such tools in healthcare provoke concerns of risk of bias-ML models underperform in women and ethnic and racial minorities. The use of ML may exacerbate existing healthcare disparities or cause post-deployment harms. Do neuroimaging data repositories and their capacity to support ML/AI-driven clinical discoveries, have both the potential to accelerate innovative medicine and harden the gaps of social inequities in neuroscience-related healthcare? In this paper, we examined the ethical concerns of ML-driven modeling of global community neuroscience needs arising from the use of data amassed within neuroimaging data repositories. We explored this in two parts; firstly, in a theoretical experiment, we argued for a South East Asian-based repository to redress global imbalances. Within this context, we then considered the ethical framework toward the inclusion vs. exclusion of the migrant worker population, a group subject to healthcare inequities. Secondly, we created a model simulating the impact of global variations in the presentation of anosmia risks in COVID-19 toward altering brain structural findings; we then performed a mini AI ethics experiment. In this experiment, we interrogated an actual pilot dataset (", "journal": "Frontiers in artificial intelligence", "date": "2024-03-05", "authors": ["ChristineLock", "Nicole Si MinTan", "Ian JamesLong", "Nicole CKeong"], "doi": "10.3389/frai.2023.1286266\n10.1016/j.neuroimage.2018.07.066\n10.1371/journal.pbio.3000344\n10.1038/s41597-019-0073-y\n10.1007/s42844-020-00020-8\n10.1038/s41597-021-00823-z\n10.1038/35066094\n10.1093/brain/awaa025\n10.1146/annurev-biodatasci-092820-114757\n10.1089/jwh.2013.4617\n10.1006/nimg.1998.0395\n10.1162/jocn.1993.5.2.162\n10.1016/j.neuroimage.2006.01.021\n10.1038/mp.2013.78\n10.1098/rspb.2010.0973\n10.1073/pnas.200033797\n10.1109/42.906426\n10.1016/S0896-6273(02)00569-X\n10.1016/j.neuroimage.2004.07.016\n10.1006/nimg.1998.0396\n10.1002/(SICI)1097-0193(1999)8:4<272::AID-HBM10>3.0.CO;2-4\n10.1093/cercor/bhg087\n10.1016/j.neuroimage.2018.02.017\n10.1016/j.neuroimage.2009.06.060\n10.1002/alz.12178\n10.1016/j.neuroimage.2006.02.051\n10.3390/ijms231911076\n10.1016/j.neuron.2018.12.021\n10.1006/nimg.2002.1132\n10.1016/j.neuroimage.2011.09.015\n10.1016/S1361-8415(01)00036-6\n10.1016/j.neuroimage.2005.09.046\n10.3389/fneur.2022.868026\n10.1038/nrg1505\n10.3389/fmed.2021.661359\n10.1001/archpsyc.60.9.878\n10.1007/s41649-017-0030-z\n10.1007/s00405-020-05965-1\n10.3389/fmed.2018.00357\n10.1038/nrg2360\n10.1007/s42844-020-00025-3\n10.1212/WNL.0b013e3181cb3e25\n10.1038/s41597-023-01946-1\n10.1093/medlaw/fws040\n10.1007/s41649-020-00138-y\n10.1016/j.neuroimage.2011.02.076\n10.1016/j.neuroimage.2010.07.020\n10.1016/j.neuroimage.2012.02.084\n10.1212/WNL.58.5.695\n10.1093/cercor/bhh032\n10.1016/j.neuroimage.2004.03.032\n10.1109/TMI.2006.887364\n10.1109/42.668698\n10.1016/j.neuroimage.2012.06.005\n10.1016/j.neuroimage.2004.07.051\n10.3389/fnins.2021.751145\n10.1371/journal.pmed.1001779\n10.1371/journal.pcbi.1004692\n10.1016/j.neuroimage.2019.116137\n10.1021/acschemneuro.0c00460\n10.1073/pnas.2211613120\n10.1016/j.neuroimage.2008.10.055\n10.1007/s41649-019-00099-x"}
{"title": "The fear of COVID-19 contagion: an exploratory EEG-fMRI study.", "abstract": "Pandemics have the potential to change how people behave and feel. The COVID-19 pandemic is no exception; thus, it may serve as a \"challenging context\" for understanding how pandemics affect people's minds. In this study, we used high-density electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) to examine the neural correlates of fear of contagion during the most critical moments of COVID-19 in Italy (i.e., October 2020-May 2021). To do that, we stimulated participants (N\u2009=\u200917; nine females) with artificial-intelligence-generated faces of people presented as healthy, recovered from COVID-19, or infected by SARS-CoV-2. The fMRI results documented a modulation of large bilateral fronto-temporo-parietal functional brain networks. Critically, we found selective recruitment of cortical (e.g., frontal lobes) and subcortical fear-related structures (e.g., amygdala and putamen) of the so-called social brain network when participants observed COVID-19-related faces. Consistently, EEG results showed distinct patterns of brain activity selectively associated with infected and recovered faces (e.g., delta and gamma rhythm). Together, these results highlight how pandemic contexts may reverberate in the human brain, thus influencing most basic social and cognitive functioning. This may explain the emergence of a cluster of psychopathologies during and after the COVID-19 pandemic. Therefore, this study underscores the need for prompt interventions to address pandemics' short- and long-term consequences on mental health.", "journal": "Scientific reports", "date": "2024-03-05", "authors": ["GiovanniFederico", "GiuseppinaCiccarelli", "GiuseppeNoce", "CarloCavaliere", "Ciro RosarioIlardi", "LiberatoreTramontano", "VincenzoAlfano", "GiuliaMele", "AngelicaDi Cecca", "MarcoSalvatore", "Maria AntonellaBrandimonte"], "doi": "10.1038/s41598-024-56014-4\n10.1016/j.bj.2020.04.007\n10.1016/S0140-6736(20)30460-8\n10.1056/NEJMp2008017\n10.1016/j.bbi.2020.05.048\n10.1016/j.tics.2005.12.004\n10.1002/da.23071\n10.1016/j.cbpra.2021.03.003\n10.1023/A:1025048802629\n10.31887/DCNS.2002.4.3/tsteimer\n10.1016/j.euroneuro.2012.07.008\n10.1016/j.tics.2016.03.011\n10.1016/j.tics.2005.03.010\n10.1007/s11469-020-00334-9\n10.7717/peerj.11380\n10.1016/j.jad.2021.11.031\n10.1146/annurev.psych.60.110707.163514\n10.1093/brain/122.4.779\n10.1017/S0140525X11000446\n10.1111/1475-3588.00047\n10.1098/rsta.2011.0081\n10.1111/j.1467-8721.2008.00566.x\n10.1016/j.neuroimage.2010.10.011\n10.1098/rspb.1998.0522\n10.1016/j.neuroimage.2004.07.060\n10.1016/j.neuropsychologia.2006.04.015\n10.1016/S1364-6613(00)01482-0\n10.1523/JNEUROSCI.17-11-04302.1997\n10.1016/S1388-2457(01)00654-X\n10.1371/journal.pone.0021714\n10.1016/S1388-2457(99)00151-0\n10.1038/sj.mp.4001469\n10.1016/j.neuropsychologia.2006.04.018\n10.1176/appi.ajp.2019.18111271\n10.1523/JNEUROSCI.2153-07.2007\n10.1111/1469-8986.3950641\n10.1016/j.biopsycho.2007.11.006\n10.1016/S0167-8760(96)00053-0\n10.1523/JNEUROSCI.0875-06.2006\n10.1162/jocn.1996.8.6.551\n10.1016/j.neuroimage.2007.07.011\n10.1167/15.1.18\n10.1093/brain/awg241\n10.1016/S0140-6736(22)00874-1\n10.1016/j.neuroimage.2012.03.068\n10.3389/fnins.2023.1130025\n10.1155/2011/879716\n10.1016/0167-8760(84)90014-X\n10.1016/j.jclinepi.2014.03.012"}
{"title": "A fully automatic parenchyma extraction method for MRI T2* relaxometry of iron loaded liver in transfusion-dependent patients.", "abstract": "To develop a fully automatic parenchyma extraction method for the T2* relaxometry of iron overload liver.\nA retrospective multicenter collection of liver MR examinations from 177 transfusion-dependent patients was conducted. The proposed method extended a semiautomatic parenchyma extraction algorithm to a fully automatic approach by introducing a modified TransUNet on the R2* (1/T2*) map for liver segmentation. Axial liver slices from 129 patients at 1.5\u00a0T were allocated to training (85%) and internal test (15%) sets. Two external test sets separately included 1.5\u00a0T data from 20 patients and 3.0\u00a0T data from 28 patients. The final T2* measurement was obtained by fitting the average signal of the extracted liver parenchyma. The agreement between T2* measurements using fully and semiautomatic parenchyma extraction methods was assessed using coefficient of variation (CoV) and Bland-Altman plots.\nDice of the deep network-based liver segmentation was 0.970\u00a0\u00b1\u00a00.019 on the internal dataset, 0.960\u00a0\u00b1\u00a00.035 on the external 1.5\u00a0T dataset, and 0.958\u00a0\u00b1\u00a00.014 on the external 3.0\u00a0T dataset. The mean difference bias between T2* measurements of the fully and semiautomatic methods were separately 0.12 (95% CI: -0.37, 0.61) ms, 0.04 (95% CI: -1.0, 1.1) ms, and 0.01 (95% CI: -0.25, 0.23) ms on the three test datasets. The CoVs between the two methods were 4.2%, 4.8% and 2.0% on the internal test set and two external test sets.\nThe developed fully automatic parenchyma extraction approach provides an efficient and operator-independent T2* measurement for assessing hepatic iron content in clinical practice.", "journal": "Magnetic resonance imaging", "date": "2024-03-03", "authors": ["ZifengLian", "QiqiLu", "BingquanLin", "LingjianChen", "JianGong", "QiugenHu", "HuafengWang", "YanqiuFeng"], "doi": "10.1016/j.mri.2024.02.017"}
{"title": "Empirical data drift detection experiments on real-world medical imaging data.", "abstract": "While it is common to monitor deployed clinical artificial intelligence (AI) models for performance degradation, it is less common for the input data to be monitored for data drift - systemic changes to input distributions. However, when real-time evaluation may not be practical (eg., labeling costs) or when gold-labels are automatically generated, we argue that tracking data drift becomes a vital addition for AI deployments. In this work, we perform empirical experiments on real-world medical imaging to evaluate three data drift detection methods' ability to detect data drift caused (a) naturally (emergence of COVID-19 in X-rays) and (b) synthetically. We find that monitoring performance alone is not a good proxy for detecting data drift and that drift-detection heavily depends on sample size and patient features. Our work discusses the need and utility of data drift detection in various scenarios and highlights gaps in knowledge for the practical application of existing methods.", "journal": "Nature communications", "date": "2024-03-01", "authors": ["AliKore", "ElyarAbbasi Bavil", "VallijahSubasri", "MoustafaAbdalla", "BenjaminFine", "ElhamDolatabadi", "MohamedAbdalla"], "doi": "10.1038/s41467-024-46142-w\n10.1148/radiol.220182\n10.1016/S2589-7500(20)30219-3\n10.1007/s00330-020-07684-x\n10.1001/jama.2019.21579\n10.1038/s41598-021-02481-y\n10.1093/ejcts/ezs584\n10.1007/s00134-011-2390-2\n10.1259/bjr.20220878\n10.1016/j.ijmedinf.2022.104930\n10.1038/oby.2008.636\n10.1001/jamanetworkopen.2021.17052\n10.1056/NEJMc2104626\n10.1016/j.clinimag.2020.04.001\n10.1136/bmj.m441\n10.1038/s41746-022-00611-y\n10.1093/jamia/ocac078\n10.1038/s41563-019-0345-0\n10.1001/jamanetworkopen.2021.7063\n10.1001/jamanetworkopen.2022.7958\n10.1177/1740774506073464\n10.1038/s41467-022-34646-2\n10.1371/journal.pone.0262838\n10.1016/j.media.2020.101797"}
{"title": "Advancing Rheumatology Care Through Machine Learning.", "abstract": "Rheumatologic diseases are marked by their complexity, involving immune-, metabolic- and mechanically mediated processes which can affect different organ systems. Despite a growing arsenal of targeted medications, many rheumatology patients fail to achieve full remission. Assessing disease activity remains challenging, as patients prioritize different symptoms and disease phenotypes vary. This is also reflected in clinical trials where the efficacy of drugs is not necessarily measured in an optimal way with the traditional outcome assessment. The recent COVID-19 pandemic has catalyzed a digital transformation in healthcare, embracing telemonitoring and patient-reported data via apps and wearables. As a further driver of digital medicine, electronic medical record (EMR) providers are actively engaged in developing algorithms for clinical decision support, heralding a shift towards patient-centered, decentralized care. Machine learning algorithms have emerged as valuable tools for handling the increasing volume of patient data, promising to enhance treatment quality and patient well-being. Convolutional neural networks (CNN) are particularly promising for radiological image analysis, aiding in the detection of specific lesions such as erosions, sacroiliitis, or osteoarthritis, with several FDA-approved applications. Clinical predictions, including numerical disease activity forecasts and medication choices, offer the potential to optimize treatment strategies. Numeric predictions can be integrated into clinical workflows, allowing for shared decision making with patients. Clustering patients based on disease characteristics provides a personalized care approach. Digital biomarkers, such as patient-reported outcomes and wearables data, offer insights into disease progression and therapy response more flexibly and outside patient consultations. In association with patient-reported outcomes, disease-specific digital biomarkers via image recognition or single-camera motion capture enables more efficient remote patient monitoring.\u00a0Digital biomarkers may also play a major role in clinical trials in the future as continuous, disease-specific outcome measurement facilitating decentralized studies. Prediction models can help with patient selection in clinical trials, such as by predicting high disease activity.\u00a0Efforts are underway to integrate these advancements into clinical workflows using digital pathways and remote patient monitoring platforms.\u00a0In summary, machine learning, digital biomarkers, and advanced imaging technologies hold immense promise for enhancing clinical decision support and clinical trials in rheumatology. Effective integration will require a multidisciplinary approach and continued validation through prospective studies.", "journal": "Pharmaceutical medicine", "date": "2024-02-29", "authors": ["ThomasH\u00fcgle"], "doi": "10.1007/s40290-024-00515-0\n10.1007/s11926-023-01121-w\n10.1038/s41591-022-01789-0\n10.1136/annrheumdis-2022-222586\n10.1186/s13075-023-03120-9\n10.1136/annrheumdis-2016-210715\n10.3899/jrheum.210990\n10.1136/annrheumdis-2018-213202\n10.2196/26323\n10.1186/s12913-022-08787-5\n10.1136/ard-2022-222626\n10.1136/annrheumdis-2022-222141\n10.1016/j.jbspin.2022.105436\n10.3389/fimmu.2023.1278247\n10.1016/s2589-7500(19)30108-6\n10.1186/s13075-021-02634-4\n10.1016/j.bbe.2021.11.004\n10.21037/qims-23-542\n10.3389/fmed.2022.1069486\n10.1093/rheumatology/keac645\n10.1371/journal.pcbi.1011073\n10.1136/annrheumdis-2021-221763\n10.1002/acr.23768\n10.1016/j.physio.2018.04.005\n10.1093/rheumatology/keaa015\n10.1038/s41598-022-22514-4\n10.1016/j.mayocp.2023.02.003\n10.1007/s12553-023-00738-2\n10.1016/j.heliyon.2023.e17575\n10.1007/978-1-0716-1787-8_22"}
{"title": "PulmoNet: a novel deep learning based pulmonary diseases detection model.", "abstract": "Pulmonary diseases are various pathological conditions that affect respiratory tissues and organs, making the exchange of gas challenging for animals inhaling and exhaling. It varies from gentle and self-limiting such as the common cold and catarrh, to life-threatening ones, such as viral pneumonia (VP), bacterial pneumonia (BP), and tuberculosis, as well as a severe acute respiratory syndrome, such as the coronavirus 2019 (COVID-19). The cost of diagnosis and treatment of pulmonary infections is on the high side, most especially in developing countries, and since radiography images (X-ray and computed tomography (CT) scan images) have proven beneficial in detecting various pulmonary infections, many machine learning (ML) models and image processing procedures have been utilized to identify these infections. The need for timely and accurate detection can be lifesaving, especially during a pandemic. This paper, therefore, suggested a deep convolutional neural network (DCNN) founded image detection model, optimized with image augmentation technique, to detect three (3) different pulmonary diseases (COVID-19, bacterial pneumonia, and viral pneumonia). The dataset containing four (4) different classes (healthy (10,325), COVID-19 (3,749), BP (883), and VP (1,478)) was utilized as training/testing data for the suggested model. The model's performance indicates high potential in detecting the three (3) classes of pulmonary diseases. The model recorded average detection accuracy of 94%, 95.4%, 99.4%, and 98.30%, and training/detection time of about 60/50\u00a0s. This result indicates the proficiency of the suggested approach when likened to the traditional texture descriptors technique of pulmonary disease recognition utilizing X-ray and CT scan images. This study introduces an innovative deep convolutional neural network model to enhance the detection of pulmonary diseases like COVID-19 and pneumonia using radiography. This model, notable for its accuracy and efficiency, promises significant advancements in medical diagnostics, particularly beneficial in developing countries due to its potential to surpass traditional diagnostic methods.", "journal": "BMC medical imaging", "date": "2024-02-29", "authors": ["AbdulRahman ToshoAbdulahi", "Roseline OluwaseunOgundokun", "Ajiboye RaimotAdenike", "Mohd AsifShah", "Yusuf KolaAhmed"], "doi": "10.1186/s12880-024-01227-2\n10.1016/j.ymssp.2022.109727\n10.1016/j.chaos.2020.110170\n10.1007/s00607-022-01057-6\n10.1016/j.compbiomed.2021.104348\n10.1016/J.COMPBIOMED.2017.04.006\n10.14336/AD.2021.1023\n10.1016/J.COMPBIOMED.2018.10.011\n10.1111/jonm.13362\n10.1002/int.22844\n10.1093/ANNONC/MDY19\n10.1016/S1473-3099(20)30134-1\n10.1148/RADIOL.2020200370\n10.1148/RYCT.2020200028\n10.1148/RADIOL.2020200490\n10.2214/AJR.20.22976\n10.3348/KJR.2020.0132\n10.1007/BF00344251\n10.1016/j.transproceed.2023.03.042\n10.1186/s13018-022-03322-y\n10.4018/IJEHMC.20220701.oa4\n10.7150/jca.26356\n10.1016/S0893-6080(03)00115-1\n10.1002/jmv.25678\n10.1007/S00330-021-07715-1\n10.1007/s10044-021-00984-y\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105581\n10.1016/J.CHAOS.2020.1104"}
{"title": "Finding the Pieces to Treat the Whole: Using Radiomics to Identify Tumor Habitats.", "abstract": null, "journal": "Radiology. Artificial intelligence", "date": "2024-02-28", "authors": ["HershSagreiya"], "doi": "10.1148/ryai.230547"}
{"title": "Automated machine learning for the identification of asymptomatic COVID-19 carriers based on chest CT images.", "abstract": "Asymptomatic COVID-19 carriers with normal chest computed tomography (CT) scans have perpetuated the ongoing pandemic of this disease. This retrospective study aimed to use automated machine learning (AutoML) to develop a prediction model based on CT characteristics for the identification of asymptomatic carriers.\nAsymptomatic carriers were from Yangzhou Third People's Hospital from August 1st, 2020, to March 31st, 2021, and the control group included a healthy population from a nonepizootic area with two negative RT\u2012PCR results within 48\u00a0h. All CT images were preprocessed using MATLAB. Model development and validation were conducted in R with the H2O package. The models were built based on six algorithms, e.g., random forest and deep neural network (DNN), and a training set (n\u2009=\u2009691). The models were improved by automatically adjusting hyperparameters for an internal validation set (n\u2009=\u2009306). The performance of the obtained models was evaluated based on a dataset from Suzhou (n\u2009=\u2009178) using the area under the curve (AUC), accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV) and F1 score.\nA total of 1,175 images were preprocessed with high stability. Six models were developed, and the performance of the DNN model ranked first, with an AUC value of 0.898 for the test set. The sensitivity, specificity, PPV, NPV, F1 score and accuracy of the DNN model were 0.820, 0.854, 0.849, 0.826, 0.834 and 0.837, respectively. A plot of a local interpretable model-agnostic explanation demonstrated how different variables worked in identifying asymptomatic carriers.\nOur study demonstrates that AutoML models based on CT images can be used to identify asymptomatic carriers. The most promising model for clinical implementation is the DNN-algorithm-based model.", "journal": "BMC medical imaging", "date": "2024-02-28", "authors": ["MinyueYin", "ChaoXu", "JinzhouZhu", "YuhanXue", "YijiaZhou", "YuHe", "JiaxiLin", "LuLiu", "JingwenGao", "XiaolinLiu", "DanShen", "CuipingFu"], "doi": "10.1186/s12880-024-01211-w\n10.1016/j.ejrad.2020.108961\n10.2214/AJR.20.22975\n10.1001/jama.2020.0757\n10.1056/NEJMoa2001316\n10.1001/jama.2020.1585\n10.1016/j.jmii.2020.05.001\n10.1016/j.artmed.2019.101752\n10.1016/j.drudis.2018.11.014\n10.2196/24018\n10.1038/s42256-020-0180-7\n10.1016/j.bspc.2023.104642\n10.1038/s41467-020-17971-2\n10.1016/j.compbiomed.2020.103792\n10.1016/j.ebiom.2019.10.057\n10.3390/diagnostics13101691\n10.3390/diagnostics13101696\n10.1016/j.pbiomolbio.2023.02.003\n10.1093/qjmed/hcab172\n10.1016/j.chaos.2020.110153\n10.3310/UDIR6682\n10.1038/s41598-023-29058-1\n10.1117/1.JMI.7.1.014504\n10.5858/arpa.2016-0427-OA\n10.1378/chest.11-1062\n10.1038/s41598-021-99735-6\n10.2214/AJR.17.18384\n10.1016/j.compbiomed.2022.106070\n10.1007/s11042-020-09894-3"}
{"title": "The future of radiology and radiologists: AI is pivotal but not the only change afoot.", "abstract": "Uncertainty regarding the future of radiologists is largely driven by the emergence of artificial intelligence (AI). If AI succeeds, will radiologists continue to monopolize imaging services? As AI accuracy progresses with alacrity, radiology reads will be excellent. Some articles show that AI can make non-radiologists experts. However, eminent figures within AI development have expressed concerns over its possible adverse uses. Bad actors, not bad AI, may account for a future in which AI is not as successful as we might hope and, as some fear, even pernicious. More relevant to current predictions over the course of AI in medicine, and radiology in particular, is how the evolution of AI is often seen in a vacuum. We cannot predict the future with certainty. But as we contemplate the potential impact of AI in radiology, we should remember that radiology does not exist in a vacuum; while AI is changing, so is everything else. The medical system, not to mention the world's population, has been severely impacted by the global COVID-19 pandemic and numerous experts expect future worldwide pandemics. We cannot predict the condition of the healthcare system in two decades but may assume that radiology will likely remain critical in any future medical practice. For now, we should responsibly use all tools at our disposal (including AI) to make ourselves as indispensable as possible. Our best chances of remaining relevant and instrumental to patient care will likely hinge on our ability to lead the changes rather than be passively impacted by them.", "journal": "Journal of medical imaging and radiation sciences", "date": "2024-02-26", "authors": ["Edmund MWeisberg", "Elliot KFishman"], "doi": "10.1016/j.jmir.2024.02.002"}
{"title": "ARDS Mortality Prediction Model Using Evolving Clinical Data and Chest Radiograph Analysis.", "abstract": "Within primary ARDS, SARS-CoV-2-associated ARDS (C-ARDS) emerged in late 2019, reaching its peak during the subsequent two years. Recent efforts in ARDS research have concentrated on phenotyping this heterogeneous syndrome to enhance comprehension of its pathophysiology.\nA retrospective study was conducted on C-ARDS patients from April 2020 to February 2021, encompassing 110 participants with a mean age of 63.2 \u00b1 11.92 (26-83 years). Of these, 61.2% (68) were male, and 25% (17) experienced severe ARDS, resulting in a mortality rate of 47.3% (52). Ventilation settings, arterial blood gases, and chest X-ray (CXR) were evaluated on the first day of invasive mechanical ventilation and between days two and three. CXR images were scrutinized using a convolutional neural network (CNN). A binary logistic regression model for predicting C-ARDS mortality was developed based on the most influential variables: age, PaO\nIntegrating data available in all intensive care units enables the prediction of C-ARDS mortality by utilizing evolving P/F ratios and CXR. This approach can assist in tailoring treatment plans and initiating early discussions to escalate care and extracorporeal life support. Machine learning algorithms for imaging classification can uncover otherwise inaccessible patterns, potentially evolving into another form of ARDS phenotyping. The combined features of these algorithms and clinical variables demonstrate superior performance compared to either element alone.", "journal": "Biomedicines", "date": "2024-02-24", "authors": ["AnaCysneiros", "TiagoGalv\u00e3o", "NunoDomingues", "PedroJorge", "LuisBento", "IgnacioMartin-Loeches"], "doi": "10.3390/biomedicines12020439\n10.1001/jama.2012.5669\n10.1038/s41572-019-0069-0\n10.1056/NEJM200005043421806\n10.1016/S0140-6736(21)00439-6\n10.1007/s00134-023-07050-7\n10.1164/rccm.202303-0558WS\n10.1016/S0140-6736(22)01485-4\n10.1186/s40560-021-00528-w\n10.1016/S2213-2600(21)00461-6\n10.1164/rccm.201211-1981OC\n10.1055/s-0039-1684049\n10.1056/NEJMoa2021436\n10.1002/14651858.CD004477.pub3\n10.1016/S2213-2600(19)30417-5\n10.1016/j.jcrc.2020.07.019\n10.1007/s00134-022-06809-8\n10.1016/j.compbiomed.2021.104463\n10.1016/S2589-7500(21)00056-X\n10.1186/s13054-022-04251-2\n10.1371/journal.pone.0265949\n10.1007/s00134-018-5332-4\n10.1186/s13613-020-0642-4\n10.1016/j.jvs.2006.08.053\n10.1136/thoraxjnl-2017-211280\n10.1513/AnnalsATS.202007-772OC\n10.1016/j.chest.2020.06.070\n10.1016/S2213-2600(19)30138-9\n10.3389/fmed.2021.772056\n10.3389/fmed.2020.599533\n10.1016/S2213-2600(19)30353-4\n10.3389/fphys.2021.730857"}
{"title": "Exploration of Interpretability Techniques for Deep COVID-19 Classification Using Chest X-ray Images.", "abstract": "The outbreak of COVID-19 has shocked the entire world with its fairly rapid spread, and has challenged different sectors. One of the most effective ways to limit its spread is the early and accurate diagnosing of infected patients. Medical imaging, such as X-ray and computed tomography (CT), combined with the potential of artificial intelligence (AI), plays an essential role in supporting medical personnel in the diagnosis process. Thus, in this article, five different deep learning models (ResNet18, ResNet34, InceptionV3, InceptionResNetV2, and DenseNet161) and their ensemble, using majority voting, have been used to classify COVID-19, pneumoni\u00e6 and healthy subjects using chest X-ray images. Multilabel classification was performed to predict multiple pathologies for each patient, if present. Firstly, the interpretability of each of the networks was thoroughly studied using local interpretability methods-occlusion, saliency, input X gradient, guided backpropagation, integrated gradients, and DeepLIFT-and using a global technique-neuron activation profiles. The mean micro F1 score of the models for COVID-19 classifications ranged from 0.66 to 0.875, and was 0.89 for the ensemble of the network models. The qualitative results showed that the ResNets were the most interpretable models. This research demonstrates the importance of using interpretability methods to compare different models before making a decision regarding the best performing model.", "journal": "Journal of imaging", "date": "2024-02-23", "authors": ["SoumickChatterjee", "FatimaSaad", "ChompunuchSarasaen", "SuhitaGhosh", "ValerieKrug", "RupaliKhatun", "RahulMishra", "NirjaDesai", "PetiaRadeva", "GeorgRose", "SebastianStober", "OliverSpeck", "AndreasN\u00fcrnberger"], "doi": "10.3390/jimaging10020045\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001316\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1007/s10044-021-00984-y\n10.1007/s13246-020-00865-4\n10.1148/radiol.2020200241\n10.1148/radiol.2020200463\n10.1148/radiol.2020200343\n10.1148/radiol.2020200330\n10.1001/jama.2020.5788\n10.1148/radiol.2020201365\n10.1515/dx-2020-0058\n10.1016/j.clinimag.2020.04.001\n10.1056/NEJMoa2002032\n10.12669/pjms.36.COVID19-S4.2778\n10.1148/radiol.2020201160\n10.1148/ryct.2020200034\n10.1016/j.compbiomed.2020.103792\n10.1038/s41598-019-55972-4\n10.21037/tcr.2018.05.02\n10.7861/futurehosp.6-2-94\n10.1002/mp.13562\n10.1504/IJBIC.2019.098405\n10.1016/j.acra.2010.11.013\n10.1148/radiol.2020200905\n10.1038/s41598-020-76282-0\n10.1038/s41598-020-76550-z\n10.1109/ACCESS.2021.3087583\n10.3390/diagnostics11091732\n10.1186/s40537-020-00392-9\n10.1007/s00521-021-06806-w\n10.1109/72.279181\n10.3390/electronics10111350\n10.3390/app12041834\n10.1038/s41572-019-0069-0\n10.1016/S2213-2600(20)30304-0\n10.1186/s13054-020-02880-z\n10.1513/AnnalsATS.202008-1026OC\n10.4018/jdwm.2007070101\n10.1016/j.neucom.2014.08.091\n10.3389/fimmu.2018.02640"}
{"title": "A Novel Approach to the Technique of Lung Region Segmentation Based on a Deep Learning Model to Diagnose COVID-19 X-ray Images.", "abstract": "The novel coronavirus pandemic has caused a global health crisis, placing immense strain on healthcare systems worldwide. Chest X-ray technology has emerged as a critical tool for the diagnosis and treatment of COVID-19. However, the manual interpretation of chest X-ray films has proven to be inefficient and time-consuming, necessitating the development of an automated classification system.\nIn response to the challenges posed by the COVID-19 pandemic, we aimed to develop a deep learning model that accurately classifies chest X-ray images, specifically focusing on lung regions, to enhance the efficiency and accuracy of COVID-19 and pneumonia diagnosis.\nWe have proposed a novel deep network called \"FocusNet\" for precise segmentation of lung regions in chest radiographs. This segmentation allows for the accurate extraction of lung contours from chest X-ray images, which are then input into the classification network, ResNet18. By training the model on these segmented lung datasets, we sought to improve the accuracy of classification.\nThe performance of our proposed system was evaluated on three types of lung regions in normal individuals, COVID-19 patients, and those with pneumonia. The average accuracy of the segmentation model (FocusNet) in segmenting lung regions was found to be above 90%. After reclassification of the segmented lung images, the specificities and sensitivities for normal, COVID-19, and pneumonia were excellent, with values of 98.00%, 99.00%, 99.50%, and 98.50%, 100.00%, and 99.00%, respectively. ResNet18 achieved impressive area under the curve (AUC) values of 0.99, 1.00, and 0.99 for classifying normal, COVID-19, and pneumonia, respectively, on the segmented lung datasets. Moreover, the AUC values of the three groups increased by 0.02, 0.02, and 0.06, respectively, when compared to the direct classification of unsegmented original images. Overall, the accuracy of lung region classification after processing the datasets was 99.3%.\nOur deep learning-based automated chest X-ray classification system, incorporating lung region segmentation using FocusNet and subsequent classification with ResNet18, has significantly improved the accuracy of diagnosing respiratory lung diseases, including COVID-19. The proposed approach has great potential to revolutionize the diagnosis of COVID-19 and other respiratory lung diseases, offering a valuable tool to support healthcare professionals during health crises.", "journal": "Current medical imaging", "date": "2024-02-23", "authors": ["XuejieDing", "QiZhou", "ZifanLiu", "Jamal AlzobairHammad Kowah", "LishengWang", "XialingHuang", "XuLiu"], "doi": "10.2174/0115734056271185231121074341"}
{"title": "Triplet-constrained deep hashing for chest X-ray image retrieval in COVID-19 assessment.", "abstract": "Radiology images of the chest, such as computer tomography scans and X-rays, have been prominently used in computer-aided COVID-19 analysis. Learning-based radiology image retrieval has attracted increasing attention recently, which generally involves image feature extraction and finding matches in extensive image databases based on query images. Many deep hashing methods have been developed for chest radiology image search due to the high efficiency of retrieval using hash codes. However, they often overlook the complex triple associations between images; that is, images belonging to the same category tend to share similar characteristics and vice versa. To this end, we develop a triplet-constrained deep hashing (TCDH) framework for chest radiology image retrieval to facilitate automated analysis of COVID-19. The TCDH consists of two phases, including (a) feature extraction and (b) image retrieval. For feature extraction, we have introduced a triplet constraint and an image reconstruction task to enhance discriminative ability of learned features, and these features are then converted into binary hash codes to capture semantic information. Specifically, the triplet constraint is designed to pull closer samples within the same category and push apart samples from different categories. Additionally, an auxiliary image reconstruction task is employed during feature extraction to help effectively capture anatomical structures of images. For image retrieval, we utilize learned hash codes to conduct searches for medical images. Extensive experiments on 30,386 chest X-ray images demonstrate the superiority of the proposed method over several state-of-the-art approaches in automated image search. The code is now available online.", "journal": "Neural networks : the official journal of the International Neural Network Society", "date": "2024-02-23", "authors": ["LinminWang", "QianqianWang", "XiaochuanWang", "YunlingMa", "LimeiZhang", "MingxiaLiu"], "doi": "10.1016/j.neunet.2024.106182"}
{"title": "CT quantification of COVID-19 pneumonia extent to predict individualized outcome.", "abstract": "\u00a0This study aimed to predict individual COVID-19 patient prognosis at hospital admission using artificial intelligence (AI)-based quantification of computed tomography (CT) pulmonary involvement.\nAssessing patient prognosis in COVID-19 pneumonia is crucial for patient management and hospital and ICU organization.\nWe retrospectively analyzed 559 patients with PCR-verified COVID-19 pneumonia referred to the hospital for a\u00a0severe disease course. We correlated the CT extent of pulmonary involvement with patient outcome. We also attempted to define cut-off values of pulmonary involvement for predicting different outcomes.\n\u00a0CT-based disease extent quantification is an independent predictor of patient morbidity and mortality, with the prognosis being impacted also by age and cardiovascular comorbidities. With the use of explored cut-off values, we divided patients into three groups based on their extent of disease: (1) less than 28\u00a0% (sensitivity 65.4\u00a0%; specificity 89.1\u00a0%), (2) ranging from 28\u00a0% (31\u00a0%) to 47\u00a0% (sensitivity 87.1\u00a0%; specificity 62.7\u00a0%), and (3) above 47\u00a0% (sensitivity 87.1\u00a0%; specificity, 62.7\u00a0%), representing low risk, risk for oxygen therapy and invasive pulmonary ventilation, and risk of death, respectively.\nCT quantification of pulmonary involvement using AI-based software helps predict COVID-19 patient outcomes (Tab. 4, Fig. 4, Ref. 38).", "journal": "Bratislavske lekarske listy", "date": "2024-02-22", "authors": ["ZuzanaBerecova", "DominikJuskanic", "MartinHazlinger", "MarekUhnak", "PavolJanega", "MarosRudnay", "RobertHatala"], "doi": "10.4149/BLL_2024_25"}
{"title": "Clinical decision support systems (CDSS) in assistance to COVID-19 diagnosis: A scoping review on types and evaluation methods.", "abstract": "Due to the COVID-19 pandemic, a precise and reliable diagnosis of this disease is critical. The use of clinical decision support systems (CDSS) can help facilitate the diagnosis of COVID-19. This scoping review aimed to investigate the role of CDSS in diagnosing COVID-19.\nWe searched four databases (Web of Science, PubMed, Scopus, and Embase) using three groups of keywords related to CDSS, COVID-19, and diagnosis. To collect data from studies, we utilized a data extraction form that consisted of eight fields. Three researchers selected relevant articles and extracted data using a data collection form. To resolve any disagreements, we consulted with a fourth researcher.\nA search of the databases retrieved 2199 articles, of which 68 were included in this review after removing duplicates and irrelevant articles. The studies used nonknowledge-based CDSS (\nCDSS for COVID-19 diagnosis have been developed mainly through machine learning (ML) methods. The greater use of these techniques can be due to their availability of public data sets about chest imaging. Although these studies indicate high accuracy for CDSS based on ML, their novelty and data set biases raise questions about replacing these systems as clinician assistants in decision-making. Further studies are needed to improve and compare the robustness and reliability of nonknowledge-based and knowledge-based CDSS in COVID-19 diagnosis.", "journal": "Health science reports", "date": "2024-02-22", "authors": ["ArefehAmeri", "AtefehAmeri", "FarzadSalmanizadeh", "KambizBahaadinbeigy"], "doi": "10.1002/hsr2.1919\n10.1007/s11277-023-10432-1\n10.1038/s41598-020-74539-2\n10.1007/978-981-16-4284-5_23\n10.1088/1757-899X/1007/1/012067"}
{"title": "Non-binary gender, vulnerable populations and mental health during the COVID-19 pandemic: Data from the COVID-19 MEntal health inTernational for the general population (COMET-G) study.", "abstract": "The COVID-19 pandemic has brought significant mental health challenges, particularly for vulnerable populations, including non-binary gender individuals. The COMET international study aimed to investigate specific risk factors for clinical depression or distress during the pandemic, also in these special populations.\nChi-square tests were used for initial screening to select only those variables which would show an initial significance. Risk Ratios (RR) were calculated, and a Multiple Backward Stepwise Linear Regression Analysis (MBSLRA) was followed with those variables given significant results at screening and with the presence of distress or depression or the lack of both of them.\nThe most important risk factors for depression were female (RR\u00a0=\u00a01.59-5.49) and non-binary gender (RR\u00a0=\u00a01.56-7.41), unemployment (RR\u00a0=\u00a01.41-6.57), not working during lockdowns (RR\u00a0=\u00a01.43-5.79), bad general health (RR\u00a0=\u00a02.74-9.98), chronic somatic disorder (RR\u00a0=\u00a01.22-5.57), history of mental disorders (depression RR\u00a0=\u00a02.31-9.47; suicide attempt RR\u00a0=\u00a02.33-9.75; psychosis RR\u00a0=\u00a02.14-10.08; Bipolar disorder RR\u00a0=\u00a02.75-12.86), smoking status (RR\u00a0=\u00a01.15-5.31) and substance use (RR\u00a0=\u00a01.77-8.01). The risk factors for distress or depression that survived MBSLRA were younger age, being widowed, living alone, bad general health, being a carer, chronic somatic disorder, not working during lockdowns, being single, self-reported history of depression, bipolar disorder, self-harm, suicide attempts and of other mental disorders, smoking, alcohol, and substance use.\nTargeted preventive interventions are crucial to safeguard the mental health of vulnerable groups, emphasizing the importance of diverse samples in future research.\nOnline data collection may have resulted in the underrepresentation of certain population groups.", "journal": "Journal of affective disorders", "date": "2024-02-22", "authors": ["Konstantinos NFountoulakis", "JelenaVrublevska", "SeriAbraham", "KristinaAdorjan", "Helal UddinAhmed", "Renato DAlarc\u00f3n", "KiyomiArai", "Sani SalihuAuwal", "MichaelBerk", "SarahBjedov", "JulioBobes", "TeresaBobes-Bascaran", "JulieBourgin-Duchesnay", "Cristina AnaBredicean", "LaurynasBukelskis", "AkakiBurkadze", "Indira Indiana CabreraAbud", "RubyCastilla-Puentes", "MarceloCetkovich", "HectorColon-Rivera", "RicardoCorral", "CarlaCortez-Vergara", "PiirikaCrepin", "DomenicoDe Berardis", "Sergio ZamoraDelgado", "DavidDe Lucena", "AvinashDe Sousa", "RamonaDi Stefano", "SeetalDodd", "Livia PriyankaElek", "AnnaElissa", "BertaErdelyi-Hamza", "GamzeErzin", "Martin JEtchevers", "PeterFalkai", "AdrianaFarcas", "IlyaFedotov", "ViktoriiaFilatova", "Nikolaos KFountoulakis", "IrynaFrankova", "FrancescoFranza", "PedroFrias", "TatianaGalako", "Cristian JGaray", "LeticiaGarcia-\u00c1lvarez", "Maria PazGarc\u00eda-Portilla", "XeniaGonda", "Tomasz MGondek", "Daniela MoreraGonz\u00e1lez", "HilaryGould", "PaoloGrandinetti", "ArturoGrau", "VioletaGroudeva", "MichalHagin", "TakayukiHarada", "Tasdik MHasan", "Nurul AzreenHashim", "JanHilbig", "SahadatHossain", "RossitzaIakimova", "MonaIbrahim", "FeliciaIftene", "YuliaIgnatenko", "MatiasIrarrazaval", "ZalihaIsmail", "JamilaIsmayilova", "AsafJacobs", "MiroJakovljevi\u0107", "NenadJak\u0161i\u0107", "AfzalJaved", "Helin YilmazKafali", "SagarKaria", "OlgaKazakova", "DoaaKhalifa", "OlenaKhaustova", "SteveKoh", "KorneliiaKosenko", "Sotirios AKoupidis", "AlishaLalljee", "JustineLiewig", "AbdulMajid", "EvgeniiaMalashonkova", "KhameliaMalik", "Najma IqbalMalik", "GulayMammadzada", "BilveshMandalia", "DonatellaMarazziti", "DarkoMar\u010dinko", "StephanieMartinez", "EimantasMatiekus", "GabrielaMejia", "Roha SaeedMemon", "Xarah Elenne MezaMart\u00ednez", "DaliaMickevi\u010di\u016bt\u0117", "RoumenMilev", "MuftauMohammed", "AlejandroMolina-L\u00f3pez", "PetrMorozov", "Nuru SuleimanMuhammad", "FilipMusta\u010d", "Mika SNaor", "AmiraNassieb", "AlvydasNavickas", "TarekOkasha", "MilenaPandova", "Anca-LiviaPanfil", "LiliyaPanteleeva", "IonPapava", "Mikaella EPatsali", "AlexeyPavlichenko", "BojanaPejuskovic", "Mariana PintoDa Costa", "MikhailPopkov", "DinaPopovic", "Nor Jannah NasutionRaduan", "Francisca VargasRam\u00edrez", "ElmarsRancans", "SalmiRazali", "FedericoRebok", "AnnaRewekant", "Elena Ninoska ReyesFlores", "Mar\u00eda TeresaRivera-Encinas", "PilarSaiz", "Manuel S\u00e1nchezde Carmona", "David SaucedoMart\u00ednez", "Jo AnneSaw", "G\u00f6rkemSaygili", "PatriciaSchneidereit", "BhumikaShah", "TomohiroShirasaka", "KetevanSilagadze", "SattiSitanggang", "OlegSkugarevsky", "AnnaSpikina", "Sridevi SiraMahalingappa", "MariaStoyanova", "AnnaSzczegielniak", "Simona ClaudiaTamasan", "GiuseppeTavormina", "Maurilio Giuseppe MariaTavormina", "Pavlos NTheodorakis", "MauricioTohen", "Eva MariaTsapakis", "DinaTukhvatullina", "IrfanUllah", "RatnarajVaidya", "Johann MVega-Dienstmaier", "OliveraVukovic", "OlgaVysotska", "NataliaWidiasih", "AnnaYashikhina", "DariaSmirnova"], "doi": "10.1016/j.jad.2024.02.050"}
{"title": "DeepCSFusion: Deep Compressive Sensing Fusion for Efficient COVID-19 Classification.", "abstract": "Worldwide, the COVID-19 epidemic, which started in 2019, has resulted in millions of deaths. The medical research community has widely used computer analysis of medical data during the pandemic, specifically deep learning models. Deploying models on devices with constrained resources is a significant challenge due to the increased storage demands associated with larger deep learning models. Accordingly, in this paper, we propose a novel compression strategy that compresses deep features with a compression ratio of 10 to 90% to accurately classify the COVID-19 and non-COVID-19 computed tomography scans. Additionally, we extensively validated the compression using various available deep learning methods to extract the most suitable features from different models. Finally, the suggested DeepCSFusion model compresses the extracted features and applies fusion to achieve the highest classification accuracy with fewer features. The proposed DeepCSFusion model was validated on the publicly available dataset \"SARS-CoV-2 CT\" scans composed of 1252 CT. This study demonstrates that the proposed DeepCSFusion reduced the computational time with an overall accuracy of 99.3%. Also, it outperforms state-of-the-art pipelines in terms of various classification measures.", "journal": "Journal of imaging informatics in medicine", "date": "2024-02-21", "authors": ["Dina ARagab", "SalemaFayed", "NohaGhatwary"], "doi": "10.1007/s10278-024-01011-2\n10.1016/S2213-2600(20)30079-5\n10.1038/s41586-020-2008-3\n10.1007/s12098-020-03263-6\n10.1016/j.compbiomed.2020.104037\n10.1109/RBME.2020.2987975\n10.1016/j.compbiomed.2022.105340\n10.1007/s11548-019-01914-4\n10.1016/j.eng.2020.04.010\n10.7717/peerj-cs.306\n10.1007/s10489-020-02076-6\n10.1016/j.compbiomed.2021.105134\n10.1016/j.bbe.2020.08.005\n10.1016/j.bspc.2021.102920\n10.1016/j.neucom.2022.11.072\n10.3390/app12052734\n10.1016/j.procs.2013.09.028\n10.1007/s11042-015-2575-8\n10.1007/s11042-017-5227-3\n10.1016/j.patcog.2018.02.006\n10.1504/IJAPR.2017.089384\n10.1109/MSP.2007.914729\n10.1109/MSP.2007.914731\n10.1016/j.procs.2022.12.111\n10.1007/BF03178082\n10.1016/j.procs.2019.12.112\n10.3390/electronics11142248\n10.1007/s00330-021-07715-1\n10.1080/07391102.2020.1788642\n10.3390/s21020455\n10.1016/j.eswa.2022.116554\n10.1016/j.compbiomed.2020.103795\n10.1109/TCBB.2021.3065361"}
{"title": "Accurate diagnosis of COVID-19 from lung CT images using transfer learning.", "abstract": "In this study, it is aimed to classify data by feature extraction from tomographic images for the diagnosis of COVID-19 using image processing and transfer learning.\nIn the proposed study, CT images are made better detectable by artificial intelligence through preliminary processes such as masking and segmentation. Then, the number of data was increased by applying data augmentation. The size of the dataset contains a large number of images in numerical terms. Therefore, the results of the models are more reliable. The dataset is split into 70% training and 30% testing. In this way, different features of the applied models were found, and positive effects were achieved on the result. Transfer Learning was used to reduce training times and further increase the success rate. To find the best method, many different pre-trained Transfer Learning models have been tried and compared with many different studies.\nA total of 8,354 images were used in the research. Of these, 2,695 consist of COVID-19 patients and the remaining healthy chest tomography images. All of these images were given to the models through masking and segmentation processes. As a result of the experimental evaluation, the best model was determined to be ResNet-50 and the highest results were found (accuracy 95.7%, precision 94.7%, recall 99.2%, specificity 88.3%, F1 score 96.9%, ROC-AUC score 97%).\nThe presence of a COVID-19 lesion in the images was identified with high accuracy and recall rate using the transfer learning model we developed using thorax CT images. This outcome demonstrates that the strategy will speed up the diagnosis of COVID-19.", "journal": "European review for medical and pharmacological sciences", "date": "2024-02-20", "authors": ["H GTas", "M B HTas", "BIrgul", "SAydin", "UKuyrukluyildiz"], "doi": "10.26355/eurrev_202402_35360"}
{"title": "Post-COVID highlights: Challenges and solutions of artificial intelligence techniques for swift identification of COVID-19.", "abstract": "Since the onset of the COVID-19 pandemic in 2019, there has been a concerted effort to develop cost-effective, non-invasive, and rapid AI-based tools. These tools were intended to alleviate the burden on healthcare systems, control the rapid spread of the virus, and enhance intervention outcomes, all in response to this unprecedented global crisis. As we transition into a post-COVID era, we retrospectively evaluate these proposed studies and offer a review of the techniques employed in AI diagnostic models, with a focus on the solutions proposed for different challenges. This review endeavors to provide insights into the diverse solutions designed to address the multifaceted challenges that arose during the pandemic. By doing so, we aim to prepare the AI community for the development of AI tools tailored to address public health emergencies effectively.", "journal": "Current opinion in structural biology", "date": "2024-02-17", "authors": ["YingyingFang", "XiaodanXing", "ShiyiWang", "SimonWalsh", "GuangYang"], "doi": "10.1016/j.sbi.2024.102778"}
{"title": "Pre- and post-COVID-19 gender trends in authorship for paediatric radiology articles worldwide: a systematic review.", "abstract": "Gender inequalities in academic medicine persist despite progress over the past decade. Evidence-based targeted interventions are needed to reduce gender inequalities.\nThis systematic review aimed to investigate the impact of COVID-19 on gender trends in authorship of paediatric radiology research worldwide.\nThis prospectively registered, PRISMA-compliant systematic review searched the following databases: PubMed, MEDLINE, Web of Science, and Scopus from January 1, 2018, to May 29, 2023, with no restrictions on country of origin. Screening and data extraction occurred independently and in duplicate. Gender of first, last, and corresponding authors were determined using an artificial intelligence-powered, validated, multinational database ( www.genderize.io ). Two time periods were categorised according to the Johns Hopkins Center for Systems Science and Engineering: pre-COVID (prior to March 2020) and peak and post-COVID (March 2020 onwards). One-sample binomial testing was used to analyse proportion of authorship based on gender. Categorical variables were described as frequencies and percentages, and compared using testing chi-square or Fisher exact testing, with a threshold of P<0.05 representing statistical significance.\nIn total, 922 articles were included with 39 countries represented. A statistically significant difference in authorship based on gender persisted during the peak and post-COVID time period (March 2020 onwards) where women represented a statistically significant lower proportion of last (35.5%) and corresponding (42.7%) authors (P<0.001, P=0.001, respectively). Statistically significant differences for first authors were not found in either period (P=0.08 and P=0.48).\nThis study identifies differences in gender trends for authorship in paediatric radiology research worldwide. Future efforts to increase authorship by women are needed.", "journal": "Pediatric radiology", "date": "2024-02-13", "authors": ["RakhshanKamran", "LiamJackman", "CynthiaChan", "Ann CLee", "AleezaKamran", "JennaAlli", "Chlo\u00ebJacklin", "EveDeck", "YujinSuk", "VictoriaJackman", "MiconGarvilles", "Susan ChengShelmerdine", "Andrea SchwarzDoria"], "doi": "10.1007/s00247-024-05855-2\n10.1136/bmjgh-2020-002922\n10.1073/pnas.1914221117\n10.1371/journal.pone.0066212\n10.1038/504211a\n10.1371/journal.pbio.2004956\n10.1057/s41599-022-01365-4\n10.1007/s11150-020-09534-7\n10.1177/2382120520915895\n10.7326/M13-0974\n10.1148/radiol.2016160950\n10.1148/radiol.2021204417\n10.1016/j.jacr.2018.12.034\n10.1007/s40134-022-00391-z\n10.1007/s00247-022-05440-5\n10.1007/s00247-020-04645-w\n10.1007/s00247-021-05213-6\n10.1136/bmj.n71\n10.1001/jamainternmed.2019.0907\n10.1161/CIRCULATIONAHA.117.032325\n10.1136/bmj.l6890\n10.1016/j.acra.2019.06.011\n10.1097/ACM.0000000000000089\n10.1186/s13244-019-0792-9\n10.1215/00703370-8976151"}
{"title": "Robust Medical Diagnosis: A Novel Two-Phase Deep Learning Framework for Adversarial\u00a0Proof Disease Detection\u00a0in Radiology Images.", "abstract": "In the realm of medical diagnostics, the utilization of deep learning techniques, notably in the context of radiology images, has emerged as a transformative force. The significance of artificial intelligence (AI), specifically machine learning (ML) and deep learning (DL), lies in their capacity to rapidly and accurately diagnose diseases from radiology images. This capability has been particularly vital during the COVID-19 pandemic, where rapid and precise diagnosis played a pivotal role in managing the spread of the virus. DL models, trained on vast datasets of radiology images, have showcased remarkable proficiency in distinguishing between normal and COVID-19-affected cases, offering a ray of hope amidst the crisis. However, as with any technological advancement, vulnerabilities emerge. Deep learning-based diagnostic models, although proficient, are not immune to adversarial attacks. These attacks, characterized by carefully crafted perturbations to input data, can potentially disrupt the models' decision-making processes. In the medical context, such vulnerabilities could have dire consequences, leading to misdiagnoses and compromised patient care. To address this, we propose a two-phase defense framework that combines advanced adversarial learning and adversarial image filtering techniques. We use a modified adversarial learning algorithm to enhance the model's resilience against adversarial examples during the training phase. During the inference phase, we apply JPEG compression to mitigate perturbations that cause misclassification. We evaluate our approach on three models based on ResNet-50, VGG-16, and Inception-V3. These models perform exceptionally in classifying radiology images (X-ray and CT) of lung regions into normal, pneumonia, and COVID-19 pneumonia categories. We then assess the vulnerability of these models to three targeted adversarial attacks: fast gradient sign method (FGSM), projected gradient descent (PGD), and basic iterative method (BIM). The results show a significant drop in model performance after the attacks. However, our defense framework greatly improves the models' resistance to adversarial attacks, maintaining high accuracy on adversarial examples. Importantly, our framework ensures the reliability of the models in diagnosing COVID-19 from clean images.", "journal": "Journal of imaging informatics in medicine", "date": "2024-02-12", "authors": ["Sheikh Burhan UlHaque", "AasimZafar"], "doi": "10.1007/s10278-023-00916-8\n10.3348/kjr.2020.0146\n10.1016/j.eswa.2015.10.015\n10.1016/j.compmedimag.2016.07.004\n10.1109/ACCESS.2017.2789324\n10.1007/s00330-022-09335-9\n10.1007/s00330-021-08334-6\n10.1007/s10278-022-00754-0\n10.1007/s10278-021-00431-8\n10.1007/s11517-022-02632-x\n10.1007/s11517-020-02299-2\n10.1007/s10278-023-00811-2\n10.1007/s10278-023-00791-3\n10.1007/s11517-022-02553-9]\n10.1007/s11517-022-02746-2]\n10.1007/s11517-022-02619-8]\n10.1007/s11548-021-02466-2]\n10.1007/s11548-020-02299-5\n10.1007/s00330-021-08049-8\n10.1007/s10916-021-01757-0\n10.1007/s10916-020-01645-z\n10.1016/j.patcog.2020.107332\n10.1007/s11548-022-02813-x\n10.1007/s11548-020-02305-w\n10.1007/s10916-020-01597-4\n10.1007/s00330-021-07715-1\n10.1007/s11517-022-02637-6\n10.1109/JIOT.2020.3013710\n10.1186/s13640-020-0490-z\n10.3390/app11094233"}
{"title": "Investigating distributions of inhaled aerosols in the lungs of post-COVID-19 clusters through a unified imaging and modeling approach.", "abstract": "Recent studies, based on clinical data, have identified sex and age as significant factors associated with an increased risk of long COVID. These two factors align with the two post-COVID-19 clusters identified by a deep learning algorithm in computed tomography (CT) lung scans: Cluster 1 (C1), comprising predominantly females with small airway diseases, and Cluster 2 (C2), characterized by older individuals with fibrotic-like patterns. This study aims to assess the distributions of inhaled aerosols in these clusters.\n140 COVID survivors examined around 112 days post-diagnosis, along with 105 uninfected, non-smoking healthy controls, were studied. Their demographic data and CT scans at full inspiration and expiration were analyzed using a combined imaging and modeling approach. A subject-specific CT-based computational model analysis was utilized to predict airway resistance and particle deposition among C1 and C2 subjects. The cluster-specific structure and function relationships were explored.\nIn C1 subjects, distinctive features included airway narrowing, a reduced homothety ratio of daughter over parent branch diameter, and increased airway resistance. Airway resistance was concentrated in the distal region, with a higher fraction of particle deposition in the proximal airways. On the other hand, C2 subjects exhibited airway dilation, an increased homothety ratio, reduced airway resistance, and a shift of resistance concentration towards the proximal region, allowing for deeper particle penetration into the lungs.\nThis study revealed unique mechanistic phenotypes of airway resistance and particle deposition in the two post-COVID-19 clusters. The implications of these findings for inhaled drug delivery effectiveness and susceptibility to air pollutants were explored.", "journal": "European journal of pharmaceutical sciences : official journal of the European Federation for Pharmaceutical Sciences", "date": "2024-02-11", "authors": ["XuanZhang", "FrankLi", "Prathish KRajaraman", "Alejandro PComellas", "Eric AHoffman", "Ching-LongLin"], "doi": "10.1016/j.ejps.2024.106724\n10.1016/j.jbiomech.2011.06.009\n10.1016/j.ebiom.2023.104777\n10.1016/j.resp.2013.10.015\n10.1016/j.resp.2010.05.011\n10.1148/radiol.212170\n10.1148/radiol.212170\n10.1016/j.jaci.2016.11.053\n10.1152/japplphysiol.01094.2014\n10.1152/japplphysiol.00113.2013\n10.1152/japplphysiol.00016.2019\n10.1152/japplphysiol.00144.2020\n10.1136/thoraxjnl-2013-203897\n10.1152/japplphysiol.00440.2018\n10.1148/radiol.220449\n10.1016/j.annemergmed.2017.01.013\n10.1056/NEJMoal014350\n10.1001/jamanetworkopen.2021.28568\n10.1186/sl2931-018-0888-7\n10.1007/sll517-017-1690-2\n10.1259/bjr.20211364\n10.1016/j.addr.2020.09.007.\n10.llll/resp.12841\n10.1016/j.ejpb.2022.03.010\n10.3390/jcml2103511\n10.2147/jir.S271292\n10.3109/15412555.2011.586658\n10.3389/fphys.2022.999263\n10.3389/ftned.2021.686878\n10.1002/wsbm.1234\n10.21037/atm-20-4955\n10.1038/nature02287\n10.1183/13993003.03101-2021\n10.1513/AnnalsATS.202008-10020C\n10.14336/ad.2016.1215\n10.1001/jamanetworkopen.2022.38804\n10.1001/jama.2020.6918\n10.1073/pnas.1715564115\n10.1152/japplphysiol.00520.2004\n10.1164/ajrccm.159.2.9707145\n10.1183/13993003.00551-2023\n10.12659/MSM.928996\n10.3390/medicina58030419\n10.1016/j.powtec.2023.119175\n10.1118/1.3193526\n10.1152/japplphysiol.00176.2020\n10.1016/j.lanepe.2023.100608\n10.1016/j.eclinm.2022.lOl668\n10.1183/13993003.00280-2023\n10.1016/j.ejps.2022.106272\n10.1016/j.powtec.2023.119163"}
{"title": "Pulmonary thrombosis associated with COVID-19 pneumonia: Beyond classical pulmonary thromboembolism.", "abstract": "Classical pulmonary thromboembolism (TE) and local pulmonary thrombosis (PT) have been suggested as mechanisms of thrombosis in COVID-19. However, robust evidence is still lacking because this was mainly based on retrospective studies, in which patients were included when TE was suspected.\nAll patients with COVID-19 pneumonia underwent computed tomography and pulmonary angiography in a prospective study. The main objective was to determine the number and percentage of thrombi surrounded by lung opacification (TSO) in each patient, as well as their relationship with percentage of lung involvement (TLI), to distinguish classical TE (with a random location of thrombi that should correspond to a percentage of TSO equivalent to the TLI) from PT. We determined TLI by artificial intelligence. Analyses at patient level (TLI and percentage of TSO) and at thrombi level (TLI and TSO) were performed.\nWe diagnosed TE in 70 out of 184 patients. Three (2-8) thrombi/patient were detected. The percentage of TSO was 100% (75-100) per patient, and TLI was 19.9% (4.6-35.2). Sixty-five patients (92.9%) were above the random scenario with higher percentage of TSO than TLI. Most thrombi were TSO (n\u2009=\u2009299, 75.1%). When evaluating by TLI (<10%, 10%-20%, 20%-30% and >30%), percentage of TSO was higher in most groups. Thrombi were mainly in subsegmental/segmental arteries, and percentage of TSO was higher in all locations.\nThrombi in COVID-19 were found within lung opacities in a higher percentage than lung involvement, regardless of TLI and clot location, supporting the hypothesis of local PT rather than \"classic TE\".", "journal": "European journal of clinical investigation", "date": "2024-02-10", "authors": ["CarlaSu\u00e1rez-Castillejo", "N\u00e9storCalvo", "LuminitaPreda", "NuriaToledo-Pons", "Aina RosaMill\u00e1n-Pons", "Joaqu\u00ednMart\u00ednez", "LuisaRam\u00f3n", "AmandaIglesias", "DanielMorell-Garc\u00eda", "Josep MiquelBau\u00e7a", "Bel\u00e9nN\u00fa\u00f1ez", "JaumeSauleda", "ErnestSala-Llinas", "AlbertoAlonso-Fern\u00e1ndez"], "doi": "10.1111/eci.14176"}
{"title": "Differential privacy preserved federated learning for prognostic modeling in COVID-19 patients using large multi-institutional chest CT dataset.", "abstract": "Notwithstanding the encouraging results of previous studies reporting on the efficiency of deep learning (DL) in COVID-19 prognostication, clinical adoption of the developed methodology still needs to be improved. To overcome this limitation, we set out to predict the prognosis of a large multi-institutional cohort of patients with COVID-19 using a DL-based model.\nThis study aimed to evaluate the performance of deep privacy-preserving federated learning (DPFL) in predicting COVID-19 outcomes using chest CT images.\nAfter applying inclusion and exclusion criteria, 3055 patients from 19 centers, including 1599 alive and 1456 deceased, were enrolled in this study. Data from all centers were split (randomly with stratification respective to each center and class) into a training/validation set (70%/10%) and a hold-out test set (20%). For the DL model, feature extraction was performed on 2D slices, and averaging was performed at the final layer to construct a 3D model for each scan. The DensNet model was used for feature extraction. The model was developed using centralized and FL approaches. For FL, we employed DPFL approaches. Membership inference attack was also evaluated in the FL strategy. For model evaluation, different metrics were reported in the hold-out test sets. In addition, models trained in two scenarios, centralized and FL, were compared using the DeLong test for statistical differences.\nThe centralized model achieved an accuracy of 0.76, while the DPFL model had an accuracy of 0.75. Both the centralized and DPFL models achieved a specificity of 0.77. The centralized model achieved a sensitivity of 0.74, while the DPFL model had a sensitivity of 0.73. A mean AUC of 0.82 and 0.81 with 95% confidence intervals of (95% CI: 0.79-0.85) and (95% CI: 0.77-0.84) were achieved by the centralized model and the DPFL model, respectively. The DeLong test did not prove statistically significant differences between the two models (p-value\u00a0=\u00a00.98). The AUC values for the inference attacks fluctuate between 0.49 and 0.51, with an average of 0.50\u00a0\u00b1\u00a00.003 and 95% CI for the mean AUC of 0.500 to 0.501.\nThe performance of the proposed model was comparable to centralized models while operating on large and heterogeneous multi-institutional datasets. In addition, the model was resistant to inference attacks, ensuring the privacy of shared data during the training process.", "journal": "Medical physics", "date": "2024-02-09", "authors": ["IsaacShiri", "YazdanSalimi", "NasimSirjani", "BehroozRazeghi", "SaraBagherieh", "MasoumehPakbin", "ZahraMansouri", "GhasemHajianfar", "Atlas HaddadiAvval", "DariushAskari", "MohammadrezaGhasemian", "SalehSandoughdaran", "AhmadSohrabi", "ElhamSadati", "SomayehLivani", "PooyaIranpour", "ShahriarKolahi", "BardiaKhosravi", "SalarBijari", "SaharSayfollahi", "Mohammad RezaAtashzar", "MohammadHasanian", "AlirezaShahhamzeh", "ArashTeimouri", "NedaGoharpey", "HesamaddinShirzad-Aski", "JalalKarimi", "Amir RezaRadmard", "KiaraRezaei-Kalantari", "Mostafa GhelichOghli", "MehrdadOveisi", "AlirezaVafaei Sadr", "SlavaVoloshynovskiy", "HabibZaidi"], "doi": "10.1002/mp.16964"}
{"title": "Estimating lung function from computed tomography at the patient and lobe level using machine learning.", "abstract": "Automated estimation of Pulmonary function test (PFT) results from Computed Tomography (CT) could advance the use of CT in screening, diagnosis, and staging of restrictive pulmonary diseases. Estimating lung function per lobe, which cannot be done with PFTs, would be helpful for risk assessment for pulmonary resection surgery and bronchoscopic lung volume reduction.\nTo automatically estimate PFT results from CT and furthermore disentangle the individual contribution of pulmonary lobes to a patient's lung function.\nWe propose I3Dr, a deep learning architecture for estimating global measures from an image that can also estimate the contributions of individual parts of the image to this global measure. We apply it to estimate the separate contributions of each pulmonary lobe to a patient's total lung function from CT, while requiring only CT scans and patient level lung function measurements for training. I3Dr consists of a lobe-level and a patient-level model. The lobe-level model extracts all anatomical pulmonary lobes from a CT scan and processes them in parallel to produce lobe level lung function estimates that sum up to a patient level estimate. The patient-level model directly estimates patient level lung function from a CT scan and is used to re-scale the output of the lobe-level model to increase performance. After demonstrating the viability of the proposed approach, the I3Dr model is trained and evaluated for PFT result estimation using a large data set of 8\u00a0433 CT volumes for training, 1\u00a0775 CT volumes for validation, and 1\u00a0873 CT volumes for\u00a0testing.\nFirst, we demonstrate the viability of our approach by showing that a model trained with a collection of digit images to estimate their sum implicitly learns to assign correct values to individual digits. Next, we show that our models can estimate lobe-level quantities, such as COVID-19 severity scores, pulmonary volume (PV), and functional pulmonary volume (FPV) from CT while only provided with patient-level quantities during training. Lastly, we train and evaluate models for producing spirometry and diffusion capacity of carbon mono-oxide (DLCO) estimates at the patient and lobe level. For producing Forced Expiratory Volume in one second (FEV1), Forced Vital Capacity (FVC), and DLCO estimates, I3Dr obtains mean absolute errors (MAE) of 0.377\u00a0L, 0.297\u00a0L, and 2.800\u00a0mL/min/mm Hg respectively. We release the resulting algorithms for lung function estimation to the research community at https://grand-challenge.org/algorithms/lobe-wise-lung-function-estimation/ CONCLUSIONS: I3Dr can estimate global measures from an image, as well as the contributions of individual parts of the image to this global measure. It offers a promising approach for estimating PFT results from CT scans and disentangling the individual contribution of pulmonary lobes to a patient's lung function. The findings presented in this work may advance the use of CT in screening, diagnosis, and staging of restrictive pulmonary diseases as well as in risk assessment for pulmonary resection surgery and bronchoscopic lung volume\u00a0reduction.", "journal": "Medical physics", "date": "2024-02-08", "authors": ["Luuk HBoulogne", "Jean-PaulCharbonnier", "ColinJacobs", "Erik H F Mvan der Heijden", "Bramvan Ginneken"], "doi": "10.1002/mp.16915"}
{"title": "A methodical exploration of imaging modalities from dataset to detection through machine learning paradigms in prominent lung disease diagnosis: a review.", "abstract": "Lung diseases, both infectious and non-infectious, are the most prevalent cause of mortality overall in the world. Medical research has identified pneumonia, lung cancer, and Corona Virus Disease 2019 (COVID-19) as prominent lung diseases prioritized over others. Imaging modalities, including X-rays, computer tomography (CT) scans, magnetic resonance imaging (MRIs), positron emission tomography (PET) scans, and others, are primarily employed in medical assessments because they provide computed data that can be utilized as input datasets for computer-assisted diagnostic systems. Imaging datasets are used to develop and evaluate machine learning (ML) methods to analyze and predict prominent lung diseases.\nThis review analyzes ML paradigms, imaging modalities' utilization, and recent developments for prominent lung diseases. Furthermore, the research also explores various datasets available publically that are being used for prominent lung diseases.\nThe well-known databases of academic studies that have been subjected to peer review, namely ScienceDirect, arXiv, IEEE Xplore, MDPI, and many more, were used for the search of relevant articles. Applied keywords and combinations used to search procedures with primary considerations for review, such as pneumonia, lung cancer, COVID-19, various imaging modalities, ML, convolutional neural networks (CNNs), transfer learning, and ensemble learning.\nThis research finding indicates that X-ray datasets are preferred for detecting pneumonia, while CT scan datasets are predominantly favored for detecting lung cancer. Furthermore, in COVID-19 detection, X-ray datasets are prioritized over CT scan datasets. The analysis reveals that X-rays and CT scans have surpassed all other imaging techniques. It has been observed that using CNNs yields a high degree of accuracy and practicability in identifying prominent lung diseases. Transfer learning and ensemble learning are complementary techniques to CNNs to facilitate analysis. Furthermore, accuracy is the most favored metric for assessment.", "journal": "BMC medical imaging", "date": "2024-02-02", "authors": ["SunilKumar", "HarishKumar", "GyanendraKumar", "Shailendra PratapSingh", "AnchitBijalwan", "ManojDiwakar"], "doi": "10.1186/s12880-024-01192-w\n10.1016/j.neunet.2023.11.006\n10.3390/bioengineering9100493\n10.3390/diagnostics11112155\n10.1016/j.jbi.2022.104151\n10.1016/j.tbench.2023.100119\n10.1016/j.eswa.2023.122029\n10.1016/j.bspc.2021.103325\n10.3390/app122010535\n10.1016/j.media.2019.03.009\n10.1136/bmj.38977.669769.2c\n10.3390/electronics11172634\n10.1136/bmjopen-2019-034568\n10.1016/j.measurement.2019.02.042"}
{"title": "Diagnostic performance of deep learning models versus radiologists in COVID-19 pneumonia: A systematic review and meta-analysis.", "abstract": "Although several studies have compared the performance of deep learning (DL) models and radiologists for the diagnosis of COVID-19 pneumonia on CT of the chest, these results have not been collectively evaluated. We performed a meta-analysis of original articles comparing the performance of DL models versus radiologists in detecting COVID-19 pneumonia.\nA systematic search was conducted on the three main medical literature databases, Scopus, Web of Science, and PubMed, for articles published as of February 1st, 2023. We included original scientific articles that compared DL models trained to detect COVID-19 pneumonia on CT to radiologists. Meta-analysis was performed to determine DL versus radiologist performance in terms of model sensitivity and specificity, taking into account inter and intra-study heterogeneity.\nTwenty-two articles met the inclusion criteria. Based on the meta-analytic calculations, DL models had significantly higher pooled sensitivity (0.933 vs. 0.829, p\u00a0<\u00a00.001) compared to radiologists with similar pooled specificity (0.905 vs. 0.897, p\u00a0=\u00a00.746). In the differentiation of COVID-19 versus community-acquired pneumonia, the DL models had significantly higher sensitivity compared to radiologists (0.915 vs. 0.836, p\u00a0=\u00a00.001).\nDL models have high performance for screening of COVID-19 pneumonia on chest CT, offering the possibility of these models for augmenting radiologists in clinical practice.", "journal": "Clinical imaging", "date": "2024-02-02", "authors": ["MohammadrezaChavoshi", "SaraZamani", "Seyed AliMirshahvalad"], "doi": "10.1016/j.clinimag.2024.110092"}
{"title": "Recent developments of telemedicine in glaucoma.", "abstract": "Telemedicine has an increasingly significant role in the fields of ophthalmology and glaucoma. This review covers recent advancements in the development and optimization of teleglaucoma techniques and applications.\nGlaucoma monitoring and diagnosis via remote tonometry, perimetry, and fundus imaging have become a possibility based on recent developments. Many applications work in combination with smart devices, virtual reality, and artificial intelligence and have been tested in patient populations against conventional \"reference-standard\" measurement tools, demonstrating promising results. Of note, there is still much progress to be made in teleglaucoma and telemedicine at large, such as accessibility to internet, broadband, and smart devices, application affordability, and reimbursement for remote services. However, continued development and optimization of these applications suggest that the implementation of remote monitoring will be a mainstay for glaucoma patient care.\nEspecially since the beginning of the COVID-19 pandemic, remote patient care has taken on an important role in medicine and ophthalmology. Remote versions of tonometry, perimetry, and fundus imaging may allow for a more patient-centered and accessible future for glaucoma care.", "journal": "Current opinion in ophthalmology", "date": "2024-01-31", "authors": ["Jason JJo", "Louis RPasquale"], "doi": "10.1097/ICU.0000000000001019\n10.1089/tmj.2023.0041."}
{"title": "Explainable deep learning diagnostic system for prediction of lung disease from medical images.", "abstract": "Around the globe, respiratory lung diseases pose a severe threat to human survival. Based on a central goal to reduce contiguous transmission from infected to healthy persons, several technologies have evolved for diagnosing lung pathologies. One of the emerging technologies is the utility of Artificial Intelligence (AI) based on computer vision for processing wide varieties of medical imaging but AI methods without explainability are often treated as a black box. Based on a view to demystifying the rationale influencing AI decisions, this paper designed and developed a novel low-cost explainable deep-learning diagnostic tool for predicting lung disease from medical images. For this, we investigated explainable deep learning (DL) models (conventional DL and vision transformers (ViTs)) for performing prediction of the existence of pneumonia, COVID19, or no-disease from both original and data augmentation (DA)-based medical images (from two chest X-ray datasets). The results show that our experimental consideration of the DA that combines the impact of cropping, rotation, and horizontal flipping (CROP+ROT+HF) for transforming input images and then passed as input to an Inception-V3 architecture yielded a performance that surpasses all the ViTs and other conventional DL approaches in most of the evaluated performance metrics. Overall, the results suggest that the utility of data augmentation schemes aided the DL methods to yield higher classification accuracies. Furthermore, we compared five different class activation mapping (CAM) algorithms (GradCAM, GradCAM++, EigenGradCAM, AblationCAM, and RandomCAM). The result shows that most of the examined CAM algorithms were effective in identifying the attention region containing the existence of pneumonia or COVID-19 from the medical images (chest X-rays). Our developed low-cost AI diagnostic tool (pilot system) can assist medical experts and radiographers in proffering early diagnosis of lung disease. For this, we selected five to seven deep learning models and the explainable algorithms were deployed on a novel web interface implemented via a Gradio framework.", "journal": "Computers in biology and medicine", "date": "2024-01-24", "authors": ["Nussair AdelHroub", "Ali NaderAlsannaa", "MaadAlowaifeer", "MotazAlfarraj", "EmmanuelOkafor"], "doi": "10.1016/j.compbiomed.2024.108012"}
{"title": "The Implementation of a Gesture Recognition System with a Millimeter Wave and Thermal Imager.", "abstract": "During the COVID-19 pandemic, the number of cases continued to rise. As a result, there was a growing demand for alternative control methods to traditional buttons or touch screens. However, most current gesture recognition technologies rely on machine vision methods. However, this method can lead to suboptimal recognition results, especially in situations where the camera is operating in low-light conditions or encounters complex backgrounds. This study introduces an innovative gesture recognition system for large movements that uses a combination of millimeter wave radar and a thermal imager, where the multi-color conversion algorithm is used to improve palm recognition on the thermal imager together with deep learning approaches to improve its accuracy. While the user performs gestures, the mmWave radar captures point cloud information, which is then analyzed through neural network model inference. It also integrates thermal imaging and palm recognition to effectively track and monitor hand movements on the screen. The results suggest that this combined method significantly improves accuracy, reaching a rate of over 80%.", "journal": "Sensors (Basel, Switzerland)", "date": "2024-01-23", "authors": ["Yi-LinCheng", "Wen-HsiangYeh", "Yu-PingLiao"], "doi": "10.3390/s24020581\n10.1080/10408363.2020.1783198\n10.3390/infrastructures5070053\n10.1109/LSENS.2018.2810093\n10.3390/computers10010005\n10.1016/j.jobe.2023.108126\n10.1016/j.cviu.2006.10.012\n10.1109/TMC.2022.3153717\n10.1109/JSEN.2020.2991741\n10.1109/JSSC.2013.2239004\n10.1109/ACCESS.2016.2617282\n10.1145/2897824.2925953\n10.1109/ACCESS.2019.2903586\n10.1109/ICCE-Taiwan55306.2022.9868981\n10.1109/ACCESS.2017.2762418"}
{"title": "Lung Imaging and Artificial Intelligence in ARDS.", "abstract": "Artificial intelligence (AI) can make intelligent decisions in a manner akin to that of the human mind. AI has the potential to improve clinical workflow, diagnosis, and prognosis, especially in radiology. Acute respiratory distress syndrome (ARDS) is a very diverse illness that is characterized by interstitial opacities, mostly in the dependent areas, decreased lung aeration with alveolar collapse, and inflammatory lung edema resulting in elevated lung weight. As a result, lung imaging is a crucial tool for evaluating the mechanical and morphological traits of ARDS patients. Compared to traditional chest radiography, sensitivity and specificity of lung computed tomography (CT) and ultrasound are higher. The state of the art in the application of AI is summarized in this narrative review which focuses on CT and ultrasound techniques in patients with ARDS. A total of eighteen items were retrieved. The primary goals of using AI for lung imaging were to evaluate the risk of developing ARDS, the measurement of alveolar recruitment, potential alternative diagnoses, and outcome. While the physician must still be present to guarantee a high standard of examination, AI could help the clinical team provide the best care possible.", "journal": "Journal of clinical medicine", "date": "2024-01-23", "authors": ["DavideChiumello", "SilviaCoppola", "GiuliaCatozzi", "FiammettaDanzo", "PierachilleSantus", "DejanRadovanovic"], "doi": "10.3390/jcm13020305\n10.1056/NEJMra2302038\n10.1186/s13613-023-01154-5\n10.1016/j.compbiomed.2021.104210\n10.1016/j.jacr.2021.04.002\n10.3390/diagnostics13172760\n10.1183/13993003.00435-2019\n10.1002/mp.13649\n10.1162/neco.2006.18.7.1527\n10.1016/j.ejrad.2019.108748\n10.3390/diagnostics11061029\n10.3389/fphys.2021.676118\n10.3346/jkms.2021.36.e46\n10.1007/s00330-020-07013-2\n10.1016/j.eng.2018.11.020\n10.1186/s40635-023-00495-6\n10.1371/journal.pone.0245384\n10.3390/diagnostics12010010\n10.1007/s00330-020-07635-6\n10.1007/s10140-023-02149-2\n10.21037/atm-20-3554\n10.1109/TUFFC.2020.3002249\n10.3390/app11020672\n10.1136/bmjopen-2020-045120\n10.1016/j.imu.2021.100687\n10.1007/s00134-005-2627-z\n10.1164/ajrccm.164.9.2103121\n10.1097/01.shk.0000169725.80068.4a\n10.1148/rg.2020200158\n10.4187/respcare.01731\n10.21037/atm.2017.07.20\n10.3389/fphys.2021.666941\n10.5858/arpa.2015-0519-RA\n10.21037/atm.2017.06.49\n10.1109/RBME.2020.2987975\n10.1038/s41746-023-00797-9\n10.1177/20552076221120317\n10.12659/MSM.924582\n10.2196/28028\n10.1097/00005373-200108000-00003\n10.1016/S0140-6736(67)90168-7\n10.1056/NEJM200005043421806\n10.1016/S0140-6736(80)90237-8\n10.1007/BF01735174\n10.1001/jama.1993.03500160092039\n10.1056/NEJMoa052052\n10.1007/s00134-012-2707-9\n10.1186/s13613-019-0497-8\n10.1109/TPAMI.2016.2644615\n10.1001/jama.2016.0291\n10.1007/s00134-020-06281-2\n10.3390/jcm11175180\n10.1007/s00134-021-06519-7\n10.1016/j.jcrc.2023.154444\n10.1002/jum.15285\n10.1111/anae.15082\n10.1016/j.crad.2020.05.001\n10.3760/cma.j.issn.2095-4352.2015.07.008\n10.1007/s00134-016-4411-7\n10.1186/cc13859\n10.1164/rccm.201003-0369OC\n10.3390/diagnostics12061405\n10.1186/s12890-015-0091-2\n10.1097/CCM.0b013e31824e68ae\n10.1097/CCM.0000000000003971\n10.1097/MCC.0000000000000042\n10.1016/j.ajem.2006.02.013\n10.1007/s00134-020-06005-6\n10.1002/jum.14627\n10.1016/S0140-6736(20)31875-4\n10.1109/JBHI.2021.3103839\n10.1038/nature14539\n10.1561/2000000039\n10.1016/j.jemermed.2021.01.041\n10.5811/westjem.2020.5.47743"}
{"title": "HRCTCov19-a high-resolution chest CT scan image dataset for COVID-19 diagnosis and differentiation.", "abstract": "Computed tomography (CT) was a widely used diagnostic technique for COVID-19 during the pandemic. High-Resolution Computed Tomography (HRCT), is a type of computed tomography that enhances image resolution through the utilization of advanced methods. Due to privacy concerns, publicly available COVID-19 CT image datasets are incredibly tough to come by, leading to it being challenging to research and create AI-powered COVID-19 diagnostic algorithms based on CT images.\nTo address this issue, we created HRCTCov19, a new COVID-19 high-resolution chest CT scan image collection that includes not only COVID-19 cases of Ground Glass Opacity (GGO), Crazy Paving, and Air Space Consolidation but also CT images of cases with negative COVID-19. The HRCTCov19 dataset, which includes slice-level and patient-level labeling, has the potential to assist in COVID-19 research, in particular for diagnosis and a distinction using AI algorithms, machine learning, and deep learning methods. This dataset, which can be accessed through the web at http://databiox.com , includes 181,106 chest HRCT images from 395 patients labeled as GGO, Crazy Paving, Air Space Consolidation, and Negative.", "journal": "BMC research notes", "date": "2024-01-23", "authors": ["IrajAbedi", "MahsaVali", "BentolhodaOtroshi", "MaryamZamanian", "HamidrezaBolhasani"], "doi": "10.1186/s13104-024-06693-z\n10.1093/cid/ciaa247\n10.1148/radiol.2020200432\n10.1016/S0140-6736(20)30183-5\n10.1007/s00330-020-06827-4\n10.1148/radiol.2020200230\n10.2214/AJR.20.22975\n10.1186/s43055-021-00415-2\n10.1148/ryct.2020200075\n10.1038/s41597-021-00900-3\n10.1016/j.imu.2020.100427\n10.1016/j.imu.2020.100341\n10.1016/j.bspc.2021.102588\n10.1186/s13104-021-05592-x\n10.3390/bioengineering8020026\n10.1259/bjr/96908158"}
{"title": "Smartphone-based detection of COVID-19 and associated pneumonia using thermal imaging and a transfer learning algorithm.", "abstract": "COVID-19-related pneumonia is typically diagnosed using chest x-ray or computed tomography images. However, these techniques can only be used in hospitals. In contrast, thermal cameras are portable, inexpensive devices that can be connected to smartphones. Thus, they can be used to detect and monitor medical conditions outside hospitals. Herein, a smartphone-based application using thermal images of a human back was developed for COVID-19 detection. Image analysis using a deep learning algorithm revealed a sensitivity and specificity of 88.7% and 92.3%, respectively. The findings support the future use of noninvasive thermal imaging in primary screening for COVID-19 and associated pneumonia.", "journal": "Journal of biophotonics", "date": "2024-01-23", "authors": ["OshritHoffer", "Rafael YBrzezinski", "AdamGanim", "PerryShalom", "ZehavaOvadia-Blechman", "LitalBen-Baruch", "NirLewis", "RacheliPeled", "CarmiShimon", "NiliNaftali-Shani", "EyalKatz", "YairZimmer", "NetaRabin"], "doi": "10.1002/jbio.202300486"}
{"title": "A Survey on Artificial Intelligence in Pulmonary Imaging.", "abstract": "Over the last decade, deep learning (DL) has contributed a paradigm shift in computer vision and image recognition creating widespread opportunities of using artificial intelligence in research as well as industrial applications. DL has been extensively studied in medical imaging applications, including those related to pulmonary diseases. Chronic obstructive pulmonary disease, asthma, lung cancer, pneumonia, and, more recently, COVID-19 are common lung diseases affecting nearly 7.4% of world population. Pulmonary imaging has been widely investigated toward improving our understanding of disease etiologies and early diagnosis and assessment of disease progression and clinical outcomes. DL has been broadly applied to solve various pulmonary image processing challenges including classification, recognition, registration, and segmentation. This paper presents a survey of pulmonary diseases, roles of imaging in translational and clinical pulmonary research, and applications of different DL architectures and methods in pulmonary imaging with emphasis on DL-based segmentation of major pulmonary anatomies such as lung volumes, lung lobes, pulmonary vessels, and airways as well as thoracic musculoskeletal anatomies related to pulmonary diseases.", "journal": "Wiley interdisciplinary reviews. Data mining and knowledge discovery", "date": "2024-01-22", "authors": ["Punam KSaha", "Syed AhmedNadeem", "Alejandro PComellas"], "doi": "10.1002/widm.1510\n10.1183/09031936.03.00405703\n10.1001/jama.2021.5469\n10.1007/s10278-017-9983-4\n10.1378/chest.123.1_suppl.21s\n10.3390/electronics8030292\n10.1109/TMI.2003.815905\n10.1109/TPAMI.2016.2644615\n10.1183/09031936.00128008\n10.5041/RMMJ.10355\n10.1148/radiol.2353040121\n10.1164/rccm.201807-1351SO\n10.1093/aje/kwf113\n10.1109/Access.2019.2944862\n10.1063/1.1144830\n10.1378/chest.121.2.609\n10.1152/ajpregu.00173.2017\n10.1586/ers.11.34\n10.3109/15412555.2012.665520\n10.3390/jcm11082265\n10.1007/s11548-019-01917-1\n10.1056/NEJMoa021322\n10.1183/09031936.04.00014304\n10.1097/RTI.0000000000000388\n10.1118/1.597428\n10.1016/j.bspc.2006.05.002\n10.1016/j.rmed.2018.11.014\n10.1016/j.media.2016.11.001\n10.3389/fcvm.2020.00025\n10.1016/j.neuroimage.2017.04.041\n10.3390/jimaging6110125\n10.1007/Bf00994018\n10.1007/s00330-010-1845-0\n10.1080/15412550802237531\n10.1109/TMI.2007.907555\n10.1148/radiol.09090548\n10.1186/s12931-018-0771-6\n10.1016/j.compmedimag.2007.02.002\n10.1016/j.rmed.2012.08.006\n10.1109/TBME.2016.2613502\n10.1117/1.JMI.5.2.024003\n10.1016/j.compmedimag.2009.04.012\n10.1097/00043764-198608000-00038\n10.1016/j.neucom.2018.09.013\n10.1088/1361-6560/ab843e\n10.1002/mp.14065\n10.1016/j.ophtha.2017.02.008\n10.1148/radiol.2021210963\n10.1088/0031-9155/58/24/8647\n10.1016/j.media.2019.101592\n10.1038/s41598-020-80936-4\n10.1109/TMI.2018.2858202\n10.1164/ajrccm.154.1.8680679\n10.1118/1.3013555\n10.1109/TBME.2013.2244601\n10.1145/3422622\n10.1109/TMI.2009.2035813\n10.1056/NEJMra003200\n10.1016/j.compbiomed.2018.10.011\n10.1001/jama.2016.17216\n10.1109/TMI.2018.2823768\n10.1016/j.diii.2020.03.014\n10.1007/s00138-020-01060-x\n10.1016/s0933-3657(99)00042-1\n10.3390/ijerph16020250\n10.1159/000480435\n10.1007/s10278-019-00227-x\n10.1162/neco.2006.18.7.1527\n10.1136/bmj.332.7549.1077\n10.1162/neco.1997.9.8.1735\n10.1016/j.acra.2008.12.024\n10.1016/s1076-6332(97)80080-3\n10.1016/j.mcna.2017.03.008\n10.1111/j.1365-2222.2008.02971.x\n10.1378/chest.128.4.2005\n10.1007/s11277-018-5777-3\n10.3906/elk-1710-157\n10.1109/JBHI.2018.2852639\n10.1093/jnci/djy225\n10.1016/j.patcog.2018.05.014\n10.1113/jphysiol.1959.sp006308\n10.1148/radiol.2019191022\n10.1161/01.atv.0000012662.29622.00\n10.1513/AnnalsATS.201412-591OC\n10.1164/rccm.201107-1317PP\n10.1136/thx.53.2.129\n10.1378/chest.117.5_suppl_1.251s\n10.1016/s1076-6332(99)80058-0\n10.3390/s19102361\n10.1016/j.ebiom.2020.103106\n10.1016/j.procs.2015.12.145\n10.1109/TMI.2013.2244903\n10.1007/s10278-016-9859-z\n10.1038/s41598-021-93980-5\n10.1164/ajrccm.161.2.9812073\n10.1016/s1076-6332(03)80517-2\n10.1164/rccm.201704-0692OC\n10.1164/rccm.201910-1948LE\n10.1016/j.media.2009.02.004\n10.1148/80.4.653\n10.1148/radiol.2017162326\n10.1038/242190a0\n10.1038/nature14539\n10.1162/neco.1989.1.4.541\n10.1109/5.726791\n10.1364/BOE.8.003440\n10.1088/1361-6560/ab79c4\n10.1056/NEJMoa2001316\n10.1056/NEJMcp010731\n10.1016/j.media.2017.07.005\n10.1038/srep26286\n10.1016/j.eng.2018.11.020\n10.1002/mp.14141\n10.1016/0893-6080(95)00061-5\n10.1007/s00521-020-05437-x\n10.1148/radiology.219.2.r01ma26498\n10.1016/j.artmed.2019.06.008\n10.1016/j.bspc.2006.12.001\n10.1109/TMI.2014.2337057\n10.1093/jnci/92.16.1308\n10.1164/rccm.2107031\n10.1513/AnnalsATS.201307-229OC\n10.1016/s0033-8389(03)00105-2\n10.1148/radiol.2481071446\n10.1016/j.lungcan.2009.03.030\n10.1007/s11548-016-1492-2\n10.1109/72.846746\n10.1007/s11277-018-5702-9\n10.1259/bjr/82634045\n10.1007/s10278-015-9801-9\n10.1109/TMI.2020.3029013\n10.1007/s00330-021-08036-z\n10.1109/Tmi.2018.2833385\n10.1016/j.media.2016.05.009\n10.1164/rccm.200211-1268OC\n10.1016/j.aej.2016.06.002\n10.1186/s40733-016-0029-3\n10.1088/0031-9155/49/20/r01\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.109944\n10.1007/s10278-019-00223-1\n10.1016/j.artmed.2008.07.017\n10.1016/j.rmed.2016.11.021\n10.1164/rccm.201411-2105LE\n10.1016/j.compmedimag.2008.04.005\n10.1109/Access.2021.3131216\n10.1148/radiology.217.2.r00nv01447\n10.1017/gheg.2018.1\n10.1371/journal.pmed.1002686\n10.1016/j.chest.2016.12.033\n10.1109/Tmi.2013.2268424\n10.3109/15412550903499522\n10.1126/science.7423187\n10.1016/j.coph.2004.01.011\n10.1102/1470-7330.2011.9020\n10.1038/323533a0\n10.1109/TMI.2009.2038224\n10.1109/42.538937\n10.1016/j.nicl.2019.102149\n10.1147/rd.33.0210\n10.1016/j.engappai.2014.07.007\n10.1016/j.media.2017.06.015\n10.4103/0971-6203.58777\n10.1146/annurev-bioeng-071516-044442\n10.1186/s40537-019-0197-0\n10.1183/09031936.00170111\n10.1164/rccm.201506-1208PP\n10.1136/thoraxjnl-2014-205160\n10.1155/2017/8314740\n10.1109/42.500140\n10.1016/j.cmpb.2019.06.005\n10.1016/S0140-6736(14)60042-8\n10.3390/diagnostics11081405\n10.1371/journal.pone.0068546\n10.1136/thx.2006.062026\n10.1001/jama.1955.02950310009002\n10.1002/ijc.10882\n10.3233/XST-210955\n10.2214/ajr.163.5.7976869\n10.1109/TMI.2005.857654\n10.1109/TMI.2005.857653\n10.1016/j.ophtha.2016.11.014\n10.1016/j.media.2010.05.005\n10.1109/42.974918\n10.1183/09031936.00111707\n10.1109/Tmi.2018.2791721\n10.1016/j.media.2017.06.014\n10.1148/radiol.2020200843\n10.1007/bf00992698\n10.3322/caac.21172\n10.1148/radiol.11110372\n10.1109/TMI.2017.2708987\n10.1148/ryai.2021200248\n10.1016/j.bspc.2019.101600\n10.1007/s12021-018-9377-x\n10.1109/Tmm.2019.2919431\n10.1038/s41598-021-03002-7\n10.1007/s10916-011-9710-5\n10.1109/TMI.2019.2922960\n10.1016/j.media.2018.10.006\n10.1016/j.neuroimage.2014.12.061\n10.3389/fncom.2015.00066\n10.2528/Pier13121310\n10.2214/Ajr.20.22976\n10.1002/widm.1255"}
{"title": "COVID-19 Detection and Diagnosis Model on CT Scans Based on AI Techniques.", "abstract": "The end of 2019 could be mounted in a rudimentary framing of a new medical problem, which globally introduces into the discussion a fulminant outbreak of coronavirus, consequently spreading COVID-19 that conducted long-lived and persistent repercussions. Hence, the theme proposed to be solved arises from the field of medical imaging, where a pulmonary CT-based standardized reporting system could be addressed as a solution. The core of it focuses on certain impediments such as the overworking of doctors, aiming essentially to solve a classification problem using deep learning techniques, namely, if a patient suffers from COVID-19, viral pneumonia, or is healthy from a pulmonary point of view. The methodology's approach was a meticulous one, denoting an empirical character in which the initial stage, given using data processing, performs an extraction of the lung cavity from the CT scans, which is a less explored approach, followed by data augmentation. The next step is comprehended by developing a CNN in two scenarios, one in which there is a binary classification (COVID and non-COVID patients), and the other one is represented by a three-class classification. Moreover, viral pneumonia is addressed. To obtain an efficient version, architectural changes were gradually made, involving four databases during this process. Furthermore, given the availability of pre-trained models, the transfer learning technique was employed by incorporating the linear classifier from our own convolutional network into an existing model, with the result being much more promising. The experimentation encompassed several models including MobileNetV1, ResNet50, DenseNet201, VGG16, and VGG19. Through a more in-depth analysis, using the CAM technique, MobilneNetV1 differentiated itself via the detection accuracy of possible pulmonary anomalies. Interestingly, this model stood out as not being among the most used in the literature. As a result, the following values of evaluation metrics were reached: loss (0.0751), accuracy (0.9744), precision (0.9758), recall (0.9742), AUC (0.9902), and F1 score (0.9750), from 1161 samples allocated for each of the three individual classes.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2024-01-22", "authors": ["Maria-AlexandraZolya", "CosminBaltag", "Drago\u0219-VasileBratu", "SimonaComan", "Sorin-AurelMoraru"], "doi": "10.3390/bioengineering11010079\n10.1093/trstmh/traa025\n10.1073/pnas.57.4.933\n10.1213/ANE.0000000000004845\n10.1007/s00330-020-06880-z\n10.1002/jmv.25781\n10.3390/v15040916\n10.1016/j.jacr.2020.02.008\n10.4329/wjr.v12.i12.289\n10.1007/s00330-020-06731-x\n10.2214/AJR.20.22961\n10.1007/s00330-020-06827-4\n10.1007/s11042-022-13843-7\n10.1016/j.rmed.2020.105980\n10.3390/diagnostics13193057\n10.1016/B978-0-12-822548-6.00080-7\n10.1007/s11517-022-02758-y\n10.3390/healthcare11172388\n10.1109/ACCESS.2017.2788044\n10.1016/j.crad.2022.11.006\n10.1016/j.media.2016.10.004\n10.1038/s41598-022-06802-7\n10.1111/acem.12831\n10.1016/j.cell.2020.04.045\n10.1016/j.compbiomed.2021.104306\n10.1016/j.bspc.2022.104481\n10.1016/j.procs.2023.01.144\n10.1016/j.compbiomed.2022.106474\n10.1007/s00521-023-08568-z\n10.1007/s13755-022-00203-w\n10.11591/eei.v12i3.4832\n10.1016/j.advengsoft.2022.103317"}
{"title": "Lightweight Techniques to Improve Generalization and Robustness of U-Net Based Networks for Pulmonary Lobe Segmentation.", "abstract": "Lung lobe segmentation in chest CT is relevant to a wide range of clinical applications. However, existing segmentation pipelines often exhibit vulnerabilities and performance degradations when applied to external datasets. This is usually attributed to the size of the available dataset or model. We show that it is possible to enhance generalizability without huge resources by carefully curating the dataset and combining machine learning with medical expertise. Multiple machine learning techniques (self-supervision (SSL), attention (A), and data augmentation (DA)) are used to train a fast and fully-automated lung lobe segmentation model based on 2D U-Net. Our study involved evaluating these techniques on a diverse dataset collected under the RACOON project, encompassing 100 CT chest scans from patients with bacterial, viral, or SARS-CoV2 infections. We compare our model to a baseline U-Net trained on the same dataset. Our approach significantly improved segmentation accuracy (Dice score of 92.8% vs. 82.3%, ", "journal": "Bioengineering (Basel, Switzerland)", "date": "2024-01-22", "authors": ["Armin ADadras", "AchrefJaziri", "EricFrodl", "Thomas JVogl", "JuliaDietz", "Andreas MBucher"], "doi": "10.3390/bioengineering11010021\n10.1007/s00330-020-07147-3\n10.1155/2009/636240\n10.1109/TMI.2005.859209\n10.1186/1475-925X-13-59\n10.1109/TMI.2012.2219881\n10.1186/s41747-020-00173-2\n10.1007/s10278-019-00223-1\n10.1148/radiol.2462070712\n10.1038/s42256-021-00307-0\n10.1109/42.932749\n10.3390/info11020125"}
{"title": "\"KAIZEN\" method realizing implementation of deep-learning models for COVID-19 CT diagnosis in real world hospitals.", "abstract": "Numerous COVID-19 diagnostic imaging Artificial Intelligence (AI) studies exist. However, none of their models were of potential clinical use, primarily owing to methodological defects and the lack of implementation considerations for inference. In this study, all development processes of the deep-learning models are performed based on strict criteria of the \"KAIZEN checklist\", which is proposed based on previous AI development guidelines to overcome the deficiencies mentioned above. We develop and evaluate two binary-classification deep-learning models to triage COVID-19: a slice model examining a Computed Tomography (CT) slice to find COVID-19 lesions; a series model examining a series of CT images to find an infected patient. We collected 2,400,200 CT slices from twelve emergency centers in Japan. Area Under Curve (AUC) and accuracy were calculated for classification performance. The inference time of the system that includes these two models were measured. For validation data, the slice and series models recognized COVID-19 with AUCs and accuracies of 0.989 and 0.982, 95.9% and 93.0% respectively. For test data, the models' AUCs and accuracies were 0.958 and 0.953, 90.0% and 91.4% respectively. The average inference time per case was 2.83\u00a0s. Our deep-learning system realizes accuracy and inference speed high enough for practical use. The systems have already been implemented in four hospitals and eight are under progression. We released an application software and implementation code for free in a highly usable state to allow its use in Japan and globally.", "journal": "Scientific reports", "date": "2024-01-20", "authors": ["NaokiOkada", "YutakaUmemura", "ShoiShi", "ShusukeInoue", "ShunHonda", "YohsukeMatsuzawa", "YuichiroHirano", "AyanoKikuyama", "MihoYamakawa", "TomokoGyobu", "NaohiroHosomi", "KensukeMinami", "NatsushiroMorita", "AtsushiWatanabe", "HiroyukiYamasaki", "KiyomitsuFukaguchi", "HirokiMaeyama", "KaoriIto", "KenOkamoto", "KouheiHarano", "NaohitoMeguro", "RyoUnita", "ShinichiKoshiba", "TakuroEndo", "TomonoriYamamoto", "TomoyaYamashita", "ToshikazuShinba", "SatoshiFujimi"], "doi": "10.1038/s41598-024-52135-y\n10.1128/JCM.01695-20\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1148/radiol.2020200527\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200642\n10.7326/M20-1495\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020200432\n10.1136/bmj.m1464\n10.1016/S2589-7500(20)30199-0\n10.1038/s42256-021-00307-0\n10.1007/s12559-020-09751-3\n10.1016/j.eng.2020.04.010\n10.1016/j.cell.2020.08.029\n10.1038/s41467-020-17971-2\n10.1016/j.compbiomed.2020.103795\n10.1186/s12938-020-00809-9\n10.1109/TMI.2020.2992546\n10.1109/JBHI.2020.3018181\n10.3390/e22050517\n10.1109/TMI.2020.2996256\n10.1136/bmj.m1328\n10.1136/bmj.m3210\n10.1136/bmjopen-2020-047709\n10.1136/bmj.m3164\n10.1038/s41591-020-1041-y\n10.1148/ryai.2020200029\n10.1109/ACCESS.2018.2877890\n10.1371/journal.pone.0258760\n10.1148/radiol.2020201473\n10.1186/s41747-020-00173-2\n10.1038/s42256-021-00338-7\n10.1038/s41591-021-01614-0\n10.1186/s13244-020-00931-1\n10.1007/s00330-020-07042-x\n10.1148/ryct.2020200492\n10.1609/aaai.v34i07.7000\n10.2307/2529886"}
{"title": "COVID-19 detection from chest X-ray images using CLAHE-YCrCb, LBP, and machine learning algorithms.", "abstract": "COVID-19 is a disease that caused a contagious respiratory ailment that killed and infected hundreds of millions. It is necessary to develop a computer-based tool that is fast, precise, and inexpensive to detect COVID-19 efficiently. Recent studies revealed that machine learning and deep learning models accurately detect COVID-19 using chest X-ray (CXR) images. However, they exhibit notable limitations, such as a large amount of data to train, larger feature vector sizes, enormous trainable parameters, expensive computational resources (GPUs), and longer run-time.\nIn this study, we proposed a new approach to address some of the above-mentioned limitations. The proposed model involves the following steps: First, we use contrast limited adaptive histogram equalization (CLAHE) to enhance the contrast of CXR images. The resulting images are converted from CLAHE to YCrCb color space. We estimate reflectance from chrominance using the Illumination-Reflectance model. Finally, we use a normalized local binary patterns histogram generated from reflectance (Cr) and YCb as the classification feature vector. Decision tree, Naive Bayes, support vector machine, K-nearest neighbor, and logistic regression were used as the classification algorithms. The performance evaluation on the test set indicates that the proposed approach is superior, with accuracy rates of 99.01%, 100%, and 98.46% across three different datasets, respectively. Naive Bayes, a probabilistic machine learning algorithm, emerged as the most resilient.\nOur proposed method uses fewer handcrafted features, affordable computational resources, and less runtime than existing state-of-the-art approaches. Emerging nations where radiologists are in short supply can adopt this prototype. We made both coding materials and datasets accessible to the general public for further improvement. Check the manuscript's availability of the data and materials under the declaration section for access.", "journal": "BMC bioinformatics", "date": "2024-01-18", "authors": ["RukundoPrince", "ZhendongNiu", "Zahid YounasKhan", "MasaboEmmanuel", "NiyishakaPatrick"], "doi": "10.1186/s12859-023-05427-5\n10.1111/exsy.12749\n10.1111/exsy.12919\n10.1111/exsy.12842\n10.1111/exsy.12742\n10.1016/j.compbiomed.2022.105233\n10.1016/j.bspc.2021.103182\n10.1016/j.eswa.2020.114054\n10.1109/ACCESS.2020.3016780\n10.1186/s12859-022-04818-4\n10.1109/ACCESS.2021.3077592\n10.1111/exsy.12776\n10.1016/j.compbiomed.2021.104816\n10.1016/j.bbe.2021.05.013\n10.1111/exsy.12694\n10.1186/s13640-019-0445-4\n10.1109/ACCESS.2019.2910605\n10.1007/s11042-020-09707-7\n10.1016/j.compbiomed.2021.104453\n10.1016/j.imu.2020.100505\n10.1016/j.infrared.2021.103847\n10.1016/j.procs.2019.12.112\n10.1016/j.patcog.2018.06.015\n10.1186/s40494-021-00504-5\n10.1111/exsy.12895\n10.1111/exsy.12813\n10.1111/exsy.12791\n10.1111/exsy.12714\n10.1016/j.eswa.2022.117410\n10.1016/j.compbiomed.2020.103792\n10.1109/ACCESS.2020.3010287\n10.1111/exsy.12797\n10.1016/j.compbiomed.2021.104319"}
{"title": "Integrating artificial intelligence-based epitope prediction in a SARS-CoV-2 antibody discovery pipeline: caution is warranted.", "abstract": "SARS-CoV-2-neutralizing antibodies (nABs) showed great promise in the early phases of the COVID-19 pandemic. The emergence of resistant strains, however, quickly rendered the majority of clinically approved nABs ineffective. This underscored the imperative to develop nAB cocktails targeting non-overlapping epitopes.\nUndertaking a nAB discovery program, we employed a classical workflow, while integrating artificial intelligence (AI)-based prediction to select non-competing nABs very early in the pipeline. We identified and in\u00a0vivo validated (in female Syrian hamsters) two highly potent nABs.\nDespite the promising results, in depth cryo-EM structural analysis demonstrated that the AI-based prediction employed with the intention to ensure non-overlapping epitopes was inaccurate. The two nABs in fact bound to the same receptor-binding epitope in a remarkably similar manner.\nOur findings indicate that, even in the Alphafold era, AI-based predictions of paratope-epitope interactions are rough and experimental validation of epitopes remains an essential cornerstone of a successful nAB lead selection.\nFull list of funders is provided at the end of the manuscript.", "journal": "EBioMedicine", "date": "2024-01-18", "authors": ["Delphine DianaAcar", "WojciechWitkowski", "MagdalenaWejda", "RuifangWei", "TimDesmet", "BertSchepens", "SieglindeDe Cae", "KoenSedeyn", "HannahEeckhaut", "DariaFijalkowska", "KennyRoose", "SandrineVanmarcke", "AnnePoupon", "DirkJochmans", "XinZhang", "RanaAbdelnabi", "Caroline SFoo", "BirgitWeynand", "DirkReiter", "NicoCallewaert", "HanRemaut", "JohanNeyts", "XavierSaelens", "SarahGerlo", "LinosVandekerckhove"], "doi": "10.1016/j.ebiom.2023.104960\n10.1101/2020.07.21.214932\n10.1101/2020.12.23.424199\n10.1101/2020.12.23.424199\n10.1101/2020.11.30.405472\n10.1101/2021.04.11.439351"}
{"title": "Machine Learning Predictive Modeling for the Identification of Moderate Coronavirus Disease 2019 During the Pandemic: A Retrospective Study.", "abstract": "Timely differentiation of moderate\u00a0COVID-19\u00a0cases from mild cases is beneficial for early treatment and saves medical resources during the pandemic. We attempted to construct a model to predict the occurrence of moderate COVID-19 through a retrospective study.\nIn this retrospective study, clinical data from patients with COVID-19 admitted to Hainan Western Central Hospital in Danzhou, China, between August 1, 2022, and August 31, 2022, was collected, including sex, age, signs on admission, comorbidities, imaging data, post-admission treatment, length of stay, and the results of laboratory tests on admission. The patients were classified into a mild-to-moderate-type group according to WHO guidance. Factors that differed between groups were included in machine learning models such as Bernoulli Na\u00efve Bayes (BNB), linear discriminant analysis, support vector machine (SVM), least absolute shrinkage and selection operator (LASSO), and logistic regression (LR) models. These models were compared to select the optimal model with the best predictive efficacy for moderate COVID-19. The predictive performance of the models was assessed using the area under the curve (AUC), sensitivity, specificity, and calibration plot.\nA total of 231 patients with COVID-19 were included in this retrospective analysis. Among them, 152 (68.83%) were mild types, 72 (31.17%) were moderate types, and there were no patients with severe or critical types. A logistic regression model combined with age, respiratory rate (RR), lactate dehydrogenase (LDH), D-dimer, and albumin was selected to predict the occurrence of moderate COVID-19. The receiver operating characteristic curve (ROC) showed that AUC, sensitivity, and specificity in the model were 0.719, 0.681, and 0.635, respectively, in predicting moderate COVID-19. Calibration curve analysis revealed that the predicted probability of the model was in good agreement with the true probability. Stratified analysis showed better predictive efficacy after modeling for people aged \u226466 years (AUC = 0.7656) and a better calibration curve.\nThe LR model, combined with age, RR, D-dimer, LDH, and albumin, can predict the occurrence of moderate COVID-19 well, especially for patients aged \u226466 years.", "journal": "Cureus", "date": "2024-01-16", "authors": ["TaoWang", "ZhanqingZhao", "WenzheLi", "JingWu", "QianruYe", "HuiXie"], "doi": "10.7759/cureus.50619"}
{"title": "Self-attention-driven retrieval of chest CT images for COVID-19 assessment.", "abstract": "Numerous methods have been developed for computer-aided diagnosis (CAD) of coronavirus disease-19 (COVID-19), based on chest computed tomography (CT) images. The majority of these methods are based on deep neural networks and often act as \"black boxes\" that cannot easily gain the trust of medical community, whereas their result is uniformly influenced by all image regions. This work introduces a novel, self-attention-driven method for content-based image retrieval (CBIR) of chest CT images. The proposed method analyzes a query CT image and returns a classification result, as well as a list of classified images, ranked according to similarity with the query. Each CT image is accompanied by a heatmap, which is derived by gradient-weighted class activation mapping (Grad-CAM) and represents the contribution of lung tissue and lesions to COVID-19 pathology. Beyond visualization, Grad-CAM weights are employed in a self-attention mechanism, in order to strengthen the influence of the most COVID-19-related image regions on the retrieval result. Experiments on two publicly available datasets demonstrate that the binary classification accuracy obtained by means of DenseNet-201 is 81.3% and 96.4%, for COVID-CT and SARS-CoV-2 datasets, respectively, with a false negative rate which is less than 3% in both datasets. In addition, the Grad-CAM-guided CBIR framework slightly outperforms the plain CBIR in most cases, with respect to nearest neighbour (NN) and first four (FF). The proposed method could serve as a computational tool for a more transparent decision-making process that could be trusted by the medical community. In addition, the employed self-attention mechanism increases the obtained retrieval performance.", "journal": "Biomedical physics & engineering express", "date": "2024-01-15", "authors": ["VictoriaFili", "MichalisSavelonas"], "doi": "10.1088/2057-1976/ad1e76"}
{"title": "Mental health of junior college students in China during COVID-19 school lockdown: Findings of on-line cross-sectional survey.", "abstract": "During the COVID-19 pandemic, junior students who had recently entered university may have experienced particular difficulties. This study aimed to investigate the incidence of anxiety, depression, and sleep status among junior college students during school closure. Junior college students from 3colleges in Anhui Province participated in this study from 6th to 20th April, 2022. The students' data were collected using a designed online questionnaire developed on the \"Wen juan xing\" website and submitted via cell phone. Ordinal logistic regression analysis indicated that female sex was an independent risk factor for increased anxiety, depression, and insomnia (anxiety: OR 1.503, 95% CI 1.191-1.897; depression: OR 1.14, 95% CI 1.023-1.270; ISI OR 2.052, 95% CI 1.646-2.559). Notably, medical specialty was an independent risk factor for depression and anxiety (anxiety: OR 1.367, 95% CI 1.078-1.734; depression: OR 1.289, 95% CI 1.148-1.448). Moreover, being a freshman was a risk factor for increased depression and insomnia (depression: OR 1.036,95% CI 0.931-1.153; insomnia: (OR 1.157,95% CI 0.961-1.394). The findings indicate that a considerable portion of junior college students experienced psychological problems due to lockdowns during the COVID-19 pandemic.", "journal": "Medicine", "date": "2024-01-11", "authors": ["FengLi", "JingWang", "JiuChen", "QianChen", "JunxiaWang", "MaoxueWang", "ShouliangMa", "BingZhang", "WenxiaHu"], "doi": "10.1097/MD.0000000000036808"}
{"title": "Artificial intelligence-based analysis of the spatial distribution of abnormal computed tomography patterns in SARS-CoV-2 pneumonia: association with disease severity.", "abstract": "The substantial heterogeneity of clinical presentations in patients with severe acute respiratory syndrome\u00a0coronavirus 2 (SARS-CoV-2) pneumonia still requires robust chest computed tomography analysis to identify high-risk patients. While extension of ground-glass opacity and consolidation from peripheral to central lung fields on chest computed tomography (CT) might be associated with severely ill conditions, quantification of the central-peripheral distribution of ground glass opacity and consolidation in assessments of SARS-CoV-2 pneumonia remains unestablished. This study aimed to examine whether the central-peripheral distributions of ground glass opacity and consolidation were associated with severe outcomes in patients with SARS-CoV-2 pneumonia independent of the whole-lung extents of these abnormal shadows.\nThis multicenter retrospective cohort included hospitalized patients with SARS-CoV-2 pneumonia between January 2020 and August 2021. An artificial intelligence-based image analysis technology was used to segment abnormal shadows, including ground glass opacity and consolidation. The area ratio of ground glass opacity and consolidation to the whole lung (GGO%, CON%) and the ratio of ground glass opacity and consolidation areas in the central lungs to those in the peripheral lungs (GGO(C/P)) and (CON(C/P)) were automatically calculated. Severe outcome was defined as in-hospital death or requirement for endotracheal intubation.\nOf 512 enrolled patients, the severe outcome was observed in 77 patients. GGO% and CON% were higher in patients with severe outcomes than in those without. Multivariable logistic models showed that GGO(C/P), but not CON(C/P), was associated with the severe outcome independent of age, sex, comorbidities, GGO%, and CON%.\nIn addition to GGO% and CON% in the whole lung, the higher the ratio of ground glass opacity in the central regions to that in the peripheral regions was, the more severe the outcomes in patients with SARS-CoV-2 pneumonia were. The proposed method might be useful to reproducibly quantify the extension of ground glass opacity from peripheral to central lungs and to estimate prognosis.", "journal": "Respiratory research", "date": "2024-01-11", "authors": ["YusukeKataoka", "NaoyaTanabe", "MasahiroShirata", "NobuyoshiHamao", "IsseiOi", "TomokiMaetani", "YusukeShiraishi", "KentaroHashimoto", "MasatoshiYamazoe", "HiroshiShima", "HitomiAjimizu", "TsuyoshiOguma", "MasahitoEmura", "KazuoEndo", "YoshinoriHasegawa", "TadashiMio", "TetsuhiroShiota", "HiroakiYasui", "HitoshiNakaji", "MichikoTsuchiya", "KeisukeTomii", "ToyohiroHirai", "IsaoIto"], "doi": "10.1186/s12931-024-02673-w\n10.1001/jama.2020.2648\n10.1016/S0140-6736(20)30183-5\n10.1038/s41467-020-18786-x\n10.1016/j.ejrad.2020.109202\n10.1007/s00330-020-06955-x\n10.1148/radiol.2020200230\n10.1148/radiol.2020200274\n10.1148/radiol.2020200280\n10.1148/radiol.2020200370\n10.1148/radiol.2020200843\n10.2214/AJR.20.22975\n10.1007/s00330-023-09427-0\n10.1148/ryct.2020200130\n10.1148/radiol.2020200463\n10.1148/ryct.2020200389\n10.1007/s00330-020-06817-6\n10.1148/ryct.2020200047\n10.1038/s41551-018-0195-0\n10.1001/jama.2016.17216\n10.1038/s41591-018-0316-z\n10.1038/s41467-020-17971-2\n10.1016/j.cell.2020.04.045\n10.1513/AnnalsATS.202101-044OC\n10.1186/s12879-022-07927-w\n10.1038/s41379-021-00814-w\n10.1016/j.ebiom.2022.104229\n10.1016/j.chest.2020.09.259\n10.1016/S1473-3099(20)30367-4\n10.1007/s00330-020-07033-y\n10.3390/diagnostics11020265\n10.21037/qims-20-564\n10.12659/MSM.925183\n10.1148/ryct.2020200034\n10.1002/jmv.25889\n10.1016/j.intimp.2022.109088\n10.1148/radiol.222462\n10.1136/thoraxjnl-2021-217080\n10.1016/j.diabet.2020.07.005\n10.1186/s40001-020-00432-3\n10.1093/cid/ciaa1012\n10.3389/fmed.2023.1271863"}
{"title": "Chronic Lung Injury after COVID-19 Pneumonia: Clinical, Radiologic, and Histopathologic Perspectives.", "abstract": "With the COVID-19 pandemic having lasted more than 3 years, concerns are growing about prolonged symptoms and respiratory complications in COVID-19 survivors, collectively termed post-COVID-19 condition (PCC). Up to 50% of patients have residual symptoms and physiologic impairment, particularly dyspnea and reduced diffusion capacity. Studies have also shown that 24%-54% of patients hospitalized during the 1st year of the pandemic exhibit radiologic abnormalities, such as ground-glass opacity, reticular opacity, bronchial dilatation, and air trapping, when imaged more than 1 year after infection. In patients with persistent respiratory symptoms but normal results at chest CT, dual-energy contrast-enhanced CT, xenon 129 MRI, and low-field-strength MRI were reported to show abnormal ventilation and/or perfusion, suggesting that some lung injury may not be detectable with standard CT. Histologic patterns in post-COVID-19 lung disease include fibrosis, organizing pneumonia, and vascular abnormality, indicating that different pathologic mechanisms may contribute to PCC. Therefore, a comprehensive imaging approach is necessary to evaluate and diagnose patients with persistent post-COVID-19 symptoms. This review will focus on the long-term findings of clinical and radiologic abnormalities and describe histopathologic perspectives. It also addresses advanced imaging techniques and deep learning approaches that can be applied to COVID-19 survivors. This field remains an active area of research, and further follow-up studies are warranted for a better understanding of the chronic stage of the disease and developing a multidisciplinary approach for patient management.", "journal": "Radiology", "date": "2024-01-09", "authors": ["Min JaeCha", "Joshua JSolomon", "Jong EunLee", "HyewonChoi", "Kum JuChae", "Kyung SooLee", "David ALynch"], "doi": "10.1148/radiol.231643"}
{"title": "An AI-based novel system for predicting respiratory support in COVID-19 patients through CT imaging analysis.", "abstract": "The proposed AI-based diagnostic system aims to predict the respiratory support required for COVID-19 patients by analyzing the correlation between COVID-19 lesions and the level of respiratory support provided to the patients. Computed tomography (CT) imaging will be used to analyze the three levels of respiratory support received by the patient: Level 0 (minimum support), Level 1 (non-invasive support such as soft oxygen), and Level 2 (invasive support such as mechanical ventilation). The system will begin by segmenting the COVID-19 lesions from the CT images and creating an appearance model for each lesion using a 2D, rotation-invariant, Markov-Gibbs random field (MGRF) model. Three MGRF-based models will be created, one for each level of respiratory support. This suggests that the system will be able to differentiate between different levels of severity in COVID-19 patients. The system will decide for each patient using a neural network-based fusion system, which combines the estimates of the Gibbs energy from the three MGRF-based models. The proposed system were assessed using 307 COVID-19-infected patients, achieving an accuracy of [Formula: see text], a sensitivity of [Formula: see text], and a specificity of [Formula: see text], indicating a high level of prediction accuracy.", "journal": "Scientific reports", "date": "2024-01-09", "authors": ["Ibrahim ShawkyFarahat", "AhmedSharafeldeen", "MohammedGhazal", "Norah SalehAlghamdi", "AliMahmoud", "JamesConnelly", "Ericvan Bogaert", "HumaZia", "TaniaTahtouh", "WaleedAladrousy", "Ahmed ElsaidTolba", "SamirElmougy", "AymanEl-Baz"], "doi": "10.1038/s41598-023-51053-9\n10.3390/life12060806\n10.1016/j.scitotenv.2020.138870\n10.1016/j.ajem.2020.08.001\n10.1016/j.scs.2020.102568\n10.1136/bmjopen-2020-042946\n10.1515/cclm-2020-1294\n10.1007/s10916-020-01597-4\n10.2196/25884\n10.1002/jmv.27352\n10.1080/09720502.2021.2015097\n10.1186/s12938-020-00807-x\n10.1016/j.eswa.2021.114883\n10.5455/jjee.204-1585312246\n10.3390/s21165482\n10.1038/s41598-021-83735-7\n10.1038/s41598-021-91305-0\n10.3390/diagnostics12030696\n10.3390/app12115377\n10.3390/diagnostics12020461\n10.1016/j.eswa.2019.05.028\n10.1007/s10462-011-9272-4\n10.1145/2990508\n10.1006/jcss.1997.1504\n10.1001/jama.2020.4326\n10.1001/jama.2020.6775\n10.1007/s11042-022-12508-9\n10.3390/cancers14071840\n10.3390/bioengineering9100493\n10.1038/s41598-022-07890-1\n10.1016/j.eswa.2021.115805\n10.1002/mp.15399\n10.1016/j.ajo.2020.01.016\n10.3390/s22093490\n10.3390/cancers15215216\n10.3390/s22207833\n10.3390/app12168326\n10.3390/s22114250\n10.1007/s00521-022-07241-1\n10.1007/s12652-022-04342-6"}
{"title": "Diagnosis of Covid-19 from CT slices using Whale Optimization Algorithm, Support Vector Machine and Multi-Layer Perceptron.", "abstract": "The coronavirus disease 2019 is a serious and highly contagious disease caused by infection with a newly discovered virus, named severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).\nA Computer Aided Diagnosis (CAD) system to assist physicians to diagnose Covid-19 from chest Computed Tomography (CT) slices is modelled and experimented.\nThe lung tissues are segmented using Otsu's thresholding method. The Covid-19 lesions have been annotated as the Regions of Interest (ROIs), which is followed by texture and shape extraction. The obtained features are stored as feature vectors and split into 80:20 train and test sets. To choose the optimal features, Whale Optimization Algorithm (WOA) with Support Vector Machine (SVM) classifier's accuracy is employed. A Multi-Layer Perceptron (MLP) classifier is trained to perform classification with the selected features.\nComparative experimentations of the proposed system with existing eight benchmark Machine Learning classifiers using real-time dataset demonstrates that the proposed system with 88.94% accuracy outperforms the benchmark classifier's results. Statistical analysis namely, Friedman test, Mann Whitney U test and Kendall's Rank Correlation Coefficient Test has been performed which indicates that the proposed method has a significant impact on the novel dataset considered.\nThe MLP classifier's accuracy without feature selection yielded 80.40%, whereas with feature selection using WOA, it yielded 88.94%.", "journal": "Journal of X-ray science and technology", "date": "2024-01-08", "authors": ["RBetshrine Rachel", "HKhanna Nehemiah", "Vaibhav KumarSingh", "Rebecca Mercy VictoriaManoharan"], "doi": "10.3233/XST-230196"}
{"title": "Auto-detection of the coronavirus disease by using deep convolutional neural networks and X-ray photographs.", "abstract": "The most widely used method for detecting Coronavirus Disease 2019 (COVID-19) is real-time polymerase chain reaction. However, this method has several drawbacks, including high cost, lengthy turnaround time for results, and the potential for false-negative results due to limited sensitivity. To address these issues, additional technologies such as computed tomography (CT) or X-rays have been employed for diagnosing the disease. Chest X-rays are more commonly used than CT scans due to the widespread availability of X-ray machines, lower ionizing radiation, and lower cost of equipment. COVID-19 presents certain radiological biomarkers that can be observed through chest X-rays, making it necessary for radiologists to manually search for these biomarkers. However, this process is time-consuming and prone to errors. Therefore, there is a critical need to develop an automated system for evaluating chest X-rays. Deep learning techniques can be employed to expedite this process. In this study, a deep learning-based method called Custom Convolutional Neural Network (Custom-CNN) is proposed for identifying COVID-19 infection in chest X-rays. The Custom-CNN model consists of eight weighted layers and utilizes strategies like dropout and batch normalization to enhance performance and reduce overfitting. The proposed approach achieved a classification accuracy of 98.19% and aims to accurately classify COVID-19, normal, and pneumonia samples.", "journal": "Scientific reports", "date": "2024-01-05", "authors": ["Ahmad MohdAzizHussein", "Abdulrauf GarbaSharifai", "Osama Moh'dAlia", "LaithAbualigah", "Khaled HAlmotairi", "Sohaib K MAbujayyab", "Amir HGandomi"], "doi": "10.1038/s41598-023-47038-3\n10.1016/j.ijsu.2020.02.034\n10.1056/NEJMoa2001191\n10.1148/ryct.2020200028\n10.1016/j.physio.2020.03.003\n10.1007/s12098-020-03263-6\n10.1016/j.nmni.2020.100669\n10.1016/j.asoc.2021.107490\n10.1148/radiol.2020200490\n10.1148/radiol.2020200527\n10.1148/radiol.2020200343\n10.1016/S1473-3099(20)30134-1\n10.1148/radiol.2020200463\n10.1109/JBHI.2021.3072076\n10.1016/j.media.2017.07.005\n10.3844/jcssp.2020.1278.1290\n10.1038/nature14539\n10.1016/j.compbiomed.2018.09.009\n10.1038/nature21056\n10.1016/j.compbiomed.2021.104319\n10.55525/tjst.1092676\n10.1007/s10096-020-03901-z\n10.3390/jpm11060482\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1038/s41598-020-74539-2\n10.1109/TMI.2020.2993291\n10.1016/j.eswa.2021.114883\n10.1038/s41598-021-88807-2\n10.1016/j.bbe.2021.12.001\n10.1007/s00521-022-06918-x\n10.1007/s10489-020-01829-7\n10.1016/j.patrec.2021.11.020\n10.3390/su14116785\n10.1016/j.bspc.2022.103977\n10.1007/s13755-021-00166-4\n10.4103/jmp.jmp_100_21\n10.3390/diagnostics13081484\n10.1016/j.matpr.2022.05.199\n10.1002/ima.22564\n10.1155/2022/3861161\n10.1007/s10044-021-00984-y\n10.1016/j.cmpb.2021.106288"}
{"title": "Potential diagnostic application of a novel deep learning- based approach for COVID-19.", "abstract": "COVID-19 is a highly communicable respiratory illness caused by the novel coronavirus SARS-CoV-2, which has had a significant impact on global public health and the economy. Detecting COVID-19 patients during a pandemic with limited medical facilities can be challenging, resulting in errors and further complications. Therefore, this study aims to develop deep learning models to facilitate automated diagnosis of COVID-19 from CT scan records of patients. The study also introduced COVID-MAH-CT, a new dataset that contains 4442 CT scan images from 133 COVID-19 patients, as well as 133 CT scan 3D volumes. We proposed and evaluated six different transfer learning models for slide-level analysis that are responsible for detecting COVID-19 in multi-slice spiral CT. Additionally, multi-head attention squeeze and excitation residual (MASERes) neural network, a novel 3D deep model was developed for patient-level analysis, which analyzes all the CT slides of a given patient as a whole and can accurately diagnose COVID-19. The codes and dataset developed in this study are available at https://github.com/alrzsdgh/COVID . The proposed transfer learning models for slide-level analysis were able to detect COVID-19 CT slides with an accuracy of more than 99%, while MASERes was able to detect COVID-19 patients from 3D CT volumes with an accuracy of 100%. These achievements demonstrate that the proposed models in this study can be useful for automatically detecting COVID-19 in both slide-level and patient-level from patients' CT scan records, and can be applied for real-world utilization, particularly in diagnosing COVID-19 cases in areas with limited medical facilities.", "journal": "Scientific reports", "date": "2024-01-04", "authors": ["AlirezaSadeghi", "MahdiehSadeghi", "AliSharifpour", "MahdiFakhar", "ZakariaZakariaei", "MohammadrezaSadeghi", "MojtabaRokni", "AtousaZakariaei", "Elham SadatBanimostafavi", "FarshidHajati"], "doi": "10.1038/s41598-023-50742-9\n10.1016/j.cpcardiol.2020.100618\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1002/rmv.2179\n10.1016/j.chest.2020.11.026\n10.1007/s00330-020-07346-y\n10.1148/radiol.2020201343\n10.1016/j.metabol.2017.01.011\n10.1016/j.neucom.2017.11.077\n10.1038/nature14539\n10.1155/2022/3626726\n10.1148/radiol.2020200905\n10.1148/ryct.2020200075\n10.4018/IJEHMC.20220701.oa4\n10.1155/2022/7672196\n10.3390/s21020455\n10.1016/j.bspc.2021.102588\n10.1016/j.compbiomed.2021.104306\n10.1109/TETCI.2022.3174868\n10.1038/s41597-021-00900-3\n10.1038/nature14539\n10.1016/j.media.2021.101985\n10.1016/j.media.2020.101693\n10.1109/JPROC.2020.3004555\n10.1186/s12880-022-00793-7\n10.1016/j.asoc.2019.105524\n10.1145/3065386\n10.1016/j.ejrad.2020.109041\n10.1016/j.compbiomed.2022.105464\n10.1007/s10278-022-00734-4"}
{"title": "Detection of COVID-19 using edge devices by a light-weight convolutional neural network from chest X-ray images.", "abstract": "Deep learning is a highly significant technology in clinical treatment and diagnostics nowadays. Convolutional Neural Network (CNN) is a new idea in deep learning that is being used in the area of computer vision. The COVID-19 detection is the subject of our medical study. Researchers attempted to increase the detection accuracy but at the cost of high model complexity. In this paper, we desire to achieve better accuracy with little training space and time so that this model easily deployed in edge devices. In this paper, a new CNN design is proposed that has three stages: pre-processing, which removes the black padding on the side initially; convolution, which employs filter banks; and feature extraction, which makes use of deep convolutional layers with skip connections. In order to train the model, chest X-ray images are partitioned into three sets: learning(0.7), validation(0.1), and testing(0.2). The models are then evaluated using the test and training data. The LMNet, CoroNet, CVDNet, and Deep GRU-CNN models are the other four models used in the same experiment. The propose model achieved 99.47% & 98.91% accuracy on training and testing respectively. Additionally, it achieved 97.54%, 98.19%, 99.49%, and 97.86% scores for precision, recall, specificity, and f1-score respectively. The proposed model obtained nearly equivalent accuracy and other similar metrics when compared with other models but greatly reduced the model complexity. Moreover, it is found that proposed model is less prone to over fitting as compared to other models.", "journal": "BMC medical imaging", "date": "2024-01-04", "authors": ["SohamkumarChauhan", "Damoder ReddyEdla", "VijayasreeBoddu", "M JayanthiRao", "RamalingaswamyCheruku", "Soumya RanjanNayak", "SheshikalaMartha", "KamppaLavanya", "Tsedenya DebebeNigat"], "doi": "10.1186/s12880-023-01155-7\n10.1038/s41586-020-2008-3\n10.1016/j.ijsu.2020.02.034\n10.1097/RTI.0000000000000404\n10.1016/S0140-6736(20)30211-7\n10.1109/ACCESS.2021.3052494\n10.1109/ACCESS.2020.3010287\n10.1109/ACCESS.2021.3077592\n10.1109/ACCESS.2021.3083516\n10.1148/radiol.2020200432\n10.1109/TII.2021.3057524\n10.1152/physiolgenomics.00029.2020\n10.1109/TIFS.2017.2710946"}
{"title": "The Scottish Medical Imaging Archive: 57.3 Million Radiology Studies Linked to Their Medical Records.", "abstract": "", "journal": "Radiology. Artificial intelligence", "date": "2024-01-03", "authors": ["RobBaxter", "ThomasNind", "JamesSutherland", "GordonMcAllister", "DouglasHardy", "AllyHume", "RuairidhMacLeod", "JacquelineCaldwell", "SusanKrueger", "LeandroTramma", "RossTeviotdale", "KennyGillen", "DonaldScobbie", "IanBaillie", "AndrewBrooks", "BiancaProdan", "WilliamKerr", "DominicSloan-Murphy", "Juan F RHerrera", "Edwin J Rvan Beek", "Parminder SinghReel", "SmartiReel", "EsmaMansouri-Benssassi", "RoyMudie", "DouglasSteele", "AlexDoney", "EmanueleTrucco", "CaroleMorris", "RobertWallace", "AndrewMorris", "MarkParsons", "EmilyJefferson"], "doi": "10.1148/ryai.220266"}
{"title": "Assessment of a deep learning model for COVID-19 classification on chest radiographs: a comparison across image acquisition techniques and clinical factors.", "abstract": "The purpose is to assess the performance of a pre-trained deep learning model in the task of classifying between coronavirus disease (COVID)-positive and COVID-negative patients from chest radiographs (CXRs) while considering various image acquisition parameters, clinical factors, and patient demographics.\nStandard and soft-tissue CXRs of 9860 patients comprised the \"original dataset,\" consisting of training and test sets and were used to train a DenseNet-121 architecture model to classify COVID-19 using three classification algorithms: standard, soft tissue, and a combination of both types of images via feature fusion. A larger more-current test set of 5893 patients (the \"current test set\") was used to assess the performance of the pretrained model. The current test set contained a larger span of dates, incorporated different variants of the virus and included different immunization statuses. Model performance between the original and current test sets was evaluated using area under the receiver operating characteristic curve (ROC AUC) [95% CI].\nThe model achieved AUC values of 0.67 [0.65, 0.70] for cropped standard images, 0.65 [0.63, 0.67] for cropped soft-tissue images, and 0.67 [0.65, 0.69] for both types of cropped images. These were all significantly lower than the performance of the model on the original test set. Investigations regarding matching the acquisition dates between the test sets (i.e., controlling for virus variants), immunization status, disease severity, and age and sex distributions did not fully explain the discrepancy in performance.\nSeveral relevant factors were considered to determine whether differences existed in the test sets, including time period of image acquisition, vaccination status, and disease severity. The lower performance on the current test set may have occurred due to model overfitting and a lack of generalizability.", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2024-01-02", "authors": ["MenaShenouda", "IsabellaFlerlage", "AditiKaveti", "Maryellen LGiger", "Samuel GArmato"], "doi": "10.1117/1.JMI.10.6.064504\n10.1148/ryct.2020200034\n10.1148/radiol.2020201160\n10.1148/radiol.2020201874\n10.1148/radiol.2020202944\n10.1097/rti.0000000000000559\n10.1148/radiol.2020203511\n10.1007/s00330-021-07797-x\n10.1016/j.cell.2020.04.045\n10.1038/s42256-021-00307-0\n10.1126/scitranslmed.abb1655\n10.1038/s41746-022-00614-9\n10.1117/12.2652106\n10.1117/1.JMI.8.S1.014503\n10.1038/s41598-021-87994-2\n10.3978/j.issn.2223-4292.2014.11.20\n10.1038/s41598-020-76550-z\n10.1109/cvpr.2017.243]\n10.48550/arXiv.1711.05225\n10.1371/journal.pmed.1002686\n10.1109/cvpr.2009.5206848\n10.1109/cvpr.2017.369\n10.1148/ryai.2020200079\n10.1609/aaai.v33i01.3301590\n10.1136/thoraxjnl-2017-211280\n10.1016/S0140-6736(86)90837-8\n10.48550/arXiv.1802.03426\n10.2307/2531595\n10.2307/4615733\n10.1006/jmps.1998.1218\n10.1148/ryai.2021210011\n10.1038/s42256-020-00251-5\n10.48550/arXiv.2003.13865\n10.48550/arXiv.1607.02533\n10.48550/arXiv.2002.11379"}
{"title": "RVCNet: A hybrid deep neural network framework for the diagnosis of lung diseases.", "abstract": "Early evaluation and diagnosis can significantly reduce the life-threatening nature of lung diseases. Computer-aided diagnostic systems (CADs) can help radiologists make more precise diagnoses and reduce misinterpretations in lung disease diagnosis. Existing literature indicates that more research is needed to correctly classify lung diseases in the presence of multiple classes for different radiographic imaging datasets. As a result, this paper proposes RVCNet, a hybrid deep neural network framework for predicting lung diseases from an X-ray dataset of multiple classes. This framework is developed based on the ideas of three deep learning techniques: ResNet101V2, VGG19, and a basic CNN model. In the feature extraction phase of this new hybrid architecture, hyperparameter fine-tuning is used. Additional layers, such as batch normalization, dropout, and a few dense layers, are applied in the classification phase. The proposed method is applied to a dataset of COVID-19, non-COVID lung infections, viral pneumonia, and normal patients' X-ray images. The experiments take into account 2262 training and 252 testing images. Results show that with the Nadam optimizer, the proposed algorithm has an overall classification accuracy, AUC, precision, recall, and F1-score of 91.27%, 92.31%, 90.48%, 98.30%, and 94.23%, respectively. Finally, these results are compared with some recent deep-learning models. For this four-class dataset, the proposed RVCNet has a classification accuracy of 91.27%, which is better than ResNet101V2, VGG19, VGG19 over CNN, and other stand-alone models. Finally, the application of the GRAD-CAM approach clearly interprets the classification of images by the RVCNet framework.", "journal": "PloS one", "date": "2023-12-28", "authors": ["Fatema BinteAlam", "PrajoyPodder", "M Rubaiyat HossainMondal"], "doi": "10.1371/journal.pone.0293125\n10.1016/j.imu.2020.100374\n10.1016/j.compbiomed.2022.105350\n10.1155/2019/4180949\n10.1148/ryai.2021190228\n10.1016/j.imu.2020.100391\n10.3389/fpubh.2022.894920\n10.1155/2022/9036457\n10.1016/j.asoc.2021.107645\n10.1007/s13246-020-00865-4\n10.1007/s10278-018-0112-9\n10.1007/s11045-020-00756-7\n10.1016/j.compbiomed.2023.106646\n10.1016/j.irbm.2020.05.003\n10.1109/TII.2021.3057683\n10.1016/j.compbiomed.2022.106156\n10.1007/s10916-022-01868-2\n10.1155/2021/2158184\n10.1007/s13755-021-00152-w\n10.3390/s21175813\n10.1016/j.cmpb.2020.105581\n10.1016/j.chaos.2020.109944\n10.1007/s10489-020-02055-x\n10.1038/s41598-021-03287-8"}
{"title": "A systematic review and meta-analysis on ChatGPT and its utilization in medical and dental research.", "abstract": "Since its release, ChatGPT has taken the world by storm with its utilization in various fields of life. This review's main goal was to offer a thorough and fact-based evaluation of ChatGPT's potential as a tool for medical and dental research, which could direct subsequent research and influence clinical practices.\nDifferent online databases were scoured for relevant articles that were in accordance with the study objectives. A team of reviewers was assembled to devise a proper methodological framework for inclusion of articles and meta-analysis.\n11 descriptive studies were considered for this review that evaluated the accuracy of ChatGPT in answering medical queries related to different domains such as systematic reviews, cancer, liver diseases, diagnostic imaging, education, and COVID-19 vaccination. The studies reported different accuracy ranges, from 18.3\u00a0% to 100\u00a0%, across various datasets and specialties. The meta-analysis showed an odds ratio (OR) of 2.25 and a relative risk (RR) of 1.47 with a 95\u00a0% confidence interval (CI), indicating that the accuracy of ChatGPT in providing correct responses was significantly higher compared to the total responses for queries. However, significant heterogeneity was present among the studies, suggesting considerable variability in the effect sizes across the included studies.\nThe observations indicate that ChatGPT has the ability to provide appropriate solutions to questions in the medical and dentistry areas, but researchers and doctors should cautiously assess its responses because they might not always be dependable. Overall, the importance of this study rests in shedding light on ChatGPT's accuracy in the medical and dentistry fields and emphasizing the need for additional investigation to enhance its performance. \u00a9 2017 Elsevier Inc. All rights reserved.", "journal": "Heliyon", "date": "2023-12-25", "authors": ["HirojBagde", "AshwiniDhopte", "Mohammad KhursheedAlam", "RehanaBasri"], "doi": "10.1016/j.heliyon.2023.e23050\n10.1038/s41551-018-0305-z\n10.2196/27850\n10.1016/j.jacr.2021.01.013\n10.1038/s41598-022-24721-5\n10.1097/MD.0000000000029587\n10.1038/s41467-022-29437-8\n10.1259/bjro.20210062\n10.30953/bhty.v4.176\n10.1101/2022.12.19.22283643\n10.48550/arXiv.2212.14402\n10.2139/ssrn.4335905\n10.1097/JCMA.0000000000000900\n10.1126/science.adg7879\n10.1148/radiol.230171\n10.1016/j.arthro.2023.01.015\n10.1038/d41586-023-00191-1\n10.1093/eurjcn/zvad022\n10.1016/j.stemcr.2022.12.009\n10.1016/j.resuscitation.2023.109729\n10.33546/bnj.2551\n10.1227/neu.0000000000002414\n10.1016/j.resuscitation.2023.109732\n10.1371/journal.pdig.0000205\n10.3352/jeehp.2023.20.5\n10.7326/0003-4819-151-4-200908180-00136\n10.1002/jrsm.1411\n10.1186/1471-2288-14-45\n10.2196/45312\n10.1093/asj/sjad069\n10.3352/jeehp.2023.20.1\n10.21203/rs.3.rs-2566942/v1\n10.1093/jncics/pkad015\n10.1101/2023.02.02.23285399\n10.1101/2023.02.21.23285886\n10.52225/narra.v3i1.103\n10.7759/cureus.35029\n10.1101/2023.03.24.23287731\n10.3350/cmh.2023.0089\n10.1148/radiol.230163\n10.58496/MJCS/2023/004\n10.48550/arxiv.2212.14882.2212.14882\n10.1016/S2589-7500(23)00021-3\n10.1016/j.nbt.2023.02.001\n10.1016/j.jacbts.2023.01.001\n10.26434/chemrxiv-2023-qgs3k\n10.1371/journal.pdig.0000198\n10.1101/2023.01.27.23285115\n10.18632/oncoscience.571\n10.1016/S2589-7500(23)00048-1\n10.1007/s10389-023-01936-y"}
{"title": "Anti-motion Ultrafast T", "abstract": "Stroke patients commonly face challenges during magnetic resonance imaging (MRI) examinations due to involuntary movements. This study aims to overcome these challenges by utilizing multiple overlapping-echo detachment (MOLED) quantitative technology. Through this technology, we also seek to detect microstructural changes of the normal-appearing corticospinal tract (NA-CST) in subacute-chronic stroke patients.\n79 patients underwent 3.0\u00a0T MRI scans, including routine scans and MOLED technique. A deep learning network was utilized for image reconstruction, and the accuracy, reliability, and resistance to motion of the MOLED technique were validated on phantoms and volunteers. Subsequently, we assessed motor dysfunction severity, ischemic lesion volume, T\nThe MOLED technique showed high accuracy (P\u00a0<\u00a00.001) and excellent repeatability, with a mean coefficient of variation (CoV) of 1.11%. It provided reliable quantitative results even under head movement, with a mean difference (Mean\nThe MOLED technique offers significant advantages for quantitatively imaging stroke patients with involuntary movements. Additionally, T", "journal": "Academic radiology", "date": "2023-12-24", "authors": ["YueZhang", "XiaoWang", "MingYe", "ZongyeLi", "YuchuanZhuang", "QinqinYang", "QichangFu", "RuiChen", "EryuanGao", "YananRen", "YongZhang", "ShuhuiCai", "ZhongChen", "CongboCai", "YanboDong", "JianfengBao", "JingliangCheng"], "doi": "10.1016/j.acra.2023.11.036"}
{"title": "Personalized and privacy-preserving federated heterogeneous medical image analysis with PPPML-HMI.", "abstract": "Heterogeneous data is endemic due to the use of diverse models and settings of devices by hospitals in the field of medical imaging. However, there are few open-source frameworks for federated heterogeneous medical image analysis with personalization and privacy protection without the demand to modify the existing model structures or to share any private data. Here, we proposed PPPML-HMI, a novel open-source learning paradigm for personalized and privacy-preserving federated heterogeneous medical image analysis. To our best knowledge, personalization and privacy protection were discussed simultaneously for the first time under the federated scenario by integrating the PerFedAvg algorithm and designing the novel cyclic secure aggregation with the homomorphic encryption algorithm. To show the utility of PPPML-HMI, we applied it to a simulated classification task namely the classification of healthy people and patients from the RAD-ChestCT Dataset, and one real-world segmentation task namely the segmentation of lung infections from COVID-19 CT scans. Meanwhile, we applied the improved deep leakage from gradients to simulate adversarial attacks and showed the strong privacy-preserving capability of PPPML-HMI. By applying PPPML-HMI to both tasks with different neural networks, a varied number of users, and sample sizes, we demonstrated the strong generalizability of PPPML-HMI in privacy-preserving federated learning on heterogeneous medical images.", "journal": "Computers in biology and medicine", "date": "2023-12-24", "authors": ["JuexiaoZhou", "LongxiZhou", "DiWang", "XiaopengXu", "HaoyangLi", "YuetanChu", "WenkaiHan", "XinGao"], "doi": "10.1016/j.compbiomed.2023.107861"}
{"title": "Low-complexity lung ultrasound video scoring by means of intensity projection-based video compression.", "abstract": "Since the outbreak of COVID-19, efforts have been made towards semi-quantitative analysis of lung ultrasound (LUS) data to assess the patient's condition. Several methods have been proposed in this regard, with a focus on frame-level analysis, which was then used to assess the condition at the video and prognostic levels. However, no extensive work has been done to analyze lung conditions directly at the video level. This study proposes a novel method for video-level scoring based on compression of LUS video data into a single image and automatic classification to assess patient's condition. The method utilizes maximum, mean, and minimum intensity projection-based compression of LUS video data over time. This enables to preserve hyper- and hypo-echoic data regions, while compressing the video down to a maximum of three images. The resulting images are then classified using a convolutional neural network (CNN). Finally, the worst predicted score given among the images is assigned to the corresponding video. The results show that this compression technique can achieve a promising agreement at the prognostic level (81.62%), while the video-level agreement remains comparable with the state-of-the-art (46.19%). Conclusively, the suggested method lays down the foundation for LUS video compression, shifting from frame-level to direct video-level analysis of LUS data.", "journal": "Computers in biology and medicine", "date": "2023-12-24", "authors": ["UmairKhan", "SajjadAfrakhteh", "FedericoMento", "GizemMert", "AndreaSmargiassi", "RiccardoInchingolo", "FrancescoTursi", "Veronica NarvenaMacioce", "TizianoPerrone", "GiovanniIacca", "LibertarioDemi"], "doi": "10.1016/j.compbiomed.2023.107885"}
{"title": "MSTAC: A Multi-Stage Automated Classification of COVID-19 Chest X-ray Images Using Stacked CNN Models.", "abstract": "This study introduces a Multi-Stage Automated Classification (MSTAC) system for COVID-19 chest X-ray (CXR) images, utilizing stacked Convolutional Neural Network (CNN) models. Suspected COVID-19 patients often undergo CXR imaging, making it valuable for disease classification. The study collected CXR images from public datasets and aimed to differentiate between COVID-19, non-COVID-19, and healthy cases. MSTAC employs two classification stages: the first distinguishes healthy from unhealthy cases, and the second further classifies COVID-19 and non-COVID-19 cases. Compared to a single CNN-Multiclass model, MSTAC demonstrated superior classification performance, achieving 97.30% accuracy and sensitivity. In contrast, the CNN-Multiclass model showed 94.76% accuracy and sensitivity. MSTAC's effectiveness is highlighted in its promising results over the CNN-Multiclass model, suggesting its potential to assist healthcare professionals in efficiently diagnosing COVID-19 cases. The system outperformed similar techniques, emphasizing its accuracy and efficiency in COVID-19 diagnosis. This research underscores MSTAC as a valuable tool in medical image analysis for enhanced disease classification.", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2023-12-22", "authors": ["ThanakornPhumkuea", "ThakerngWongsirichot", "KasikritDamkliang", "AsmaNavasakulpong", "JarutasAndritsch"], "doi": "10.3390/tomography9060173\n10.1101/2020.02.07.937862\n10.1007/s12098-020-03263-6\n10.1056/NEJMoa2001316\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.3786\n10.1016/j.jaci.2020.04.029\n10.3390/tomography9050129\n10.1001/jama.2020.2783\n10.3390/tomography8030100\n10.1016/j.acra.2020.04.016\n10.3390/tomography9020056\n10.1016/j.ijid.2020.03.017\n10.1056/NEJMoa2030340\n10.1371/journal.pone.0252440\n10.1016/j.cmpb.2020.105532\n10.1016/j.bbe.2021.05.013\n10.1016/j.asoc.2020.107052\n10.1109/ACCESS.2020.3010287\n10.1016/j.patrec.2020.09.010\n10.1038/s41598-020-76550-z\n10.1155/2021/8890226\n10.1038/s41598-021-88807-2\n10.1038/s41598-021-99015-3\n10.1038/s41598-021-97428-8\n10.1038/s41598-022-11990-3\n10.59275/j.melba.2020-48g7\n10.4015/S1016237218500412\n10.1613/jair.953\n10.1016/j.ins.2019.10.048\n10.5121/ijdkp.2015.5201"}
{"title": "COVID-19 infection segmentation using hybrid deep learning and image processing techniques.", "abstract": "The coronavirus disease 2019 (COVID-19) epidemic has become a worldwide problem that continues to affect people's lives daily, and the early diagnosis of COVID-19 has a critical importance on the treatment of infected patients for medical and healthcare organizations. To detect COVID-19 infections, medical imaging techniques, including computed tomography (CT) scan images and X-ray images, are considered some of the helpful medical tests that healthcare providers carry out. However, in addition to the difficulty of segmenting contaminated areas from CT scan images, these approaches also offer limited accuracy for identifying the virus. Accordingly, this paper addresses the effectiveness of using deep learning (DL) and image processing techniques, which serve to expand the dataset without the need for any augmentation strategies, and it also presents a novel approach for detecting COVID-19 virus infections in lung images, particularly the infection prediction issue. In our proposed method, to reveal the infection, the input images are first preprocessed using a threshold then resized to 128\u2009\u00d7\u2009128. After that, a density heat map tool is used for coloring the resized lung images. The three channels (red, green, and blue) are then separated from the colored image and are further preprocessed through image inverse and histogram equalization, and are subsequently fed, in independent directions, into three separate U-Nets with the same architecture for segmentation. Finally, the segmentation results are combined and run through a convolution layer one by one to get the detection. Several evaluation metrics using the CT scan dataset were used to measure the performance of the proposed approach in comparison with other state-of-the-art techniques in terms of accuracy, sensitivity, precision, and the dice coefficient. The experimental results of the proposed approach reached 99.71%, 0.83, 0.87, and 0.85, respectively. These results show that coloring the CT scan images dataset and then dividing each image into its RGB image channels can enhance the COVID-19 detection, and it also increases the U-Net power in the segmentation when merging the channel segmentation results. In comparison to other existing segmentation techniques employing bigger 512\u2009\u00d7\u2009512 images, this study is one of the few that can rapidly and correctly detect the COVID-19 virus with high accuracy on smaller 128\u2009\u00d7\u2009128 images using the metrics of accuracy, sensitivity, precision, and dice coefficient.", "journal": "Scientific reports", "date": "2023-12-21", "authors": ["SamarAntar", "Hussein Karam HusseinAbd El-Sattar", "Mohammad HAbdel-Rahman", "FayedF M Ghaleb"], "doi": "10.1038/s41598-023-49337-1\n10.1016/j.clinimag.2021.01.019\n10.1007/s12098-020-03263-6\n10.1126/scitranslmed.abc1931\n10.1136/bmj.m1403\n10.1148/radiol.2020200642\n10.1021/acsnano.0c02624\n10.1080/14737159.2020.1757437\n10.1016/j.compbiomed.2023.106646\n10.1038/s41598-022-20804-5\n10.1155/2022/5329014\n10.1016/j.jnlest.2022.100161\n10.1038/s41598-021-88807-2\n10.1016/j.eswa.2020.113909\n10.1007/s10140-020-01886-y\n10.1016/j.bspc.2020.102365\n10.22266/ijies2023.0831.35\n10.18576/isl/120523\n10.1016/j.neucom.2019.11.023\n10.1109/ACCESS.2021.3053998\n10.1016/j.compbiomed.2022.105233\n10.3389/fonc.2022.931141\n10.1038/s41591-018-0268-3\n10.1016/j.compbiomed.2017.08.022\n10.1007/s10916-016-0552-z\n10.1155/2022/3626726\n10.1016/S0140-6736(20)30183-5\n10.1101/2020.04.24.20078584\n10.5281/zenodo.3757476\n10.1109/RBME.2020.2987975\n10.1002/ima.22469\n10.1109/MCSE.2007.55\n10.1016/j.ijmedinf.2020.104284\n10.1007/s10489-020-01888-w\n10.1016/j.ijcce.2023.03.005\n10.14569/IJACSA.2023.0140372\n10.1038/s41598-021-99015-3\n10.1016/j.bspc.2022.104192\n10.1093/bib/bbaa170\n10.1016/j.compbiomed.2021.104650\n10.1038/s41598-023-30941-0\n10.5937/fme2101206L\n10.1016/j.clinimag.2021.01.030\n10.1007/s13246-020-00865-4\n10.1109/TNNLS.2021.3084827\n10.1186/s40537-021-00444-8\n10.1049/ipr2.12419\n10.1109/TMI.2019.2959609\n10.4018/IJEHMC.20220701.oa4\n10.1038/s41598-020-76550-z\n10.1007/s10489-020-01826-w\n10.1109/TMI.2020.2996645\n10.7717/peerj-cs.364\n10.3390/app12104825\n10.1002/mp.14676\n10.1007/s11042-021-11299-9\n10.1007/s11063-022-10785-x\n10.1038/s41598-023-33614-0\n10.1016/j.bspc.2021.102859\n10.1109/42.14513\n10.1186/s40537-020-00392-9"}
{"title": "Noncontact remote sensing of abnormal blood pressure using a deep neural network: a novel approach for hypertension screening.", "abstract": "As the global burden of hypertension continues to increase, early diagnosis and treatment play an increasingly important role in improving the prognosis of patients. In this study, we developed and evaluated a method for predicting abnormally high blood pressure (HBP) from infrared (upper body) remote thermograms using a deep learning (DL) model.\nThe data used in this cross-sectional study were drawn from a coronavirus disease 2019 (COVID-19) pilot cohort study comprising data from 252 volunteers recruited from 22 July to 4 September 2020. Original video files were cropped at 5 frame intervals to 3,800 frames per slice. Blood pressure (BP) information was measured using a Welch Allyn 71WT monitor prior to infrared imaging, and an abnormal increase in BP was defined as a systolic blood pressure (SBP) \u2265140 mmHg and/or diastolic blood pressure (DBP) \u226590 mmHg. The PanycNet DL model was developed using a deep neural network to predict abnormal BP based on infrared thermograms.\nA total of 252 participants were included, of which 62.70% were male and 37.30% were female. The rate of abnormally high HBP was 29.20% of the total number. In the validation group (upper body), precision, recall, and area under the receiver operating characteristic curve (AUC) values were 0.930, 0.930, and 0.983 [95% confidence interval (CI): 0.904-1.000], respectively, and the head showed the strongest predictive ability with an AUC of 0.868 (95% CI: 0.603-0.994).\nThis is the first technique that can perform screening for hypertension without contact using existing equipment and data. It is anticipated that this technique will be suitable for mass screening of the population for abnormal BP in public places and home BP monitoring.", "journal": "Quantitative imaging in medicine and surgery", "date": "2023-12-18", "authors": ["ZeyeLiu", "HangLi", "WenchaoLi", "DonglinZhuang", "FengwenZhang", "WenbinOuyang", "ShouzhengWang", "LucaBertolaccini", "EbrahamAlskaf", "XiangbinPan"], "doi": "10.21037/qims-23-970\n10.1016/S0140-6736(20)30752-2\n10.1371/journal.pone.0276222\n10.1146/annurev.publhealth.27.021405.102132\n10.1161/CIRCULATIONAHA.116.008731\n10.1136/bmj.m3222\n10.1097/HJH.0000000000002958\n10.1080/16549716.2021.2000092\n10.15585/mmwr.mm6545a3\n10.1088/1361-6579/acd164\n10.1109/JBHI.2022.3206477\n10.1126/sciadv.adh0615\n10.1016/S0929-6646(09)60017-6\n10.1117/1.JMI.8.S1.010901\n10.1001/jama.1988.03720130047028\n10.13026/wfr2-5973\n10.13026/wfr2-5973\n10.1161/01.CIR.101.23.e215\n10.1137/18M1216134\n10.3390/jcdd10020074\n10.1093/cvr/cvad065\n10.1002/ehf2.14288\n10.4102/phcfm.v12i1.2160\n10.3390/s21020346\n10.1088/0967-3334/33/3/R33\n10.1016/j.amjsurg.2008.06.015\n10.1186/s41256-020-00145-4\n10.2478/jtim-2021-0011\n10.1038/s41591-021-01595-0\n10.1089/tmj.2017.0257\n10.1161/HYPERTENSIONAHA.120.14742\n10.1016/j.jacc.2020.01.046\n10.1016/j.cjca.2021.09.004"}
{"title": "Diagnosis of COVID-19 with simultaneous accurate prediction of cardiac abnormalities from chest computed tomographic images.", "abstract": "COVID-19 has potential consequences on the pulmonary and cardiovascular health of millions of infected people worldwide. Chest computed tomographic (CT) imaging has remained the first line of diagnosis for individuals infected with SARS-CoV-2. However, differentiating COVID-19 from other types of pneumonia and predicting associated cardiovascular complications from the same chest-CT images have remained challenging. In this study, we have first used transfer learning method to distinguish COVID-19 from other pneumonia and healthy cases with 99.2% accuracy. Next, we have developed another CNN-based deep learning approach to automatically predict the risk of cardiovascular disease (CVD) in COVID-19 patients compared to the normal subjects with 97.97% accuracy. Our model was further validated against cardiac CT-based markers including cardiac thoracic ratio (CTR), pulmonary artery to aorta ratio (PA/A), and presence of calcified plaque. Thus, we successfully demonstrate that CT-based deep learning algorithms can be employed as a dual screening diagnostic tool to diagnose COVID-19 and differentiate it from other pneumonia, and also predicts CVD risk associated with COVID-19 infection.", "journal": "PloS one", "date": "2023-12-14", "authors": ["MoumitaMoitra", "MahaAlafeef", "ArjunNarasimhan", "VikramKakaria", "ParikshitMoitra", "DipanjanPan"], "doi": "10.1371/journal.pone.0290494\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200823\n10.1038/s41569-020-0413-9\n10.1016/j.acra.2020.09.012\n10.1016/j.nano.2013.10.012\n10.1016/j.jaccas.2020.04.015\n10.1002/ejhf.1990\n10.1016/j.ajem.2020.04.048\n10.1172/jci.insight.148980\n10.1021/ja201918u\n10.1002/cmmi.449\n10.1016/j.inffus.2021.04.008\n10.1016/j.inffus.2021.02.016\n10.1002/smll.201500728\n10.1166/jnn.2010.3034\n10.1002/wnan.1436\n10.1021/mp400044j\n10.1002/adfm.201602966\n10.1021/acsnano.0c03822\n10.1021/acsnano.0c06392\n10.1039/d1cc01410b\n10.1002/bit.27812\n10.1021/acsnano.1c05226\n10.1038/s41596-021-00546-w\n10.1007/s12274-017-1518-2\n10.7150/thno.7581\n10.1158/0008-5472.CAN-17-1225\n10.1148/radiol.2020200642\n10.1016/j.bios.2018.03.044\n10.1007/s10796-021-10132-w\n10.1007/s10796-021-10123-x\n10.1016/j.eswa.2022.116540\n10.1016/j.asoc.2021.108291\n10.1016/j.asoc.2021.108250\n10.1007/s00521-022-06918-x\n10.1016/j.patcog.2021.108135\n10.1038/srep14986\n10.1007/s11517-020-02299-2\n10.1109/ACCESS.2020.3016780\n10.1016/j.knosys.2022.108207\n10.1038/s41598-020-74164-z\n10.1039/c7an01932g\n10.1016/j.compbiomed.2021.105127\n10.1016/j.compbiomed.2021.104306\n10.1371/journal.pone.0280352\n10.1371/journal.pone.0282608\n10.1053/j.ajkd.2016.11.026\n10.1186/s12968-015-0184-3\n10.1038/s41569-020-0360-5\n10.1093/cvr/cvaa106\n10.1161/JAHA.120.016219\n10.3390/s21175940\n10.1109/MIPR.2018.00032\n10.1186/s12938-021-00908-1\n10.1007/978-3-030-59354-4_15\n10.1161/01.CIR.0000120390.68287.BB\n10.1183/13993003.02168-2016\n10.1016/j.foodcont.2021.108439\n10.1038/s41598-020-74539-2\n10.1021/acsami.9b14110\n10.1007/s40846-017-0239-z\n10.1016/j.inffus.2020.11.005\n10.1016/j.eswa.2021.115681\n10.1007/s11548-020-02275-z\n10.1038/s41467-020-17971-2\n10.1073/pnas.2009165117\n10.1148/ryct.2020200277\n10.1053/euhj.1997.0862\n10.1056/NEJMoa1203830\n10.2147/COPD.S131413\n10.1016/0002-9149(90)90505-u\n10.1016/j.amjcard.2008.08.010\n10.1038/s41467-021-23235-4\n10.1016/j.jcct.2017.11.007"}
{"title": "IoT-based COVID-19 detection using recalling-enhanced recurrent neural network optimized with golden eagle optimization algorithm.", "abstract": "New potential for healthcare has been made possible by the development of the Internet of Medical Things (IoMT) with deep learning. This is applied for a broad range of applications. Normal medical devices together with sensors can gather important data when connected to the Internet, and deep learning uses this data to reveal symptoms and patterns and activate remote care. In recent years, the COVID-19 pandemic caused more mortality. Millions of people have been affected by this virus, and the number of infections is continually rising daily. To detect COVID-19, researchers attempt to utilize medical imaging and deep learning-based methods. Several methodologies were suggested utilizing chest X-ray (CXR) images for COVID-19 diagnosis. But these methodologies do not provide satisfactory accuracy. To overcome these drawbacks, a recalling-enhanced recurrent neural network optimized with golden eagle optimization algorithm (RERNN-GEO) is proposed in this paper. The intention of this work is to provide IoT-based deep learning method for the premature identification of COVID-19. This paradigm can be able to ease the workload of radiologists and medical specialists and also help with pandemic control. RERNN-GEO is a deep learning-based method; this is utilized in chest X-ray (CXR) images for COVID-19 diagnosis. Here, the Gray-Level Co-Occurrence Matrix (GLCM) window adaptive algorithm is used for extracting features to enable accurate diagnosis. By utilizing this algorithm, the proposed method attains better accuracy (33.84%, 28.93%, and 33.03%) and lower execution time (11.06%, 33.26%, and 23.33%) compared with the existing methods. This method can be capable of helping the clinician/radiologist to validate the initial assessment related to COVID-19.", "journal": "Medical & biological engineering & computing", "date": "2023-12-14", "authors": ["NoneKarthick S", "NoneGomathi N"], "doi": "10.1007/s11517-023-02973-1\n10.1109/JIOT.2020.3013710\n10.1002/cpe.7729\n10.1109/JSEN.2020.3030905\n10.1016/j.isatra.2022.03.017\n10.1016/j.bspc.2022.104197\n10.1142/S0218001423540010\n10.1109/JSEN.2021.3076767\n10.1109/JIOT.2020.3044031\n10.1109/JIOT.2021.3055804\n10.1016/j.bspc.2021.102960\n10.1080/0952813X.2021.1960634\n10.1007/s12195-020-00629-w\n10.1016/j.iot.2021.100377\n10.1109/JIOT.2021.3051080\n10.1109/ACCESS.2021.3068276\n10.1016/j.imu.2021.100588\n10.1002/ima.22552\n10.1007/s10489-020-01889-9\n10.1109/ACCESS.2020.3030090\n10.1002/spe.3011\n10.1007/s13042-020-01248-7\n10.1109/JIOT.2020.3034074\n10.1016/j.bspc.2020.102149\n10.1007/s00607-021-00971-5\n10.1007/s00354-022-00176-0\n10.1001/jama.2020.12839\n10.1016/j.procs.2020.03.223\n10.1016/j.procs.2020.03.382\n10.1016/j.ins.2020.01.045\n10.1016/j.cie.2020.107050\n10.1016/j.slast.2021.10.011"}
{"title": "Diagnosis and detection of pneumonia using weak-label based on X-ray images: a multi-center study.", "abstract": "Development and assessment the deep learning weakly supervised algorithm for the classification and detection pneumonia via X-ray.\nThis retrospective study analyzed two publicly available dataset that contain X-ray images of pneumonia cases and normal cases. The first dataset from Guangzhou Women and Children's Medical Center. It contains a total of 5,856 X-ray images, which are divided into training, validation, and test sets with 8:1:1 ratio for algorithm training and testing. The deep learning algorithm ResNet34 was employed to build diagnostic model. And the second public dataset were collated by researchers from Qatar University and the University of Dhaka along with collaborators from Pakistan and Malaysia and some medical doctors. A total of 1,300 images of COVID-19 positive cases, 1,300 normal images and 1,300 images of viral pneumonia for external validation. Class activation map (CAM) were used to location the pneumonia lesions.\nThe ResNet34 model for pneumonia detection achieved an AUC of 0.9949 [0.9910-0.9981] (with an accuracy of 98.29% a sensitivity of 99.29% and a specificity of 95.57%) in the test dataset. And for external validation dataset, the model obtained an AUC of 0.9835[0.9806-0.9864] (with an accuracy of 94.62%, a sensitivity of 92.35% and a specificity of 99.15%). Moreover, the CAM can accurately locate the pneumonia area.\nThe deep learning algorithm can accurately detect pneumonia and locate the pneumonia area based on weak supervision information, which can provide potential value for helping radiologists to improve their accuracy of detection pneumonia patients through X-ray images.", "journal": "BMC medical imaging", "date": "2023-12-13", "authors": ["KairouGuo", "JiangboCheng", "KaiyuanLi", "LanhuiWang", "YadongLv", "DesenCao"], "doi": "10.1186/s12880-023-01174-4\n10.1371/journal.pone.0256630\n10.1016/S2213-2600(13)70075-4\n10.1378/chest.101.4.1005\n10.1093/cid/cix082\n10.1183/09031936.01.00213501\n10.1002/jhm.955\n10.1109/TMI.2016.2538465\n10.1109/TBME.2018.2844188\n10.1016/j.media.2010.02.004\n10.1007/s00330-015-4030-7\n10.3389/fonc.2021.610785\n10.3389/fonc.2020.631831\n10.1016/j.media.2019.04.012\n10.1016/j.compbiomed.2021.105014\n10.1016/j.clinimag.2021.07.004\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.110495\n10.1016/j.cell.2018.02.010\n10.1007/s11263-015-0816-y\n10.1186/s40537-016-0043-6\n10.1109/ACCESS.2020.3044858\n10.3390/biology10111174\n10.1016/j.compbiomed.2020.103869\n10.1016/j.chaos.2020.110245\n10.1109/TMI.2020.3042773\n10.1145/3431804\n10.1016/j.asoc.2022.109464\n10.1148/radiol.2020200463\n10.1007/s00330-020-06801-0\n10.1007/s00330-020-06816-7\n10.1016/j.ejrad.2021.110002"}
{"title": "Context-aware and local-aware fusion with transformer for medical image segmentation.", "abstract": "", "journal": "Physics in medicine and biology", "date": "2023-12-12", "authors": ["HanguangXiao", "LiLi", "QiyuanLiu", "QihangZhang", "JunqiLiu", "ZhiLiu"], "doi": "10.1088/1361-6560/ad14c6"}
{"title": "SARS-CoV-2 Detection: Radiology based Multi-modal Multi-task Framework.", "abstract": "The global community is still grappling with the SARS-CoV-2 pandemic, declared by the World Health Organization in March 2020. Radiology is an important screening method for the early detection of SARS-CoV-2. Doctors typically recommend that patients undergo one of the radiology procedures during the early stages of diagnosis. Recent research has focused on developing deep learning-based architectures that use either X-Rays or CT-Scans, but not both. This paper presents a multi-modal, multi-task learning framework that uses either the X-Rays or CT-Scans to identify SARS-CoV-2 patients. The framework employs a shared feature embedding that utilizes common information from both X-Rays and CT-Scans, as well as task-specific feature embeddings that are independent of the type of chest screening. The shared and task-specific embeddings are combined to obtain the final classification results, which have been shown to have an accuracy of 98.23% and 98.83% in detecting SARS-CoV-2 using X-Rays and CT-Scans, respectively.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2023-12-12", "authors": ["NikhilanandArya", "KwanitGupta", "SriparnaSaha"], "doi": "10.1109/EMBC40787.2023.10340386"}
{"title": "Towards an Informed CNN for Bone SR-microCT Image Classification with an Unsupervised Patched-based Image Clustering.", "abstract": "Bone microscale differences cannot be readily recognizable to humans from Synchrotron Radiation micro-Computed Tomography (SR-microCT) images. Premises are possible with Deep Learning (DL) imaging analysis. Despite this, more attention to high-level features leads models to require help identifying relevant details to support a decision. Within this context, we propose a method for classifying healthy, osteoporotic, and COVID-19 femoral heads SR-microCT images informing a vgg16 about the most subtle microscale differences using unsupervised patched-based clustering. Our strategy allows achieving up to 9.8% accuracy improvement in classifying healthy from osteoporotic images over uninformed methods, while 59.1% of accuracy between osteoporosis and COVID-19.Clinical relevance-We established a starting point for classifying healthy, osteoporotic, and COVID-19 femoral heads from SR-microCTs with human non-discriminative features, with 60.91% accuracy in healthy-osteporotic image classification.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2023-12-12", "authors": ["IsabellaPoles", "EleonoraD'Arnese", "FedericaBuccino", "LauraVergani", "Marco DSantambrogio"], "doi": "10.1109/EMBC40787.2023.10341140"}
{"title": "Sequestration of imaging studies in MIDRC: stratified sampling to balance demographic characteristics of patients in a multi-institutional data commons.", "abstract": "The Medical Imaging and Data Resource Center (MIDRC) is a multi-institutional effort to accelerate medical imaging machine intelligence research and create a publicly available image repository/commons as well as a sequestered commons for performance evaluation and benchmarking of algorithms. After de-identification, approximately 80% of the medical images and associated metadata become part of the open commons and 20% are sequestered from the open commons. To ensure that both commons are representative of the population available, we introduced a stratified sampling method to balance the demographic characteristics across the two datasets.\nOur method uses multi-dimensional stratified sampling where several demographic variables of interest are sequentially used to separate the data into individual strata, each representing a unique combination of variables. Within each resulting stratum, patients are assigned to the open or sequestered commons. This algorithm was used on an example dataset containing 5000 patients using the variables of race, age, sex at birth, ethnicity, COVID-19 status, and image modality and compared resulting demographic distributions to na\u00efve random sampling of the dataset over 2000 independent trials.\nResulting prevalence of each demographic variable matched the prevalence from the input dataset within one standard deviation. Mann-Whitney U test results supported the hypothesis that sequestration by stratified sampling provided more balanced subsets than na\u00efve randomization, except for demographic subcategories with very low prevalence.\nThe developed multi-dimensional stratified sampling algorithm can partition a large dataset while maintaining balance across several variables, superior to the balance achieved from na\u00efve randomization.", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2023-12-11", "authors": ["NatalieBaughan", "Heather MWhitney", "KarenDrukker", "BerkmanSahiner", "TingtingHu", "Grace HyunKim", "MichaelMcNitt-Gray", "Kyle JMyers", "Maryellen LGiger"], "doi": "10.1117/1.JMI.10.6.064501\n10.1109/RBME.2020.2987975\n10.1002/mp.15359\n10.1117/1.JMI.8.S1.014503\n10.1038/s42256-021-00307-0\n10.1117/1.JMI.8.S1.010902\n10.1093/jamia/ocac070\n10.1117/1.JMI.10.6.061104\n10.1016/j.patter.2021.100347\n10.1073/pnas.1919012117\n10.1016/S2589-7500(22)00063-2\n10.1016/j.jacr.2022.06.008\n10.1016/j.ebiom.2023.104467\n10.1093/jalm/jfac085\n10.1038/s41591-021-01595-0\n10.1111/insr.12188\n10.1016/j.cct.2015.07.011\n10.1081/BIP-200062277\n10.1214/aoms/1177730491\n10.1117/12.2654247\n10.1117/1.JMI.10.6.061105\n10.1109/TNNLS.2012.2222925\n10.1117/12.2610239"}
{"title": "Future direction for the deployment of deep learning artificial intelligence: Vision threatening disease detection in underserved communities during COVID-19.", "abstract": "Vision-threatening diseases (VTDs) are the leading causes of vision loss and blindness. Use of deep learning artificial intelligence (DLAI) in early detection and subspecialty referral is critical to saving vision years and maintaining quality of life. To address this, we propose a comprehensive community-based screening retinal approach that incorporates DLAI to mitigate disparities and address need in an underserved urban community.\nWe evaluated two DLAI software designed for 45\u00b0 retinal image analysis. DLAI was deployed in clinical settings to triage cases for ophthalmic referrals. Functionality was evaluated to propose implementation in a community screening.\nOur community screenings have incorporated various imaging modalities to improve VTD pick up rate: nonmydriatic color retinal imaging (18%), fundus autofluorescence (AF) (23%), and ocular coherence tomography B and angiography scans (35%). Robotic teleconsultation increased follow-up reached 100%. In clinical settings, DLAI reduced image analysis time (EyeArt\u2122 = under 38 s, SELENA+\u2122 =10.6 s) and highlighted multiple VTDs. High concordance was observed between human graders and DLAI (\nIntegration of DLAI in our ocular screening protocol can be used to reach underserved communities, especially when traditional health-care access is limited.", "journal": "Saudi journal of ophthalmology : official journal of the Saudi Ophthalmological Society", "date": "2023-12-11", "authors": ["RitaVought", "VictoriaVought", "BernardSzirth", "Albert SKhouri"], "doi": "10.4103/sjopt.sjopt_16_23"}
{"title": "Diversity of symptom phenotypes in SARS-CoV-2 community infections observed in multiple large datasets.", "abstract": "Variability in case severity and in the range of symptoms experienced has been apparent from the earliest months of the COVID-19 pandemic. From a clinical perspective, symptom variability might indicate various routes/mechanisms by which infection leads to disease, with different routes requiring potentially different treatment approaches. For public health and control of transmission, symptoms in community cases were the prompt upon which action such as PCR testing and isolation was taken. However, interpreting symptoms presents challenges, for instance, in balancing the sensitivity and specificity of individual symptoms with the need to maximise case finding, whilst managing demand for limited resources such as testing. For both clinical and transmission control reasons, we require an approach that allows for the possibility of distinct symptom phenotypes, rather than assuming variability along a single dimension. Here we address this problem by bringing together four large and diverse datasets deriving from routine testing, a population-representative household survey and participatory smartphone surveillance in the United Kingdom. Through the use of cutting-edge unsupervised classification techniques from statistics and machine learning, we characterise symptom phenotypes among symptomatic SARS-CoV-2 PCR-positive community cases. We first analyse each dataset in isolation and across age bands, before using methods that allow us to compare multiple datasets. While we observe separation due to the total number of symptoms experienced by cases, we also see a separation of symptoms into gastrointestinal, respiratory and other types, and different symptom co-occurrence patterns at the extremes of age. In this way, we are able to demonstrate the deep structure of symptoms of COVID-19 without usual biases due to study design. This is expected to have implications for the identification and management of community SARS-CoV-2 cases and could be further applied to symptom-based management of other diseases and syndromes.", "journal": "Scientific reports", "date": "2023-12-09", "authors": ["MartynFyles", "Karina-DorisVihta", "Carole HSudre", "HarryLong", "RajenkiDas", "CarolineJay", "TomWingfield", "FergusCumming", "WilliamGreen", "PantelisHadjipantelis", "JoniKirk", "Claire JSteves", "SebastienOurselin", "Graham FMedley", "ElizabethFearon", "ThomasHouse"], "doi": "10.1038/s41598-023-47488-9\n10.1038/s41562-021-01079-8\n10.1002/14651858.CD013665/full\n10.1371/journal.pmed.1003777\n10.1038/nrgastro.2011.49\n10.1080/1744666X.2017.1257940\n10.1089/cap.2017.0082\n10.1111/cob.12331\n10.1002/cam4.3235"}
{"title": "On the feasibility of Vis-NIR spectroscopy and machine learning for real time SARS-CoV-2 detection.", "abstract": "The pandemic caused by Covid-19 is still present around the world. Despite advances in combating the disease, such as vaccine development, identifying infected individuals is still essential to optimize the control of human-to-human transmission of the virus. The main technique for detecting the virus is the RT-PCR method, which, despite its high relative cost, has a high accuracy in detecting the coronavirus. Given this, a method capable of performing the identification quickly, accurately, and inexpensively is necessary. Thus, this work aimed to analyze the feasibility of a new technique for identifying SARS-CoV-2 through the use of optical spectroscopy in the visible and near-infrared range (Vis-NIR) combined with machine learning algorithms. Spectral signals were obtained from nasopharyngeal swab samples previously analyzed using the RT-PCR method. The specimens were provided by the Molecular Diagnosis Laboratory of Covid-19 at Univasf. A total of 314 samples were analyzed, comprising 42 testing positive and 272 testing negative for Covid-19. Digital signal processing techniques, such as Savitzky-Golay filters and statistical methods were used to eliminate spurious elements from the original data and extract relevant features. Supervised machine learning algorithms such as SVM, Random Forest, and Naive Bayes classifiers were used to perform automatic sample identification. To evaluate the performance of the models, a 5-fold cross-validation technique was applied. With the proposed methodology, it was possible to achieve an accuracy of 75%, a sensitivity of 80%, and a specificity of 70%, in addition to an area under the ROC curve of 0.81, in the identification of nasopharyngeal swab samples from previously diagnosed individuals. From these results, it was possible to conclude that Vis-NIR spectroscopy is a promising, fast and relatively low cost technique to identify the SARS-CoV-2.", "journal": "Spectrochimica acta. Part A, Molecular and biomolecular spectroscopy", "date": "2023-12-09", "authors": ["Bruno Fonseca OliveiraCoelho", "S\u00e1vio Luiz PereiraNunes", "Chirles Ara\u00fajode Fran\u00e7a", "Daniel Dos SantosCosta", "Rodrigo Felicianodo Carmo", "Ricardo MenezesPrates", "Eduardo Furtado SimasFilho", "Rodrigo PereiraRamos"], "doi": "10.1016/j.saa.2023.123735"}
{"title": "Accuracy of artificial intelligence CT quantification in predicting COVID-19 subjects' prognosis.", "abstract": "Artificial intelligence (AI)-aided analysis of chest CT expedites the quantification of abnormalities and may facilitate the diagnosis and assessment of the prognosis of subjects with COVID-19.\nThis study investigates the performance of an AI-aided quantification model in predicting the clinical outcomes of hospitalized subjects with COVID-19 and compares it with radiologists' performance.\nA total of 90 subjects with COVID-19 (men, n = 59 [65.6%]; age, 52.9\u00b116.7 years) were recruited in this cross-sectional study. Quantification of the total and compromised lung parenchyma was performed by two expert radiologists using a volumetric image analysis software and compared against an AI-assisted package consisting of a modified U-Net model for segmenting COVID-19 lesions and an off-the-shelf U-Net model augmented with COVID-19 data for segmenting lung volume. The fraction of compromised lung parenchyma (%CL) was calculated. Based on clinical results, the subjects were divided into two categories: critical (n = 45) and noncritical (n = 45). All admission data were compared between the two groups.\nThere was an excellent agreement between the radiologist-obtained and AI-assisted measurements (intraclass correlation coefficient = 0.88, P < 0.001). Both the AI-assisted and radiologist-obtained %CLs were significantly higher in the critical subjects (P = 0.009 and 0.02, respectively) than in the noncritical subjects. In the multivariate logistic regression analysis to distinguish the critical subjects, an AI-assisted %CL \u226535% (odds ratio [OR] = 17.0), oxygen saturation level of <88% (OR = 33.6), immunocompromised condition (OR = 8.1), and other comorbidities (OR = 15.2) independently remained as significant variables in the models. Our proposed model obtained an accuracy of 83.9%, a sensitivity of 79.1%, and a specificity of 88.6% in predicting critical outcomes.\nAI-assisted measurements are similar to quantitative radiologist-obtained measurements in determining lung involvement in COVID-19 subjects.", "journal": "PloS one", "date": "2023-12-08", "authors": ["ArvinArian", "Mohammad-MehdiMehrabi Nejad", "MostafaZoorpaikar", "NavidHasanzadeh", "SamanSotoudeh-Paima", "ShahriarKolahi", "MasoumehGity", "HamidSoltanian-Zadeh"], "doi": "10.1371/journal.pone.0294899\n10.1016/S2213-2600(20)30161-2\n10.1007/s11547-020-01293-w\n10.1148/ryct.2020200047\n10.5114/pjr.2020.98009\n10.1007/s00330-020-07623-w\n10.1155/2021/9941570\n10.2214/AJR.20.22976\n10.1109/RBME.2020.2987975\n10.1067/j.cpradiol.2020.06.009\n10.1109/RBME.2020.2990959\n10.5812/iranjradiol-117992\n10.17632/pfmgfpwnmm.1\n10.1186/s41747-020-00173-2\n10.7717/peerj-cs.368\n10.12928/telkomnika.v18i3.14753\n10.1007/s12194-021-00630-6\n10.1002/1097-0142(1950)3:1<32::AID-CNCR2820030106>3.0.CO;2-3\n10.1007/s11432-020-2849-3\n10.3348/kjr.2020.0215\n10.1007/s00330-020-07013-2\n10.1038/s41598-020-79470-0\n10.1007/s00330-020-07269-8\n10.1016/j.acra.2020.09.004\n10.1016/j.clinimag.2021.06.036\n10.1186/s41747-023-00334-z\n10.1007/s00330-021-08485-6"}
{"title": "Predicting survival of Iranian COVID-19 patients infected by various variants including omicron from CT Scan images and clinical data using deep neural networks.", "abstract": "", "journal": "Heliyon", "date": "2023-12-07", "authors": ["MahyarGhafoori", "MehrabHamidi", "Rassa GhavamiModegh", "AlirezaAziz-Ahari", "NedaHeydari", "ZeynabTavafizadeh", "OmidPournik", "SasanEmdadi", "SaeedSamimi", "AmirMohseni", "MohammadrezaKhaleghi", "HamedDashti", "Hamid RRabiee"], "doi": "10.1016/j.heliyon.2023.e21965"}
{"title": "Exploring the pathologist's role in understanding COVID-19: from pneumonia to long-COVID lung sequelae.", "abstract": "The crucial role of pathologists in enhancing our understanding of SARS-CoV-2-related disease, from initial pneumonia manifestations to persistent long COVID lung symptoms, is the focus of this review. Pathological explorations have offered unprecedented insights into the early stages of severe COVID-19, shedding light on the interplay between the virus and subsequent complications, thereby shaping clinical approaches. Growing interest is directed to residual lung abnormalities of COVID-19 survivors. Although various radiological studies reported long-lasting pulmonary changes (e.g., ground glass opacities, reticulations, and bronchiectasis), the true incidence of pulmonary fibrosis and corresponding pathological findings in these patients remains largely unknown. There are a few high-impact and knowledgeable works on late complications in COVID-19 survivors, several coming from explant or autopsy cases, and rare cases from in vivo sampling. The study of biopsy samples has further deepened our knowledge of the aftermath of COVID-19 on lung tissue, uncovering alterations at the cellular level and shifts in vascular and epithelial dynamics. Despite the substantial progress made, future research is needed to devise a uniform strategy for interpreting lung biopsies, with a focus on leveraging advanced tools such as molecular and digital pathology techniques, along with artificial intelligence.", "journal": "Pathologica", "date": "2023-12-06", "authors": ["Gheorghe-EmilianOlteanu", "FedericaPezzuto", "FrancescaLunardi", "FrancescoFortarezza", "AlessandraDubini", "FiorellaCalabrese"], "doi": "10.32074/1591-951X-906\n10.1016/j.jinf.2020.02.018\n10.1136/bmj.m792\n10.1038/s41379-021-00793-y\n10.1016/s1473-3099(20)30434-5\n10.1038/s41379-020-00661-1\n10.1056/nejmoa2015432\n10.1002/2Fpath.5653\n10.1001/jama.2020.5394\n10.1186/s12931-023-02464-9\n10.1056/nejmoa2021436\n10.3390/2Fdiagnostics12010095\n10.1111/2Fmyc.13342\n10.1016/s1473-3099(20)30847-1\n10.1126/scitranslmed.abe4282\n10.1093/ajcp/aqaa264\n10.1016/s2213-2600(21)00077-1\n10.5858/arpa.2021-0519-sa\n10.3389/fmed.2022.778489\n10.1164/2Frccm.202004-1278IM\n10.1159/000514822\n10.1016/2Fj.eclinm.2021.101209\n10.36416/2F1806-3756/2Fe20210438\n10.1183/2F13993003.02411-2021\n10.1007/s15010-021-01678-7\n10.1111/his.14930"}
{"title": "Effective multi-class lungdisease classification using the hybridfeature engineering mechanism.", "abstract": "The utilization of computational models in the field of medical image classification is an ongoing and unstoppable trend, driven by the pursuit of aiding medical professionals in achieving swift and precise diagnoses. Post COVID-19, many researchers are studying better classification and diagnosis of lung diseases particularly, as it was reported that one of the very few diseases greatly affecting human beings was related to lungs. This research study, as presented in the paper, introduces an advanced computer-assisted model that is specifically tailored for the classification of 13 lung diseases using deep learning techniques, with a focus on analyzing chest radiograph images. The work flows from data collection, image quality enhancement, feature extraction to a comparative classification performance analysis. For data collection, an open-source data set consisting of 112,000 chest X-Ray images was used. Since, the quality of the pictures was significant for the work, enhanced image quality is achieved through preprocessing techniques such as Otsu-based binary conversion, contrast limited adaptive histogram equalization-driven noise reduction, and Canny edge detection. Feature extraction incorporates connected regions, histogram of oriented gradients, gray-level co-occurrence matrix and Haar wavelet transformation, complemented by feature selection via regularized neighbourhood component analysis. The paper proposes an optimized hybrid model, improved Aquila optimization convolutional neural networks (CNN), which is a combination of optimized CNN and DENSENET121 with applied batch equalization, which provides novelty for the model compared with other similar works. The comparative evaluation of classification performance among CNN, DENSENET121 and the proposed hybrid model is also done to find the results. The findings highlight the proposed hybrid model's supremacy, boasting 97.00% accuracy, 94.00% precision, 96.00% sensitivity, 96.00% specificity and 95.00% F1-score. In the future, potential avenues encompass exploring explainable machine learning for discerning model decisions and optimizing performance through strategic model restructuring.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2023-12-06", "authors": ["BinjuSaju", "NeethuTressa", "Rajesh KumarDhanaraj", "SumeghTharewal", "Jincy ChundamannilMathew", "DaniloPelusi"], "doi": "10.3934/mbe.2023896"}
{"title": "Performance in prognostic capacity and efficiency of the Thoracic Care Suite GE AI tool applied to chest radiography of patients with COVID-19 pneumonia.", "abstract": "Rapid progression of COVID-19 pneumonia may put patients at risk of requiring ventilatory support, such as non-invasive mechanical ventilation or endotracheal intubation. Implementing tools that detect COVID-19 pneumonia can improve the patient's healthcare. We aim to evaluate the efficacy and efficiency of the artificial intelligence (AI) tool GE Healthcare's Thoracic Care Suite (featuring Lunit INSIGHT CXR, TCS) to predict the ventilatory support need based on pneumonic progression of COVID-19 on consecutive chest X-rays.\nOutpatients with confirmed SARS-CoV-2 infection, with chest X-ray (CXR) findings probable or indeterminate for COVID-19 pneumonia, who required a second CXR due to unfavorableclinical course, were collected. The number of affected lung fields for the two CXRs was assessed using the AI tool.\nOne hundred fourteen patients (57.4\u00b114.2 years, 65-57%-men) were retrospectively collected. Fifteen (13.2%) required ventilatory support. Progression of pneumonic extension \u22650.5 lung fields per day compared to pneumonia onset, detected using the TCS tool, increased the risk of requiring ventilatory support by 4-fold. Analyzing the AI output required 26s of radiological time.\nApplying the AI tool, Thoracic Care Suite, to CXR of patients with COVID-19 pneumonia allows us to anticipate ventilatory support requirements requiring less than half a minute.", "journal": "Radiologia", "date": "2023-12-05", "authors": ["J MPlasencia-Mart\u00ednez", "RP\u00e9rez-Costa", "MBallesta-Ruiz", "J MGarc\u00eda-Santos"], "doi": "10.1016/j.rxeng.2022.11.007"}
{"title": "Empowering COVID-19 detection: Optimizing performance through fine-tuned EfficientNet deep learning architecture.", "abstract": "The worldwide COVID-19 pandemic has profoundly influenced the health and everyday experiences of individuals across the planet. It is a highly contagious respiratory disease requiring early and accurate detection to curb its rapid transmission. Initial testing methods primarily revolved around identifying the genetic composition of the coronavirus, exhibiting a relatively low detection rate and requiring a time-intensive procedure. To address this challenge, experts have suggested using radiological imagery, particularly chest X-rays, as a valuable approach within the diagnostic protocol. This study investigates the potential of leveraging radiographic imaging (X-rays) with deep learning algorithms to swiftly and precisely identify COVID-19 patients. The proposed approach elevates the detection accuracy by fine-tuning with appropriate layers on various established transfer learning models. The experimentation was conducted on a COVID-19 X-ray dataset containing 2000 images. The accuracy rates achieved were impressive of 99.55%, 97.32%, 99.11%, 99.55%, 99.11% and 100% for Xception, InceptionResNetV2, ResNet50 , ResNet50V2, EfficientNetB0 and EfficientNetB4 respectively. The fine-tuned EfficientNetB4 achieved an excellent accuracy score, showcasing its potential as a robust COVID-19 detection model. Furthermore, EfficientNetB4 excelled in identifying Lung disease using Chest X-ray dataset containing 4,350 Images, achieving remarkable performance with an accuracy of 99.17%, precision of 99.13%, recall of 99.16%, and f1-score of 99.14%. These results highlight the promise of fine-tuned transfer learning for efficient lung detection through medical imaging, especially with X-ray images. This research offers radiologists an effective means of aiding rapid and precise COVID-19 diagnosis and contributes valuable assistance for healthcare professionals in accurately identifying affected patients.", "journal": "Computers in biology and medicine", "date": "2023-12-03", "authors": ["Md AlaminTalukder", "Md AbuLayek", "MohsinKazi", "Md AshrafUddin", "SunilAryal"], "doi": "10.1016/j.compbiomed.2023.107789"}
{"title": "MERGE: A model for multi-input biomedical federated learning.", "abstract": "Driven by the deep learning (DL) revolution, artificial intelligence (AI) has become a fundamental tool for many biomedical tasks, including analyzing and classifying diagnostic images. Imaging, however, is not the only source of information. Tabular data, such as personal and genomic data and blood test results, are routinely collected but rarely considered in DL pipelines. Nevertheless, DL requires large datasets that often must be pooled from different institutions, raising non-trivial privacy concerns. Federated learning (FL) is a cooperative learning paradigm that aims to address these issues by moving models instead of data across different institutions. Here, we present a federated multi-input architecture using images and tabular data as a methodology to enhance model performance while preserving data privacy. We evaluated it on two showcases: the prognosis of COVID-19 and patients' stratification in Alzheimer's disease, providing evidence of enhanced accuracy and F1 scores against single-input models and improved generalizability against non-federated models.", "journal": "Patterns (New York, N.Y.)", "date": "2023-11-30", "authors": ["BrunoCasella", "WalterRiviera", "MarcoAldinucci", "GloriaMenegaz"], "doi": "10.1016/j.patter.2023.100856\n10.1111/exsy.12764\n10.1016/j.cmpb.2022.106821\n10.1016/S0140-6736(20)32519-8\n10.1016/j.neucom.2018.09.013\n10.1109/CVPR.2019.00874\n10.1088/1741-2552/ac0f4b\n10.1007/978-3-030-41593-8_12\n10.1093/jamia/ocz090\n10.1016/j.future.2020.10.007\n10.2200/S00960ED2V01Y201910AIM043\n10.1016/j.media.2021.102216\n10.1145/3575637.3575644\n10.1145/3533708\n10.1016/j.media.2020.101714\n10.1002/jmri.28365\n10.1088/2632-2153/abb214\n10.1098/rsta.2020.0258\n10.3389/frai.2021.694875\n10.1109/BHI56158.2022.9926813\n10.48550/arXiv.2211.01914\n10.1093/jamia/ocaa341\n10.1109/ISBI.2019.8759317\n10.1016/j.inffus.2017.02.003\n10.1145/3543848\n10.1016/j.neucom.2021.03.090\n10.1007/s13755-022-00174-y\n10.1109/ICPR.2018.8546308\n10.48550/arXiv.1811.00347\n10.48550/arXiv:1804.05788\n10.1109/TMI.2014.2377694\n10.1109/ISBI.2016.7493369\n10.1109/TBME.2014.2372011\n10.1109/ICASSP43922.2022.9747913\n10.1016/j.compbiomed.2020.103831\n10.1016/j.patcog.2016.10.009\n10.1016/j.neucom.2020.01.006\n10.3389/fnins.2022.965871\n10.1109/BIBE55377.2022.00017\n10.1109/TMI.2020.2992546\n10.1016/j.patcog.2022.108817\n10.1145/3510031\n10.1145/3529836.3529904\n10.1109/OJCS.2022.3206407\n10.1145/3501816\n10.1016/j.bdr.2015.04.001\n10.1109/IoTDI54339.2022.00011\n10.48550/arXiv.2205.11096\n10.1016/j.compbiomed.2022.105350\n10.48550/arXiv:2004.03698\n10.1109/SMC52423.2021.9659179\n10.1016/j.inffus.2020.09.002\n10.1016/j.neuroimage.2011.01.008\n10.1109/JBHI.2021.3097721\n10.3390/brainsci13020306\n10.1109/CVPR.2016.90\n10.1145/3534678.3539121\n10.48550/arXiv.2303.10630\n10.5281/zenodo.8218953\n10.1145/3203217.3205340\n10.1145/arXiv:1511.08458\n10.1109/ICDE53745.2022.00077\n10.1007/978-3-031-23119-3_1"}
{"title": "An Optimal Model Combining SqueezeNet and Machine Learning Methods for Lung Disease Diagnosis.", "abstract": "Artificial intelligence (AI) is rapidly evolving in healthcare, with transformative potential. AI revolutionizes medical imaging by enabling online self-diagnosis for patients and improving diagnostic accuracy for healthcare professionals. While valuable datasets aid machine learning in disease detection, challenges persist in diagnosing similar lung conditions from chest X-rays. Integrating AI into healthcare holds promise for enhanced outcomes and efficiency.\nIn this article, we aim to present a new AI model that solves this challenge by allowing the differentiation, diagnosis and classification of three distinct diseases, whose symptoms are very similar. The fundamental contribution is to reduce the number of parameters used while maintaining the same level of precision for use in embedded systems.\nOur proposed model combines the power of the neural network using the SqueezeNet architecture with a set of machine learning algorithms as classifiers, including logistic regression, support vector machine (SVM), k-nearest neighbors (KNN), decision tree, and naive Bayes. The chest Xray dataset used in the proposed model consists of CXR images that are classified into four categories: pneumonia, tuberculosis, COVID-19, and normal cases.\nOur proposed model demonstrated remarkable accuracy (97,32%), precision (97,33), F1 score (97,31%), recall (97,30%), and AUC (99,40), which is close to the best model. Whereas, the number of parameters used by our model (4,6 M) is very small compared to the best model in the literature (47M).\nThe model demonstrated good classification accuracy. In addition, the proposed model has the ability to use fewer parameters, which means it requires less internal memory and computing resources. </p>.", "journal": "Current medical imaging", "date": "2023-11-30", "authors": ["AbdallahMaiti", "AbdallahAbarda", "MohamedHanini", "AhmedOussous"], "doi": "10.2174/0115734056258742230920062315"}
{"title": "Deep learning-based analysis of COVID-19 X-ray images: Incorporating clinical significance and assessing misinterpretation.", "abstract": "COVID-19, pneumonia, and tuberculosis have had a significant effect on recent global health. Since 2019, COVID-19 has been a major factor underlying the increase in respiratory-related terminal illness. Early-stage interpretation and identification of these diseases from X-ray images is essential to aid medical specialists in diagnosis. In this study, (COV-X-net19) a convolutional neural network model is developed and customized with a soft attention mechanism to classify lung diseases into four classes: normal, COVID-19, pneumonia, and tuberculosis using chest X-ray images. Image preprocessing is carried out by adjusting optimal parameters to preprocess the images before undertaking training of the classification models. Moreover, the proposed model is optimized by experimenting with different architectural structures and hyperparameters to further boost performance. The performance of the proposed model is compared with eight state-of-the-art transfer learning models for a comparative evaluation. Results suggest that the COV-X-net19 outperforms other models with a testing accuracy of 95.19%, precision of 96.49% and F1-score of 95.13%. Another novel approach of this study is to find out the probable reason behind image misclassification by analyzing the handcrafted imaging features with statistical evaluation. A statistical analysis known as analysis of variance test is performed, to identify at which point the model can identify a class accurately, and at which point the model cannot identify the class. The potential features responsible for the misclassification are also found. Moreover, Random Forest Feature importance technique and Minimum Redundancy Maximum Relevance technique are also explored. The methods and findings of this study can benefit in the clinical perspective in early detection and enable a better understanding of the cause of misclassification.", "journal": "Digital health", "date": "2023-11-29", "authors": ["Md RahadIslam Bhuiyan", "SamiAzam", "SidratulMontaha", "Risul IslamJim", "AsifKarim", "Inam UllahKhan", "MarkBrady", "Md ZahidHasan", "FrisoDe Boer", "Md Saddam HossainMukta"], "doi": "10.1177/20552076231215915\n10.1016/j.eswa.2020.114054\n10.1109/EBBT.2019.8741582\n10.1016/j.compmedimag.2021.102008\n10.1016/j.compmedimag.2021.101933\n10.1101/2021.05.12.21257114\n10.1016/j.cmpb.2020.105532\n10.1109/Confluence47617.2020.9057809\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.chaos.2020.110495\n10.3390/pathogens12010017\n10.1016/j.aei.2021.101317\n10.3390/biology10121347\n10.1109/ICISS49785.2020.9316100\n10.3389/fmed.2022.924979\n10.3390/biomedicines10112835\n10.1109/CVPR.2018.00474\n10.1016/j.bspc.2020.102365\n10.1109/SIU53274.2021.9478028\n10.3390/biology11111654"}
{"title": "COVision: convolutional neural network for the differentiation of COVID-19 from common pulmonary conditions using CT scans.", "abstract": "With the growing amount of COVID-19 cases, especially in developing countries with limited medical resources, it is essential to accurately and efficiently diagnose COVID-19. Due to characteristic ground-glass opacities (GGOs) and other types of lesions being present in both COVID-19 and other acute lung diseases, misdiagnosis occurs often - 26.6% of the time in manual interpretations of CT scans. Current deep-learning models can identify COVID-19 but cannot distinguish it from other common lung diseases like bacterial pneumonia. Concretely, COVision is a deep-learning model that can differentiate COVID-19 from other common lung diseases, with high specificity using CT scans and other clinical factors. COVision was designed to minimize overfitting and complexity by decreasing the number of hidden layers and trainable parameters while still achieving superior performance. Our model consists of two parts: the CNN which analyzes CT scans and the CFNN (clinical factors neural network) which analyzes clinical factors such as age, gender, etc. Using federated averaging, we ensembled our CNN with the CFNN to create a comprehensive diagnostic tool. After training, our CNN achieved an accuracy of 95.8% and our CFNN achieved an accuracy of 88.75% on a validation set. We found a statistical significance that COVision performs better than three independent radiologists with at least 10 years of experience, especially in differentiating COVID-19 from pneumonia. We analyzed our CNN's activation maps through Grad-CAMs and found that lesions in COVID-19 presented peripherally, closer to the pleura, whereas pneumonia lesions presented centrally.", "journal": "BMC pulmonary medicine", "date": "2023-11-29", "authors": ["Kush VParikh", "Timothy JMathew"], "doi": "10.1186/s12890-023-02723-x\n10.1016/j.ejca.2011.11.036\n10.1016/j.asoc.2022.108780\n10.1007/s11263-019-01228-7"}
{"title": "Segmentation of lung lobes and lesions in chest CT for the classification of COVID-19 severity.", "abstract": "To precisely determine the severity of COVID-19-related pneumonia, computed tomography (CT) is an imaging modality beneficial for patient monitoring and therapy planning. Thus, we aimed to develop a deep learning-based image segmentation model to automatically assess lung lesions related to COVID-19 infection and calculate the total severity score (TSS). The entire dataset consisted of 124 COVID-19 patients acquired from Chulabhorn Hospital, divided into 28 cases without lung lesions and 96 cases with lung lesions categorized severity by radiologists regarding TSS. The model used a 3D-UNet along with DenseNet and ResNet models that had already been trained to separate the lobes of the lungs and figure out the percentage of lung involvement due to COVID-19 infection. It also used the Dice similarity coefficient (DSC) to measure TSS. Our final model, consisting of 3D-UNet integrated with DenseNet169, achieved segmentation of lung lobes and lesions with the Dice similarity coefficients of 91.52% and 76.89%, respectively. The calculated TSS values were similar to those evaluated by radiologists, with an R2 of 0.842. The correlation between the ground-truth TSS and model prediction was greater than that of the radiologist, which was 0.890 and 0.709, respectively.", "journal": "Scientific reports", "date": "2023-11-29", "authors": ["PrachayaKhomduean", "PongpatPhuaudomcharoen", "TotsapornBoonchu", "UnchalisaTaetragool", "KamonwanChamchoy", "NatWimolsiri", "TanadulJarrusrojwuttikul", "AmmarutChuajak", "UdomchaiTechavipoo", "NumfonTweeatsani"], "doi": "10.1038/s41598-023-47743-z\n10.1056/NEJMoa2002032\n10.1056/NEJMoa2001017\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMcp2009575\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020201160\n10.1148/ryct.2020200280\n10.1097/rti.0000000000000541\n10.2214/AJR.20.22954\n10.21037/atm-20-2119a\n10.1136/bmjopen-2020-042946\n10.1183/13993003.04188-2020\n10.1148/radiol.2020201343\n10.2214/AJR.20.22975\n10.1016/j.jacr.2020.03.006\n10.1080/07853890.2020.1851044\n10.1097/rti.0000000000000524\n10.1148/radiol.2020200230\n10.1148/ryct.2020200047\n10.1007/s00330-020-06817-6\n10.1007/s00330-021-08432-5\n10.1016/j.ijid.2020.09.1449\n10.1148/radiol.2020202439\n10.1148/ryai.2020200048\n10.3390/sym12111787\n10.1038/s42256-022-00483-7\n10.1109/TMI.2014.2337057\n10.1038/s41598-022-06854-9\n10.1038/s41598-020-80936-4\n10.1007/s00330-022-08741-3\n10.3390/diagnostics11050893\n10.1007/s11263-007-0090-8\n10.1016/j.compbiomed.2021.105089\n10.1109/TPAMI.2015.2408351\n10.3390/a13100263\n10.1080/01431169108955241\n10.1016/j.compbiomed.2021.104319"}
{"title": "Trends in Influenza Vaccination Rates among a Medicaid Population from 2016 to 2021.", "abstract": "Seasonal influenza is a leading cause of death in the U.S., causing significant morbidity, mortality, and economic burden. Despite the proven efficacy of vaccinations, rates remain notably low, especially among Medicaid enrollees. Leveraging Medicaid claims data, this study characterizes influenza vaccination rates among Medicaid enrollees and aims to elucidate factors influencing vaccine uptake, providing insights that might also be applicable to other vaccine-preventable diseases, including COVID-19. This study used Medicaid claims data from nine U.S. states (2016-2021], encompassing three types of claims: fee-for-service, major Medicaid managed care plan, and combined. We included Medicaid enrollees who had an in-person healthcare encounter during an influenza season in this period, excluding those under 6 months of age, over 65 years, or having telehealth-only encounters. Vaccination was the primary outcome, with secondary outcomes involving in-person healthcare encounters. Chi-square tests, multivariable logistic regression, and Fisher's exact test were utilized for statistical analysis. A total of 20,868,910 enrollees with at least one healthcare encounter in at least one influenza season were included in the study population between 2016 and 2021. Overall, 15% (N = 3,050,471) of enrollees received an influenza vaccine between 2016 and 2021. During peri-COVID periods, there was an increase in vaccination rates among enrollees compared to pre-COVID periods, from 14% to 16%. Children had the highest influenza vaccination rates among all age groups at 29%, whereas only 17% were of 5-17 years, and 10% were of the 18-64 years were vaccinated. We observed differences in the likelihood of receiving the influenza vaccine among enrollees based on their health conditions and medical encounters. In a study of Medicaid enrollees across nine states, 15% received an influenza vaccine from July 2016 to June 2021. Vaccination rates rose annually, peaking during peri-COVID seasons. The highest uptake was among children (6 months-4 years), and the lowest was in adults (18-64 years). Female gender, urban residency, and Medicaid-managed care affiliation positively influenced uptake. However, mental health and substance abuse disorders decreased the likelihood. This study, reliant on Medicaid claims data, underscores the need for outreach services.", "journal": "Vaccines", "date": "2023-11-25", "authors": ["BehzadNaderalvojoud", "Nilpa DShah", "Jane NMutanga", "ArturBelov", "RebeccaStaiger", "Jonathan HChen", "BarbeeWhitaker", "TinaHernandez-Boussard"], "doi": "10.3390/vaccines11111712\n10.15585/mmwr.mm7117e1\n10.1093/cid/cix1060\n10.1016/j.vaccine.2018.05.057\n10.1093/infdis/jiaa800\n10.1503/cmaj.151059\n10.1093/cid/ciy737\n10.1086/657309\n10.15585/mmwr.rr6803a1\n10.1002/cpt.2480\n10.1371/journal.pone.0249785\n10.1016/j.vaccine.2018.06.065\n10.1002/pbc.29351\n10.3390/vaccines10101610\n10.1177/2150132720958532\n10.1016/j.vaccine.2021.04.013\n10.2105/AJPH.2021.306575\n10.1016/j.vaccine.2020.10.030\n10.1016/j.vaccine.2022.03.058\n10.15585/mmwr.mm7045a3\n10.1016/S2214-109X(21)00512-X\n10.1016/j.vaccine.2021.08.091\n10.7326/M21-1550\n10.1016/j.vaccine.2020.08.008\n10.1016/j.amjmed.2015.10.031\n10.1016/j.vaccine.2015.02.029\n10.1016/j.genhosppsych.2007.08.015\n10.1097/00005650-200202000-00007\n10.1371/journal.pone.0266692\n10.2190/PM.46.1.a\n10.4103/2230-8210.85579\n10.15585/mmwr.ss6501a1"}
{"title": "The Diagnostic Utility of Artificial Intelligence-Guided Computed Tomography-Based Severity Scores for Predicting Short-Term Clinical Outcomes in Adults with COVID-19 Pneumonia.", "abstract": "Chest computed tomography (CT) imaging with the use of an artificial intelligence (AI) analysis program has been helpful for the rapid evaluation of large numbers of patients during the COVID-19 pandemic. We have previously demonstrated that adults with COVID-19 infection with high-risk obstructive sleep apnea (OSA) have poorer clinical outcomes than COVID-19 patients with low-risk OSA. In the current secondary analysis, we evaluated the association of AI-guided CT-based severity scores (SSs) with short-term outcomes in the same cohort. In total, 221 patients (mean age of 52.6 \u00b1 15.6 years, 59% men) with eligible chest CT images from March to May 2020 were included. The AI program scanned the CT images in 3D, and the algorithm measured volumes of lobes and lungs as well as high-opacity areas, including ground glass and consolidation. An SS was defined as the ratio of the volume of high-opacity areas to that of the total lung volume. The primary outcome was the need for supplemental oxygen and hospitalization over 28 days. A receiver operating characteristic (ROC) curve analysis of the association between an SS and the need for supplemental oxygen revealed a cut-off score of 2.65 on the CT images, with a sensitivity of 81% and a specificity of 56%. In a multivariate logistic regression model, an SS > 2.65 predicted the need for supplemental oxygen, with an odds ratio (OR) of 3.98 (95% confidence interval (CI) 1.80-8.79; ", "journal": "Journal of clinical medicine", "date": "2023-11-25", "authors": ["ZeynepAtceken", "YelizCelik", "CetinAtasoy", "Y\u00fckselPeker"], "doi": "10.3390/jcm12227039\n10.1007/s00330-020-06827-4\n10.1186/s40779-020-0233-6\n10.1148/radiol.2020200343\n10.2214/AJR.20.22976\n10.2214/AJR.20.22975\n10.1148/radiol.2020200230\n10.1007/s00330-020-06817-6\n10.1148/radiol.2020200370\n10.1155/2021/6697677\n10.1097/RLI.0000000000000674\n10.1148/ryct.2020200047\n10.1007/s00330-020-07623-w\n10.1097/RTI.0000000000000524\n10.1148/radiol.2020201473\n10.1097/RLI.0000000000000672\n10.3348/kjr.2020.0423\n10.1007/s10462-021-09985-z\n10.1148/ryai.2020200048\n10.1038/s41591-020-0931-3\n10.1002/14651858.cd013787\n10.1513/AnnalsATS.202011-1409OC\n10.2214/AJR.22.27598\n10.1038/s41467-020-17971-2\n10.1148/radiol.2020200905\n10.1259/bjr.20210759\n10.1186/s12879-023-08303-y\n10.1016/j.ejro.2021.100370\n10.1111/jpc.14828\n10.3390/healthcare10122493"}
{"title": "COVID-19 Detection via Ultra-Low-Dose X-ray Images Enabled by Deep Learning.", "abstract": "The detection of Coronavirus disease 2019 (COVID-19) is crucial for controlling the spread of the virus. Current research utilizes X-ray imaging and artificial intelligence for COVID-19 diagnosis. However, conventional X-ray scans expose patients to excessive radiation, rendering repeated examinations impractical. Ultra-low-dose X-ray imaging technology enables rapid and accurate COVID-19 detection with minimal additional radiation exposure. In this retrospective cohort study, ULTRA-X-COVID, a deep neural network specifically designed for automatic detection of COVID-19 infections using ultra-low-dose X-ray images, is presented. The study included a multinational and multicenter dataset consisting of 30,882 X-ray images obtained from approximately 16,600 patients across 51 countries. It is important to note that there was no overlap between the training and test sets. The data analysis was conducted from 1 April 2020 to 1 January 2022. To evaluate the effectiveness of the model, various metrics such as the area under the receiver operating characteristic curve, receiver operating characteristic, accuracy, specificity, and F1 score were utilized. In the test set, the model demonstrated an AUC of 0.968 (95% CI, 0.956-0.983), accuracy of 94.3%, specificity of 88.9%, and F1 score of 99.0%. Notably, the ULTRA-X-COVID model demonstrated a performance comparable to conventional X-ray doses, with a prediction time of only 0.1 s per image. These findings suggest that the ULTRA-X-COVID model can effectively identify COVID-19 cases using ultra-low-dose X-ray scans, providing a novel alternative for COVID-19 detection. Moreover, the model exhibits potential adaptability for diagnoses of various other diseases.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2023-11-25", "authors": ["Isah SalimAhmad", "NaLi", "TangshengWang", "XuanLiu", "JingjingDai", "YinpingChan", "HaoyangLiu", "JunmingZhu", "WeibinKong", "ZefengLu", "YaoqinXie", "XiaokunLiang"], "doi": "10.3390/bioengineering10111314\n10.1016/S0140-6736(20)30419-0\n10.1007/s10489-020-01902-1\n10.1007/s11042-022-13509-4\n10.1109/TMBMC.2021.3099367\n10.1016/j.compbiomed.2022.106070\n10.1148/ryct.2020200196\n10.1016/j.compbiomed.2021.104319\n10.1016/j.compbiomed.2021.104425\n10.1016/j.compbiomed.2021.104689\n10.1109/TNNLS.2021.3114747\n10.1007/s00530-021-00794-6\n10.3390/sym12040651\n10.1016/j.imu.2020.100360\n10.1016/j.ejrad.2020.109041\n10.1371/journal.pone.0250952\n10.31224/osf.io/wx89s\n10.1007/s13246-020-00865-4\n10.1038/s41598-020-76550-z\n10.1038/d41586-021-00396-2\n10.1007/s10489-021-02393-4\n10.1007/s10489-022-03893-7\n10.1007/s11548-019-01912-6\n10.20944/preprints202003.0300.v1\n10.1016/j.media.2020.101794\n10.1007/s10044-021-00984-y\n10.1007/s12559-020-09775-9\n10.5455/jjee.204-1585312246\n10.1145/3418094.3418096\n10.1142/S0218001421510046\n10.1109/TMI.2020.3040950\n10.1109/ACCESS.2021.3086229\n10.3390/math11061489\n10.1007/s00500-020-05275-y\n10.48550/arXiv.2110.08427\n10.3390/s21041480"}
{"title": "DualAttNet: Synergistic fusion of image-level and fine-grained disease attention for multi-label lesion detection in chest X-rays.", "abstract": "Chest radiographs are the most commonly performed radiological examinations for lesion detection. Recent advances in deep learning have led to encouraging results in various thoracic disease detection tasks. Particularly, the architecture with feature pyramid network performs the ability to recognise targets with different sizes. However, such networks are difficult to focus on lesion regions in chest X-rays due to their high resemblance in vision. In this paper, we propose a dual attention supervised module for multi-label lesion detection in chest radiographs, named DualAttNet. It efficiently fuses global and local lesion classification information based on an image-level attention block and a fine-grained disease attention algorithm. A binary cross entropy loss function is used to calculate the difference between the attention map and ground truth at image level. The generated gradient flow is leveraged to refine pyramid representations and highlight lesion-related features. We evaluate the proposed model on VinDr-CXR, ChestX-ray8 and COVID-19 datasets. The experimental results show that DualAttNet surpasses baselines by 0.6% to 2.7% mAP and 1.4% to 4.7% AP", "journal": "Computers in biology and medicine", "date": "2023-11-25", "authors": ["QingXu", "WentingDuan"], "doi": "10.1016/j.compbiomed.2023.107742"}
{"title": "A deep learning feature extraction-based hybrid approach for detecting pediatric pneumonia in chest X-ray images.", "abstract": "Pneumonia is a disease caused by bacteria, viruses, and fungi that settle in the alveolar sacs of the lungs and can lead to serious health complications in humans. Early detection of pneumonia is necessary for early treatment to manage and cure the disease. Recently, machine learning-based pneumonia detection methods have focused on pneumonia in adults. Machine learning relies on manual feature engineering, whereas deep learning can automatically detect and extract features from data. This study proposes a deep learning feature extraction-based hybrid approach that combines deep learning and machine learning to detect pediatric pneumonia, which is difficult to standardize. The proposed hybrid approach enhances the accuracy of detecting pediatric pneumonia and simplifies the approach by eliminating the requirement for advanced feature extraction. The experiments indicate that the hybrid approach using a Medium Neural Network based on AlexNet feature extraction achieved a 97.9% accuracy rate and 98.0% sensitivity rate. The results show that the proposed approach achieved higher accuracy rates than state-of-the-art approaches.", "journal": "Physical and engineering sciences in medicine", "date": "2023-11-22", "authors": ["UfukBal", "AlkanBal", "\u00d6zge TaylanMoral", "FatihD\u00fczg\u00fcn", "NidaG\u00fcrb\u00fcz"], "doi": "10.1007/s13246-023-01347-z\n10.3390/diagnostics13040743\n10.1145/3065386\n10.3390/app10020559\n10.3390/electronics10131512\n10.1016/j.measurement.2020.108046\n10.3390/s20041068\n10.1016/j.cmpb.2019.06.023\n10.1371/journal.pone.0256630\n10.3390/diagnostics10060417\n10.1136/bmjopen-2020-044461\n10.1007/s11554-021-01086-y"}
{"title": "Natural language processing system for rapid detection and intervention of mental health crisis chat messages.", "abstract": "Patients experiencing mental health crises often seek help through messaging-based platforms, but may face long wait times due to limited message triage capacity. Here we build and deploy a machine-learning-enabled system to improve response times to crisis messages in a large, national telehealth provider network. We train a two-stage natural language processing (NLP) system with key word filtering followed by logistic regression on 721 electronic medical record chat messages, of which 32% are potential crises (suicidal/homicidal ideation, domestic violence, or non-suicidal self-injury). Model performance is evaluated on a retrospective test set (4/1/21-4/1/22, N\u2009=\u2009481) and a prospective test set (10/1/22-10/31/22, N\u2009=\u2009102,471). In the retrospective test set, the model has an AUC of 0.82 (95% CI: 0.78-0.86), sensitivity of 0.99 (95% CI: 0.96-1.00), and PPV of 0.35 (95% CI: 0.309-0.4). In the prospective test set, the model has an AUC of 0.98 (95% CI: 0.966-0.984), sensitivity of 0.98 (95% CI: 0.96-0.99), and PPV of 0.66 (95% CI: 0.626-0.692). The daily median time from message receipt to crisis specialist triage ranges from 8 to 13\u2009min, compared to 9\u2009h before the deployment of the system. We demonstrate that a NLP-based machine learning model can reliably identify potential crisis chat messages in a telehealth setting. Our system integrates into existing clinical workflows, suggesting that with appropriate training, humans can successfully leverage ML systems to facilitate triage of crisis messages.", "journal": "NPJ digital medicine", "date": "2023-11-22", "authors": ["AkshaySwaminathan", "Iv\u00e1nL\u00f3pez", "Rafael Antonio GarciaMar", "TylerHeist", "TomMcClintock", "KaitlinCaoili", "MadelineGrace", "MatthewRubashkin", "Michael NBoggs", "Jonathan HChen", "OlivierGevaert", "DavidMou", "Matthew KNock"], "doi": "10.1038/s41746-023-00951-3\n10.3390/ijerph15071425\n10.1080/10503307.2020.1781952\n10.1155/2016/8708434\n10.1213/ANE.0000000000004575\n10.1503/cmaj.202066\n10.1186/s12916-019-1466-7\n10.1186/s12916-019-1425-3\n10.1093/biomet/26.4.404\n10.2307/2531595"}
{"title": "Visual transformer and deep CNN prediction of high-risk COVID-19 infected patients using fusion of CT images and clinical data.", "abstract": "Despite the globally reducing hospitalization rates and the much lower risks of Covid-19 mortality, accurate diagnosis of the infection stage and prediction of outcomes are clinically of interest. Advanced current technology can facilitate automating the process and help identifying those who are at higher risks of developing severe illness. This work explores and represents deep-learning-based schemes for predicting clinical outcomes in Covid-19 infected patients, using Visual Transformer and Convolutional Neural Networks (CNNs), fed with 3D data fusion of CT scan images and patients' clinical data.\nWe report on the efficiency of Video Swin Transformers and several CNN models fed with fusion datasets and CT scans only vs. a set of conventional classifiers fed with patients' clinical data only. A relatively large clinical dataset from 380 Covid-19 diagnosed patients was used to train/test the models.\nResults show that the 3D Video Swin Transformers fed with the fusion datasets of 64 sectional CT scans\u2009+\u200967 clinical labels outperformed all other approaches for predicting outcomes in Covid-19-infected patients amongst all techniques (i.e., TPR\u2009=\u20090.95, FPR\u2009=\u20090.40, F0.5 score\u2009=\u20090.82, AUC\u2009=\u20090.77, Kappa\u2009=\u20090.6).\nWe demonstrate how the utility of our proposed novel 3D data fusion approach through concatenating CT scan images with patients' clinical data can remarkably improve the performance of the models in predicting Covid-19 infection outcomes.\nFindings indicate possibilities of predicting the severity of outcome using patients' CT images and clinical data collected at the time of admission to hospital.", "journal": "BMC medical informatics and decision making", "date": "2023-11-18", "authors": ["Sara Saberi MoghadamTehrani", "MaralZarvani", "PariaAmiri", "ZahraGhods", "MasoomehRaoufi", "Seyed Amir AhmadSafavi-Naini", "AmiraliSoheili", "MohammadGharib", "HamidAbbasi"], "doi": "10.1186/s12911-023-02344-8\n10.1038/s41586-020-2008-3\n10.1038/s41564-020-0695-z\n10.1016/S2213-2600(20)30076-X\n10.1002/jmv.25728\n10.1080/22221751.2020.1741327\n10.1016/j.media.2021.102096\n10.1038/s41598-020-78888-w\n10.1016/j.scs.2020.102589\n10.1016/j.compbiomed.2020.103795\n10.1016/j.compbiomed.2020.103792\n10.1038/s41467-020-18684-2\n10.1038/s41598-021-83967-7\n10.2196/25884\n10.1016/j.cmpb.2021.105996\n10.1016/j.cmpb.2021.105951\n10.1016/j.cmpb.2020.105704\n10.1016/j.ejrad.2021.109583\n10.1109/JBHI.2020.3034296\n10.1038/s41746-020-00341-z\n10.1016/j.cmpb.2020.105581\n10.1016/j.cmpb.2021.106336\n10.1016/j.cmpb.2021.106406\n10.1016/j.jbi.2021.103751\n10.1109/JTEHM.2021.3134096\n10.4097/kjae.2017.70.4.407\n10.1037/h0071325\n10.1016/j.crbeha.2021.100044\n10.1007/s10994-006-6226-1\n10.1198/106186008X344522\n10.1016/j.ins.2017.05.008\n10.1146/annurev-statistics-031017-100325\n10.1109/TCYB.2020.2983860\n10.1145/2907070\n10.1142/S0218001409007326\n10.1016/j.ins.2018.06.020\n10.1016/j.patrec.2020.03.016"}
{"title": "A scoping review of interpretability and explainability concerning artificial intelligence methods in medical imaging.", "abstract": "To review eXplainable Artificial Intelligence/(XAI) methods available for medical imaging/(MI).\nA scoping review was conducted following the Joanna Briggs Institute's methodology. The search was performed on Pubmed, Embase, Cinhal, Web of Science, BioRxiv, MedRxiv, and Google Scholar. Studies published in French and English after 2017 were included. Keyword combinations and descriptors related to explainability, and MI modalities were employed. Two independent reviewers screened abstracts, titles and full text, resolving differences through discussion.\n228 studies met the criteria. XAI publications are increasing, targeting MRI (n\u00a0=\u00a073), radiography (n\u00a0=\u00a047), CT (n\u00a0=\u00a046). Lung (n\u00a0=\u00a082) and brain (n\u00a0=\u00a074) pathologies, Covid-19 (n\u00a0=\u00a048), Alzheimer's disease (n\u00a0=\u00a025), brain tumors (n\u00a0=\u00a015) are the main pathologies explained. Explanations are presented visually (n\u00a0=\u00a0186), numerically (n\u00a0=\u00a067), rule-based (n\u00a0=\u00a011), textually (n\u00a0=\u00a011), and example-based (n\u00a0=\u00a06). Commonly explained tasks include classification (n\u00a0=\u00a089), prediction (n\u00a0=\u00a047), diagnosis (n\u00a0=\u00a039), detection (n\u00a0=\u00a029), segmentation (n\u00a0=\u00a013), and image quality improvement (n\u00a0=\u00a06). The most frequently provided explanations were local (78.1\u00a0%), 5.7\u00a0% were global, and 16.2\u00a0% combined both local and global approaches. Post-hoc approaches were predominantly employed. The used terminology varied, sometimes indistinctively using explainable (n\u00a0=\u00a0207), interpretable (n\u00a0=\u00a0187), understandable (n\u00a0=\u00a0112), transparent (n\u00a0=\u00a061), reliable (n\u00a0=\u00a031), and intelligible (n\u00a0=\u00a03).\nThe number of XAI publications in medical imaging is increasing, primarily focusing on applying XAI techniques to MRI, CT, and radiography for classifying and predicting lung and brain pathologies. Visual and numerical output formats are predominantly used. Terminology standardisation remains a challenge, as terms like \"explainable\" and \"interpretable\" are sometimes being used indistinctively. Future XAI development should consider user needs and perspectives.", "journal": "European journal of radiology", "date": "2023-11-18", "authors": ["M\u00e9lanieChampendal", "HenningM\u00fcller", "John OPrior", "Cl\u00e1udia S\u00e1Dos Reis"], "doi": "10.1016/j.ejrad.2023.111159"}
{"title": "Ensemble classification of integrated CT scan datasets in detecting COVID-19 using feature fusion from contourlet transform and CNN.", "abstract": "The COVID-19 disease caused by coronavirus is constantly changing due to the emergence of different variants and thousands of people are dying every day worldwide. Early detection of this new form of pulmonary disease can reduce the mortality rate. In this paper, an automated method based on machine learning (ML) and deep learning (DL) has been developed to detect COVID-19 using computed tomography (CT) scan images extracted from three publicly available datasets (A total of 11,407 images; 7397 COVID-19 images and 4010 normal images). An unsupervised clustering approach that is a modified region-based clustering technique for segmenting COVID-19 CT scan image has been proposed. Furthermore, contourlet transform and convolution neural network (CNN) have been employed to extract features individually from the segmented CT scan images and to fuse them in one feature vector. Binary differential evolution (BDE) approach has been employed as a feature optimization technique to obtain comprehensible features from the fused feature vector. Finally, a ML/DL-based ensemble classifier considering bagging technique has been employed to detect COVID-19 from the CT images. A fivefold and generalization cross-validation techniques have been used for the validation purpose. Classification experiments have also been conducted with several pre-trained models (AlexNet, ResNet50, GoogleNet, VGG16, VGG19) and found that the ensemble classifier technique with fused feature has provided state-of-the-art performance with an accuracy of 99.98%.", "journal": "Scientific reports", "date": "2023-11-17", "authors": ["MdNur-A-Alam", "Mostofa KamalNasir", "MominulAhsan", "Md AbdulBased", "JulfikarHaider", "MarcinKowalski"], "doi": "10.1038/s41598-023-47183-9\n10.1001/jama.2020.12839\n10.23750/abm.v91i1.9397\n10.1016/j.patcog.2021.107848\n10.1038/s41579-020-00459-7\n10.1016/j.jbi.2021.103751\n10.1155/2021/6677314\n10.3389/fcimb.2020.587269\n10.1016/j.patrec.2020.10.001\n10.1016/j.eswa.2021.116377\n10.3390/electronics12030710\n10.1016/j.asoc.2022.108765\n10.1016/j.patcog.2021.108135\n10.1016/j.patcog.2021.108071\n10.1016/j.compbiomed.2020.104037\n10.1016/j.patcog.2021.107828\n10.1016/j.knosys.2021.106849\n10.1016/j.bbe.2021.04.006\n10.1155/2018/2634861\n10.1016/j.patcog.2010.06.006\n10.1007/s10140-020-01886-y\n10.1148/radiology.154.1.3964935\n10.1109/tsp.2003.812841\n10.4028/www.scientific.net/amm.321-324.1061\n10.1007/s42979-021-00690-w\n10.1504/IJIM.2015.070024\n10.1109/TPAMI.2005.159\n10.31661/JBPE.V0I0.2002-1072\n10.1088/1361-6498/ab23cc\n10.1007/s10462-009-9124-7\n10.1038/s41597-021-00900-3\n10.3390/s21041480\n10.1016/j.procs.2020.09.258\n10.1109/tcbb.2021.3065361\n10.3390/e22050517\n10.1155/2021/8785636\n10.2196/19569\n10.1016/j.ejrad.2020.109041\n10.1038/s41591-020-0931-3\n10.1016/j.irbm.2020.05.003\n10.1016/j.compbiomed.2020.104037\n10.1183/13993003.00775-2020\n10.1016/j.patrec.2021.08.009\n10.3390/s21041278"}
{"title": "The role of iodinated contrast media in computed tomography structured Reporting and Data Systems (RADS): a narrative review.", "abstract": "In recent years, there has been a large-scale dissemination of guidelines in radiology in the form of Reporting & Data Systems (RADS). The use of iodinated contrast media (ICM) has a fundamental role in enhancing the diagnostic capabilities of computed tomography (CT) but poses certain risks. The scope of the present review is to summarize the current role of ICM only in clinical reporting guidelines for CT that have adopted the \"RADS\" approach, focusing on three specific questions per each RADS: (I) what is the scope of the scoring system; (II) how is ICM used in the scoring system; (III) what is the impact of ICM enhancement on the scoring.\nWe analyzed the original articles for each of the latest versions of RADS that can be used in CT [PubMed articles between January, 2005 and March, 2023 in English and American College of Radiology (ACR) official website].\nWe found 14 RADS suitable for use in CT out of 28 RADS described in the literature. Four RADS were validated by the ACR: Colonography-RADS (C-RADS), Liver Imaging-RADS (LI-RADS), Lung CT Screening-RADS (Lung-RADS), and Neck Imaging-RADS (NI-RADS). One RADS was validated by the ACR in collaboration with other cardiovascular scientific societies: Coronary Artery Disease-RADS 2.0 (CAD-RADS). Nine RADS were proposed by other scientific groups: Bone Tumor Imaging-RADS (BTI-RADS), Bone\u2011RADS, Coronary Artery Calcium Data & Reporting System (CAC-DRS), Coronavirus Disease 2019 Imaging-RADS (COVID-RADS), COVID-19-RADS (CO-RADS), Interstitial Lung Fibrosis Imaging-RADS (ILF-RADS), Lung-RADS (LU-RADS), Node-RADS, and Viral Pneumonia Imaging-RADS (VP-RADS).\nThis overview suggests that ICM is not strictly necessary for the study of bones and calcifications (CAC-DRS, BTI-RADS, Bone-RADS), lung parenchyma (Lung-RADS, LU-RADS, COVID-RADS, CO-RADS, VP-RADS and ILF-RADS), and in CT colonography (C-RADS). On the other hand, ICM plays a key role in CT angiography (CAD-RADS), in the study of liver parenchyma (LI-RADS), and in the evaluation of soft tissues and lymph nodes (NI-RADS, Node-RADS). Future studies are needed in order to evaluate the impact of the new iodinated and non-iodinate contrast media, artificial intelligence tools and dual energy CT in the assignment of RADS scores.", "journal": "Quantitative imaging in medicine and surgery", "date": "2023-11-16", "authors": ["MarcoParillo", "Aart Jvan der Molen", "PatrickAsbach", "Fabian Henry J\u00fcrgenElsholtz", "AndreaLaghi", "MaximeRonot", "Jim SWu", "Carlo AugustoMallio", "Carlo CosimoQuattrocchi"], "doi": "10.21037/qims-23-603\n10.1016/j.radi.2022.09.001\n10.1177/15385744221108040\n10.1007/s12070-023-03554-2\n10.1016/j.jvsv.2022.09.006\n10.1148/rg.2019190087\n10.1007/s13244-017-0588-8\n10.1007/s00330-021-08327-5\n10.21037/qims-20-1325\n10.21037/qims.2020.01.09\n10.21037/qims-2020-27\n10.1007/s10140-020-01751-y\n10.1148/radiol.2361041926\n10.1016/j.jacr.2016.04.031\n10.1016/j.jacr.2018.05.006\n10.1016/j.jcct.2022.07.002\n10.1007/s00330-021-07745-9\n10.1007/s00256-022-04022-8\n10.1016/j.jcct.2018.03.008\n10.1007/s00330-020-06863-0\n10.1148/radiol.2020201473\n10.1097/RCT.0000000000001075\n10.1016/j.carj.2014.03.004\n10.1007/s00330-020-07572-4\n10.21037/qims-20-587\n10.3174/ajnr.A6586\n10.1097/RLI.0000000000000696\n10.1177/0284185114534414\n10.1177/0284185114536157\n10.3109/02841851.2010.509739\n10.1097/RLI.0000000000000686\n10.1148/radiol.2015142631\n10.1007/s11604-021-01233-2\n10.21037/qims-20-1037\n10.21037/qims-20-782\n10.1007/s11548-023-02862-w\n10.1016/j.ejrad.2022.110251\n10.1016/j.diii.2022.01.004\n10.1016/j.jcct.2021.05.004\n10.1016/j.atherosclerosis.2019.12.001"}
{"title": "A Novel COVID-19 Diagnosis Approach Utilizing a Comprehensive Set of Diagnostic Information (CSDI).", "abstract": "The aim of the study was to develop a computerized method for distinguishing COVID-19-affected cases from cases of pneumonia. This task continues to be a real challenge in the practice of diagnosing COVID-19 disease. In the study, a new approach was proposed, using a comprehensive set of diagnostic information (CSDI) including, among other things, medical history, demographic data, signs and symptoms of the disease, and laboratory results. These data have the advantage of being much more reliable compared with data based on a single source of information, such as radiological imaging. On this basis, a comprehensive process of building predictive models was carried out, including such steps as data preprocessing, feature selection, training, and evaluation of classification models. During the study, 9 different methods for feature selection were used, while the grid search method and 12 popular classification algorithms were employed to build classification models. The most effective model achieved a classification accuracy (ACC) of 85%, a sensitivity (TPR) equal to 83%, and a specificity (TNR) of 88%. The model was built using the random forest method with 15 features selected using the recursive feature elimination selection method. The results provide an opportunity to build a computer system to assist the physician in the diagnosis of the COVID-19 disease.", "journal": "Journal of clinical medicine", "date": "2023-11-14", "authors": ["UlzhalgasZhunissova", "R\u00f3\u017caDzier\u017cak", "ZbigniewOmiotek", "VolodymyrLytvynenko"], "doi": "10.3390/jcm12216912\n10.2174/1573397118666220412114514\n10.1016/S2213-2600(20)30079-5\n10.1001/jama.2020.1585\n10.1007/s12652-020-02883-2\n10.3390/diagnostics13111856\n10.3390/vaccines11020374\n10.1038/s41598-023-34908-z\n10.1002/mp.14609\n10.1016/j.bea.2021.100003\n10.1016/j.eswa.2020.114054\n10.1007/s12539-021-00499-4\n10.1515/cclm-2020-1294\n10.3390/healthcare10030541\n10.1155/2021/6677314\n10.1155/2022/4838009\n10.1088/1742-6596/1879/2/022083\n10.4103/jehp.jehp_1424_20\n10.1016/S0140-6736(20)30183-5\n10.1016/j.eswa.2020.113661\n10.3390/computers10030031\n10.1108/WJE-10-2020-0537\n10.4236/jdaip.2022.101005\n10.2196/27468\n10.1016/j.smhl.2020.100178\n10.32604/cmc.2021.012874\n10.1016/j.clinimag.2020.04.001\n10.1016/j.bbe.2020.08.008\n10.1016/j.bspc.2022.104268\n10.1007/s10489-022-03893-7\n10.1016/j.eswa.2021.116377\n10.1155/2022/7672196\n10.1038/s41598-021-97497-9\n10.1016/j.compbiomed.2021.104348\n10.5121/ijdkp.2015.5201\n10.14569/IJACSA.2021.0120670\n10.1186/s12911-021-01521-x\n10.1016/j.cmpb.2022.107295\n10.4103/2045-9912.326002\n10.3390/s22134820\n10.1016/j.jcv.2020.104431\n10.2196/21439\n10.1038/s41598-023-28943-z\n10.1007/s11042-020-10340-7\n10.3934/mbe.2023244"}
{"title": "MRI with generalized diffusion encoding reveals damaged white matter in patients previously hospitalized for COVID-19 and with persisting symptoms at follow-up.", "abstract": "There is mounting evidence of the long-term effects of COVID-19 on the central nervous system, with patients experiencing diverse symptoms, often suggesting brain involvement. Conventional brain MRI of these patients shows unspecific patterns, with no clear connection of the symptomatology to brain tissue abnormalities, whereas diffusion tensor studies and volumetric analyses detect measurable changes in the brain after COVID-19. Diffusion MRI exploits the random motion of water molecules to achieve unique sensitivity to structures at the microscopic level, and new sequences employing generalized diffusion encoding provide structural information which are sensitive to intravoxel features. In this observational study, a total of 32 persons were investigated: 16 patients previously hospitalized for COVID-19 with persisting symptoms of post-COVID condition (mean age 60 years: range 41-79, all male) at 7-month follow-up and 16 matched controls, not previously hospitalized for COVID-19, with no post-COVID symptoms (mean age 58 years, range 46-69, 11 males). Standard MRI and generalized diffusion encoding MRI were employed to examine the brain white matter of the subjects. To detect possible group differences, several tissue microstructure descriptors obtainable with the employed diffusion sequence, the fractional anisotropy, mean diffusivity, axial diffusivity, radial diffusivity, microscopic anisotropy, orientational coherence (", "journal": "Brain communications", "date": "2023-11-13", "authors": ["DenebBoito", "AndersEklund", "AndersTisell", "RichardLevi", "Evren\u00d6zarslan", "IdaBlystad"], "doi": "10.1093/braincomms/fcad284"}
{"title": "COVID-19 screening in low resource settings using artificial intelligence for chest radiographs and point-of-care blood tests.", "abstract": "Artificial intelligence (AI) systems for detection of COVID-19 using chest X-Ray (CXR) imaging and point-of-care blood tests were applied to data from four low resource African settings. The performance of these systems to detect COVID-19 using various input data was analysed and compared with antigen-based rapid diagnostic tests. Participants were tested using the gold standard of RT-PCR test (nasopharyngeal swab) to determine whether they were infected with SARS-CoV-2. A total of 3737 (260 RT-PCR positive) participants were included. In our cohort, AI for CXR images was a poor predictor of COVID-19 (AUC = 0.60), since the majority of positive cases had mild symptoms and no visible pneumonia in the lungs. AI systems using differential white blood cell counts (WBC), or a combination of WBC and C-Reactive Protein (CRP) both achieved an AUC of 0.74 with a suggested optimal cut-off point at 83% sensitivity and 63% specificity. The antigen-RDT tests in this trial obtained 65% sensitivity at 98% specificity. This study is the first to validate AI tools for COVID-19 detection in an African setting. It demonstrates that screening for COVID-19 using AI with point-of-care blood tests is feasible and can operate at a higher sensitivity level than antigen testing.", "journal": "Scientific reports", "date": "2023-11-12", "authors": ["KeelinMurphy", "JosephineMuhairwe", "StevenSchalekamp", "Bramvan Ginneken", "IreneAyakaka", "KameleMashaete", "BulembaKatende", "Alastairvan Heerden", "ShannonBosman", "ThandananiMadonsela", "LuciaGonzalez Fernandez", "AitaSignorell", "MoniekBresser", "KlausReither", "Tracy RGlass"], "doi": "10.1038/s41598-023-46461-w\n10.2196/19673\n10.1007/s00330-020-07347-x\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020201473\n10.1002/14651858.CD013639.pub2\n10.1007/s00330-020-06827-4\n10.1515/cclm-2020-0398\n10.1002/jmv.26143\n10.1515/cclm-2020-0593\n10.1002/ctm2.23\n10.1002/14651858.CD013787\n10.1148/radiol.2020201491\n10.1038/s41467-020-17971-2\n10.1038/s41467-020-18685-1\n10.1148/radiol.2020202439\n10.1148/radiol.2020200905\n10.1109/TMI.2020.2995508\n10.1109/TCBB.2021.3065361\n10.1007/s00330-021-07715-1\n10.1183/13993003.00775-2020\n10.1109/TMI.2020.2994908\n10.1002/mp.15582\n10.1117/1.JMI.8.S1.014001\n10.1148/radiol.2020201874\n10.1109/TMI.2020.2993291\n10.1016/j.media.2021.102299\n10.1371/journal.pone.0242301\n10.1371/journal.pone.0262052\n10.1109/JBHI.2020.3037127\n10.1016/j.patcog.2020.107613\n10.1148/radiol.2020203511\n10.1148/radiol.2020202944\n10.1016/j.patcog.2021.108289\n10.1038/s41746-021-00553-x\n10.1371/journal.pone.0255301\n10.2196/24048\n10.1016/S2589-7500(20)30274-0\n10.1016/S2589-7500(21)00272-7\n10.1093/clinchem/hvaa200\n10.1371/journal.pgph.0001488\n10.2307/2531595\n10.3389/fimmu.2021.720363\n10.1002/jmv.26097\n10.1093/ije/dyab012\n10.1371/journal.pmed.1003735\n10.1371/journal.pmed.1004011\n10.1128/spectrum.00853-22\n10.1136/bmjopen-2020-046130"}
{"title": "Novel machine-learning analysis of SARS-CoV-2 infection\u00a0in a subclinical nonhuman primate model using radiomics and blood biomarkers.", "abstract": "Detection of the physiological response to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection is challenging in the absence of overt clinical signs but remains\u00a0necessary to understand a full\u00a0subclinical disease spectrum. In this study, our objective was to use radiomics (from computed tomography images) and blood biomarkers to predict SARS-CoV-2 infection in a nonhuman primate model (NHP) with inapparent clinical disease. To accomplish this aim, we built machine-learning models to predict SARS-CoV-2 infection in a NHP model of subclinical disease using baseline-normalized radiomic and blood sample analyses data from SARS-CoV-2-exposed and control (mock-exposed) crab-eating macaques. We applied a novel adaptation of the minimum redundancy maximum relevance (mRMR) feature-selection technique, called mRMR-permute, for statistically-thresholded and unbiased feature selection. Through performance comparison of eight machine-learning models trained on 14 feature sets, we demonstrated that a logistic regression model trained on the mRMR-permute feature set can predict SARS-CoV-2 infection with very high accuracy. Eighty-nine percent of mRMR-permute selected features had strong and significant class effects. Through this work, we identified a key set of radiomic and blood biomarkers that can be used to predict infection status even in the absence of clinical signs. Furthermore, we proposed and demonstrated the utility of a novel feature-selection technique called mRMR-permute. This work lays the foundation for the prediction and classification of SARS-CoV-2 disease severity.", "journal": "Scientific reports", "date": "2023-11-11", "authors": ["Winston TChu", "Marcelo ACastro", "SyedReza", "Timothy KCooper", "SeanBartlinski", "DaraBradley", "Scott MAnthony", "GabriellaWorwa", "Courtney LFinch", "Jens HKuhn", "IanCrozier", "JeffreySolomon"], "doi": "10.1038/s41598-023-46694-9\n10.1056/NEJMoa2006100\n10.7326/M20-6976\n10.1001/jamainternmed.2020.5664\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1021/acsnano.0c02624\n10.1148/radiol.2020200343\n10.1186/s12879-021-05855-9\n10.1016/j.clinimag.2021.04.033\n10.6061/clinics/2021/e3503\n10.1007/s00330-021-08435-2\n10.1371/journal.pone.0264711\n10.1148/ryct.2020200126\n10.1038/s41579-020-00459-7\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2002032\n10.1016/j.jfma.2020.04.007\n10.1038/s41586-020-2787-6\n10.1371/journal.ppat.1010161\n10.1016/j.molmed.2021.12.001\n10.1038/s41592-018-0019-x\n10.1109/TPAMI.2005.159\n10.1155/2012/320698\n10.2174/1386207321666180601074349\n10.1155/2015/604910\n10.1371/journal.pone.0184129\n10.1007/s00330-020-07032-z\n10.1097/RTI.0000000000000544\n10.1109/JBHI.2020.3036722\n10.1186/s12967-020-02692-3\n10.1002/mco2.14\n10.1016/j.intimp.2020.106705\n10.1093/labmed/lmaa111\n10.1093/clinchem/hvaa200\n10.1097/RCT.0000000000001094\n10.3390/cells10102754\n10.1183/13993003.01754-2020\n10.1186/s12931-020-01429-6\n10.1183/13993003.01217-2020\n10.1097/RLI.0000000000000672\n10.1148/radiol.2020200274\n10.1097/RLI.0000000000000670\n10.5888/pcd18.210123\n10.1371/journal.pbio.1000412\n10.1016/j.antiviral.2016.02.013\n10.1016/j.acra.2020.08.023\n10.1016/j.acra.2023.02.027\n10.1158/0008-5472.CAN-17-0339\n10.1016/j.mri.2012.05.001\n10.1097/CORR.0000000000000843"}
{"title": "Improving explainable AI with patch perturbation-based evaluation pipeline: a COVID-19 X-ray image analysis case study.", "abstract": "Recent advances in artificial intelligence (AI) have sparked interest in developing explainable AI (XAI) methods for clinical decision support systems, especially in translational research. Although using XAI methods may enhance trust in black-box models, evaluating their effectiveness has been challenging, primarily due to the absence of human (expert) intervention, additional annotations, and automated strategies. In order to conduct a thorough assessment, we propose a patch perturbation-based approach to automatically evaluate the quality of explanations in medical imaging analysis. To eliminate the need for human efforts in conventional evaluation methods, our approach executes poisoning attacks during model retraining by generating both static and dynamic triggers. We then propose a comprehensive set of evaluation metrics during the model inference stage to facilitate the evaluation from multiple perspectives, covering a wide range of correctness, completeness, consistency, and complexity. In addition, we include an extensive case study to showcase the proposed evaluation strategy by applying widely-used XAI methods on COVID-19 X-ray imaging classification tasks, as well as a thorough review of existing XAI methods in medical imaging analysis with evaluation availability. The proposed patch perturbation-based workflow offers model developers an automated and generalizable evaluation strategy to identify potential pitfalls and optimize their proposed explainable solutions, while also aiding end-users in comparing and selecting appropriate XAI methods that meet specific clinical needs in real-world clinical research and practice.", "journal": "Scientific reports", "date": "2023-11-10", "authors": ["JiminSun", "WenqiShi", "Felipe OGiuste", "Yog SVaghani", "LingziTang", "May DWang"], "doi": "10.1038/s41598-023-46493-2\n10.1146/annurev-bioeng-071516-044442\n10.1109/RBME.2020.2987975\n10.1016/S2589-7500(20)30219-3\n10.1016/S2589-7500(20)30218-1\n10.3390/app11115088\n10.1016/j.inffus.2019.12.012\n10.3390/electronics10050593\n10.1016/j.cmpb.2020.105608\n10.1109/JTEHM.2021.3134096\n10.1016/j.media.2021.102046\n10.3390/app10165683\n10.1109/JBHI.2022.3205167\n10.1016/j.compbiomed.2020.103869\n10.1016/j.compbiomed.2020.103792\n10.1007/s11042-022-12156-z\n10.1016/j.asoc.2020.106744\n10.3390/s21217116\n10.1007/s10489-020-01900-3\n10.3892/etm.2020.8797\n10.1109/TNNLS.2016.2599820\n10.1007/s11263-017-1059-x\n10.1148/radiol.2020203511\n10.1109/TKDE.2009.191\n10.1145/3422622\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1007/s11604-017-0702-3\n10.2217/iim.12.13\n10.1148/rg.246045065\n10.1007/s12194-012-0179-9\n10.1007/s00330-006-0470-4"}
{"title": "MRI Deep Learning-Based Automatic Segmentation of Interventricular Septum for Black-Blood Myocardial T2* Measurement in Thalassemia.", "abstract": "The T2* value of interventricular septum is routinely reported for grading myocardial iron load in thalassemia major, and automatic segmentation of septum could shorten analysis time and reduce interobserver variability.\nTo develop a deep learning-based method for automatic septum segmentation from black-blood MR images for the myocardial T2* measurement of thalassemia patients.\nRetrospective.\nOne hundred forty-six transfusion-dependent thalassemia patients with cardiac MR examinations from two centers. Data from Center 1 (1.5\u2009T) were assigned to the training (100 examinations) and internal testing (20 examinations) sets; data from Center 2 were assigned to the external testing set (26 examinations; 10 at 1.5\u2009T and 16 at 3.0\u2009T).\n1.5\u2009T and 3.0\u2009T, multiecho gradient-echo sequence.\nA modified attention U-Net for septum segmentation was constructed and trained, and its performance evaluated on unseen internal and external datasets. T2* was measured by fitting the average septum signal, separately segmented by automatic and manual methods.\nAgreement between manual and automatic septum segmentations was assessed with the Dice coefficient, and T2* agreement was assessed using the Bland-Altman plot and the coefficient of variation (CoV).\nThe median Dice coefficient of deep network-based septum segmentation was 0.90 [0.05] on the internal dataset, 0.82 [0.10] on the external 1.5\u2009T dataset, and 0.86 [0.14] on the external 3.0\u2009T dataset. T2* measurements using automatic segmentation corresponded with those from manual segmentation, with a mean difference of 0.02 (95% LoA: -0.74 to 0.79) msec, 0.43 (95% LoA: -2.1 to 3.0) msec, and 0.36 (95% LoA: -0.72 to 1.4) msec on the three datasets. The CoVs between the two methods were 3.1%, 7.0%, and 6.1% on the internal and two external datasets, respectively.\nThe proposed septum segmentation yielded myocardial T2* measurements which were highly consistent with those obtained by manual segmentation. This automatic approach may facilitate data processing and avoid operator-dependent variability in practice.\n4 TECHNICAL EFFICACY: Stage 1.", "journal": "Journal of magnetic resonance imaging : JMRI", "date": "2023-11-09", "authors": ["ZifengLian", "QiqiLu", "BingquanLin", "LingjianChen", "PengPeng", "YanqiuFeng"], "doi": "10.1002/jmri.29113"}
{"title": "Impact of respiratory motion on ", "abstract": "A total of 101 patients who underwent oncological \nIn total, 168 lesions with and without respiratory motion correction were analyzed. Our results indicated that most \nRespiratory motion has a significant impact on ", "journal": "Journal of applied clinical medical physics", "date": "2023-11-08", "authors": ["Yu-HungChen", "Kuo-YiKan", "Shu-HsinLiu", "Hsin-HonLin", "Kun-HanLue"], "doi": "10.1002/acm2.14200"}
{"title": "Phenotyping the virulence of SARS-CoV-2 variants in hamsters by digital pathology and machine learning.", "abstract": "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has continued to evolve throughout the coronavirus disease-19 (COVID-19) pandemic, giving rise to multiple variants of concern (VOCs) with different biological properties. As the pandemic progresses, it will be essential to test in near real time the potential of any new emerging variant to cause severe disease. BA.1 (Omicron) was shown to be attenuated compared to the previous VOCs like Delta, but it is possible that newly emerging variants may regain a virulent phenotype. Hamsters have been proven to be an exceedingly good model for SARS-CoV-2 pathogenesis. Here, we aimed to develop robust quantitative pipelines to assess the virulence of SARS-CoV-2 variants in hamsters. We used various approaches including RNAseq, RNA in situ hybridization, immunohistochemistry, and digital pathology, including software assisted whole section imaging and downstream automatic analyses enhanced by machine learning, to develop methods to assess and quantify virus-induced pulmonary lesions in an unbiased manner. Initially, we used Delta and Omicron to develop our experimental pipelines. We then assessed the virulence of recent Omicron sub-lineages including BA.5, XBB, BQ.1.18, BA.2, BA.2.75 and EG.5.1. We show that in experimentally infected hamsters, accurate quantification of alveolar epithelial hyperplasia and macrophage infiltrates represent robust markers for assessing the extent of virus-induced pulmonary pathology, and hence virus virulence. In addition, using these pipelines, we could reveal how some Omicron sub-lineages (e.g., BA.2.75 and EG.5.1) have regained virulence compared to the original BA.1. Finally, to maximise the utility of the digital pathology pipelines reported in our study, we developed an online repository containing representative whole organ histopathology sections that can be visualised at variable magnifications (https://covid-atlas.cvr.gla.ac.uk). Overall, this pipeline can provide unbiased and invaluable data for rapidly assessing newly emerging variants and their potential to cause severe disease.", "journal": "PLoS pathogens", "date": "2023-11-07", "authors": ["Gavin RMeehan", "VanessaHerder", "JayAllan", "XinyiHuang", "KarenKerr", "Diogo CorreaMendonca", "GeorgiosIlia", "Derek WWright", "KyriakiNomikou", "QuanGu", "SergiMolina Arias", "FlorianHansmann", "AlexandrosHardas", "CharalamposAttipa", "GiudittaDe Lorenzo", "VanessaCowton", "NicoleUpfold", "NatashaPalmalux", "Jonathan CBrown", "Wendy SBarclay", "Ana Da SilvaFilipe", "WilhelmFurnon", "Arvind HPatel", "MassimoPalmarini"], "doi": "10.1371/journal.ppat.1011589\n10.1038/s41579-022-00841-7\n10.1038/s41591-022-01911-2\n10.1038/s41591-022-01911-2\n10.1038/s41586-022-04411-y\n10.1016/j.chom.2022.10.003\n10.1038/s41467-023-38188-z\n10.1038/s41467-023-38435-3\n10.1038/s41586-021-03792-w\n10.1038/s41577-022-00720-5\n10.1016/j.lanepe.2022.100556\n10.1093/infdis/jiac411\n10.1016/j.jinf.2023.05.019\n10.1038/s41586-022-04441-6\n10.1038/s41564-022-01143-7\n10.1038/s41586-022-04462-1\n10.1038/s41467-022-31200-y\n10.1073/pnas.2009799117\n10.1093/cid/ciaa325\n10.1038/s41467-021-25156-8\n10.1038/s41586-022-04442-5\n10.1016/j.cell.2022.09.018\n10.1038/s41586-021-04266-9\n10.1038/s42003-023-05081-w\n10.1038/s41586-022-05482-7\n10.1038/s41467-023-37059-x\n10.1126/scitranslmed.abq3059\n10.1128/mbio.03044-21\n10.1038/s41467-023-39049-5\n10.1371/journal.ppat.1011057\n10.1038/s41467-023-40033-2\n10.1038/s41467-022-33632-y\n10.1016/S0140-6736(22)00326-9\n10.1016/S1473-3099(21)00475-8\n10.1002/path.5331\n10.1128/JVI.01010-21\n10.1038/s41467-021-25030-7\n10.1016/j.immuni.2021.01.017\n10.5858/arpa.2018-0343-RA\n10.1371/journal.pbio.2004086\n10.1038/s41586-020-2342-5\n10.1038/s41586-020-2787-6\n10.1074/jbc.270.12.6531\n10.1177/44.7.8675988\n10.1016/j.chom.2022.10.003\n10.1101/2022.12.27.521986\n10.1038/s41586-022-04856-1\n10.1016/j.jpi.2022.100184\n10.1371/journal.ppat.1009705\n10.1177/03009858211057197\n10.1038/s41467-020-19684-y\n10.1016/S1473-3099(20)30434-5\n10.1038/s41379-020-0536-x\n10.1038/s41379-020-0603-3\n10.1172/JCI68782\n10.1016/j.stem.2020.02.010\n10.1038/nature14112\n10.1016/j.stem.2019.12.014\n10.1101/2023.09.08.23295024\n10.1053/j.semdp.2023.02.001\n10.1016/j.prp.2023.154362\n10.1002/path.5966\n10.1016/j.cell.2022.04.035\n10.1371/journal.pbio.3001091\n10.1038/s41598-017-17204-5\n10.1038/nmeth.3317\n10.1093/bioinformatics/btt656\n10.1186/s13059-014-0550-8\n10.1186/s12859-017-1559-2"}
{"title": "Deep learning, 3D ultrastructural analysis reveals quantitative differences in platelet and organelle packing in COVID-19/SARSCoV2 patient-derived platelets.", "abstract": "Platelets contribute to COVID-19 clinical manifestations, of which microclotting in the pulmonary vasculature has been a prominent symptom. To investigate the potential diagnostic contributions of overall platelet morphology and their \u03b1-granules and mitochondria to the understanding of platelet hyperactivation and micro-clotting, we undertook a 3D ultrastructural approach. Because differences might be small, we used the high-contrast, high-resolution technique of focused ion beam scanning EM (FIB-SEM) and employed deep learning computational methods to evaluate nearly 600 individual platelets and 30\u2009000 included organelles within three healthy controls and three severely ill COVID-19 patients. Statistical analysis reveals that the \u03b1-granule/mitochondrion-to-plateletvolume ratio is significantly greater in COVID-19 patient platelets indicating a denser packing of organelles, and a more compact platelet. The COVID-19 patient platelets were significantly smaller -by 35% in volume - with most of the difference in organelle packing density being due to decreased platelet size. There was little to no 3D ultrastructural evidence for differential activation of the platelets from COVID-19 patients. Though limited by sample size, our studies suggest that factors outside of the platelets themselves are likely responsible for COVID-19 complications. Our studies show how deep learning 3D methodology can become the gold standard for 3D ultrastructural studies of platelets.\nCOVID-19 patients exhibit a range of symptoms including microclotting. Clotting is a complex process involving both circulating proteins and platelets, a cell within the blood. Increased clotting is suggestive of an increased level of platelet activation. If this were true, we reasoned that parts of the platelet involved in the release of platelet contents during clotting would have lost their content and appear as expanded, empty \u201cghosts.\u201d To test this, we drew blood from severely ill COVID-19 patients and compared the platelets within the blood draws to those from healthy volunteers. All procedures were done under careful attention to biosafety and approved by health authorities. We looked within the platelets for empty ghosts by the high magnification technique of electron microscopy. To count the ghosts, we developed new computer software. In the end, we found little difference between the COVID patient platelets and the healthy donor platelets. The results suggest that circulating proteins outside of the platelet are more important to the strong clotting response. The software developed will be used to analyze other disease states.", "journal": "Platelets", "date": "2023-11-07", "authors": ["Sagar SMatharu", "Cassidy SNordmann", "Kurtis ROttman", "RahulAkkem", "DouglasPalumbo", "Denzel R DCruz", "KennethCampbell", "GailSievert", "JamieSturgill", "James ZPorterfield", "SmitaJoshi", "Hammodah RAlfar", "ChiPeng", "Irina DPokrovskaya", "Jeffrey AKamykowski", "Jeremy PWood", "BethGarvy", "Maria AAronova", "Sidney WWhiteheart", "Richard DLeapman", "BrianStorrie"], "doi": "10.1080/09537104.2023.2264978\n10.1016/j.eclinm.2020.100434\n10.1056/NEJMoa2015432\n10.1161/CIRCULATIONAHA.120.047430\n10.1186/s12959-021-00284-9\n10.1001/jamanetworkopen.2020.10478\n10.1186/s13054-020-03025-y\n10.1186/s12933-020-01165-7\n10.1161/CIRCULATION-AHA.120.047659\n10.1055/s-0040-1712097\n10.1111/jth.14830\n10.1042/BSR20210611\n10.3390/ijms21218234\n10.1016/j.blre.2009.04.001\n10.1182/blood-2012-07-445080\n10.1182/bloodadvances.2018019158\n10.1111/jth.15243\n10.1016/j.metabol.2023.155596\n10.3390/ijms21144866\n10.1080/09537104.2021.1881951\n10.1002/rth2.12260\n10.3109/01913121003725721\n10.1371/journal.pone.0097784\n10.3389/fcvm.2022.1031092\n10.1016/s0268-960x(05)80020-7\n10.1073/pnas.1518628112\n10.1111/jth.13225\n10.1182/blood-2016-03-705681\n10.1117/1.JBO.18.1.017001\n10.3389/fmed.2017.00146\n10.1016/j.clinimag.2021.04.035\n10.1080/095371002220148332\n10.1080/09537104.2023.2184183"}
{"title": "Bell's palsy or an aggressive infiltrating basaloid carcinoma post-mRNA vaccination for COVID-19? A case report and review of the literature.", "abstract": "We report on an aggressive, infiltrating, metastatic, and ultimately lethal basaloid type of carcinoma arising shortly after an mRNA vaccination for COVID-19. The wife of the patient, since deceased, gave the consent for publishing the case. The malignancy was of cutaneous origin and the case showed symptoms consistent with Bell's palsy and trigeminal neuralgia beginning four days post-vaccination (right side head temporal pain). The temporal pain was suggestive for inflammation and impairment of T cell immune activation. Magnetic Resonance Imaging (MRI) showed a vascular loop on the left lateral aspect of the 5", "journal": "EXCLI journal", "date": "2023-11-06", "authors": ["Anthony MKyriakopoulos", "GregNigh", "Peter AMcCullough", "Maria DOlivier", "StephanieSeneff"], "doi": "10.17179/excli2023-6145\n10.1016/j.ijporl.2014.12.010\n10.1007/s10571-016-0366-z\n10.1002/lary.1978.88.5.787\n10.1007/978-981-15-3266-5_3\n10.1038/s41380-022-01831-0\n10.1016/j.ajoc.2022.101679\n10.3389/fphar.2017.00561\n10.1136/jcp.2006.040337\n10.1055/s-0036-1571834\n10.1186/s43055-022-00746-8\n10.3324/haematol.2011.054718\n10.1002/ccr3.2832\n10.3238/arztebl.2019.0054\n10.4049/jimmunol.2100637\n10.1111/j.0303-6987.2006.00436.x\n10.1007/s00262-004-0593-x\n10.5070/D38342r5bj\n10.1111/ene.14561\n10.1016/j.cdip.2007.05.005\n10.1111/j.1749-4486.2007.01597.x\n10.1158/2326-6066.CIR-20-0814\n10.1186/s40463-022-00591-9\n10.1007/s11481-021-10015-6\n10.1016/j.csbj.2022.10.022\n10.3892/br.2018.1151\n10.4103/2229-5178.105456\n10.3390/jcm9113529\n10.1016/j.annemergmed.2013.06.022\n10.1016/j.amsu.2018.10.025\n10.3389/fmed.2021.798095\n10.1016/j.autrev.2012.05.008\n10.1042/BSR20210611\n10.1056/NEJMoa2002032\n10.3389/fmolb.2022.790706\n10.3174/ajnr.A1212\n10.1001/jamapediatrics.2022.3581\n10.3238/arztebl.2013.0451\n10.3389/fphar.2021.643254\n10.1016/j.anndiagpath.2004.07.001\n10.1007/s40675-017-0062-7\n10.1126/science.aau6977\n10.1016/j.amsu.2022.104758\n10.3390/ijms23010307\n10.1038/s41568-021-00431-4\n10.1007/s00296-022-05091-7\n10.1134/S0006297921030032\n10.3389/fmed.2022.853941\n10.1186/s40425-017-0228-3\n10.1515/cclm-2022-0787\n10.3390/cancers14030476\n10.1001/jamaneurol.2020.1127\n10.1038/s41467-019-12275-6\n10.1001/jama.1972.03200120046013\n10.1155/2012/157187\n10.3389/fimmu.2021.656700\n10.1212/WNL.0000000000009455\n10.1007/s11606-020-06463-0\n10.1136/jcp.2006.041608\n10.1038/s41598-022-09410-7\n10.1038/s41598-020-74101-0\n10.4049/jimmunol.1401572\n10.1155/2017/8929745\n10.1016/j.amsu.2022.103897\n10.1007/s12105-021-01353-1\n10.1111/j.1365-2133.2005.06486.x\n10.1016/j.fct.2022.113008\n10.5021/ad.2011.23.2.213\n10.47176/mjiri.36.85\n10.1096/fj.01-0995fje\n10.1016/j.vaccine.2015.07.035\n10.7759/cureus.15064\n10.4103/0974-2077.184040\n10.3389/fimmu.2021.658428\n10.1155/2017/2692604MLA\n10.1016/0030-4220(87)90175-7\n10.1111/j.1600-0560.2010.01632.x\n10.1007/s10194-011-0324-6\n10.1097/PAI.0000000000000394\n10.1016/s0190-9622(84)80334-5\n10.1016/j.anndiagpath.2015.01.004\n10.1007/s12105-008-0089-7\n10.3389/fimmu.2019.02022\n10.1007/s10571-020-00915-1\n10.1007/s00018-022-04219-z\n10.1007/s00415-019-09282-4"}
{"title": "Causal association of COVID-19 with brain structure changes: Findings from a non-overlapping 2-sample Mendelian randomization study.", "abstract": "Recent cohort studies suggested that SARS-CoV-2 infection is associated with changes in brain structure. However, the potential causal relationship remains unclear. We performed a two-sample Mendelian randomization analysis to determine whether genetic susceptibility of COVID-19 is causally associated with changes in cortical and subcortical areas of the brain. This 2-sample MR (Mendelian Randomization) study is an instrumental variable analysis of data from the COVID-19 Host Genetics Initiative (HGI) meta-analyses round 5 excluding UK Biobank participants (COVID-19 infection, N\u00a0=\u00a01,348,701; COVID-19 severity, N\u00a0=\u00a01,557,411), the Enhancing NeuroImaging Genetics through Meta Analysis (ENIGMA) Global and regional cortical measures, N\u00a0=\u00a033,709; combined hemispheric subcortical volumes, N\u00a0=\u00a038,851), and UK Biobank (left/right subcortical volumes, N\u00a0=\u00a019,629). A replication analysis was performed on summary statistics from different COVID-19 GWAS study (COVID-19 infection, N\u00a0=\u00a080,932; COVID-19 severity, N\u00a0=\u00a072,733). We found that the genetic susceptibility of COVID-19 was not significantly associated with changes in brain structures, including cortical and subcortical brain structure. Similar results were observed for different (1) MR estimates, (2) COVID-19 GWAS summary statistics, and (3) definitions of COVID-19 infection and severity. This study suggests that the genetic susceptibility of COVID-19 is not causally associated with changes in cortical and subcortical brain structure.", "journal": "Journal of the neurological sciences", "date": "2023-11-06", "authors": ["PingjianDing", "RongXu"], "doi": "10.1016/j.jns.2023.120864\n10.1038/s41586-022-04569-5"}
{"title": "How intra-source imbalanced datasets impact the performance of deep learning for COVID-19 diagnosis using chest X-ray images.", "abstract": "Over the past decade, the use of deep learning has been widely increasing in the medical image diagnosis field. Deep learning-based methods' (DLMs) performance strongly relies on training data. Therefore, researchers often focus on collecting as much data as possible from different medical facilities or developing approaches to avoid the impact of inter-category imbalance (ICI), which means a difference in data quantity among categories. However, due to the ICI within each medical facility, medical data are often isolated and acquired in different settings among medical facilities, known as the issue of intra-source imbalance (ISI) characteristic. This imbalance also impacts the performance of DLMs but receives negligible attention. In this study, we study the impact of the ISI on DLMs by comparison of the version of a deep learning model that was trained separately by an intra-source imbalanced chest X-ray (CXR) dataset and an intra-source balanced CXR dataset for COVID-19 diagnosis. The finding is that using the intra-source imbalanced dataset causes a serious training bias, although the dataset has a good inter-category balance. In contrast, the deep learning model performed a reliable diagnosis when trained on the intra-source balanced dataset. Therefore, our study reports clear evidence that the intra-source balance is vital for training data to minimize the risk of poor performance of DLMs.", "journal": "Scientific reports", "date": "2023-11-04", "authors": ["ZhangZhang", "XiaoyongZhang", "KeiIchiji", "IvoBukovsk\u00fd", "NoriyasuHomma"], "doi": "10.1038/s41598-023-45368-w\n10.1016/S0140-6736(20)30185-9\n10.1001/jama.2020.3786\n10.1371/journal.pone.0253407\n10.1148/radiol.2020201160\n10.1016/j.cmpb.2020.105608\n10.1007/s11042-021-11319-8\n10.1038/s41598-021-87523-1\n10.1038/s41598-020-76550-z\n10.1016/j.artmed.2021.102158\n10.1038/s41598-021-95561-y\n10.1007/s11263-019-01228-7\n10.1186/s40537-019-0192-5\n10.1109/TNNLS.2021.3070467\n10.21608/ijci.2021.207825\n10.21227/w3aw-rv39\n10.21227/m4j2-ap59\n10.1016/j.media.2020.101797\n10.1016/j.cell.2018.02.010\n10.1093/jamia/ocv080\n10.3978/j.issn.2223-4292.2014.11.20\n10.1007/978-3-319-16199-0_2\n10.1016/j.patrec.2005.10.010\n10.1002/bimj.200410135"}
{"title": "Exploring transfer learning in chest radiographic images within the interplay between COVID-19 and diabetes.", "abstract": "The intricate relationship between COVID-19 and diabetes has garnered increasing attention within the medical community. Emerging evidence suggests that individuals with diabetes may experience heightened vulnerability to COVID-19 and, in some cases, develop diabetes as a post-complication following the viral infection. Additionally, it has been observed that patients taking cough medicine containing steroids may face an elevated risk of developing diabetes, further underscoring the complex interplay between these health factors. Based on previous research, we implemented deep-learning models to diagnose the infection ", "journal": "Frontiers in public health", "date": "2023-11-03", "authors": ["MuhammadShoaib", "NasirSayed", "BabarShah", "TariqHussain", "Ahmad AliAlZubi", "Sufian AhmadAlZubi", "FarmanAli"], "doi": "10.3389/fpubh.2023.1297909\n10.3389/fcdhc.2022.826006\n10.2337/dc21-1318\n10.1007/s12070-021-02883-4\n10.3389/fendo.2022.974540\n10.1001/jama.2020.3786\n10.3389/fpubh.2023.1191377\n10.1101/2020.02.11.20021493\n10.1097/SLA.0000000000003925\n10.1148/radiol.2020200527\n10.1007/s11042-022-12500-3\n10.1148/ryct.2020200028\n10.1148/radiol.2462070712\n10.1016/j.crad.2020.03.003\n10.1007/s40267-020-00721-1\n10.1109/TPAMI.2016.2644615\n10.59275/j.melba.2020-48g7\n10.48550/arXiv.2003.11597\n10.48550/arXiv.1901.07031\n10.1109/TPAMI.2016.2577031\n10.1007/s10462-022-10237-x\n10.3390/electronics11223831\n10.1016/j.comnet.2020.107138\n10.3390/s22103959\n10.21037/atm.2020.03.132\n10.1155/2022/9179998\n10.3390/RS12183020\n10.1016/j.aej.2023.05.036\n10.3389/fmed.2023.1157000\n10.3390/healthcare11091204\n10.1371/journal.pone.0285121\n10.1007/978-3-319-10590-1_53"}
{"title": "UNet segmentation network of COVID-19 CT images with multi-scale attention.", "abstract": "In recent years, the global outbreak of COVID-19 has posed an extremely serious life-safety risk to humans, and in order to maximize the diagnostic efficiency of physicians, it is extremely valuable to investigate the methods of lesion segmentation in images of COVID-19. Aiming at the problems of existing deep learning models, such as low segmentation accuracy, poor model generalization performance, large model parameters and difficult deployment, we propose an UNet segmentation network integrating multi-scale attention for COVID-19 CT images. Specifically, the UNet network model is utilized as the base network, and the structure of multi-scale convolutional attention is proposed in the encoder stage to enhance the network's ability to capture multi-scale information. Second, a local channel attention module is proposed to extract spatial information by modeling local relationships to generate channel domain weights, to supplement detailed information about the target region to reduce information redundancy and to enhance important information. Moreover, the network model encoder segment uses the Meta-ACON activation function to avoid the overfitting phenomenon of the model and to improve the model's representational ability. A large number of experimental results on publicly available mixed data sets show that compared with the current mainstream image segmentation algorithms, the pro-posed method can more effectively improve the accuracy and generalization performance of COVID-19 lesions segmentation and provide help for medical diagnosis and analysis.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2023-11-03", "authors": ["MingjuChen", "SihangYi", "MeiYang", "ZhiwenYang", "XingyueZhang"], "doi": "10.3934/mbe.2023747"}
{"title": "Developing a Deep Neural Network model for COVID-19 diagnosis based on CT scan images.", "abstract": "COVID-19 is most commonly diagnosed using a testing kit but chest X-rays and computed tomography (CT) scan images have a potential role in COVID-19 diagnosis. Currently, CT diagnosis systems based on Artificial intelligence (AI) models have been used in some countries. Previous research studies used complex neural networks, which led to difficulty in network training and high computation rates. Hence, in this study, we developed the 6-layer Deep Neural Network (DNN) model for COVID-19 diagnosis based on CT scan images. The proposed DNN model is generated to improve accurate diagnostics for classifying sick and healthy persons. Also, other classification models, such as decision trees, random forests and standard neural networks, have been investigated. One of the main contributions of this study is the use of the global feature extractor operator for feature extraction from the images. Furthermore, the 10-fold cross-validation technique is utilized for partitioning the data into training, testing and validation. During the DNN training, the model is generated without dropping out of neurons in the layers. The experimental results of the lightweight DNN model demonstrated that this model has the best accuracy of 96.71% compared to the previous classification models for COVID-19 diagnosis.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2023-11-03", "authors": ["Javad HassannatajJoloudari", "FaezehAzizi", "IssaNodehi", "Mohammad AliNematollahi", "FatemeKamrannejhad", "EdrisHassannatajjeloudari", "RoohallahAlizadehsani", "Sheikh Mohammed SharifulIslam"], "doi": "10.3934/mbe.2023725"}
{"title": "MultiCOVID: a multi modal deep learning approach for COVID-19 diagnosis.", "abstract": "The rapid spread of the severe acute respiratory syndrome coronavirus 2 led to a global overextension of healthcare. Both Chest X-rays (CXR) and blood test have been demonstrated to have predictive value on Coronavirus Disease 2019 (COVID-19) diagnosis on different prevalence scenarios. With the objective of improving and accelerating the diagnosis of COVID-19, a multi modal prediction algorithm (MultiCOVID) based on CXR and blood test was developed, to discriminate between COVID-19, Heart Failure and Non-COVID Pneumonia and healthy (Control) patients. This retrospective single-center study includes CXR and blood test obtained between January 2017 and May 2020. Multi modal prediction models were generated using opensource DL algorithms. Performance of the MultiCOVID algorithm was compared with interpretations from five experienced thoracic radiologists on 300 random test images using the McNemar-Bowker test. A total of 8578 samples from 6123 patients (mean age 66\u00a0\u00b1\u00a018\u00a0years of standard deviation, 3523 men) were evaluated across datasets. For the entire test set, the overall accuracy of MultiCOVID was 84%, with a mean AUC of 0.92 (0.89-0.94). For 300 random test images, overall accuracy of MultiCOVID was significantly higher (69.6%) compared with individual radiologists (range, 43.7-58.7%) and the consensus of all five radiologists (59.3%, P\u2009<\u2009.001). Overall, we have developed a multimodal deep learning algorithm, MultiCOVID, that discriminates among COVID-19, heart failure, non-COVID pneumonia and healthy patients using both CXR and blood test with a significantly better performance than experienced thoracic radiologists.", "journal": "Scientific reports", "date": "2023-11-01", "authors": ["MaxHardy-Werbin", "Jos\u00e9 MariaMaiques", "MarcosBusto", "IsabelCirera", "AlfonsAguirre", "NievesGarcia-Gisbert", "FlavioZuccarino", "SantiagoCarbullanca", "Luis AlexanderDel Carpio", "DidacRamal", "\u00c1ngelGayete", "JordiMart\u00ednez-Roldan", "AlbertMarquez-Colome", "BeatrizBellosillo", "JoanGibert"], "doi": "10.1038/s41598-023-46126-8\n10.1038/s42256-021-00338-7\n10.1136/bmj.m2426\n10.7717/peerj.9482\n10.1038/s41746-020-00343-x\n10.3389/fpubh.2017.00307\n10.3390/info11020108\n10.1016/j.media.2021.102225\n10.1038/s42256-019-0138-9\n10.1002/jmv.26506\n10.4269/ajtmh.20-1536\n10.1016/j.htct.2020.03.001\n10.1038/s41598-021-90265-9\n10.1093/cid/ciaa1175\n10.1016/S2589-7500(20)30274-0\n10.1002/jmv.26384\n10.1148/radiol.2019191225\n10.1038/s41598-020-76550-z\n10.1148/radiol.2020203511\n10.1148/radiol.2020202944\n10.3390/jcm11082157\n10.1038/s41598-022-11990-3\n10.1038/s41598-023-34559-0\n10.1038/s41598-020-74539-2\n10.7554/eLife.70640"}
{"title": "Deep learning prediction of steep and flat corneal curvature using fundus photography in post-COVID telemedicine era.", "abstract": "Recently, fundus photography (FP) is being increasingly used. Corneal curvature is an essential factor in refractive errors and is associated with several pathological corneal conditions. As FP-based examination systems have already been widely distributed, it would be helpful for telemedicine to extract information such as corneal curvature using FP. This study aims to develop a deep learning model based on FP for corneal curvature prediction by categorizing corneas into steep, regular, and flat groups. The EfficientNetB0 architecture with transfer learning was used to learn FP patterns to predict flat, regular, and steep corneas. In validation, the model achieved a multiclass accuracy of 0.727, a Matthews correlation coefficient of 0.519, and an unweighted Cohen's \u03ba of 0.590. The areas under the receiver operating characteristic curves for binary prediction of flat and steep corneas were 0.863 and 0.848, respectively. The optic nerve and its peripheral areas were the main focus of the model. The developed algorithm shows that FP can potentially be used as an imaging modality to estimate corneal curvature in the post-COVID-19 era, whereby patients may benefit from the detection of abnormal corneal curvatures using FP in the telemedicine setting.", "journal": "Medical & biological engineering & computing", "date": "2023-10-27", "authors": ["Joon YulChoi", "HyungsuKim", "Jin KukKim", "In SikLee", "Ik HeeRyu", "Jung SooKim", "Tae KeunYoo"], "doi": "10.1007/s11517-023-02952-6\n10.1167/iovs.18-23887\n10.1371/journal.pone.0039313\n10.1001/archopht.1985.01050050068020\n10.1111/aos.14636\n10.1155/2015/794854\n10.1016/j.ajo.2022.11.011\n10.1016/j.cmpb.2022.106735\n10.1136/bjo-2022-321295\n10.1007/s00417-020-04879-2\n10.1080/08820538.2020.1789675\n10.1016/j.eswa.2022.119430\n10.1002/cpe.7405\n10.1038/s41551-022-00867-5\n10.1167/tvst.12.6.11\n10.1038/s41598-021-89743-x\n10.1007/s13167-022-00292-3\n10.1016/j.optom.2019.11.001\n10.1038/s41598-021-97375-4\n10.1097/ICL.0b013e3181eb8418\n10.3928/1542-8877-19910701-04\n10.1016/j.jcrs.2017.06.036\n10.1016/j.cmpb.2021.106086\n10.1016/j.asoc.2020.106859\n10.3906/tar-2007-105\n10.1038/s41598-022-22984-6\n10.1002/cpe.6908\n10.1371/journal.pone.0187336\n10.1186/s12859-022-04971-w\n10.1038/s41746-017-0013-1\n10.1007/s11517-021-02321-1\n10.1097/01.icl.0000086494.50288.70\n10.1016/S2589-7500(21)00043-1\n10.1038/s41598-020-80839-4\n10.1159/000329597\n10.3389/fcell.2023.1124005\n10.1007/s00417-022-05738-y\n10.1038/s43587-022-00171-6\n10.3389/fcell.2021.653692\n10.1038/s41433-021-01795-5\n10.1111/opo.12909\n10.1002/ima.22717\n10.21037/atm-21-1772\n10.1016/S0161-6420(97)30004-9\n10.1016/S1726-4901(09)70038-3\n10.1007/s10792-020-01464-8\n10.1016/j.jtos.2021.12.004\n10.3928/1081-597X-20040901-32\n10.1167/iovs.62.3.15\n10.1016/j.cmpb.2023.107522\n10.1148/ryai.210064\n10.1167/tvst.9.2.6\n10.1016/S2589-7500(20)30288-0\n10.1167/iovs.10-5226\n10.1016/j.jclinepi.2015.04.005"}
{"title": "Severe Acute Respiratory Syndrome Coronavirus 2 Pneumonia in Critically Ill Patients: A Cluster Analysis According to Baseline Characteristics, Biological Features, and Chest CT Scan on Admission.", "abstract": "Inconsistent results from COVID-19 studies raise the issue of patient heterogeneity.\nThe objective of this study was to identify homogeneous subgroups of patients (clusters) using baseline characteristics including inflammatory biomarkers and the extent of lung parenchymal lesions on CT, and to compare their outcomes.\nRetrospective single-center study.\nMedical ICU of the University Hospital of Clermont-Ferrand, France.\nAll consecutive adult patients aged greater than or equal to 18 years, admitted between March 20, 2020, and August 31, 2021, for COVID-19 pneumonia.\nCharacteristics at baseline, during ICU stay, and outcomes at day 60 were recorded. On the chest CT performed at admission the extent of lung parenchyma lesions was established by artificial intelligence software.\nClusters were determined by hierarchical clustering on principal components using principal component analysis of admission characteristics including plasma interleukin-6, human histocompatibility leukocyte antigen-DR expression rate on blood monocytes (HLA-DR) monocytic-expression rate (mHLA-DR), and the extent of lung parenchymal lesions. Factors associated with day 60 mortality were investigated by univariate survival analysis. Two hundred seventy patients were included. Four clusters were identified and three were fully described. Cluster 1 (obese patients, with moderate hypoxemia, moderate extent of lung parenchymal lesions, no inflammation, and no down-regulation of mHLA-DR) had a better prognosis at day 60 (hazard ratio [HR] = 0.27 [0.15-0.46], p < 0.01), whereas cluster 2 (older patients with comorbidities, moderate extent of lung parenchyma lesions but significant hypoxemia, inflammation, and down-regulation of mHLA-DR) and cluster 3 (patients with severe parenchymal disease, hypoxemia, inflammatory reaction, and down-regulation of mHLA-DR) had an increased risk of mortality (HR = 2.07 [1.37-3.13], p < 0.01 and HR = 1.52 [1-2.32], p = 0.05, respectively). In multivariate analysis, only clusters 1 and 2 were independently associated with day 60 death.\nThree clusters with distinct characteristics and outcomes were identified. Such clusters could facilitate the identification of targeted populations for the next trials.", "journal": "Critical care medicine", "date": "2023-10-27", "authors": ["K\u00e9vinGrapin", "RomainDe Bauchene", "BenjaminBonnet", "AudreyMirand", "LucieCassagnes", "LaureCalvet", "Fran\u00e7oisThouy", "RadhiaBouzgarrou", "C\u00e9cileHenquell", "BertrandEvrard", "MireilleAdda", "BertrandSouweine", "ClaireDupuis"], "doi": "10.1097/CCM.0000000000006105"}
{"title": "Evaluating Retinal Disease Diagnosis with an Interpretable Lightweight CNN Model Resistant to Adversarial Attacks.", "abstract": "Optical Coherence Tomography (OCT) is an imperative symptomatic tool empowering the diagnosis of retinal diseases and anomalies. The manual decision towards those anomalies by specialists is the norm, but its labor-intensive nature calls for more proficient strategies. Consequently, the study recommends employing a Convolutional Neural Network (CNN) for the classification of OCT images derived from the OCT dataset into distinct categories, including Choroidal NeoVascularization (CNV), Diabetic Macular Edema (DME), Drusen, and Normal. The average k-fold (k = 10) training accuracy, test accuracy, validation accuracy, training loss, test loss, and validation loss values of the proposed model are 96.33%, 94.29%, 94.12%, 0.1073, 0.2002, and 0.1927, respectively. Fast Gradient Sign Method (FGSM) is employed to introduce non-random noise aligned with the cost function's data gradient, with varying epsilon values scaling the noise, and the model correctly handles all noise levels below 0.1 epsilon. Explainable AI algorithms: Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) are utilized to provide human interpretable explanations approximating the behaviour of the model within the region of a particular retinal image. Additionally, two supplementary datasets, namely, COVID-19 and Kidney Stone, are assimilated to enhance the model's robustness and versatility, resulting in a level of precision comparable to state-of-the-art methodologies. Incorporating a lightweight CNN model with 983,716 parameters, 2.37\u00d7108 floating point operations per second (FLOPs) and leveraging explainable AI strategies, this study contributes to efficient OCT-based diagnosis, underscores its potential in advancing medical diagnostics, and offers assistance in the Internet-of-Medical-Things.", "journal": "Journal of imaging", "date": "2023-10-27", "authors": ["MohanBhandari", "Tej BahadurShahi", "ArjunNeupane"], "doi": "10.3390/jimaging9100219\n10.1016/j.ophtha.2023.07.027\n10.47709/ijmdsa.v2i1.2394\n10.1016/j.health.2023.100216\n10.14569/IJACSA.2023.0140532\n10.1007/s00500-023-08928-w\n10.1155/2023/8276738\n10.1016/j.breast.2019.10.001\n10.3390/su11164371\n10.1109/OJCOMS.2022.3188750\n10.1109/ACCESS.2018.2870052\n10.1016/j.compbiomed.2022.106156\n10.1007/s10916-022-01868-2\n10.1038/s41598-022-15634-4\n10.1155/2022/8014979\n10.1007/s13755-022-00182-y\n10.1016/j.eswa.2023.120617\n10.1016/j.cell.2018.02.010\n10.1364/BOE.5.003568\n10.1007/s00417-018-04224-8\n10.1364/BOE.10.006204\n10.1016/j.jestch.2021.101091\n10.3390/s23156706\n10.17632/rscbjbr9sj.2\n10.1109/TKDE.2019.2912815\n10.3390/s22051780\n10.3389/frai.2021.752558\n10.1016/j.compeleceng.2022.108484\n10.1016/j.cmpb.2020.105581\n10.1007/s10489-020-02055-x\n10.24017/Science.2022.2.11\n10.3389/fpubh.2023.1109236"}
{"title": "A Hybrid Decision Tree and Deep Learning Approach Combining Medical Imaging and Electronic Medical Records to Predict Intubation Among Hospitalized Patients With COVID-19: Algorithm Development and Validation.", "abstract": "Early prediction of the need for invasive mechanical ventilation (IMV) in patients hospitalized with COVID-19 symptoms can help in the allocation of resources appropriately and improve patient outcomes by appropriately monitoring and treating patients at the greatest risk of respiratory failure. To help with the complexity of deciding whether a patient needs IMV, machine learning algorithms may help bring more prognostic value in a timely and systematic manner. Chest radiographs (CXRs) and electronic medical records (EMRs), typically obtained early in patients admitted with COVID-19, are the keys to deciding whether they need IMV.\nWe aimed to evaluate the use of a machine learning model to predict the need for intubation within 24 hours by using a combination of CXR and EMR data in an end-to-end automated pipeline. We included historical data from 2481 hospitalizations at The Mount Sinai Hospital in New York City.\nCXRs were first resized, rescaled, and normalized. Then lungs were segmented from the CXRs by using a U-Net algorithm. After splitting them into a training and a test set, the training set images were augmented. The augmented images were used to train an image classifier to predict the probability of intubation with a prediction window of 24 hours by retraining a pretrained DenseNet model by using transfer learning, 10-fold cross-validation, and grid search. Then, in the final fusion model, we trained a random forest algorithm via 10-fold cross-validation by combining the probability score from the image classifier with 41 longitudinal variables in the EMR. Variables in the EMR included clinical and laboratory data routinely collected in the inpatient setting. The final fusion model gave a prediction likelihood for the need of intubation within 24 hours as well.\nAt a prediction probability threshold of 0.5, the fusion model provided 78.9% (95% CI 59%-96%) sensitivity, 83% (95% CI 76%-89%) specificity, 0.509 (95% CI 0.34-0.67) F\nWe show that, when linked with EMR data, an automated deep learning image classifier improved performance in identifying hospitalized patients with severe COVID-19 at risk for intubation. With additional prospective and external validation, such a model may assist risk assessment and optimize clinical decision-making in choosing the best care plan during the critical stages of COVID-19.", "journal": "JMIR formative research", "date": "2023-10-26", "authors": ["Kim-Anh-NhiNguyen", "PranaiTandon", "SaharGhanavati", "Satya NarayanaCheetirala", "PremTimsina", "RobertFreeman", "DavidReich", "Matthew ALevin", "MadhuMazumdar", "Zahi AFayad", "ArashKia"], "doi": "10.2196/46905\n10.1038/s41586-020-2271-3\n10.1016/s0140-6736(20)30183-5\n10.1056/NEJMoa2002032\n10.1016/j.ijid.2020.03.017\n10.4081/monaldi.2020.1285\n10.4081/monaldi.2020.1285\n10.1016/j.resp.2022.103842\n10.1007/s00134-020-06022-5\n10.1001/jama.2020.6775\n10.1001/jama.2020.6627\n10.1056/NEJMc2010419\n10.1186/s13613-020-00692-6\n10.1164/rccm.201803-0589oc\n10.1016/j.jcrc.2020.10.033\n10.1148/ryai.2020200098\n10.1016/j.acra.2021.01.016\n10.1016/j.jacr.2022.03.013\n10.1038/s41598-022-24721-5\n10.1038/s41598-022-24721-5\n10.1136/bmjinnov-2020-000593\n10.2147/jmdh.s322431\n10.1016/j.acra.2021.05.002\n10.1259/bjr.20210221\n10.1038/s41591-021-01506-3\n10.1002/acr.24883\n10.1259/bjro.20210062\n10.7326/l15-5093-2\n10.1007/978-3-319-24574-4_28\n10.48550/arXiv.2005.10052\n10.1109/cvpr.2017.243\n10.1109/iccv.2017.74\n10.1038/s41746-020-00341-z\n10.1038/s41746-020-00341-z\n10.1023/A:1010933404324\n10.1023/A:1010933404324\n10.1002/sam.11348\n10.1111/j.1600-0587.2012.07348.x\n10.1007/978-0-387-30162-4_415\n10.3390/s22135007\n10.1016/S2589-7500(21)00039-X\n10.1186/s12938-022-01045-z\n10.1186/s12938-022-01045-z\n10.1097/CCE.0000000000000381"}
{"title": "Self-supervised learning with self-distillation on COVID-19 medical image classification.", "abstract": "Currently, COVID-19 is a highly infectious disease that can be clinically diagnosed based on diagnostic radiology. Deep learning is capable of mining the rich information implied in inpatient imaging data and accomplishing the classification of different stages of the disease process. However, a large amount of training data is essential to train an excellent deep-learning model. Unfortunately, due to factors such as privacy and labeling difficulties, annotated data for COVID-19 is extremely scarce, which encourages us to propose a more effective deep learning model that can effectively assist specialist physicians in COVID-19 diagnosis.\nIn this study,we introduce Masked Autoencoder (MAE) for pre-training and fine-tuning directly on small-scale target datasets. Based on this, we propose Self-Supervised Learning with Self-Distillation on COVID-19 medical image classification (SSSD-COVID). In addition to the reconstruction loss computation on the masked image patches, SSSD-COVID further performs self-distillation loss calculations on the latent representation of the encoder and decoder outputs. The additional loss calculation can transfer the knowledge from the global attention of the decoder to the encoder which acquires only local attention.\nOur model achieves 97.78\u00a0% recognition accuracy on the SARS-COV-CT dataset containing 2481 images and is further validated on the COVID-CT dataset containing 746 images, which achieves 81.76\u00a0% recognition accuracy. Further introduction of external knowledge resulted in experimental accuracies of 99.6% and 95.27\u00a0% on these two datasets, respectively.\nSSSD-COVID can obtain good results on the target dataset alone, and when external information is introduced, the performance of the model can be further improved to significantly outperform other models.Overall, the experimental results show that our method can effectively mine COVID-19 features from rare data and can assist professional physicians in decision-making to improve the efficiency of COVID-19 disease detection.", "journal": "Computer methods and programs in biomedicine", "date": "2023-10-25", "authors": ["ZhiyongTan", "YuhaiYu", "JianaMeng", "ShuangLiu", "WeiLi"], "doi": "10.1016/j.cmpb.2023.107876"}
{"title": "Persistent post-COVID headache is associated with suppression of scale-free functional brain dynamics in non-hospitalized individuals.", "abstract": "Post-acute coronavirus disease 2019 (COVID-19) syndrome (PACS) is a growing concern, with headache being a particularly debilitating symptom with high prevalence. The long-term effects of COVID-19 and post-COVID headache on brain function remain poorly understood, particularly among non-hospitalized individuals. This study focused on the power-law scaling behavior of functional brain dynamics, indexed by the Hurst exponent (H). This measure is suppressed during physiological and psychological distress and was thus hypothesized to be reduced in individuals with post-COVID syndrome, with greatest reductions among those with persistent headache.\nResting-state blood oxygenation level-dependent (BOLD) functional magnetic resonance imaging data were collected for 57 individuals who had COVID-19 (32 with no headache, 14 with ongoing headache, 11 recovered) and 17 controls who had cold and flu-like symptoms but \u00a0tested negative for COVID-19. Individuals were assessed an average of 4-5 months after COVID testing, in a cross-sectional, observational study design.\nNo significant differences in H values were found between non-headache COVID-19 and control groups., while those with ongoing headache had significantly reduced H values, and those who had recovered from headache had elevated H values, relative to non-headache groups. Effects were greatest in temporal, sensorimotor, and insular brain regions. Reduced H in these regions was also associated with decreased BOLD activity and local functional connectivity.\nThese findings provide new insights into the neurophysiological mechanisms that underlie persistent post-COVID headache, with reduced BOLD scaling as a potential biomarker that is specific to this debilitating condition.", "journal": "Brain and behavior", "date": "2023-10-24", "authors": ["Nathan WChurchill", "EugenieRoudaia", "JJean Chen", "AsafGilboa", "AllisonSekuler", "XiangJi", "FuqiangGao", "ZhongminLin", "MarioMasellis", "MagedGoubran", "Jennifer SRabin", "BenjaminLam", "IvyCheng", "RobertFowler", "ChrisHeyn", "Sandra EBlack", "Bradley JMacIntosh", "Simon JGraham", "Tom ASchweizer"], "doi": "10.1002/brb3.3212\n10.1109/18.650984\n10.1021/acschemneuro.0c00122\n10.1371/journal.pone.0006626\n10.1016/j.neuroimage.2013.05.099\n10.1016/j.jad.2022.06.061\n10.1186/s10194-017-0735-0\n10.1002/hbm.22687\n10.1002/hbm.24962\n10.3389/fneur.2023.1136408\n10.1038/srep30895\n10.3389/fphys.2012.00186\n10.1007/s004249900135\n10.1002/hbm.25741\n10.1111/ene.15040\n10.1111/j.1526-4637.2011.01183.x\n10.1016/j.neuron.2007.08.023\n10.1080/01616412.2021.2024732\n10.1073/pnas.012579499\n10.1007/s00259-021-05215-4\n10.1523/JNEUROSCI.2111-11.2011\n10.1016/j.tics.2014.04.003\n10.1007/s00221-010-2340-1\n10.1111/head.14472\n10.1186/s10194-018-0894-7\n10.1016/j.nurpra.2021.05.003\n10.1111/head.12756\n10.1007/s42399-021-00964-7\n10.1007/s10194-006-0307-1\n10.1016/j.pain.2013.06.006\n10.1038/s41591-021-01283-z\n10.1016/j.cmi.2021.05.033\n10.1103/PhysRevE.49.1685\n10.1016/S0022-0736(95)80017-4\n10.1016/S0140-6736(21)00847-3\n10.1007/s00259-021-05294-3\n10.5603/PJNNS.a2022.0049\n10.1016/j.neuron.2014.03.020\n10.1186/s10194-022-01450-8\n10.1016/j.neuroimage.2009.12.021\n10.1212/WNL.0000000000008962\n10.1073/pnas.1007841107\n10.1186/s10194-018-0933-4\n10.1016/j.jiph.2021.04.013\n10.1007/s12264-013-1468-6\n10.1109/TSP.2007.896269\n10.1109/MSP.2007.4286563\n10.1002/hbm.20593\n10.1016/j.neuroimage.2007.01.054\n10.1016/j.neuroimage.2003.12.030\n10.1016/j.jneumeth.2008.04.012"}
{"title": "Challenges of AI driven diagnosis of chest X-rays transmitted through smart phones: a\u00a0case study in COVID-19.", "abstract": "Healthcare delivery during the initial days of outbreak of COVID-19 pandemic was badly impacted due to large number of severely infected patients posing an unprecedented global challenge. Although the importance of Chest X-rays (CXRs) in meeting this challenge has now been widely recognized, speedy diagnosis of CXRs remains an outstanding challenge because of fewer Radiologists. The exponential increase in Smart Phone ownership globally, including LMICs, provides an opportunity for exploring AI-driven diagnostic tools when provided with large volumes of CXRs transmitted through Smart Phones. However, the challenges associated with such systems have not been studied to the best of our knowledge. In this paper, we show that the predictions of AI-driven models on CXR images transmitted through Smart Phones via applications, such as WhatsApp, suffer both in terms of Predictability and Explainability, two key aspects of any automated Medical Diagnosis system. We find that several existing Deep learning based models exhibit prediction instability-disagreement between the prediction outcome of the original image and the transmitted image. Concomitantly we find that the explainability of the models deteriorate substantially, prediction on the transmitted CXR is often driven by features present outside the lung region, clearly a manifestation of Spurious Correlations. Our study reveals that there is significant compression of high-resolution CXR images, sometimes as high as 95%, and this could be the reason behind these two problems. Apart from demonstrating these problems, our main contribution is to show that Multi-Task learning (MTL) can serve as an effective bulwark against the aforementioned problems. We show that MTL models exhibit substantially more robustness, 40% over existing baselines. Explainability of such models, when measured by a saliency score dependent on out-of-lung features, also show a 35% improvement. The study is conducted on WaCXR dataset, a curated dataset of 6562 image pairs corresponding to original uncompressed and WhatsApp compressed CXR images. Keeping in mind that there are no previous datasets to study such problems, we open-source this data along with all implementations.", "journal": "Scientific reports", "date": "2023-10-24", "authors": ["MariammaAntony", "Siva TejaKakileti", "RachitShah", "SabyasachiSahoo", "ChiranjibBhattacharyya", "GeethaManjunath"], "doi": "10.1038/s41598-023-44653-y\n10.1148/radiol.2020201365\n10.3978/j.issn.2223-4292.2014.11.04\n10.1016/j.afjem.2021.11.001\n10.1038/s41598-020-76550-z\n10.1038/s42256-020-00257-z\n10.1038/s42256-021-00338-7\n10.1038/s41598-020-70479-z\n10.1007/s10044-021-00984-y\n10.1038/s41598-021-93832-2\n10.1038/s41598-020-74539-2\n10.1007/s13246-020-00865-4\n10.1016/j.patrec.2020.09.010\n10.1038/s41598-021-95537-y\n10.1038/s41598-020-78060-4\n10.1007/s11760-021-02098-8\n10.1109/ACCESS.2020.3010287\n10.1038/s41598-021-95561-y\n10.1007/s12553-021-00520-2\n10.1007/s00521-020-05636-6\n10.3390/bdcc5040073\n10.1038/s41598-021-02003-w\n10.1038/s41598-022-10568-3\n10.1038/s42256-021-00307-0\n10.1016/j.patcog.2021.108243\n10.1016/j.eswa.2022.119475\n10.1016/j.media.2021.102299\n10.1148/ryai.2019180041\n10.1109/ACCESS.2020.3003810\n10.1038/s41746-020-0273-z\n10.1007/s00521-019-04051-w"}
{"title": "Accelerating artificial intelligence: How federated learning can protect privacy, facilitate collaboration, and improve outcomes.", "abstract": "Cross-institution collaborations are constrained by data-sharing challenges. These challenges hamper innovation, particularly in artificial intelligence, where models require diverse data to ensure strong performance. Federated learning (FL) solves data-sharing challenges. In typical collaborations, data is sent to a central repository where models are trained. With FL, models are sent to participating sites, trained locally, and model weights aggregated to create a master model with improved performance. At the 2021 Radiology Society of North America's (RSNA) conference, a panel was conducted titled \"Accelerating AI: How Federated Learning Can Protect Privacy, Facilitate Collaboration and Improve Outcomes.\" Two groups shared insights: researchers from the EXAM study (EMC CXR AI Model) and members of the National Cancer Institute's Early Detection Research Network's (EDRN) pancreatic cancer working group. EXAM brought together 20 institutions to create a model to predict oxygen requirements of patients seen in the emergency department with COVID-19 symptoms. The EDRN collaboration is focused on improving outcomes for pancreatic cancer patients through earlier detection. This paper describes major insights from the panel, including direct quotes. The panelists described the impetus for FL, the long-term potential vision of FL, challenges faced in FL, and the immediate path forward for FL.", "journal": "Health informatics journal", "date": "2023-10-21", "authors": ["MalharPatel", "IttaiDayan", "Elliot KFishman", "MonaFlores", "Fiona JGilbert", "MichalGuindy", "Eugene JKoay", "MichaelRosenthal", "Holger RRoth", "Marius GLinguraru"], "doi": "10.1177/14604582231207744"}
{"title": "Machine learning-based computer-aided simple triage (CAST) for COVID-19 pneumonia as compared with triage by board-certified chest radiologists.", "abstract": "Several reporting systems have been proposed for providing standardized language and diagnostic categories aiming for expressing the likelihood that lung abnormalities on CT images represent COVID-19. We developed a machine learning (ML)-based CT texture analysis software for simple triage based on the RSNA Expert Consensus Statement system. The purpose of this study was to conduct a multi-center and multi-reader study to determine the capability of ML-based computer-aided simple triage (CAST)\u00a0software based on RSNA expert consensus statements for diagnosis of COVID-19 pneumonia.\nFor this multi-center study, 174 cases who had undergone CT and polymerase chain reaction (PCR) tests for COVID-19 were retrospectively included. Their CT data were then assessed by CAST and consensus from three board-certified chest radiologists, after which all cases were classified as either positive or negative. Diagnostic performance was then compared by McNemar's test. To determine radiological finding evaluation capability of CAST, three other board-certified chest radiologists assessed CAST results for radiological findings into five criteria. Finally, accuracies of all radiological evaluations were compared by McNemar's test.\nA comparison of diagnosis for COVID-19 pneumonia based on RT-PCR results for cases with COVID-19 pneumonia findings on CT showed no significant difference of diagnostic performance between ML-based CAST software and consensus evaluation (p\u2009>\u20090.05). Comparison of agreement on accuracy for all radiological finding evaluations showed that emphysema evaluation accuracy for investigator A (AC\u2009=\u200991.7%) was significantly lower than that for investigators B (100%, p\u2009=\u20090.0009) and C (100%, p\u2009=\u20090.0009).\nThis multi-center study shows COVID-19 pneumonia triage by CAST can be considered at least as valid as that by chest expert radiologists and may be capable for playing as useful a complementary role for management of suspected COVID-19 pneumonia patients as well as the RT-PCR test in routine clinical practice.", "journal": "Japanese journal of radiology", "date": "2023-10-20", "authors": ["YoshiharuOhno", "TakatoshiAoki", "MasahiroEndo", "HisanobuKoyama", "HiroshiMoriya", "FumitoOkada", "TakanoriHigashino", "HarukaSato", "NorikoOyama-Manabe", "TakafumiHaraguchi", "KazumasaArakita", "KotaAoyagi", "YoshihiroIkeda", "ShigeoKaminaga", "AkiraTaniguchi", "NaokiSugihara"], "doi": "10.1007/s11604-023-01495-y\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200642\n10.1148/radiol.2020200343\n10.1148/radiol.2020200432\n10.1148/ryct.2020200152\n10.1097/RTI.0000000000000524\n10.1148/radiol.2020201473\n10.1007/s00330-020-06863-0\n10.5152/dir.2020.20351\n10.1016/j.crad.2020.04.002\n10.1097/RTI.0000000000000541\n10.1177/0846537120968919\n10.1007/s00330-020-07273-y\n10.1016/j.crad.2020.06.005\n10.3389/fmed.2020.573468\n10.1016/j.ejro.2022.100438\n10.1148/radiol.2462070712\n10.1016/j.ejrad.2020.109410\n10.1007/s11604-022-01270-5\n10.1177/02841851211044973\n10.1148/radiol.2020200370\n10.1111/j.1699-0463.1989.tb00464.x\n10.2307/2529310\n10.1259/bjr.20130245\n10.1007/s00330-021-07957-z\n10.1007/s00330-021-07937-3\n10.1097/RTI.0000000000000618\n10.1007/s00330-021-08409-4"}
{"title": "LCSB-inception: Reliable and effective light-chroma separated branches for Covid-19 detection from chest X-ray images.", "abstract": "According to the World Health Organization, an estimate of more than five million infections and 355,000 deaths have been recorded worldwide since the emergence of the coronavirus disease (COVID-19). Various researchers have developed interesting and effective deep learning frameworks to tackle this disease. However, poor feature extraction from the Chest X-ray images and the high computational cost of the available models impose difficulties to an accurate and fast Covid-19 detection framework. Thus, the major purpose of this study is to offer an accurate and efficient approach for extracting COVID-19 features from chest X-rays that is also less computationally expensive than earlier research. To achieve the specified goal, we explored the Inception V3 deep artificial neural network. This study proposed LCSB-Inception; a two-path (L and AB channel) Inception V3 network along the first three convolutional layers. The RGB input image is first transformed to CIE LAB coordinates (L channel which is aimed at learning the textural and edge features of the Chest X-Ray and AB channel which is aimed at learning the color variations of the Chest X-ray images). The L achromatic channel and the AB channels filters are set to 50%L-50%AB. This method saves between one-third and one-half of the parameters in the divided branches. We further introduced a global second-order pooling at the last two convolutional blocks for more robust image feature extraction against the conventional max-pooling. The detection accuracy of the LCSB-Inception is further improved by employing the Contrast Limited Adaptive Histogram Equalization (CLAHE) image enhancement technique on the input image before feeding them to the network. The proposed LCSB-Inception network is experimented on using two loss functions (Categorically smooth loss and categorically Cross-entropy) and two learning rates whereas Accuracy, Precision, Sensitivity, Specificity F1-Score, and AUC Score were used for evaluation via the chestX-ray-15k (Data_1) and COVID-19 Radiography dataset (Data_2). The proposed models produced an acceptable outcome with an accuracy of 0.97867 (Data_1) and 0.98199 (Data_2) according to the experimental findings. In terms of COVID-19 identification, the suggested models outperform conventional deep learning models and other state-of-the-art techniques presented in the literature based on the results.", "journal": "Computers in biology and medicine", "date": "2023-10-20", "authors": ["Chiagoziem CUkwuoma", "ZhiguangQin", "Victor KwakuAgbesi", "Chukwuebuka JEjiyi", "OlusolaBamisile", "Ijeoma AChikwendu", "Bole WTienin", "Md AltabHossin"], "doi": "10.1016/j.compbiomed.2022.106195\n10.1016/j.jinf.2020.02.016\n10.1016/j.imu.2022.100945\n10.1016/j.cpcardiol.2020.100618\n10.1016/j.diii.2020.03.014\n10.7150/ijbs.45053\n10.1016/S1473-3099(20)30190-0\n10.1002/jmv.25721\n10.1002/jmv.25786\n10.1148/radiol.2020200642\n10.1016/j.jcct.2011.07.001\n10.1109/prai53619.2021.9551094\n10.3390/diagnostics12051152\n10.1016/j.sciaf.2022.e01151\n10.1155/2022/9210947\n10.33889/IJMEMS.2020.5.4.052\n10.12788/fp.0045\n10.3390/app10020559\n10.1007/s42600-021-00132-9\n10.1109/CVPR.2016.308\n10.33889/IJMEMS.2020.5.4.052\n10.1016/j.radi.2022.03.011\n10.1109/cvpr.2009.5206848\n10.1148/radiol.2020200330\n10.1007/s42600-020-00110-7\n10.1038/s41598-020-76550-z\n10.1016/j.cmpb.2020.105581\n10.1007/s10489-020-01902-1\n10.1007/s13246-020-00865-4\n10.1109/ACCESS.2021.3077592\n10.1016/j.patrec.2021.11.020\n10.1007/s42600-021-00151-6\n10.3390/covid1010034\n10.1109/ACCESS.2020.3010287\n10.1007/s10489-020-01831-z\n10.3390/ijerph182111086"}
{"title": "Why Medical Students Pursue Radiology: A Current Longitudinal Survey on Motivations and Controversial Issues in Radiology.", "abstract": "Radiology is an increasingly competitive specialty. Various current factors influence medical students' decision to pursue a radiology career, including artificial intelligence (AI), remote reading, and COVID-19. This study seeks to determine the decision-making factors of all alumni from our medical school who matched into a radiology residency, and to gather opinions on emerging radiology topics.\nA survey querying decision-making factors and opinions on current radiology topics was distributed to all alumni from our medical school (first graduating class in 2011) who previously matched into a diagnostic or interventional radiology residency program (n\u00a0=\u00a057). Wilcoxon Rank-Sum and Fisher's Exact tests were used to determine statistical significance.\nForty-three of fifty-seven responses were received (75% response rate). The most influential factor that sparked respondents' interest in radiology was a radiology elective (25/43, 58%). Students who will finish radiology training in 2023 or later were more likely to be influenced by a mentor (15/23, 65%) than those who finished radiology training before 2023 (5/20, 25%) (p\u00a0=\u00a00.04). Respondents reported a 1.6/5 concern about AI negatively impacting their future career in radiology. There was 1.7/5 concern about performing radiology procedures on patients during the COVID-19 pandemic. Respondents predicted that remote reading would have a 3.2/5 positive impact on helping them achieve their preferred lifestyle. Job satisfaction among attending radiologists is rated at 4.3/5.\nRadiology electives had the greatest influence in piquing students' interest in radiology, while mentorship is assuming increasing influence. AI is perceived as a relatively minimal threat to negatively impact radiologists' jobs. Respondents had little concern about performing radiology procedures during the COVID-19 pandemic. Remote reading is viewed as having a moderately positive impact on lifestyle. Responding radiologists enjoy notably high job satisfaction.", "journal": "Academic radiology", "date": "2023-10-19", "authors": ["EastonNeitzel", "EricvanSonnenberg", "KellyLynch", "ChaseIrwin", "LisaShah-Patel", "Mark DMamlouk"], "doi": "10.1016/j.acra.2023.09.025"}
{"title": "Dynamically Synthetic Images for Federated Learning of medical images.", "abstract": "To develop deep learning models for medical diagnosis, it is important to collect more medical data from several medical institutions. Due to the regulations for privacy concerns, it is infeasible to collect data from various medical institutions to one institution for centralized learning. Federated Learning (FL) provides a feasible approach to jointly train the deep learning model with data stored in various medical institutions instead of collected together. However, the resulting FL models could be biased towards institutions with larger training datasets.\nIn this study, we propose the applicable method of Dynamically Synthetic Images for Federated Learning (DSIFL) that aims to integrate the information of local institutions with heterogeneous types of data. The main technique of DSIFL is to develop a synthetic method that can dynamically adjust the number of synthetic images similar to local data that are misclassified by the current model. The resulting global model can handle the diversity in heterogeneous types of data collected in local medical institutions by including the training of synthetic images similar to misclassified cases in local collections.\nIn model performance evaluation metrics, we focus on the accuracy of each client's dataset. Finally, the accuracy of the model of DSIFL in the experiments can achieve the higher accuracy of the FL approach.\nIn this study, we propose the framework of DSIFL that achieves improvements over the conventional FL approach. We conduct empirical studies with two kinds of medical images. We compare the performance by variants of FL vs. DSIFL approaches. The performance by individual training is used as the baseline, whereas the performance by centralized learning is used as the target for the comparison studies. The empirical findings suggest that the DSIFL has improved performance over the FL via the technique of dynamically synthetic images in training.", "journal": "Computer methods and programs in biomedicine", "date": "2023-10-19", "authors": ["Jacky Chung-HaoWu", "Hsuan-WenYu", "Tsung-HungTsai", "Henry Horng-ShingLu"], "doi": "10.1016/j.cmpb.2023.107845"}
{"title": "Computer-aided diagnosis of chest X-ray for COVID-19 diagnosis in external validation study by radiologists with and without deep learning system.", "abstract": "To evaluate the diagnostic performance of our deep learning (DL) model of COVID-19 and investigate whether the diagnostic performance of radiologists was improved by referring to our model. Our datasets contained chest X-rays (CXRs) for the following three categories: normal (NORMAL), non-COVID-19 pneumonia (PNEUMONIA), and COVID-19 pneumonia (COVID). We used two public datasets and private dataset collected from eight hospitals for the development and external validation of our DL model (26,393 CXRs). Eight radiologists performed two reading sessions: one session was performed with reference to CXRs only, and the other was performed with reference to both CXRs and the results of the DL model. The evaluation metrics for the reading session were accuracy, sensitivity, specificity, and area under the curve (AUC). The accuracy of our DL model was 0.733, and that of the eight radiologists without DL was 0.696\u2009\u00b1\u20090.031. There was a significant difference in AUC between the radiologists with and without DL for COVID versus NORMAL or PNEUMONIA (p\u2009=\u20090.0038). Our DL model alone showed better diagnostic performance than that of most radiologists. In addition, our model significantly improved the diagnostic performance of radiologists for COVID versus NORMAL or PNEUMONIA.", "journal": "Scientific reports", "date": "2023-10-17", "authors": ["AkiMiyazaki", "KengoIkejima", "MizuhoNishio", "MinoruYabuta", "HidetoshiMatsuo", "KojiOnoue", "TakaakiMatsunaga", "EikoNishioka", "AtsushiKono", "DaisukeYamada", "KenOba", "ReiichiIshikura", "TakamichiMurakami"], "doi": "10.1038/s41598-023-44818-9\n10.1148/RADIOL.2020200432\n10.1016/j.tmaid.2020.101627\n10.1016/J.CLINIMAG.2020.04.001\n10.1016/J.COMPBIOMED.2020.103792\n10.3390/S21238045\n10.1371/journal.pone.0191151\n10.1016/J.MEDIA.2020.101794\n10.1155/2021/9996737\n10.3390/S21175702\n10.1148/radiol.2020202944\n10.1148/radiol.2020201874\n10.1148/RADIOL.2020203511\n10.1038/s41598-020-76550-z\n10.1016/j.media.2020.101797\n10.1038/s41598-020-74539-2\n10.1038/s41598-022-11990-3\n10.1117/12.2549075\n10.1007/s00330-020-07628-5\n10.1016/j.media.2021.102225"}
{"title": "Improved myelin water fraction mapping with deep neural networks using synthetically generated 3D data.", "abstract": "We introduce a generative model for synthesis of large scale 3D datasets for quantitative parameter mapping of myelin water fraction (MWF). Our model combines a MR physics signal decay model with an accurate probabilistic multi-component parametric T2 model. We synthetically generate a wide variety of high quality signals and corresponding parameters from a wide range of naturally occurring prior parameter values. To capture spatial variation, the generative signal decay model is combined with a generative spatial model conditioned on generic tissue segmentations. Synthesized 3D datasets can be used to train any convolutional neural network (CNN) based architecture for MWF estimation. Our source code is available at: https://github.com/quin-med-harvard-edu/synthmap Reduction of acquisition time at the expense of lower SNR, as well as accuracy and repeatability of MWF estimation techniques, are key factors that affect the adoption of MWF mapping in clinical practice. We demonstrate that the synthetically trained CNN provides superior accuracy over the competing methods under the constraints of naturally occurring noise levels as well as on the synthetically generated images at low SNR levels. Normalized root mean squared error (nRMSE) is less than 7% on synthetic data, which is significantly lower than competing methods. Additionally, the proposed method yields a coefficient of variation (CoV) that is at least 4x better than the competing method on intra-session test-retest reference dataset.", "journal": "Medical image analysis", "date": "2023-10-17", "authors": ["Serge DidenkoVasylechko", "Simon KWarfield", "SilaKurugol", "OnurAfacan"], "doi": "10.1016/j.media.2023.102966\n10.1016/j.zemedi.2020.04.001"}
{"title": "Medical Imaging Applications of Federated Learning.", "abstract": "Since its introduction in 2016, researchers have applied the idea of Federated Learning (FL) to several domains ranging from edge computing to banking. The technique's inherent security benefits, privacy-preserving capabilities, ease of scalability, and ability to transcend data biases have motivated researchers to use this tool on healthcare datasets. While several reviews exist detailing FL and its applications, this review focuses solely on the different applications of FL to medical imaging datasets, grouping applications by diseases, modality, and/or part of the body. This Systematic Literature review was conducted by querying and consolidating results from ArXiv, IEEE Xplorer, and PubMed. Furthermore, we provide a detailed description of FL architecture, models, descriptions of the performance achieved by FL models, and how results compare with traditional Machine Learning (ML) models. Additionally, we discuss the security benefits, highlighting two primary forms of privacy-preserving techniques, including homomorphic encryption and differential privacy. Finally, we provide some background information and context regarding where the contributions lie. The background information is organized into the following categories: architecture/setup type, data-related topics, security, and learning types. While progress has been made within the field of FL and medical imaging, much room for improvement and understanding remains, with an emphasis on security and data issues remaining the primary concerns for researchers. Therefore, improvements are constantly pushing the field forward. Finally, we highlighted the challenges in deploying FL in medical imaging applications and provided recommendations for future directions.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-10-14", "authors": ["Sukhveer SinghSandhu", "Hamed TaheriGorji", "PanteaTavakolian", "KouhyarTavakolian", "AlirezaAkhbardeh"], "doi": "10.3390/diagnostics13193140\n10.1371/journal.pdig.0000033\n10.1038/s41746-020-00323-1\n10.1016/j.ejmp.2021.02.006\n10.1145/3533708\n10.1145/3412357\n10.1145/3501296\n10.1038/s42256-020-0186-1\n10.3390/diagnostics12112835\n10.3390/s22020450\n10.1109/JBHI.2022.3185673\n10.1109/COMST.2023.3315746\n10.1016/j.csbj.2021.05.010\n10.1007/s40747-020-00247-z\n10.1016/j.cie.2020.106854\n10.21037/qims-20-595\n10.1007/s12021-021-09550-7\n10.1145/3298981\n10.1109/JPROC.2021.3054390\n10.1109/JBHI.2022.3185956\n10.1007/s10796-021-10144-6\n10.3390/s22145195\n10.1016/j.media.2020.101765\n10.1371/journal.pone.0255397\n10.1016/j.media.2021.101992\n10.1038/s41598-022-05539-7\n10.3389/fnins.2016.00365\n10.1038/s41598-020-69250-1\n10.1007/s11063-022-11014-1\n10.1038/s41467-022-33407-5\n10.1109/TNNLS.2022.3223144\n10.1109/TMI.2022.3220757\n10.1101/2020.05.10.20096073\n10.1109/JSEN.2021.3076767\n10.1007/s00500-021-06514-6\n10.1038/s41591-021-01506-3\n10.1109/JIOT.2021.3056185\n10.1038/s41746-021-00431-6\n10.1016/j.asoc.2021.107330\n10.1371/journal.pone.0252573\n10.3390/s21155025\n10.1007/s00330-021-08334-6\n10.3390/s22103728\n10.1109/OJCS.2022.3206407\n10.3389/fpubh.2022.892499\n10.1109/TCBB.2022.3184319\n10.1007/s00330-019-06417-z\n10.1016/j.cmpb.2022.107318\n10.3390/diagnostics12071669\n10.3390/app11052145\n10.1109/JBHI.2022.3149288\n10.1109/JBHI.2022.3198440\n10.3390/cancers14051169\n10.1109/JBHI.2020.3040015\n10.1093/jamia/ocaa341\n10.1097/ICU.0000000000000846\n10.1016/j.xops.2021.100069\n10.1016/j.media.2022.102424\n10.1109/TMI.2022.3220750\n10.1016/j.media.2022.102564\n10.1038/s41598-022-07186-4\n10.1016/j.radonc.2022.06.009\n10.1016/j.radonc.2022.09.023\n10.2196/25869\n10.4329/wjr.v14.i6.114\n10.1109/TMI.2022.3222126"}
{"title": "An improved COVID-19 classification model on chest radiography by dual-ended multiple attention learning.", "abstract": "As a highly contagious disease, COVID-19 has not only had a great impact on the life, study and work of hundreds of millions of people around the world, but also had a huge impact on the global health care system. Therefore, any technical tool that allows for rapid screening and high-precision diagnosis of COVID-19 infections can be of vital help. In order to reduce the burden on health care system, the computer-aided diagnosis of COVID-19 has become a current research hotspot. X-ray imaging is a common and low-cost tool that can help with the COVID-19 diagnosis. The data used for this study has 15,153 CXR images, containing 10,192 normal lungs, 3,631 COVID-19 positive cases and 1,345 images of viral pneumonia. For this computer-aided task, we propose the dual-ended multiple attention learning model (DMAL). The model incorporates multiple attention learning into both networks, and the two networks are linked using an integration module. Specifically, in both networks, the backbone network is used to extract global features and the branch network captures local area information; the integration module combines multi-stage features; and the attention module containing element, channel and spatial attention prompts the model to focus on multi-scale information relevant to the disease. We evaluate the proposed DMAL network using relevant competitive methods as well as ten advanced deep learning models in the image domain and obtain the best performance with 99.67%, 99.53%, 99.66%, 99.60% and 99.76% in terms of Accuracy, Precision, Sensitivity, F1 Scores and Specificity. The proposed method will help in the rapid screening and high-precision diagnosis of COVID-19, given the general trend of such severe global infections. Our code and model are available in [https://github.com/Graziagh/DMALNet].", "journal": "IEEE journal of biomedical and health informatics", "date": "2023-10-13", "authors": ["YongxianFan", "HaoGong"], "doi": "10.1109/JBHI.2023.3324333"}
{"title": "Clinical outcome prediction using observational supervision with electronic health records and audit logs.", "abstract": "Audit logs in electronic health record (EHR) systems capture interactions of providers with clinical data. We determine if machine learning (ML) models trained using audit logs in conjunction with clinical data (\"observational supervision\") outperform ML models trained using clinical data alone in clinical outcome prediction tasks, and whether they are more robust to temporal distribution shifts in the data.\nUsing clinical and audit log data from Stanford Healthcare, we trained and evaluated various ML models including logistic regression, support vector machine (SVM) classifiers, neural networks, random forests, and gradient boosted machines (GBMs) on clinical EHR data, with and without audit logs for two clinical outcome prediction tasks: major adverse kidney events within 120 days of ICU admission (MAKE-120) in acute kidney injury (AKI) patients and 30-day readmission in acute stroke patients. We further tested the best performing models using patient data acquired during different time-intervals to evaluate the impact of temporal distribution shifts on model performance.\nPerformance generally improved for all models when trained with clinical EHR data and audit log data compared with those trained with only clinical EHR data, with GBMs tending to have the overall best performance. GBMs trained with clinical EHR data and audit logs outperformed GBMs trained without audit logs in both clinical outcome prediction tasks: AUROC 0.88 (95% CI: 0.85-0.91) vs. 0.79 (95% CI: 0.77-0.81), respectively, for MAKE-120 prediction in AKI patients, and AUROC 0.74 (95% CI: 0.71-0.77) vs. 0.63 (95% CI: 0.62-0.64), respectively, for 30-day readmission prediction in acute stroke patients. The performance of GBM models trained using audit log and clinical data degraded less in later time-intervals than models trained using only clinical data.\nObservational supervision with audit logs improved the performance of ML models trained to predict important clinical outcomes in patients with AKI and acute stroke, and improved robustness to temporal distribution shifts.", "journal": "Journal of biomedical informatics", "date": "2023-10-13", "authors": ["NanditaBhaskhar", "WuiIp", "Jonathan HChen", "Daniel LRubin"], "doi": "10.1016/j.jbi.2023.104522"}
{"title": "Clinical utilization of artificial intelligence-based COVID-19 pneumonia quantification using chest computed tomography - a multicenter retrospective cohort study in Japan.", "abstract": "Computed tomography (CT) imaging and artificial intelligence (AI)-based analyses have aided in the diagnosis and prediction of the severity of COVID-19. However, the potential of AI-based CT quantification of pneumonia in assessing patients with COVID-19 has not yet been fully explored. This study aimed to investigate the potential of AI-based CT quantification of COVID-19 pneumonia to predict the critical outcomes and clinical characteristics of patients with residual lung lesions.\nThis retrospective cohort study included 1,200 hospitalized patients with COVID-19 from four hospitals. The incidence of critical outcomes (requiring the support of high-flow oxygen or invasive mechanical ventilation or death) and complications during hospitalization (bacterial infection, renal failure, heart failure, thromboembolism, and liver dysfunction) was compared between the groups of pneumonia with high/low-percentage lung lesions, based on AI-based CT quantification. Additionally, 198 patients underwent CT scans 3 months after admission to analyze prognostic factors for residual lung lesions.\nThe pneumonia group with a high percentage of lung lesions (N\u2009=\u2009400) had a higher incidence of critical outcomes and complications during hospitalization than the low percentage group (N\u2009=\u2009800). Multivariable analysis demonstrated that AI-based CT quantification of pneumonia was independently associated with critical outcomes (adjusted odds ratio [aOR] 10.5, 95% confidence interval [CI] 5.59-19.7), as well as with oxygen requirement (aOR 6.35, 95% CI 4.60-8.76), IMV requirement (aOR 7.73, 95% CI 2.52-23.7), and mortality rate (aOR 6.46, 95% CI 1.87-22.3). Among patients with follow-up CT scans (N\u2009=\u2009198), the multivariable analysis revealed that the pneumonia group with a high percentage of lung lesions on admission (aOR 4.74, 95% CI 2.36-9.52), older age (aOR 2.53, 95% CI 1.16-5.51), female sex (aOR 2.41, 95% CI 1.13-5.11), and medical history of hypertension (aOR 2.22, 95% CI 1.09-4.50) independently predicted persistent residual lung lesions.\nAI-based CT quantification of pneumonia provides valuable information beyond qualitative evaluation by physicians, enabling the prediction of critical outcomes and residual lung lesions in patients with COVID-19.", "journal": "Respiratory research", "date": "2023-10-06", "authors": ["HiromuTanaka", "TomokiMaetani", "ShotaroChubachi", "NaoyaTanabe", "YusukeShiraishi", "TakanoriAsakura", "HoNamkoong", "TakashiShimada", "ShuheiAzekawa", "ShiroOtake", "KensukeNakagawara", "TakahiroFukushima", "MayukoWatase", "HidekiTerai", "MamoruSasaki", "SoichiroUeda", "YukariKato", "NorihiroHarada", "ShojiSuzuki", "ShuichiYoshida", "HirokiTateno", "YoshitakeYamada", "MasahiroJinzaki", "ToyohiroHirai", "YukinoriOkada", "RyujiKoike", "MakotoIshii", "NaokiHasegawa", "AkinoriKimura", "SeiyaImoto", "SatoruMiyano", "SeishiOgawa", "TakanoriKanai", "KoichiFukunaga"], "doi": "10.1186/s12931-023-02530-2\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2021436\n10.1056/NEJMoa2007764\n10.1016/j.ijid.2023.04.399\n10.1016/j.cell.2020.04.045\n10.1007/s00330-020-07033-y\n10.1016/j.chest.2021.04.004\n10.1016/j.ejrad.2020.109202\n10.1148/radiol.2021204522\n10.1148/radiol.2020200823\n10.1148/radiol.2020201433\n10.1016/j.jpha.2020.03.004\n10.1038/s41467-020-17971-2\n10.1038/s41598-020-80261-w\n10.1007/s00330-021-08334-6\n10.1016/j.acra.2021.03.001\n10.1001/jama.2020.12603\n10.1136/bmj.n1648\n10.1148/radiol.220019\n10.1097/RCT.0000000000001224\n10.21037/atm-20-4004\n10.1038/s41586-022-05163-5\n10.1016/j.ijid.2021.09.070\n10.1111/dom.14857\n10.1186/s12931-022-02222-3\n10.1513/AnnalsATS.202101-044OC\n10.1038/s41387-022-00217-z\n10.1093/cid/ciaa1012\n10.1186/s12879-022-07927-w\n10.1016/j.clinimag.2021.06.036\n10.1007/s10278-021-00430-9\n10.1007/s00330-020-07013-2\n10.3348/kjr.2020.0293\n10.1097/RTI.0000000000000572\n10.1007/s10140-020-01867-1\n10.1007/s00330-021-08049-8\n10.1186/s12890-023-02418-3\n10.1681/ASN.2020050615\n10.1002/ejhf.1871\n10.1161/CIRCULATIONAHA.120.046702\n10.1016/S2468-1253(20)30057-1\n10.1016/j.ijid.2020.05.055\n10.3389/fmed.2021.689568\n10.1172/JCI137244\n10.1038/s41579-022-00846-2\n10.1016/S2213-2600(21)00174-0\n10.1016/j.chest.2021.02.062\n10.1016/j.ejrad.2021.109676\n10.3389/fmed.2021.711435\n10.1148/radiol.2021204141\n10.1016/j.ejrad.2021.110031\n10.1016/j.atherosclerosis.2021.03.041"}
{"title": "Artificial Intelligence and Infectious Disease Imaging.", "abstract": "The mass production of the graphics processing unit and the coronavirus disease 2019 (COVID-19) pandemic have provided the means and the motivation, respectively, for rapid developments in artificial intelligence (AI) and medical imaging techniques. This has led to new opportunities to improve patient care but also new challenges that must be overcome before these techniques are put into practice. In particular, early AI models reported high performances but failed to perform as well on new data. However, these mistakes motivated further innovation focused on developing models that were not only accurate but also stable and generalizable to new data. The recent developments in AI in response to the COVID-19 pandemic will reap future dividends by facilitating, expediting, and informing other medical AI applications and educating the broad academic audience on the topic. Furthermore, AI research on imaging animal models of infectious diseases offers a unique problem space that can fill in evidence gaps that exist in clinical infectious disease research. Here, we aim to provide a focused assessment of the AI techniques leveraged in the infectious disease imaging research space, highlight the unique challenges, and discuss burgeoning solutions.", "journal": "The Journal of infectious diseases", "date": "2023-10-03", "authors": ["Winston TChu", "Syed M SReza", "James TAnibal", "AdamLanda", "IanCrozier", "Ula\u015fBa\u011fci", "Bradford JWood", "JeffreySolomon"], "doi": "10.1093/infdis/jiad158\n10.48550/arXiv.2010.11929\n10.48550/arXiv.2103.07055\n10.48550/arXiv.1910.10683\n10.48550/arXiv.2110.04257\n10.48550/arXiv.1904.05342\n10.48550/arXiv.2106.03598\n10.48550/arXiv.2207.10062\n10.48550/arXiv.1811.10154\n10.48550/arXiv.1803.06959\n10.48550/arXiv.1806.02891\n10.48550/arXiv.1808.00033\n10.48550/arXiv.1606.05386\n10.48550/arXiv.1901.04592\n10.48550/arXiv.1806.10574\n10.48550/arXiv.1804.02477\n10.48550/arXiv.1707.01154\n10.48550/arXiv.1803.04765\n10.48550/arXiv.1806.07552\n10.48550/arXiv.1802.00682\n10.48550/arXiv.1702.08608\n10.48550/arXiv.1905.12081\n10.48550/arXiv.1712.06657\n10.48550/arXiv.1312.6034\n10.48550/arXiv.1506.06579\n10.48550/arXiv.1902.10186\n10.1101/2020.05.14.096727"}
{"title": "Highly accelerated free-breathing real-time myocardial tagging for exercise cardiovascular magnetic resonance.", "abstract": "Exercise cardiovascular magnetic resonance (Ex-CMR) myocardial tagging would enable quantification of myocardial deformation after exercise. However, current electrocardiogram (ECG)-segmented sequences are limited for Ex-CMR.\nWe developed a highly accelerated balanced steady-state free-precession real-time tagging technique for 3\u00a0T. A 12-fold acceleration was achieved using incoherent sixfold random Cartesian sampling, twofold truncated outer phase encoding, and a deep learning resolution enhancement model. The technique was tested in two prospective studies. In a rest study of 27 patients referred for clinical CMR and 19 healthy subjects, a set of ECG-segmented for comparison and two sets of real-time tagging images for repeatability assessment were collected in 2-chamber and short-axis views with spatiotemporal resolution 2.0\u2009\u00d7\u20092.0\u00a0mm\nIn the rest study, deformation was successfully quantified in 90% of cases. There was a good correlation (r\u2009=\u20090.71) between ECG-segmented and real-time measures of GCS, and repeatability was good to excellent (ICC\u2009=\u20090.86 [0.71, 0.94]) with a CoV of 4.7%. In the Ex-CMR study, deformation was successfully quantified in 96% of subjects pre-exercise and 84% of subjects post-exercise. Short-axis and 2-chamber tagline quality were 1.6\u2009\u00b1\u20090.7 and 1.9\u2009\u00b1\u20090.8 at rest and 1.9\u2009\u00b1\u20090.7 and 2.5\u2009\u00b1\u20090.8 after exercise, respectively. Short-axis and 2-chamber artifact level was 1.2\u2009\u00b1\u20090.5 and 1.4\u2009\u00b1\u20090.7 at rest and 1.3\u2009\u00b1\u20090.6 and 1.5\u2009\u00b1\u20090.8 post-exercise, respectively.\nWe developed a highly accelerated real-time tagging technique and demonstrated its potential for Ex-CMR quantification of myocardial deformation. Further studies are needed to assess the clinical utility of our technique.", "journal": "Journal of cardiovascular magnetic resonance : official journal of the Society for Cardiovascular Magnetic Resonance", "date": "2023-10-03", "authors": ["Manuel AMorales", "SiyeopYoon", "AhmedFahmy", "FahimeGhanbari", "ShiroNakamori", "JenniferRodriguez", "JenniferYue", "Jordan AStreet", "Daniel AHerzka", "Warren JManning", "RezaNezafat"], "doi": "10.1186/s12968-023-00961-w\n10.1161/CIR.0000000000001052\n10.1148/radiol.2021210188\n10.1016/j.jcmg.2021.06.015\n10.1016/j.jcmg.2020.09.039\n10.1161/JAHA.120.019811\n10.1148/radiology.171.3.2717762\n10.1148/radiology.169.1.3420283\n10.1186/1532-429X-13-36\n10.1016/j.jcmg.2016.12.025\n10.1016/S0002-9149(98)00879-0\n10.1161/01.CIR.98.3.217\n10.1016/S0002-9149(97)00640-1\n10.1152/ajpheart.01017.2003\n10.1002/jmri.24263\n10.1161/JAHA.116.003811\n10.1152/jappl.1985.58.6.2047\n10.1152/advances.1999.277.6.S244\n10.1016/j.echo.2012.05.002\n10.1016/0033-0620(69)90027-9\n10.1161/01.CIR.98.25.2836\n10.1186/s12968-020-00652-w\n10.1016/S0895-6111(98)00021-4\n10.1002/jmri.28462\n10.1081/JCMR-200065637\n10.1002/mrm.22668\n10.1002/1522-2586(200009)12:3<430::AID-JMRI8>3.0.CO;2-V\n10.1002/mrm.10688\n10.1002/mrm.10171\n10.1002/(SICI)1522-2594(199911)42:5<952::AID-MRM16>3.0.CO;2-S\n10.1002/mrm.1910380414\n10.1002/mrm.21391\n10.1118/1.4906247\n10.1038/s41598-020-70551-8\n10.1109/TMI.2018.2863670\n10.1109/TMI.2017.2760978\n10.1002/mrm.27480\n10.1002/nbm.4405\n10.1148/radiol.2020192173\n10.1002/mrm.10422\n10.1148/radiol.2303030181\n10.1002/mrm.20719\n10.1016/j.media.2021.102223\n10.1186/1532-429X-14-60\n10.1038/s41592-019-0686-2\n10.1016/j.tjem.2018.08.001\n10.1002/mrm.21363\n10.1002/mrm.21267\n10.1002/mrm.10361"}
{"title": "TVFx - CoVID-19 X-Ray images classification approach using neural networks based feature thresholding technique.", "abstract": "COVID-19, the global pandemic of twenty-first century, has caused major challenges and setbacks for researchers and medical infrastructure worldwide. The CoVID-19 influences on the patients respiratory system cause flooding of airways in the lungs. Multiple techniques have been proposed since the outbreak each of which is interdepended on features and larger training datasets. It is challenging scenario to consolidate larger datasets for accurate and reliable decision support. This research article proposes a chest X-Ray images classification approach based on feature thresholding in categorizing the CoVID-19 samples. The proposed approach uses the threshold value-based Feature Extraction (TVFx) technique and has been validated on 661-CoVID-19 X-Ray datasets in providing decision support for medical experts. The model has three layers of training datasets to attain a sequential pattern based on various learning features. The aligned feature-set of the proposed technique has successfully categorized CoVID-19 active samples into mild, serious, and extreme categories as per medical standards. The proposed technique has achieved an accuracy of 97.42% in categorizing and classifying given samples sets.", "journal": "BMC medical imaging", "date": "2023-10-03", "authors": ["Syed ThouheedAhmed", "Syed MuzamilBasha", "MuthukumaranVenkatesan", "Sandeep KumarMathivanan", "SauravMallik", "NajahAlsubaie", "Mohammed SAlqahtani"], "doi": "10.1186/s12880-023-01100-8\n10.1016/j.eswa.2020.114054\n10.1016/j.asoc.2021.107669\n10.3390/s21041480\n10.1016/j.radi.2020.04.017\n10.1007/s11547-022-01456-x\n10.1186/s12859-023-05321-0\n10.1016/j.chaos.2020.109944\n10.1109/ACCESS.2020.3003810\n10.1007/s10916-019-1392-4\n10.1016/j.compbiomed.2023.106646\n10.3390/biology10111174\n10.1007/s42399-020-00363-4\n10.1109/TMI.2020.2993291"}
{"title": "CryoREAD: de novo structure modeling for nucleic acids in cryo-EM maps using deep learning.", "abstract": "DNA and RNA play fundamental roles in various cellular processes, where their three-dimensional structures provide information critical to understanding the molecular mechanisms of their functions. Although an increasing number of nucleic acid structures and their complexes with proteins are determined by cryogenic electron microscopy (cryo-EM), structure modeling for DNA and RNA remains challenging particularly when the map is determined at a resolution coarser than atomic level. Moreover, computational methods for nucleic acid structure modeling are relatively scarce. Here, we present CryoREAD, a fully automated de novo DNA/RNA atomic structure modeling method using deep learning. CryoREAD identifies phosphate, sugar and base positions in a cryo-EM map using deep learning, which are traced and modeled into a three-dimensional structure. When tested on cryo-EM maps determined at 2.0 to 5.0\u2009\u00c5 resolution, CryoREAD built substantially more accurate models than existing methods. We also applied the method to cryo-EM maps of biomolecular complexes in severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).", "journal": "Nature methods", "date": "2023-10-03", "authors": ["XiaoWang", "GenkiTerashi", "DaisukeKihara"], "doi": "10.1038/s41592-023-02032-5\n10.1038/s41592-019-0500-1\n10.5281/zenodo.8274181"}
{"title": "Explainable Artificial Intelligence (XAI) for Deep Learning Based Medical Imaging Classification.", "abstract": "Recently, deep learning has gained significant attention as a noteworthy division of artificial intelligence (AI) due to its high accuracy and versatile applications. However, one of the major challenges of AI is the need for more interpretability, commonly referred to as the black-box problem. In this study, we introduce an explainable AI model for medical image classification to enhance the interpretability of the decision-making process. Our approach is based on segmenting the images to provide a better understanding of how the AI model arrives at its results. We evaluated our model on five datasets, including the COVID-19 and Pneumonia Chest X-ray dataset, Chest X-ray (COVID-19 and Pneumonia), COVID-19 Image Dataset (COVID-19, Viral Pneumonia, Normal), and COVID-19 Radiography Database. We achieved testing and validation accuracy of 90.6% on a relatively small dataset of 6432 images. Our proposed model improved accuracy and reduced time complexity, making it more practical for medical diagnosis. Our approach offers a more interpretable and transparent AI model that can enhance the accuracy and efficiency of medical diagnosis.", "journal": "Journal of imaging", "date": "2023-09-27", "authors": ["RawanGhnemat", "SawsanAlodibat", "QasemAbu Al-Haija"], "doi": "10.3390/jimaging9090177\n10.1016/j.slast.2021.10.011\n10.1016/j.jbi.2021.103751\n10.1109/TIP.2021.3058783\n10.1007/978-3-030-96302-6_42\n10.1007/s00521-020-05636-6\n10.1007/s41060-021-00295-9\n10.1038/s41598-020-76550-z\n10.3390/bdcc6040126\n10.1109/TNNLS.2021.3086570\n10.1038/s41598-021-87994-2\n10.1155/2022/2123662\n10.1016/j.media.2021.101993\n10.1109/JBHI.2022.3205167\n10.1007/s00521-022-06918-x\n10.3390/asi5050102\n10.1109/JBHI.2021.3111415\n10.3390/ai3020028\n10.1007/s11548-020-02305-w\n10.1038/s41598-021-95680-6\n10.3390/ai3010009\n10.3390/electronics12030590\n10.3390/life12111709\n10.4018/IJIIT.306969\n10.3389/frobt.2022.1090012\n10.3390/e23010018\n10.1002/widm.1424\n10.3390/ai2030020\n10.1109/IEMTRONICS51293.2020.9216455\n10.1017/S0269888914000289\n10.1016/j.jjimei.2021.100039\n10.1007/s10489-020-01829-7\n10.1016/j.smhl.2022.100332\n10.1080/10580530.2020.1849465\n10.3390/ai1040032\n10.3390/ai4030030\n10.3390/ai1030027\n10.1109/IEMTRONICS52119.2021.9422617\n10.1109/JTEHM.2021.3134096\n10.3390/ai1020009\n10.1109/TFUZZ.2022.3144448\n10.1109/ACCESS.2020.3044858"}
{"title": "Hybrid Diagnostic Model for Improved COVID-19 Detection in Lung Radiographs Using Deep and Traditional Features.", "abstract": "A recently discovered coronavirus (COVID-19) poses a major danger to human life and health across the planet. The most important step in managing and combating COVID-19 is to accurately screen and diagnose affected people. The imaging technology of lung X-ray is a useful imaging identification/detection approach among them. The help of such computer-aided machines and diagnoses to examine lung X-ray images of COVID-19 instances can give supplemental assessment ideas to specialists, easing their workload to some level. The novel concept of this study is a hybridized approach merging pertinent manual features with deep spatial features for the classification of COVID-19. Further, we employed traditional transfer learning techniques in this investigation, utilizing four different pre-trained CNN-based deep learning models, with the Inception model showing a reasonably accurate result and a diagnosis accuracy of 82.17%. We provide a successful diagnostic approach that blends deep characteristics with machine learning classification to further increase clinical performance. It employs a complete diagnostic model. Two datasets were used to test the suggested approach, and it did quite well on several of them. On 1102 lung X-ray scans, the model was originally evaluated. The results of the experiments indicate that the suggested SVM model has a diagnostic accuracy of 95.57%. When compared to the Xception model's baseline, the diagnostic accuracy had risen by 17.58 percent. The sensitivity, specificity, and AUC of the proposed models were 95.37 percent, 95.39%, and 95.77%, respectively. To show the adaptability of our approach, we also verified our proposed model on other datasets. Finally, we arrived at results that were conclusive. When compared to research of a comparable kind, our suggested CNN model has a greater accuracy of classification and diagnostic effectiveness.", "journal": "Biomimetics (Basel, Switzerland)", "date": "2023-09-27", "authors": ["Imran ArshadChoudhry", "Adnan NQureshi", "KhursheedAurangzeb", "SaeedIqbal", "MusaedAlhussein"], "doi": "10.3390/biomimetics8050406\n10.1016/j.ijsu.2020.02.034\n10.1148/radiol.2020200330\n10.1177/0284185121992655\n10.1186/s12992-022-00804-w\n10.1016/j.arcmed.2022.03.001\n10.1038/s41598-020-76550-z\n10.1007/s40747-020-00199-4\n10.1007/s13246-020-00865-4\n10.1007/s10044-021-00984-y\n10.7861/futurehosp.6-2-94\n10.3348/kjr.2017.18.4.570\n10.1016/j.chaos.2020.109944\n10.1016/j.cmpb.2020.105581\n10.3390/app11199023\n10.3390/jpm12020309\n10.3390/math10142472\n10.3390/bioengineering9090457\n10.1016/j.bbe.2022.07.009\n10.14741/Ijcet/22774106/5.2.2015.121\n10.1186/s40537-019-0197-0\n10.1007/BF00994018\n10.1371/journal.pntd.0008056\n10.1007/BF00116251\n10.1006/jcss.1997.1504"}
{"title": "Machine learning for cross-scale microscopy of viruses.", "abstract": "Despite advances in virological sciences and antiviral research, viruses continue to emerge, circulate, and threaten public health. We still lack a comprehensive understanding of how cells and individuals remain susceptible to infectious agents. This deficiency is in part due to the complexity of viruses, including the cell states controlling virus-host interactions. Microscopy samples distinct cellular infection stages in a multi-parametric, time-resolved manner at molecular resolution and is increasingly enhanced by machine learning and deep learning. Here we discuss how state-of-the-art artificial intelligence (AI) augments light and electron microscopy and advances virological research of cells. We describe current procedures for image denoising, object segmentation, tracking, classification, and super-resolution and showcase examples of how AI has improved the acquisition and analyses of microscopy data. The power of AI-enhanced microscopy will continue to help unravel virus infection mechanisms, develop antiviral agents, and improve viral vectors.", "journal": "Cell reports methods", "date": "2023-09-27", "authors": ["AnthonyPetkidis", "VardanAndriasyan", "Urs FGreber"], "doi": "10.1016/j.crmeth.2023.100557\n10.1038/nature12294\n10.1002/jmv.26326\n10.1007/s10875-022-01289-3\n10.1038/s41579-020-0382-3\n10.3390/v13081568\n10.3390/v10040202\n10.3390/molecules24030481\n10.1093/jmicro/dfad024\n10.1038/s41556-018-0251-8\n10.1039/d1ra00278c\n10.1038/s41586-023-05925-9\n10.3390/v10040166\n10.1038/nrm.2017.71\n10.1021/acs.chemrev.9b00692\n10.1038/s41573-020-00117-w\n10.1007/978-3-030-00934-2_30\n10.1038/s41592-022-01507-1\n10.1126/sciadv.abl7150\n10.48550/arxiv.2211.08512\n10.1016/j.chom.2013.09.004\n10.1016/j.isci.2021.102543\n10.1126/science.aaw1219\n10.1038/s41587-020-0739-1\n10.1038/s41586-019-1049-y\n10.1038/s41598-019-43943-8\n10.1038/nmeth.2869\n10.1126/science.aar7042\n10.1038/s41592-020-01037-8\n10.1242/jcs.252544\n10.1016/j.cell.2018.10.056\n10.1038/nature14539\n10.1038/s41592-019-0403-1\n10.1038/s41592-022-01589-x\n10.1038/nbt.4106\n10.1038/nmeth.2019\n10.1038/s41592-021-01262-9\n10.1186/s12859-021-04344-9\n10.1017/S1431927622006328\n10.1016/j.cels.2020.04.003\n10.1038/s41592-020-01018-x\n10.1038/s41592-019-0582-9\n10.1093/bioinformatics/btx180\n10.48550/arxiv.1603.04467\n10.1038/s41467-021-22518-0\n10.1038/s41592-020-01023-0\n10.1101/2022.06.07.495102\n10.1109/ICEngTechnol.2017.8308186\n10.48550/arxiv.1603.07285\n10.1145/3065386\n10.1109/CVPR.2015.7298965\n10.1007/978-3-319-24574-4_28\n10.48550/arxiv.1411.1784\n10.1109/CVPR.2017.632\n10.1109/ICCV.2017.244\n10.48550/arxiv.1711.04340\n10.1145/3505244\n10.48550/arxiv.2010.11929\n10.1038/s41592-018-0261-2\n10.1109/WACV45572.2020.9093435\n10.1038/s41587-021-01094-0\n10.1109/ICCV.2017.322\n10.1002/bies.202000257\n10.1371/journal.ppat.1010470\n10.1126/science.abd3072\n10.26508/lsa.202101124\n10.1038/s41586-020-2714-x\n10.1371/journal.pbio.3001490\n10.1016/j.immuni.2022.03.020\n10.7554/eLife.74153\n10.1038/s41598-020-77170-3\n10.1038/nmeth.1486\n10.1371/journal.pcbi.1009797\n10.1371/journal.pcbi.1007673\n10.1038/s41467-022-35004-y\n10.1109/ISBI45749.2020.9098598\n10.1109/ISBI52829.2022.9761696\n10.1038/s41592-018-0216-7\n10.48550/arxiv.1901.11365\n10.48550/arxiv.1803.04189\n10.1109/ISBI45749.2020.9098336\n10.3389/fcomp.2020.00005\n10.48550/arxiv.2006.06072\n10.1109/CVPR52688.2022.00207\n10.1364/OPTICA.5.000458\n10.1038/s41592-020-0853-5\n10.1038/s41592-021-01236-x\n10.12720/joig.1.4.166-170\n10.1038/s41467-018-07619-7\n10.1038/nmeth.4473\n10.1038/s41592-019-0612-7\n10.1109/TMI.2018.2865709\n10.1038/s41592-020-01008-z\n10.48550/arxiv.2303.06274\n10.1186/gb-2006-7-10-r100\n10.1016/j.chom.2023.03.005\n10.1038/s41592-022-01663-4\n10.1007/s11626-020-00517-7\n10.1016/j.xpro.2021.101017\n10.48550/arxiv.2303.12712\n10.48550/arxiv.2304.02643\n10.1038/mt.2011.102\n10.1083/jcb.144.4.657\n10.1016/j.jsb.2005.06.002\n10.1016/j.chom.2011.07.006\n10.1128/JVI.02568-15\n10.1038/nmeth.1237\n10.1016/j.ymeth.2016.09.016\n10.1038/s41564-018-0288-2\n10.1016/j.celrep.2018.04.013\n10.1063/5.0034891\n10.1364/OPTICA.6.000506\n10.1038/s42256-022-00595-0\n10.1109/CVPR.2016.90\n10.1126/science.1127344\n10.1126/science.1226359\n10.1038/ncomms6980\n10.7554/eLife.40183\n10.1038/s41592-021-01136-0\n10.1038/s41592-021-01058-x\n10.1038/s41587-021-01092-2\n10.1364/JOSA.62.000055\n10.1086/111605\n10.1038/s41592-022-01652-7\n10.1016/j.biocel.2018.05.014\n10.1038/ncomms12471\n10.1038/nmeth.4605\n10.1109/ACSSC.2003.1292216\n10.1101/sqb.1962.027.001.005\n10.1016/s0065-3527(08)60012-3\n10.1084/jem.112.2.373\n10.1016/j.coviro.2011.06.001\n10.1016/0092-8674(78)90252-0\n10.1038/s41586-020-2833-4\n10.1038/s41586-020-2829-0\n10.1038/308032a0\n10.1017/S0033583500004297\n10.1016/S0022-2836(05)80271-2\n10.1006/jsbi.1996.0030\n10.1126/science.abm9506\n10.1038/s41586-021-03819-2\n10.1093/bioinformatics/btx188\n10.1007/978-3-319-46723-8_49\n10.1038/s41586-021-03977-3\n10.1083/jcb.202010039\n10.1038/nmeth.4405\n10.1016/j.jsb.2006.05.009\n10.1038/s41467-021-24887-y\n10.1016/j.chom.2018.07.018\n10.1038/s41592-021-01275-4\n10.1038/s41592-022-01746-2\n10.1016/j.cmpb.2019.05.026\n10.1016/j.cmpb.2020.105766\n10.1016/j.csbj.2021.10.001\n10.1007/s00418-018-1759-5\n10.1111/cmi.13280\n10.1126/science.1175862\n10.1128/JVI.01102-12\n10.1016/j.coviro.2021.03.006\n10.1007/978-1-4939-8678-1_30\n10.1103/PhysRevE.103.042310\n10.1038/s41562-022-01394-8\n10.1109/TCSVT.2016.2589879\n10.1109/CVPR42600.2020.01070\n10.48550/arxiv.1804.09170\n10.1145/3386252\n10.1109/TPAMI.2018.2857768\n10.1038/s41598-020-61808-3\n10.1109/5.58325\n10.1038/nprot.2016.105\n10.1371/journal.pone.0080999\n10.1038/nature08779\n10.1093/bioinformatics/btx069\n10.1101/293431\n10.1186/s12915-021-01086-1\n10.1038/s41467-020-18764-3\n10.1016/j.inffus.2019.12.012\n10.1038/s41591-021-01343-4\n10.1038/s41568-018-0016-5"}
{"title": "Development of a novel machine learning model based on laboratory and imaging indices to predict acute cardiac injury in cancer patients with COVID-19 infection: a retrospective observational study.", "abstract": "Due to the increased risk of acute cardiac injury (ACI) and poor prognosis in cancer patients with COVID-19 infection, our aim was to develop a novel and interpretable model for predicting ACI occurrence in cancer patients with COVID-19 infection.\nThis retrospective observational study screened 740 cancer patients with COVID-19 infection from December 2022 to April 2023. The least absolute shrinkage and selection operator (LASSO) regression was used for the preliminary screening of the indices. To enhance the model accuracy, we introduced an alpha index to further screen and rank the indices based on their significance. Random forest (RF) was used to construct the prediction model. The Shapley Additive Explanation (SHAP) and Local Interpretable Model-Agnostic Explanation (LIME) methods were utilized to explain the model.\nAccording to the inclusion criteria, 201 cancer patients with COVID-19, including 36 variables indices, were included in the analysis. The top eight indices (albumin, lactate dehydrogenase, cystatin C, neutrophil count, creatine kinase isoenzyme, red blood cell distribution width, D-dimer and chest computed tomography) for predicting the occurrence of ACI in cancer patients with COVID-19 infection were included in the RF model. The model achieved an area under curve (AUC) of 0.940, an accuracy of 0.866, a sensitivity of 0.750 and a specificity of 0.900. The calibration curve and decision curve analysis showed good calibration and clinical practicability. SHAP results demonstrated that albumin was the most important index for predicting the occurrence of ACI. LIME results showed that the model could predict the probability of ACI in each cancer patient infected with COVID-19 individually.\nWe developed a novel machine-learning model that demonstrates high explainability and accuracy in predicting the occurrence of ACI in cancer patients with COVID-19 infection, using laboratory and imaging indices.", "journal": "Journal of cancer research and clinical oncology", "date": "2023-09-25", "authors": ["GuangcaiWan", "XuefengWu", "XiaoweiZhang", "HongshuaiSun", "XiuyanYu"], "doi": "10.1007/s00432-023-05417-3\n10.1007/s00330-022-09325-x\n10.1016/j.cytogfr.2021.10.007\n10.1161/CIRCULATIONAHA.121.056817\n10.1016/j.clim.2022.109218\n10.1016/j.compbiomed.2021.104665\n10.3390/ijms232415698\n10.1136/bmj.g7594\n10.3389/fpubh.2021.678276\n10.1016/j.artmed.2023.102490\n10.1038/s41591-023-02325-4\n10.1007/s00330-020-07622-x\n10.1093/cvr/cvaa193\n10.1016/j.chaos.2021.111779\n10.1055/s-0040-1716544\n10.1001/jamaoncol.2022.6815\n10.1016/j.annonc.2021.02.024\n10.1001/jamacardio.2020.1017\n10.1001/jamacardio.2021.5505\n10.1007/s40121-022-00671-3\n10.1152/japplphysiol.00325.2021\n10.1371/journal.pone.0255154\n10.1002/jmv.28646\n10.1016/j.ejrad.2023.110827\n10.1016/j.eclinm.2023.101975\n10.1002/jmv.28722\n10.1016/j.ebiom.2022.103821\n10.1007/s11886-020-01421-y\n10.1016/j.mrrev.2022.108411\n10.1186/s12911-023-02132-4\n10.1080/16078454.2022.2089830\n10.1016/j.jaci.2023.01.022\n10.2196/21439\n10.1007/s12041-021-01262-w\n10.1016/S0140-6736(18)31923-8\n10.1093/eurheartj/ehaa408\n10.1038/s41591-021-01576-3\n10.1161/CIRCRESAHA.123.321876\n10.1186/s13054-020-03179-9\n10.1007/s10456-022-09860-7\n10.1186/s40249-022-00946-4\n10.1016/j.compbiomed.2023.106619\n10.1002/cam4.4888"}
{"title": "The evolving role of data & safety monitoring boards for real-world clinical trials.", "abstract": "Clinical trials provide the \"gold standard\" evidence for advancing the practice of medicine, even as they evolve to integrate real-world data sources. Modern clinical trials are increasingly incorporating real-world data sources - data not intended for research and often collected in free-living contexts. We refer to trials that incorporate real-world data sources as real-world trials. Such trials may have the potential to enhance the generalizability of findings, facilitate pragmatic study designs, and evaluate real-world effectiveness. However, key differences in the design, conduct, and implementation of real-world vs traditional trials have ramifications in data management that can threaten their desired rigor.\nThree examples of real-world trials that leverage different types of data sources - wearables, medical devices, and electronic health records are described. Key insights applicable to all three trials in their relationship to Data and Safety Monitoring Boards (DSMBs) are derived.\nInsight and recommendations are given on four topic areas: A. Charge of the DSMB; B. Composition of the DSMB; C. Pre-launch Activities; and D. Post-launch Activities. We recommend stronger and additional focus on data integrity.\nClinical trials can benefit from incorporating real-world data sources, potentially increasing the generalizability of findings and overall trial scale and efficiency. The data, however, present a level of informatic complexity that relies heavily on a robust data science infrastructure. The nature of monitoring the data and safety must evolve to adapt to new trial scenarios to protect the rigor of clinical trials.", "journal": "Journal of clinical and translational science", "date": "2023-09-25", "authors": ["Bryan JBunning", "HaleyHedlin", "Jonathan HChen", "Jody DCiolino", "Johannes OpsahlFerstad", "EmilyFox", "AriadnaGarcia", "AlanGo", "RameshJohari", "JustinLee", "David MMaahs", "Kenneth WMahaffey", "KristaOpsahl-Ong", "MarcoPerez", "KaylinRochford", "DavidScheinker", "HeidiSpratt", "Mintu PTurakhia", "ManishaDesai"], "doi": "10.1017/cts.2023.582\n10.1080/10543406.2022.2080698"}
{"title": "A cross-sectional case-control study on the structural connectome in recovered hospitalized COVID-19 patients.", "abstract": "COVID-19 can induce neurological sequelae, negatively affecting the quality of life. Unravelling this illness's impact on structural brain connectivity, white-matter microstructure (WMM), and cognitive performance may help elucidate its implications. This cross-sectional study aimed to investigate differences in these factors between former hospitalised COVID-19 patients (COV) and healthy controls. Group differences in structural brain connectivity were explored using Welch-two sample t-tests and two-sample Mann-Whitney U tests. Multivariate linear models were constructed (one per region) to examine fixel-based group differences. Differences in cognitive performance between groups were investigated using Wilcoxon Rank Sum tests. Possible effects of bundle-specific FD measures on cognitive performance were explored using a two-group path model. No differences in whole-brain structural organisation were found. Bundle-specific metrics showed reduced fiber density (p\u2009=\u20090.012, Hedges' g\u2009=\u20090.884) and fiber density cross-section (p\u2009=\u20090.007, Hedges' g\u2009=\u20090.945) in the motor segment of the corpus callosum in COV compared to healthy controls. Cognitive performance on the motor praxis and digit symbol substitution tests was worse in COV than healthy controls (p\u2009<\u20090.001, r\u2009=\u20090.688; p\u2009=\u20090.013, r\u2009=\u2009422, respectively). Associations between the cognitive performance and bundle-specific FD measures differed significantly between groups. WMM and cognitive performance differences were observed between COV and healthy controls.", "journal": "Scientific reports", "date": "2023-09-22", "authors": ["ElkeLathouwers", "AhmedRadwan", "JeroenBlommaert", "LaraStas", "BrunoTassignon", "Sabine DAllard", "FilipDe Ridder", "ElisabethDe Waele", "NicoleHoornaert", "PatrickLacor", "RembertMertens", "MaartenNaeyaert", "HubertRaeymaekers", "LucieSeyler", "Anne-MarieVanbinst", "LienVan Liedekerke", "JeroenVan Schependom", "PeterVan Schuerbeek", "StevenProvyn", "BartRoelands", "MarieVandekerckhove", "RomainMeeusen", "StefanSunaert", "GuyNagels", "JohanDe Mey", "KevinDe Pauw"], "doi": "10.1038/s41598-023-42429-y\n10.1038/s41564-020-0695-z\n10.1038/s41579-020-00459-7\n10.1016/j.bbih.2021.100290\n10.1186/s42466-021-00116-1\n10.1136/thoraxjnl-2020-215818\n10.1177/01410768211032850\n10.1016/j.eclinm.2021.101044\n10.1016/j.cell.2022.06.008\n10.1038/s41593-020-00758-5\n10.1002/path.5471\n10.1016/j.eclinm.2020.100683\n10.1016/j.mehy.2020.110320\n10.1002/jcla.24403\n10.1038/s41586-022-04569-5\n10.1016/j.eclinm.2020.100484\n10.1172/JCI147329\n10.1007/s00221-023-06545-5\n10.1016/j.jpsychires.2020.06.022\n10.1016/j.bbi.2020.06.008\n10.1002/alz.12255\n10.1111/ene.14775\n10.1016/j.ebiom.2021.103512\n10.2967/jnumed.121.262128\n10.1007/s13365-022-01079-y\n10.1002/hbm.22099\n10.1016/j.neuroimage.2021.118417\n10.3171/2013.2.JNS121294\n10.1016/j.neuropsychologia.2012.11.018\n10.1016/j.neuroimage.2015.05.039\n10.1016/j.neuroimage.2016.09.029\n10.3389/fneur.2022.1029302\n10.1016/j.neuroimage.2012.06.005\n10.1016/j.neuroimage.2015.06.092\n10.1016/j.neuroimage.2007.02.016\n10.1016/j.neuroimage.2006.01.021\n10.1016/j.neuroimage.2022.119029\n10.3357/AMHP.4343.2015\n10.18637/jss.v048.i02\n10.1177/0049124114543236\n10.1080/10705511.2014.937847\n10.1016/j.neuroimage.2011.10.045\n10.1212/WNL.0000000000001439\n10.3390/cimb44030073\n10.1126/sciimmunol.ade2798\n10.1016/j.fct.2022.113008\n10.1001/jama.2021.24110\n10.1016/j.cmi.2021.10.005\n10.3390/vaccines11020378\n10.1093/brain/awab435"}
{"title": "Using Social Media to Help Understand Patient-Reported Health Outcomes of Post-COVID-19 Condition: Natural Language Processing Approach.", "abstract": "While scientific knowledge of post-COVID-19 condition (PCC) is growing, there remains significant uncertainty in the definition of the disease, its expected clinical course, and its impact on daily functioning. Social media platforms can generate valuable insights into patient-reported health outcomes as the content is produced at high resolution by patients and caregivers, representing experiences that may be unavailable to most clinicians.\nIn this study, we aimed to determine the validity and effectiveness of advanced natural language processing approaches built to derive insight into PCC-related patient-reported health outcomes from social media platforms Twitter and Reddit. We extracted PCC-related terms, including symptoms and conditions, and measured their occurrence frequency. We compared the outputs with human annotations and clinical outcomes and tracked symptom and condition term occurrences over time and locations to explore the pipeline's potential as a surveillance tool.\nWe used bidirectional encoder representations from transformers (BERT) models to extract and normalize PCC symptom and condition terms from English posts on Twitter and Reddit. We compared 2 named entity recognition models and implemented a 2-step normalization task to map extracted terms to unique concepts in standardized terminology. The normalization steps were done using a semantic search approach with BERT biencoders. We evaluated the effectiveness of BERT models in extracting the terms using a human-annotated corpus and a proximity-based score. We also compared the validity and reliability of the extracted and normalized terms to a web-based survey with more than 3000 participants from several countries.\nUmlsBERT-Clinical had the highest accuracy in predicting entities closest to those extracted by human annotators. Based on our findings, the top 3 most commonly occurring groups of PCC symptom and condition terms were systemic (such as fatigue), neuropsychiatric (such as anxiety and brain fog), and respiratory (such as shortness of breath). In addition, we also found novel symptom and condition terms that had not been categorized in previous studies, such as infection and pain. Regarding the co-occurring symptoms, the pair of fatigue and headaches was among the most co-occurring term pairs across both platforms. Based on the temporal analysis, the neuropsychiatric terms were the most prevalent, followed by the systemic category, on both social media platforms. Our spatial analysis concluded that 42% (10,938/26,247) of the analyzed terms included location information, with the majority coming from the United States, United Kingdom, and Canada.\nThe outcome of our social media-derived pipeline is comparable with the results of peer-reviewed articles relevant to PCC symptoms. Overall, this study provides unique insights into patient-reported health outcomes of PCC and valuable information about the patient's journey that can help health care providers anticipate future needs.\nRR2-10.1101/2022.12.14.22283419.", "journal": "Journal of medical Internet research", "date": "2023-09-19", "authors": ["ElhamDolatabadi", "DianaMoyano", "MichaelBales", "SofijaSpasojevic", "RohanBhambhoria", "JunaidBhatti", "ShyamolimaDebnath", "NicholasHoell", "XinLi", "CelineLeng", "SashaNanda", "JadSaab", "EsmatSahak", "FannySie", "SaraUppal", "Nirma KhatriVadlamudi", "AntoanetaVladimirova", "ArturYakimovich", "XiaoxueYang", "Sedef AkinliKocak", "Angela MCheung"], "doi": "10.2196/45767\n10.1016/j.ebiom.2021.103722\n10.1101/2021.06.03.21258317v2\n10.1101/2021.06.03.21258317\n10.1016/j.eclinm.2021.101019\n10.1136/bmj.m2815\n10.1007/s00246-022-03020-w\n10.1007/s00246-022-03020-w\n10.1136/bmj.n1098\n10.1136/bmj.n693\n10.1056/NEJMp0900702\n10.2196/jmir.1157\n10.1186/s40504-017-0065-7\n10.1186/s40504-017-0065-7\n10.1186/1471-2458-12-242\n10.1186/1471-2458-12-242\n10.1186/s40249-018-0468-6\n10.1186/s40249-018-0468-6\n10.1186/s12889-019-7103-8\n10.1186/s12889-019-7103-8\n10.1038/s41598-021-00766-w\n10.1609/icwsm.v5i1.14137\n10.1111/1541-4337.12540\n10.2196/19509\n10.1111/phn.12809\n10.1111/phn.12809\n10.1109/JBHI.2020.3001216\n10.3390/ijerph17072365\n10.2196/22635\n10.3389/frai.2023.1023281\n10.18653/v1/2021.naacl-main.139\n10.1177/00207314211017469\n10.1177/00207314211017469\n10.2196/25314\n10.2196/41529\n10.1093/bib/bbaa296\n10.18653/v1/d19-1410\n10.1371/journal.pmed.1003773\n10.1177/0038038517708140\n10.1177/0038038517708140\n10.18653/v1/2021.bionlp-1.20\n10.1038/s41579-022-00846-2\n10.1007/s11908-013-0341-5"}
{"title": "Multi-site, Multi-domain Airway Tree Modeling.", "abstract": "Open international challenges are becoming the de facto standard for assessing computer vision and image analysis algorithms. In recent years, new methods have extended the reach of pulmonary airway segmentation that is closer to the limit of image resolution. Since EXACT'09 pulmonary airway segmentation, limited effort has been directed to the quantitative comparison of newly emerged algorithms driven by the maturity of deep learning based approaches and extensive clinical efforts for resolving finer details of distal airways for early intervention of pulmonary diseases. Thus far, public annotated datasets are extremely limited, hindering the development of data-driven methods and detailed performance evaluation of new algorithms. To provide a benchmark for the medical imaging community, we organized the Multi-site, Multi-domain Airway Tree Modeling (ATM'22), which was held as an official challenge event during the MICCAI 2022 conference. ATM'22 provides large-scale CT scans with detailed pulmonary airway annotation, including 500 CT scans (300 for training, 50 for validation, and 150 for testing). The dataset was collected from different sites and it further included a portion of noisy COVID-19 CTs with ground-glass opacity and consolidation. Twenty-three teams participated in the entire phase of the challenge and the algorithms for the top ten teams are reviewed in this paper. Both quantitative and qualitative results revealed that deep learning models embedded with the topological continuity enhancement achieved superior performance in general. ATM'22 challenge holds as an open-call design, the training data and the gold standard evaluation are available upon successful registration via its homepage (https://atm22.grand-challenge.org/).", "journal": "Medical image analysis", "date": "2023-09-17", "authors": ["MinghuiZhang", "YangqianWu", "HanxiaoZhang", "YuleiQin", "HaoZheng", "WenTang", "CoreyArnold", "ChenhaoPei", "PengxinYu", "YangNan", "GuangYang", "SimonWalsh", "Dominic CMarshall", "MatthieuKomorowski", "PuyangWang", "DazhouGuo", "DakaiJin", "Ya'nanWu", "ShuiqingZhao", "RunshengChang", "BoyuZhang", "XingLu", "AbdulQayyum", "MoonaMazher", "QiSu", "YonghuangWu", "Ying'aoLiu", "YufeiZhu", "JianchengYang", "AshkanPakzad", "BojidarRangelov", "Raul San JoseEstepar", "Carlos CanoEspinosa", "JiayuanSun", "Guang-ZhongYang", "YunGu"], "doi": "10.1016/j.media.2023.102957"}
{"title": "Federated Semi-Supervised Medical Image Segmentation via Prototype-Based Pseudo-Labeling and Contrastive Learning.", "abstract": "Existing federated learning works mainly focus on the fully supervised training setting. In realistic scenarios, however, most clinical sites can only provide data without annotations due to the lack of resources or expertise. In this work, we are concerned with the practical yet challenging federated semi-supervised segmentation (FSSS), where labeled data are only with several clients and other clients can just provide unlabeled data. We take an early attempt to tackle this problem and propose a novel FSSS method with prototype-based pseudo-labeling and contrastive learning. First, we transmit a labeled-aggregated model, which is obtained based on prototype similarity, to each unlabeled client, to work together with the global model for debiased pseudo labels generation via a consistency- and entropy-aware selection strategy. Second, we transfer image-level prototypes from labeled datasets to unlabeled clients and conduct prototypical contrastive learning on unlabeled models to enhance their discriminative power. Finally, we perform the dynamic model aggregation with a designed consistency-aware aggregation strategy to dynamically adjust the aggregation weights of each local model. We evaluate our method on COVID-19 X-ray infected region segmentation, COVID-19 CT infected region segmentation and colorectal polyp segmentation, and experimental results consistently demonstrate the effectiveness of our proposed method. Codes areavailable at https://github.com/zhangbaiming/FedSemiSeg.", "journal": "IEEE transactions on medical imaging", "date": "2023-09-13", "authors": ["HuisiWu", "BaimingZhang", "ChengChen", "JingQin"], "doi": "10.1109/TMI.2023.3314430"}
{"title": "Effectiveness of Audio Output from an Artificial Intelligence Method for Layperson Recognition of Pulmonary Edema or COVID Lung Infection on Ultrasound Images.", "abstract": null, "journal": "Journal of the American Society of Echocardiography : official publication of the American Society of Echocardiography", "date": "2023-09-12", "authors": ["Bruce JKimura", "Devin RWaltman", "Paul JHan", "Thomas JWaltman"], "doi": "10.1016/j.echo.2023.09.004"}
{"title": "Dual-stream EfficientNet with adversarial sample augmentation for COVID-19 computer aided diagnosis.", "abstract": "Though a series of computer aided measures have been taken for the rapid and definite diagnosis of 2019 coronavirus disease (COVID-19), they generally fail to achieve high enough accuracy, including the recently popular deep learning-based methods. The main reasons are that: (a) they generally focus on improving the model structures while ignoring important information contained in the medical image itself; (b) the existing small-scale datasets have difficulty in meeting the training requirements of deep learning. In this paper, a dual-stream network based on the EfficientNet is proposed for the COVID-19 diagnosis based on CT scans. The dual-stream network takes into account the important information in both spatial and frequency domains of CT scans. Besides, Adversarial Propagation (AdvProp) technology is used to address the insufficient training data usually faced by the deep learning-based computer aided diagnosis and also the overfitting issue. Feature Pyramid Network (FPN) is utilized to fuse the dual-stream features. Experimental results on the public dataset COVIDx CT-2A demonstrate that the proposed method outperforms the existing 12 deep learning-based methods for COVID-19 diagnosis, achieving an accuracy of 0.9870 for multi-class classification, and 0.9958 for binary classification. The source code is available at https://github.com/imagecbj/covid-efficientnet.", "journal": "Computers in biology and medicine", "date": "2023-09-12", "authors": ["WeijieXu", "LinaNie", "BeijingChen", "WeipingDing"], "doi": "10.1016/j.compbiomed.2023.107451"}
{"title": "A Systematic Review on Deep Structured Learning for COVID-19 Screening Using Chest CT from 2020 to 2022.", "abstract": "The emergence of the COVID-19 pandemic in Wuhan in 2019 led to the discovery of a novel coronavirus. The World Health Organization (WHO) designated it as a global pandemic on 11 March 2020 due to its rapid and widespread transmission. Its impact has had profound implications, particularly in the realm of public health. Extensive scientific endeavors have been directed towards devising effective treatment strategies and vaccines. Within the healthcare and medical imaging domain, the application of artificial intelligence (AI) has brought significant advantages. This study delves into peer-reviewed research articles spanning the years 2020 to 2022, focusing on AI-driven methodologies for the analysis and screening of COVID-19 through chest CT scan data. We assess the efficacy of deep learning algorithms in facilitating decision making processes. Our exploration encompasses various facets, including data collection, systematic contributions, emerging techniques, and encountered challenges. However, the comparison of outcomes between 2020 and 2022 proves intricate due to shifts in dataset magnitudes over time. The initiatives aimed at developing AI-powered tools for the detection, localization, and segmentation of COVID-19 cases are primarily centered on educational and training contexts. We deliberate on their merits and constraints, particularly in the context of necessitating cross-population train/test models. Our analysis encompassed a review of 231 research publications, bolstered by a meta-analysis employing search keywords (", "journal": "Healthcare (Basel, Switzerland)", "date": "2023-09-09", "authors": ["K CSantosh", "DebasmitaGhoshRoy", "SuprimNakarmi"], "doi": "10.3390/healthcare11172388\n10.1038/s41586-020-2008-3\n10.1142/S0218001422520103\n10.1016/j.ijid.2020.02.060\n10.1007/s10916-020-01645-z\n10.1016/S0140-6736(20)30183-5\n10.1136/bmj.m800\n10.1016/S0140-6736(20)30627-9\n10.1016/j.ijsu.2020.02.034\n10.1016/j.ins.2022.01.062\n10.1016/c2020-0-00344-0\n10.1007/s42979-020-00195-y\n10.18280/ria.330605\n10.1016/j.dsx.2020.05.008\n10.1038/s41591-020-0824-5\n10.1148/radiol.2020201365\n10.1109/RBME.2020.2990959\n10.2307/1592697\n10.1080/03079457.2012.680432\n10.1056/NEJMoa030747\n10.1016/bs.aivir.2018.01.001\n10.1056/NEJMoa1211721\n10.1073/pnas.57.4.933\n10.1186/s12985-015-0439-5\n10.1080/07391102.2020.1758788\n10.1080/07391102.2020.1848634\n10.1109/RBME.2020.2987975\n10.1016/S2589-7500(20)30054-6\n10.1016/j.dsx.2020.04.012\n10.1371/journal.pmed.1000100\n10.1016/j.drudis.2018.01.039\n10.1038/s41591-018-0316-z\n10.1038/nbt.4233\n10.1146/annurev-bioeng-071516-044442\n10.1038/nature14539\n10.1259/bjro.20190031\n10.1016/j.ibmed.2020.100013\n10.1007/s42399-020-00528-1\n10.1016/j.chaos.2020.110338\n10.1109/ACCESS.2021.3058537\n10.1109/TMI.2020.2996645\n10.1186/s40537-020-00392-9\n10.1109/TMI.2020.2995508\n10.1117/12.2580641\n10.1038/s41598-020-80787-z\n10.3390/computers10010006\n10.1162/neco.1989.1.4.541\n10.1007/s00330-020-07044-9\n10.1101/2020.04.13.20063941\n10.1007/s00521-020-05437-x\n10.1007/s00259-020-04929-1\n10.1148/ryai.2020200048\n10.1007/s10096-020-03901-z\n10.1038/s41551-020-00633-5\n10.1080/07391102.2020.1788642\n10.1109/ACCESS.2020.3025164\n10.1109/ACCESS.2020.3018498\n10.1109/TMI.2020.2996256\n10.1109/JBHI.2020.3042523\n10.3389/fmed.2020.608525\n10.1155/2020/8843664\n10.1016/j.ejrad.2020.109402\n10.1016/j.imu.2020.100427\n10.1016/j.irbm.2020.05.003\n10.1016/j.ejrad.2020.109041\n10.1109/TBDATA.2020.3035935\n10.1109/JBHI.2020.3030853\n10.1148/radiol.2020200905\n10.1148/radiol.2020202439\n10.1038/s41467-020-18685-1\n10.1109/ACCESS.2020.3001973\n10.1109/TMI.2020.2995965\n10.1109/ACCESS.2020.3033069\n10.1109/ACCESS.2020.3040245\n10.1016/j.chaos.2020.110190\n10.1016/j.compbiomed.2020.104092\n10.1016/j.compbiomed.2020.104037\n10.1016/j.patrec.2020.10.001\n10.2196/19569\n10.7717/peerj-cs.303\n10.1109/TMI.2020.2994908\n10.32604/cmes.2020.011920\n10.1109/ACCESS.2020.3005510\n10.1016/j.eng.2020.04.010\n10.1183/13993003.00775-2020\n10.1109/TMI.2020.2992546\n10.1038/s41598-020-76282-0\n10.1148/radiol.2020201491\n10.1371/journal.pone.0236621\n10.1007/s11356-020-10133-3\n10.1016/j.compbiomed.2021.104348\n10.1016/j.media.2021.102054\n10.1007/s00138-020-01128-8\n10.1109/TCBB.2021.3065361\n10.1007/s00330-021-07797-x\n10.1007/s00521-021-06344-5\n10.1007/s00530-021-00826-1\n10.1007/s10278-021-00431-8\n10.1007/s10489-020-01965-0\n10.1007/s10489-021-02393-4\n10.1007/s11042-021-11158-7\n10.1007/s11042-021-11299-9\n10.1007/s11042-021-11319-8\n10.1007/s12652-021-02967-7\n10.1186/s12967-021-02992-2\n10.1007/s13755-021-00140-0\n10.1007/s40846-021-00630-2\n10.1038/s41467-020-20657-4\n10.1038/s41598-020-80261-w\n10.1038/s41598-021-99015-3\n10.1007/s11548-020-02286-w\n10.1016/j.bspc.2021.102588\n10.1038/s41746-020-00369-1\n10.1007/s42979-021-00785-4\n10.21037/atm-21-1156\n10.1016/j.patcog.2021.108071\n10.1109/TVCG.2021.3114851\n10.3390/diagnostics11010041\n10.3390/diagnostics11091712\n10.3390/diagnostics11091732\n10.1007/s12539-020-00408-1\n10.1007/s11042-020-10010-8\n10.1111/exsy.12742\n10.1111/exsy.12776\n10.3389/frai.2021.598932\n10.3390/healthcare9091099\n10.2147/IDR.S296346\n10.1117/1.JMI.8.S1.014502\n10.1016/j.patcog.2021.107826\n10.1016/j.bbe.2021.06.011\n10.1016/j.inffus.2020.11.005\n10.1016/j.csbj.2021.02.016\n10.1016/j.neucom.2021.06.012\n10.1016/j.ymeth.2021.07.001\n10.1016/j.eswa.2021.115805\n10.1016/j.irbm.2021.01.004\n10.1016/j.compbiomed.2021.105014\n10.1016/j.compbiomed.2021.104835\n10.1016/j.ibmed.2021.100027\n10.1016/j.cmpbup.2021.100022\n10.1016/j.patrec.2021.10.027\n10.1016/j.compbiomed.2021.104857\n10.1016/j.asoc.2020.106912\n10.1016/j.compbiomed.2021.104895\n10.1016/j.imu.2021.100681\n10.1016/j.knosys.2021.106849\n10.1016/j.patrec.2021.06.021\n10.1002/mp.15231\n10.1371/journal.pone.0259179\n10.1186/s12859-021-04083-x\n10.3390/s21020455\n10.3390/s21062215\n10.3390/s21217286\n10.3233/shti210617\n10.1007/s00330-020-07553-7\n10.3233/XST-200784\n10.18280/ts.380115\n10.1007/s00521-021-05910-1\n10.32604/cmc.2021.014767\n10.1007/s10489-021-02292-8\n10.1007/s11265-021-01714-7\n10.1007/s12559-021-09915-9\n10.1007/s12652-021-03282-x\n10.1038/s41597-021-00900-3\n10.1038/s41598-021-83424-5\n10.1038/s41598-021-93658-y\n10.3390/app11094233\n10.3390/app11157004\n10.1155/2021/5527271\n10.1109/TII.2021.3057524\n10.1109/TBDATA.2021.3056564\n10.3390/diagnostics11081405\n10.3906/elk-2105-243\n10.3390/e23020204\n10.1088/1361-6560/ac34b2\n10.7150/ijbs.53982\n10.1002/ima.22558\n10.1155/2021/5528441\n10.3390/jpm11101008\n10.1016/j.compbiomed.2021.104425\n10.1016/j.patcog.2021.107828\n10.1016/j.bbe.2021.10.004\n10.1016/j.asoc.2021.107918\n10.1016/j.bspc.2021.102518\n10.1016/j.comcom.2021.06.011\n10.1016/j.inffus.2020.10.004\n10.1016/j.neucom.2020.07.144\n10.1016/j.media.2020.101836\n10.1007/s13246-021-01075-2\n10.3390/s21062174\n10.32604/cmc.2020.012585\n10.32604/cmc.2021.018514\n10.32604/cmc.2021.017337\n10.32604/cmc.2022.018547\n10.32604/iasc.2021.016800\n10.32604/cmc.2021.017385\n10.1016/j.bbe.2021.05.013\n10.1016/j.asoc.2020.106897\n10.1007/s10489-020-01826-w\n10.1007/s00330-020-07156-2\n10.1117/12.2588672\n10.32604/cmc.2022.019443\n10.1007/s00330-021-07715-1\n10.1007/s00521-022-07250-0\n10.1007/s00521-022-07052-4\n10.1177/1063293X211021435\n10.1007/s10439-022-02958-5\n10.1007/s10462-021-10127-8\n10.1007/s10723-022-09615-0\n10.1007/s11042-022-12484-0\n10.1007/s12065-022-00777-0\n10.1007/s12553-022-00677-4\n10.1007/s12652-022-03901-1\n10.1186/s12911-022-02022-1\n10.1007/s13369-022-07271-w\n10.1007/s13721-022-00382-2\n10.21037/atm-22-534\n10.1155/2022/1658615\n10.1111/exsy.13010\n10.3389/frai.2022.919672\n10.3390/life12070958\n10.1016/j.compbiomed.2022.105383\n10.1016/j.compbiomed.2021.105127\n10.1016/j.artmed.2022.102427\n10.1016/j.eswa.2022.116540\n10.1016/j.displa.2022.102150\n10.1016/j.asoc.2022.108765\n10.1016/j.compbiomed.2022.105298\n10.1016/j.imu.2022.101059\n10.1016/j.compbiomed.2022.105464\n10.1016/j.compbiomed.2022.105806\n10.1016/j.compbiomed.2022.105604\n10.1016/j.compbiomed.2022.105340\n10.1016/j.eswa.2022.118227\n10.1016/j.media.2022.102459\n10.1016/j.compbiomed.2021.105182\n10.1016/j.ipm.2022.103025\n10.3390/tomography8020071\n10.3390/tomography8030134\n10.1016/j.compbiomed.2022.105461\n10.1038/s41598-022-05532-0\n10.1155/2022/9771212\n10.1155/2022/1307944\n10.1109/JBHI.2022.3196489\n10.1155/2022/4509394\n10.3390/healthcare10010166\n10.1016/j.compeleceng.2022.108266\n10.1016/j.jksuci.2021.07.005\n10.1016/j.asoc.2022.109683\n10.3390/app12157554\n10.1109/ACCESS.2022.3176883\n10.3390/biology11010033\n10.3390/math10214160\n10.1097/RCT.0000000000001303\n10.3390/diagnostics12051283\n10.3389/fgene.2022.980338\n10.3389/fmed.2021.729287\n10.3389/fmed.2022.940960\n10.1007/s10489-022-03893-7\n10.1007/s10522-021-09946-7\n10.1038/s41598-022-06802-7\n10.32604/cmc.2022.018487\n10.3390/v14081667\n10.32604/cmc.2022.019354\n10.32604/csse.2022.022158\n10.32604/iasc.2022.020386\n10.1007/s10916-021-01747-2\n10.1007/s10916-020-01562-1"}
{"title": "Development and external validation of a mixed-effects deep learning model to diagnose COVID-19 from CT imaging.", "abstract": "The automatic analysis of medical images has the potential improve diagnostic accuracy while reducing the strain on clinicians. Current methods analyzing 3D-like imaging data, such as computerized tomography imaging, often treat each image slice as individual slices. This may not be able to appropriately model the relationship between slices.\nOur proposed method utilizes a mixed-effects model within the deep learning framework to model the relationship between slices. We externally validated this method on a data set taken from a different country and compared our results against other proposed methods. We evaluated the discrimination, calibration, and clinical usefulness of our model using a range of measures. Finally, we carried out a sensitivity analysis to demonstrate our methods robustness to noise and missing data.\nIn the external geographic validation set our model showed excellent performance with an AUROC of 0.930 (95%CI: 0.914, 0.947), with a sensitivity and specificity, PPV, and NPV of 0.778 (0.720, 0.828), 0.882 (0.853, 0.908), 0.744 (0.686, 0.797), and 0.900 (0.872, 0.924) at the 0.5 probability cut-off point. Our model also maintained good calibration in the external validation dataset, while other methods showed poor calibration.\nDeep learning can reduce stress on healthcare systems by automatically screening CT imaging for COVID-19. Our method showed improved generalizability in external validation compared to previous published methods. However, deep learning models must be robustly assessed using various performance measures and externally validated in each setting. In addition, best practice guidelines for developing and reporting predictive models are vital for the safe adoption of such models.", "journal": "Frontiers in medicine", "date": "2023-09-08", "authors": ["JoshuaBridge", "YandaMeng", "WenyueZhu", "ThomasFitzmaurice", "CarolineMcCann", "CliffAddison", "ManhuiWang", "CristinMerritt", "StuFranks", "MariaMackey", "SteveMessenger", "RenrongSun", "YitianZhao", "YalinZheng"], "doi": "10.3389/fmed.2023.1113030\n10.1136/bmj.n494\n10.1136/bmj.m1808\n10.1186/s12916-020-01810-8\n10.1097/CM9.0000000000000788\n10.1016/j.jcct.2020.08.013\n10.1148/radiol.2020200905\n10.1148/radiol.2020201491\n10.1038/s41598-017-16620-x\n10.3390/jimaging6060044\n10.1016/j.ejro.2020.100295\n10.22037/aaem.v8i1.665\n10.1155/2020/5436025\n10.1136/bmj.m1328\n10.1038/s42256-021-00307-0\n10.1136/bmj.g7594\n10.1148/ryai.2020200029\n10.7326/M18-1376\n10.1136/bmj.b2393\n10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2\n10.1097/EDE.0b013e3181c30fb2\n10.1186/s12916-019-1466-7\n10.1016/j.jclinepi.2015.12.005\n10.2307/2531595\n10.1214/ss/1009213286\n10.1186/s12916-019-1425-3\n10.1177/0272989X06295361\n10.48550/arXiv.1706.03825\n10.48550/arXiv.1603.04467\n10.1186/1471-2105-12-77\n10.48550/arXiv.1412.6980\n10.1002/sim.7992\n10.17816/DD46826\n10.1016/j.cell.2020.04.045\n10.48550/arXiv.1412.6572\n10.1016/j.jclinepi.2021.02.011\n10.1038/s41591-021-01517-0\n10.1136/bmjopen-2020-048008"}
{"title": "FP-CNN: Fuzzy pooling-based convolutional neural network for lung ultrasound image classification with explainable AI.", "abstract": "The COVID-19 pandemic wreaks havoc on healthcare systems all across the world. In pandemic scenarios like COVID-19, the applicability of diagnostic modalities is crucial in medical diagnosis, where non-invasive ultrasound imaging has the potential to be a useful biomarker. This research develops a computer-assisted intelligent methodology for ultrasound lung image classification by utilizing a fuzzy pooling-based convolutional neural network FP-CNN with underlying evidence of particular decisions. The fuzzy-pooling method finds better representative features for ultrasound image classification. The FPCNN model categorizes ultrasound images into one of three classes: covid, disease-free (normal), and pneumonia. Explanations of diagnostic decisions are crucial to ensure the fairness of an intelligent system. This research has used Shapley Additive Explanation (SHAP) to explain the prediction of the FP-CNN models. The prediction of the black-box model is illustrated using the SHAP explanation of the intermediate layers of the black-box model. To determine the most effective model, we have tested different state-of-the-art convolutional neural network architectures with various training strategies, including fine-tuned models, single-layer fuzzy pooling models, and fuzzy pooling at all pooling layers. Among different architectures, the Xception model with all pooling layers having fuzzy pooling achieves the best classification results of 97.2% accuracy. We hope our proposed method will be helpful for the clinical diagnosis of covid-19 from lung ultrasound (LUS) images.", "journal": "Computers in biology and medicine", "date": "2023-09-08", "authors": ["Md MahmodulHasan", "Muhammad MinoarHossain", "Mohammad MotiurRahman", "AkmAzad", "Salem AAlyami", "Mohammad AliMoni"], "doi": "10.1016/j.compbiomed.2023.107407"}
{"title": "Image-based and machine learning-guided multiplexed serology test for SARS-CoV-2.", "abstract": "We present a miniaturized immunofluorescence assay (mini-IFA) for measuring antibody response in patient blood samples. The method utilizes machine learning-guided image analysis and enables simultaneous measurement of immunoglobulin M (IgM), IgA, and IgG responses against different viral antigens in an automated and high-throughput manner. The assay relies on antigens expressed through transfection, enabling use at a low biosafety level and fast adaptation to emerging pathogens. Using severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) as the model pathogen, we demonstrate that this method allows differentiation between vaccine-induced and infection-induced antibody responses. Additionally, we established a dedicated web page for quantitative visualization of sample-specific results and their distribution, comparing them with controls and other samples. Our results provide a proof of concept for the approach, demonstrating fast and accurate measurement of antibody responses in a research setup with prospects for clinical diagnostics.", "journal": "Cell reports methods", "date": "2023-09-06", "authors": ["ViljaPieti\u00e4inen", "MinttuPolso", "EdeMigh", "ChristianGuckelsberger", "MariaHarmati", "AkosDiosdi", "LauraTurunen", "AnttiHassinen", "SwapnilPotdar", "AnnikaKoponen", "Edina GyukitySebestyen", "FerencKovacs", "AndrasKriston", "RekaHollandi", "KatalinBurian", "GabriellaTerhes", "AdamVisnyovszki", "EszterFodor", "ZsomborLacza", "AnuKantele", "PekkaKolehmainen", "LauraKakkola", "TomasStrandin", "LevLevanov", "OlliKallioniemi", "LajosKemeny", "IlkkaJulkunen", "OlliVapalahti", "KrisztinaBuzas", "LassiPaavolainen", "PeterHorvath", "JussiHepojoki"], "doi": "10.1016/j.crmeth.2023.100565\n10.1002/bies.202000257\n10.1038/s41591-020-0913-5\n10.3390/v13020143\n10.1126/science.abd3072\n10.1101/2021.01.25.427969\n10.1038/nmeth.3323\n10.1016/j.cels.2020.04.003\n10.1016/j.cels.2017.05.012\n10.1177/1087057114527313\n10.1002/cpmc.100\n10.1001/jamapediatrics.2021.0001\n10.1177/00131644600200010\n10.1037/h0031619\n10.1176/appi.ajp.2012.12070999\n10.1093/bioinformatics/btaa138\n10.1001/jamanetworkopen.2021.4302\n10.1111/2049-632X.12034\n10.3390/pathogens10020138\n10.1038/s41591-020-0965-6\n10.2807/1560-7917.ES.2020.25.11.2000266"}
{"title": "[Corona virus disease 2019 lesion segmentation network based on an adaptive joint loss function].", "abstract": "Corona virus disease 2019 (COVID-19) is an acute respiratory infectious disease with strong contagiousness, strong variability, and long incubation period. The probability of misdiagnosis and missed diagnosis can be significantly decreased with the use of automatic segmentation of COVID-19 lesions based on computed tomography images, which helps doctors in rapid diagnosis and precise treatment. This paper introduced the level set generalized Dice loss function (LGDL) in conjunction with the level set segmentation method based on COVID-19 lesion segmentation network and proposed a dual-path COVID-19 lesion segmentation network (Dual-SAUNet++) to address the pain points such as the complex symptoms of COVID-19 and the blurred boundaries that are challenging to segment. LGDL is an adaptive weight joint loss obtained by combining the generalized Dice loss of the mask path and the mean square error of the level set path. On the test set, the model achieved Dice similarity coefficient of (87.81 \u00b1 10.86)%, intersection over union of (79.20 \u00b1 14.58)%, sensitivity of (94.18 \u00b1 13.56)%, specificity of (99.83 \u00b1 0.43)% and Hausdorff distance of 18.29 \u00b1 31.48 mm. Studies indicated that Dual-SAUNet++ has a great anti-noise capability and it can segment multi-scale lesions while simultaneously focusing on their area and border information. The method proposed in this paper assists doctors in judging the severity of COVID-19 infection by accurately segmenting the lesion, and provides a reliable basis for subsequent clinical treatment.\n\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u611f\u67d3\u662f\u4e00\u79cd\u4f20\u67d3\u6027\u5f3a\u3001\u53d8\u5f02\u6027\u5f3a\u3001\u6f5c\u4f0f\u671f\u957f\u7684\u6025\u6027\u547c\u5438\u9053\u4f20\u67d3\u75c5\u3002\u57fa\u4e8e\u7535\u5b50\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u6210\u50cf\u7684\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u611f\u67d3\u75c5\u7076\u81ea\u52a8\u5206\u5272\u53ef\u4ee5\u8f85\u52a9\u533b\u751f\u8fdb\u884c\u5feb\u901f\u8bca\u65ad\u548c\u7cbe\u786e\u6cbb\u7597\uff0c\u80fd\u6709\u6548\u5730\u51cf\u5c11\u8bef\u8bca\u6f0f\u8bca\u7684\u98ce\u9669\u3002\u9488\u5bf9\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u611f\u67d3\u75c5\u7076\u5f81\u8c61\u590d\u6742\u4e14\u8fb9\u754c\u6a21\u7cca\u96be\u4ee5\u5206\u5272\u7b49\u75db\u70b9\uff0c\u672c\u6587\u5728\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u611f\u67d3\u75c5\u7076\u5206\u5272\u7f51\u7edc\u7684\u57fa\u7840\u4e0a\u7ed3\u5408\u6c34\u5e73\u96c6\u5206\u5272\u65b9\u6cd5\u5f15\u5165\u4e86\u6c34\u5e73\u96c6\u5e7f\u4e49\u9ab0\u5b50\u635f\u5931\u51fd\u6570\uff08LGDL\uff09\uff0c\u63d0\u51fa\u4e86\u53cc\u8def\u5f84\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u611f\u67d3\u75c5\u7076\u5206\u5272\u7f51\u7edc\uff08Dual-SAUNet++\uff09\uff0c\u5176\u4e2dLGDL\u662f\u7531\u63a9\u819c\u8def\u5f84\u7684\u5e7f\u4e49\u9ab0\u5b50\u635f\u5931\u548c\u6c34\u5e73\u96c6\u8def\u5f84\u7684\u5747\u65b9\u8bef\u5dee\u8054\u5408\u6240\u5f97\u7684\u81ea\u9002\u5e94\u6743\u91cd\u635f\u5931\u3002\u672c\u6587\u6240\u63d0\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u7684\u6234\u65af\u76f8\u4f3c\u7cfb\u6570\u4e3a\uff0887.81 \u00b1 10.86\uff09%\uff0c\u4ea4\u5e76\u6bd4\u4e3a\uff0879.20 \u00b1 14.58\uff09%\uff0c\u654f\u611f\u5ea6\u4e3a\uff0894.18 \u00b1 13.56\uff09%\uff0c\u7279\u5f02\u5ea6\u4e3a\uff0899.83 \u00b1 0.43\uff09%\uff0c\u8c6a\u65af\u591a\u592b\u8ddd\u79bb\u4e3a\uff0818.29 \u00b1 31.48\uff09 mm\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cDual-SAUNet++\u80fd\u591f\u540c\u65f6\u5173\u6ce8\u75c5\u7076\u7684\u9762\u79ef\u548c\u8fb9\u754c\u4fe1\u606f\uff0c\u53ef\u4ee5\u6709\u6548\u5206\u5272\u51fa\u591a\u5c3a\u5ea6\u75c5\u7076\u4e14\u5177\u6709\u8f83\u5f3a\u7684\u6297\u566a\u80fd\u529b\u3002\u7efc\u4e0a\uff0c\u672c\u6587\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u7cbe\u786e\u5206\u5272\u75c5\u7076\u533a\u57df\uff0c\u53ef\u8f85\u52a9\u533b\u751f\u5224\u65ad\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u611f\u67d3\u7684\u4e25\u91cd\u7a0b\u5ea6\uff0c\u4e3a\u540e\u7eed\u4e34\u5e8a\u6cbb\u7597\u63d0\u4f9b\u53ef\u9760\u4f9d\u636e\u3002.", "journal": "Sheng wu yi xue gong cheng xue za zhi = Journal of biomedical engineering = Shengwu yixue gongchengxue zazhi", "date": "2023-09-05", "authors": ["HanguangXiao", "HuanqiLi", "ZhiqiangRan", "QihangZhang", "BolongZhang", "YujiaWei", "XiuhongZhu"], "doi": "10.7507/1001-5515.202206051\n10.1109/ACCESS.2020.3027738\n10.1016/j.eng.2020.04.010\n10.1016/j.neucom.2015.05.044\n10.1007/s11704-015-4543-x\n10.1016/j.compmedimag.2015.07.003\n10.7507/1001-5515.202008032\n10.1007/s11760-022-02183-6\n10.1016/j.bspc.2021.102800\n10.3390/app11178039\n10.7717/peerj-cs.349\n10.1109/TMI.2020.3000314\n10.1002/ima.22527\n10.1609/aaai.v35i10.17066\n10.1109/TMI.2020.2996645\n10.3389/fbioe.2020.605132"}
{"title": "Complex network-based classification of radiographic images for COVID-19 diagnosis.", "abstract": "In this work, we present a network-based technique for chest X-ray image classification to help the diagnosis and prognosis of patients with COVID-19. From visual inspection, we perceive that healthy and COVID-19 chest radiographic images present different levels of geometric complexity. Therefore, we apply fractal dimension and quadtree as feature extractors to characterize such differences. Moreover, real-world datasets often present complex patterns, which are hardly handled by only the physical features of the data (such as similarity, distance, or distribution). This issue is addressed by complex networks, which are suitable tools for characterizing data patterns and capturing spatial, topological, and functional relationships in data. Specifically, we propose a new approach combining complexity measures and complex networks to provide a modified high-level classification technique to be applied to COVID-19 chest radiographic image classification. The computational results on the Kaggle COVID-19 Radiography Database show that the proposed method can obtain high classification accuracy on X-ray images, being competitive with state-of-the-art classification techniques. Lastly, a set of network measures is evaluated according to their potential in distinguishing the network classes, which resulted in the choice of communicability measure. We expect that the present work will make significant contributions to machine learning at the semantic level and to combat COVID-19.", "journal": "PloS one", "date": "2023-09-01", "authors": ["WeiguangLiu", "RafaelDelalibera Rodrigues", "JianglongYan", "Yu-TaoZhu", "Everson Jos\u00e9de Freitas Pereira", "GenLi", "QiushengZheng", "LiangZhao"], "doi": "10.1371/journal.pone.0290968\n10.1016/S1473-3099(20)30484-9\n10.1038/s41579-018-0118-9\n10.1038/s41579-020-00459-7\n10.1016/S0140-6736(20)30183-5\n10.1126/science.abb3221\n10.1016/j.ijid.2020.01.050\n10.1101/2020.01.23.20018549v2\n10.21037/atm-20-5248\n10.1109/TNNLS.2012.2195027\n10.1038/30918\n10.1126/science.286.5439.509\n10.1103/RevModPhys.74.47\n10.1137/S003614450342480\n10.1126/science.156.3775.636\n10.1007/BF00288933\n10.1016/j.ins.2014.09.048\n10.1109/TNNLS.2017.2726082\n10.1103/PhysRevE.77.036111\n10.1186/s41747-020-00173-2\n10.1145/3397391.3397412\n10.3390/jpm12020309\n10.1038/s41598-020-76282-0\n10.3390/diagnostics11122208\n10.1007/s10044-020-00950-0\n10.3390/s21217286\n10.3390/app11199023\n10.2174/18756603MTA3nMTAc5\n10.1038/s41598-021-03287-8\n10.1016/j.compbiomed.2022.106156\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1140/epjs/s11734-021-00159-0"}
{"title": "Comparative performances of machine learning algorithms in radiomics and impacting factors.", "abstract": "There are no current recommendations on which machine learning (ML) algorithms should be used in radiomics. The objective was to compare performances of ML algorithms in radiomics when applied to different clinical questions to determine whether some strategies could give the best and most stable performances regardless of datasets. This study compares the performances of nine feature selection algorithms combined with fourteen binary classification algorithms on ten datasets. These datasets included radiomics features and clinical diagnosis for binary clinical classifications including COVID-19 pneumonia or sarcopenia on CT, head and neck, orbital or uterine lesions on MRI. For each dataset, a train-test split was created. Each of the 126 (9\u2009\u00d7\u200914) combinations of feature selection algorithms and classification algorithms was trained and tuned using a ten-fold cross validation, then AUC was computed. This procedure was repeated three times per dataset. Best overall performances were obtained with JMI and JMIM as feature selection algorithms and random forest and linear regression models as classification algorithms. The choice of the classification algorithm was the factor explaining most of the performance variation (10% of total variance). The choice of the feature selection algorithm explained only 2% of variation, while the train-test split explained 9%.", "journal": "Scientific reports", "date": "2023-08-29", "authors": ["AntoineDecoux", "LoicDuron", "PaulHabert", "VictoireRoblot", "EminaArsovic", "GuillaumeChassagnon", "ArmelleArnoux", "LaureFournier"], "doi": "10.1038/s41598-023-39738-7\n10.1038/s41416-019-0699-8\n10.1038/nrclinonc.2017.141\n10.1186/s40779-023-00458-8\n10.1148/ryai.2020200029\n10.1038/s42256-021-00307-0\n10.1007/s00330-022-09187-3\n10.1038/s41746-022-00592-y\n10.1007/s00330-018-5695-5\n10.1016/j.neuroimage.2017.06.061\n10.1016/j.media.2020.101860\n10.1097/RLI.0000000000000722\n10.1007/s00330-022-08579-9\n10.3348/kjr.2018.0070\n10.1080/01621459.1937.10503522\n10.1109/ACCESS.2019.2928975\n10.1016/j.medmic.2020.100013\n10.1038/srep13087\n10.1371/journal.pone.0256152\n10.1186/1471-2105-7-91\n10.1186/s13040-017-0154-4"}
{"title": "Automated Lung Ultrasound Pulmonary Disease Quantification Using an Unsupervised Machine Learning Technique for COVID-19.", "abstract": "COVID-19 is an ongoing global health pandemic. Although COVID-19 can be diagnosed with various tests such as PCR, these tests do not establish pulmonary disease burden. Whereas point-of-care lung ultrasound (POCUS) can directly assess the severity of characteristic pulmonary findings of COVID-19, the advantage of using US is that it is inexpensive, portable, and widely available for use in many clinical settings. For automated assessment of pulmonary findings, we have developed an unsupervised learning technique termed the calculated lung ultrasound (CLU) index. The CLU can quantify various types of lung findings, such as A or B lines, consolidations, and pleural effusions, and it uses these findings to calculate a CLU index score, which is a quantitative measure of pulmonary disease burden. This is accomplished using an unsupervised, patient-specific approach that does not require training on a large dataset. The CLU was tested on 52 lung ultrasound examinations from several institutions. CLU demonstrated excellent concordance with radiologist findings in different pulmonary disease states. Given the global nature of COVID-19, the CLU would be useful for sonographers and physicians in resource-strapped areas with limited ultrasound training and diagnostic capacities for more accurate assessment of pulmonary status.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-08-26", "authors": ["HershSagreiya", "Michael AJacobs", "AlirezaAkhbardeh"], "doi": "10.3390/diagnostics13162692\n10.2214/AJR.20.23034\n10.1016/j.clinimag.2020.04.001\n10.1148/radiol.2020200988\n10.1136/oemed-2020-106731\n10.1378/chest.14-1313\n10.1016/j.ultrasmedbio.2020.05.012\n10.1186/s13244-019-0775-x\n10.1148/radiol.2020200847\n10.1111/1742-6723.13546\n10.1371/journal.pone.0236312\n10.1002/jum.15285\n10.1016/j.acra.2020.07.002\n10.1007/s00134-020-06212-1\n10.26355/eurrev_202003_20549\n10.4103/0970-2113.156245\n10.3390/jcm10153255\n10.1118/1.3682173\n10.1109/TSMC.1979.4310076\n10.1016/0165-1684(94)90060-4\n10.1007/s00134-020-05996-6\n10.1186/s13089-015-0028-2\n10.1109/TUFFC.2021.3107598\n10.1371/journal.pone.0255886\n10.1121/10.0004855\n10.1121/10.0007272\n10.3390/s21165486\n10.1109/ACCESS.2020.3016780\n10.3390/jimaging8030065\n10.1007/s10396-020-01074-y\n10.1136/bmj-2021-069679\n10.1007/s40477-021-00636-1\n10.1186/s13075-019-1888-9\n10.1016/j.jacr.2021.08.022\n10.1016/j.rmed.2021.106384"}
{"title": "Self-Supervised Learning Application on COVID-19 Chest X-ray Image Classification Using Masked AutoEncoder.", "abstract": "The COVID-19 pandemic has underscored the urgent need for rapid and accurate diagnosis facilitated by artificial intelligence (AI), particularly in computer-aided diagnosis using medical imaging. However, this context presents two notable challenges: high diagnostic accuracy demand and limited availability of medical data for training AI models. To address these issues, we proposed the implementation of a Masked AutoEncoder (MAE), an innovative self-supervised learning approach, for classifying 2D Chest X-ray images. Our approach involved performing imaging reconstruction using a Vision Transformer (ViT) model as the feature encoder, paired with a custom-defined decoder. Additionally, we fine-tuned the pretrained ViT encoder using a labeled medical dataset, serving as the backbone. To evaluate our approach, we conducted a comparative analysis of three distinct training methods: training from scratch, transfer learning, and MAE-based training, all employing COVID-19 chest X-ray images. The results demonstrate that MAE-based training produces superior performance, achieving an accuracy of 0.985 and an AUC of 0.9957. We explored the mask ratio influence on MAE and found ratio = 0.4 shows the best performance. Furthermore, we illustrate that MAE exhibits remarkable efficiency when applied to labeled data, delivering comparable performance to utilizing only 30% of the original training dataset. Overall, our findings highlight the significant performance enhancement achieved by using MAE, particularly when working with limited datasets. This approach holds profound implications for future disease diagnosis, especially in scenarios where imaging information is scarce.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2023-08-26", "authors": ["XinXing", "GongboLiang", "ChrisWang", "NathanJacobs", "Ai-LingLin"], "doi": "10.3390/bioengineering10080901\n10.1007/s12559-022-10072-w\n10.1155/2022/5052435\n10.3390/electronics12020467\n10.1088/1742-6596/1168/2/022022\n10.1016/j.jacr.2020.01.006\n10.1145/3065386\n10.1109/JPROC.2020.3004555\n10.1093/nsr/nwx106\n10.1038/s41598-020-76550-z\n10.1007/s42979-021-00782-7\n10.1038/s42003-020-1079-x\n10.1016/j.media.2022.102529"}
{"title": "Biased Deep Learning Methods in Detection of COVID-19 Using CT Images: A Challenge Mounted by Subject-Wise-Split ISFCT Dataset.", "abstract": "Accurate detection of respiratory system damage including COVID-19 is considered one of the crucial applications of deep learning (DL) models using CT images. However, the main shortcoming of the published works has been unreliable reported accuracy and the lack of repeatability with new datasets, mainly due to slice-wise splits of the data, creating dependency between training and test sets due to shared data across the sets. We introduce a new dataset of CT images (ISFCT Dataset) with labels indicating the subject-wise split to train and test our DL algorithms in an unbiased manner. We also use this dataset to validate the real performance of the published works in a subject-wise data split. Another key feature provides more specific labels (eight characteristic lung features) rather than being limited to COVID-19 and healthy labels. We show that the reported high accuracy of the existing models on current slice-wise splits is not repeatable for subject-wise splits, and distribution differences between data splits are demonstrated using t-distribution stochastic neighbor embedding. We indicate that, by examining subject-wise data splitting, less complicated models show competitive results compared to the exiting complicated models, demonstrating that complex models do not necessarily generate accurate and repeatable results.", "journal": "Journal of imaging", "date": "2023-08-25", "authors": ["ShivaParsarad", "NargesSaeedizadeh", "Ghazaleh JamalipourSoufi", "ShamimShafieyoon", "FarzanehHekmatnia", "Andrew ParvizZarei", "SamiraSoleimany", "AmirYousefi", "HengamehNazari", "PegahTorabi", "AbbasS Milani", "Seyed AliMadani Tonekaboni", "HosseinRabbani", "AliHekmatnia", "RaheleKafieh"], "doi": "10.3390/jimaging9080159\n10.1109/RBME.2020.2987975\n10.1148/radiol.2020200370\n10.1007/s00521-020-05437-x\n10.1109/ACCESS.2020.3003810\n10.1093/gigascience/gix019\n10.1038/s41746-019-0178-x\n10.2196/27468\n10.1038/s41598-021-93832-2\n10.1016/j.cell.2020.04.045\n10.1016/j.bbe.2021.05.013\n10.59275/j.melba.2020-48g7\n10.1101/2020.07.08.20149161\n10.1155/2022/7672196\n10.1007/s10522-021-09946-7\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1007/s10439-022-02958-5\n10.1038/s41598-022-05532-0\n10.1136/bmjopen-2020-044640\n10.3389/fpubh.2020.00152\n10.1148/rg.2018170048\n10.53347/rID-66157\n10.1148/rg.236035101\n10.1186/s13244-019-0789-4\n10.1016/j.imu.2020.100427\n10.1016/j.cobme.2018.12.005"}
{"title": "Predicting intensive care need for COVID-19 patients using deep learning on chest radiography.", "abstract": "Image-based prediction of coronavirus disease 2019 (COVID-19) severity and resource needs can be an important means to address the COVID-19 pandemic. In this study, we propose an artificial intelligence/machine learning (AI/ML) COVID-19 prognosis method to predict patients' needs for intensive care by analyzing chest X-ray radiography (CXR) images using deep learning.\nThe dataset consisted of 8357 CXR exams from 5046 COVID-19-positive patients as confirmed by reverse transcription polymerase chain reaction (RT-PCR) tests for the SARS-CoV-2 virus with a training/validation/test split of 64%/16%/20% on a by patient level. Our model involved a DenseNet121 network with a sequential transfer learning technique employed to train on a sequence of gradually more specific and complex tasks: (1)\u00a0fine-tuning a model pretrained on ImageNet using a previously established CXR dataset with a broad spectrum of pathologies; (2)\u00a0refining on another established dataset to detect pneumonia; and (3)\u00a0fine-tuning using our in-house training/validation datasets to predict patients' needs for intensive care within 24, 48, 72, and 96\u00a0h following the CXR exams. The classification performances were evaluated on our independent test set (CXR exams of 1048 patients) using the area under the receiver operating characteristic curve (AUC) as the figure of merit in the task of distinguishing between those COVID-19-positive patients who required intensive care following the imaging exam and those who did not.\nOur proposed AI/ML model achieved an AUC (95% confidence interval) of 0.78 (0.74, 0.81) when predicting the need for intensive care 24\u00a0h in advance, and at least 0.76 (0.73, 0.80) for 48\u00a0h or more in advance using predictions based on the AI prognostic marker derived from CXR images.\nThis AI/ML prediction model for patients' needs for intensive care has the potential to support both clinical decision-making and resource management.", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2023-08-23", "authors": ["HuiLi", "KarenDrukker", "QiyuanHu", "Heather MWhitney", "Jordan DFuhrman", "Maryellen LGiger"], "doi": "10.1117/1.JMI.10.4.044504\n10.7861/clinmed.2019-coron\n10.1148/radiol.2020203173\n10.1002/mp.13264\n10.3322/caac.21552\n10.1016/j.jacr.2017.12.028\n10.1016/j.jmir.2019.09.005\n10.1259/bjr.20190855\n10.1148/radiol.2020202944\n10.1117/1.JMI.8.S1.014503\n10.1148/radiol.2020201491\n10.1038/s41591-020-0931-3\n10.1038/s41598-022-15013-z\n10.1088/1361-6560/abbf9e\n10.1007/s11739-020-02475-0\n10.1148/ryai.2020200079\n10.2214/AJR.20.22976\n10.1117/1.JMI.8.S1.014501\n10.3174/ajnr.A7072\n10.1016/j.chest.2020.04.003\n10.1016/j.clinimag.2020.04.001\n10.1148/ryct.2020200034\n10.1186/s40560-021-00527-x\n10.1007/s10479-022-04984-x\n10.1111/jcmm.17098\n10.1038/s41598-022-07890-1\n10.1038/s41746-021-00456-x\n10.1038/s41598-021-83967-7\n10.7717/peerj.10337\n10.3390/jcm9061668\n10.1371/journal.pone.0236618\n10.1109/TMI.2016.2528162\n10.1002/mp.12453\n10.3390/diagnostics12061457\n10.1109/TMI.2018.2870343\n10.1117/1.JMI.3.3.034501\n10.1117/12.2083124\n10.3390/diagnostics12010135\n10.1109/CVPR.2017.243\n10.1371/journal.pmed.1002686\n10.1109/CVPR.2009.5206848\n10.1109/CVPR.2017.369\n10.1002/(SICI)1097-0258(19980515)17:9<1033::AID-SIM784>3.0.CO;2-Z\n10.1006/jmps.1998.1218\n10.1080/01621459.1987.10478410\n10.1109/ICCV.2017.74\n10.1080/01621459.1958.10501452\n10.4103/0974-7788.76794\n10.1016/j.acra.2008.04.022\n10.1038/s41746-021-00453-0\n10.1148/radiol.2020200905\n10.1148/radiol.2020201874\n10.1016/j.cmpb.2020.105581\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.109944\n10.1183/13993003.00775-2020\n10.1016/j.cell.2020.04.045\n10.1186/s42234-020-00050-8"}
{"title": "CoTrFuse: a novel framework by fusing CNN and transformer for medical image segmentation.", "abstract": "Medical image segmentation is a crucial and intricate process in medical image processing and analysis. With the advancements in artificial intelligence, deep learning techniques have been widely used in recent years for medical image segmentation. One such technique is the U-Net framework based on the U-shaped convolutional neural networks (CNN) and its variants. However, these methods have limitations in simultaneously capturing both the global and the remote semantic information due to the restricted receptive domain caused by the convolution operation's intrinsic features. Transformers are attention-based models with excellent global modeling capabilities, but their ability to acquire local information is limited. To address this, we propose a network that combines the strengths of both CNN and Transformer, called CoTrFuse. The proposed CoTrFuse network uses EfficientNet and Swin Transformer as dual encoders. The Swin Transformer and CNN Fusion module are combined to fuse the features of both branches before the skip connection structure. We evaluated the proposed network on two datasets: the ISIC-2017 challenge dataset and the COVID-QU-Ex dataset. Our experimental results demonstrate that the proposed CoTrFuse outperforms several state-of-the-art segmentation methods, indicating its superiority in medical image segmentation. The codes are available athttps://github.com/BinYCn/CoTrFuse.", "journal": "Physics in medicine and biology", "date": "2023-08-22", "authors": ["YuanbinChen", "TaoWang", "HuiTang", "LongxuanZhao", "XinlinZhang", "TaoTan", "QinquanGao", "MinDu", "TongTong"], "doi": "10.1088/1361-6560/acede8"}
{"title": "Clinical application of radiological AI for pulmonary nodule evaluation: Replicability and susceptibility to the population shift caused by the COVID-19 pandemic.", "abstract": "replicability and generalizability of medical AI are the recognized challenges that hinder a broad AI deployment in clinical practice. Pulmonary nodes detection and characterization based on chest CT images is one of the demanded use cases for automatization by means of AI, and multiple AI solutions addressing this task are becoming available. Here, we evaluated and compared the performance of several commercially available radiological AI with the same clinical task on the same external datasets acquired before and during the pandemic of COVID-19.\n5 commercially available AI models for pulmonary nodule detection were tested on two external datasets labelled by experts according to the intended clinical task. Dataset1 was acquired before the pandemic and did not contain radiological signs of COVID-19; dataset2 was collected during the pandemic and did contain radiological signs of COVID-19. ROC-analysis was applied separately for the dataset1 and dataset2 to select probability thresholds for each dataset separately. AUROC, sensitivity and specificity metrics were used to assess and compare the results of AI performance.\nStatistically significant differences in AUROC values were observed between the AI models for the dataset1. Whereas for the dataset2 the differences of AUROC values became statistically insignificant. Sensitivity and specificity differed statistically significantly between the AI models for the dataset1. This difference was insignificant for the dataset2 when we applied the probability threshold initially selected for the dataset1. An update of the probability threshold based on the dataset2 created statistically significant differences of sensitivity and specificity between AI models for the dataset2. For 3 out of 5 AI models, the update of the probability threshold was valuable to compensate for the degradation of AI model performances with the population shift caused by the pandemic.\nPopulation shift in the data is able to deteriorate differences of AI models performance. Update of the probability threshold together with the population shift seems to be valuable to preserve AI models performance without retraining them.", "journal": "International journal of medical informatics", "date": "2023-08-22", "authors": ["YuriyVasilev", "AntonVladzymyrskyy", "KirillArzamasov", "OlgaOmelyanskaya", "IgorShulkin", "DaryaKozikhina", "InnaGoncharova", "RomanReshetnikov", "SergeyChetverikov", "IvanBlokhin", "TatianaBobrovskaya", "AnnaAndreychenko"], "doi": "10.1016/j.ijmedinf.2023.105190"}
{"title": "GOLF-Net: Global and local association fusion network for COVID-19 lung infection segmentation.", "abstract": "The global spread of the Corona Virus Disease 2019 (COVID-19) has caused significant health hazards, leading researchers to explore new methods for detecting lung infections that can supplement molecular diagnosis. Computer tomography (CT) has emerged as a promising tool, although accurately segmenting infected areas in COVID-19 CT scans, especially given the limited available data, remains a challenge for deep learning models. To address this issue, we propose a novel segmentation network, the GlObal and Local association Fusion Network (GOLF-Net), that combines global and local features from Convolutional Neural Networks and Transformers, respectively. Our network leverages attention mechanisms to enhance the correlation and representation of local features, improving the accuracy of infected area segmentation. Additionally, we implement transfer learning to pretrain our network parameters, providing a robust solution to the issue of limited COVID-19 CT data. Our experimental results demonstrate that the segmentation performance of our network exceeds that of most existing models, with a Dice coefficient of 95.09% and an IoU of 92.58%. \u00a9 2014 Hosting by Elsevier B.V. All rights reserved.", "journal": "Computers in biology and medicine", "date": "2023-08-19", "authors": ["XinyuXu", "LinGao", "LiangYu"], "doi": "10.1016/j.compbiomed.2023.107361"}
{"title": "Development and validation of a hybrid deep learning-machine learning approach for severity assessment of COVID-19 and other pneumonias.", "abstract": "The Coronavirus Disease 2019 (COVID-19) is transitioning into the endemic phase. Nonetheless, it is crucial to remain mindful that pandemics related to infectious respiratory diseases (IRDs) can emerge unpredictably. Therefore, we aimed to develop and validate a severity assessment model for IRDs, including COVID-19, influenza, and novel influenza, using CT images on a multi-centre data set. Of the 805 COVID-19 patients collected from a single centre, 649 were used for training and 156 were used for internal validation (D1). Additionally, three external validation sets were obtained from 7 cohorts: 1138 patients with COVID-19 (D2), and 233 patients with influenza and novel influenza (D3). A hybrid model, referred to as Hybrid-DDM, was constructed by combining two deep learning models and a machine learning model. Across datasets D1, D2, and D3, the Hybrid-DDM exhibited significantly improved performance compared to the baseline model. The areas under the receiver operating curves (AUCs) were 0.830 versus 0.767 (p\u2009=\u20090.036) in D1, 0.801 versus 0.753 (p\u2009<\u20090.001) in D2, and 0.774 versus 0.668 (p\u2009<\u20090.001) in D3. This study indicates that the Hybrid-DDM model, trained using COVID-19 patient data, is effective and can also be applicable to patients with other types of viral pneumonia.", "journal": "Scientific reports", "date": "2023-08-18", "authors": ["DoohyunPark", "RyoungwooJang", "Myung JinChung", "Hyun JoonAn", "SeongwonBak", "EuijoonChoi", "DosikHwang"], "doi": "10.1038/s41598-023-40506-w\n10.1038/s41371-020-00451-x\n10.1038/s41574-020-00435-4\n10.12997/jla.2020.9.3.435\n10.1371/journal.pone.0241265\n10.5888/pcd18.210123\n10.1016/j.diii.2020.03.014\n10.5114/pjr.2019.85812\n10.1148/rg.2018170048\n10.1016/j.ejrad.2004.03.010\n10.1007/s00330-022-08869-2\n10.3390/jcm9051514\n10.3390/diagnostics13132223\n10.1038/s41598-023-31312-5\n10.1007/s10586-023-03972-5\n10.1148/radiol.2020202439\n10.1016/j.acra.2020.09.004\n10.1007/s00330-020-07042-x\n10.1016/j.media.2021.102054\n10.1038/s41598-022-07890-1\n10.1016/j.media.2020.101836\n10.1016/j.bspc.2023.104672\n10.1007/s10278-017-0028-9\n10.1007/978-3-319-24574-4_28\n10.1109/CVPR.2016.90\n10.1002/mp.14609\n10.1023/A:1010933404324\n10.2307/2531595\n10.2174/1573405617666210713113439\n10.1038/s41598-020-76550-z\n10.1016/j.ejrad.2020.109041\n10.1148/radiol.2020200274\n10.1148/radiol.2020200463\n10.34218/IJM.11.12.2020.204\n10.1136/thorax.57.5.438\n10.1186/s13052-020-0770-3"}
{"title": "Regional, circuit and network heterogeneity of brain abnormalities in psychiatric disorders.", "abstract": "The substantial individual heterogeneity that characterizes people with mental illness is often ignored by classical case-control research, which relies on group mean comparisons. Here we present a comprehensive, multiscale characterization of the heterogeneity of gray matter volume (GMV) differences in 1,294 cases diagnosed with one of six conditions (attention-deficit/hyperactivity disorder, autism spectrum disorder, bipolar disorder, depression, obsessive-compulsive disorder and schizophrenia) and 1,465 matched controls. Normative models indicated that person-specific deviations from population expectations for regional GMV were highly heterogeneous, affecting the same area in <7% of people with the same diagnosis. However, these deviations were embedded within common functional circuits and networks in up to 56% of cases. The salience-ventral attention system was implicated transdiagnostically, with other systems selectively involved in depression, bipolar disorder, schizophrenia and attention-deficit/hyperactivity disorder. Phenotypic differences between cases assigned the same diagnosis may thus arise from the heterogeneous localization of specific regional deviations, whereas phenotypic similarities may be attributable to the dysfunction of common functional circuits and networks.", "journal": "Nature neuroscience", "date": "2023-08-15", "authors": ["AshleaSegal", "LindenParkes", "KevinAquino", "Seyed MostafaKia", "ThomasWolfers", "BarbaraFranke", "MartineHoogman", "Christian FBeckmann", "Lars TWestlye", "Ole AAndreassen", "AndrewZalesky", "Ben JHarrison", "Christopher GDavey", "CarlesSoriano-Mas", "Narc\u00edsCardoner", "JegganTiego", "MuratY\u00fccel", "LeahBraganza", "ChaoSuo", "MichaelBerk", "SueCotton", "Mark ABellgrove", "Andre FMarquand", "AlexFornito"], "doi": "10.1038/s41593-023-01404-6"}
{"title": "Natural language processing to convert unstructured COVID-19 chest-CT reports into structured reports.", "abstract": "Structured reporting has been demonstrated to increase report completeness and to reduce error rate, also enabling data mining of radiological reports. Still, structured reporting is perceived by radiologists as a fragmented reporting style, limiting their freedom of expression.\nA deep learning-based natural language processing method was developed to automatically convert unstructured COVID-19 chest CT reports into structured reports.\nTwo hundred-two COVID-19 chest CT were retrospectively reviewed by two experienced radiologists, who wrote for each exam a free-form text radiological report and coherently filled the template provided by the Italian Society of Medical and Interventional Radiology, used as ground-truth. A semi-supervised convolutional neural network was implemented to extract 62 categorical variables from the report. Two iterations were carried-out, the first without fine-tuning, the second one performing a fine-tuning. The performance was measured using the mean accuracy and the F1 mean score. An error analysis was performed to identify errors entirely attributable to incorrect processing of the model.\nThe algorithm achieved a mean accuracy of 93.7% and an F1 score 93.8% in the first iteration. Most of the errors were exclusively attributable to wrong inference (46%). In the second iteration the model achieved for both parameters 95,8% and percentage of errors attributable to wrong inference decreased to 26%.\nThe convolutional neural network achieved an optimal performance in the automated conversion of free-form text into structured radiological reports, overcoming all the limitation attributed to structured reporting and finally paving the way for data mining of radiological report.", "journal": "European journal of radiology open", "date": "2023-08-14", "authors": ["Salvatore ClaudioFanni", "ChiaraRomei", "GiovanniFerrando", "FedericaVolpi", "Caterina AidaD'Amore", "ClaudioBedini", "SandroUbbiali", "SalvatoreValentino", "EmanueleNeri"], "doi": "10.1016/j.ejro.2023.100512\n10.1186/s13244-019-0831-6\n10.1007/s00261-021-02966-4\n10.1186/s12909-019-1538-6\n10.1016/j.ejrad.2021.109621\n10.1186/s13244-020-00901-7\n10.1007/s00330-021-08485-6\n10.1007/s00330-020-06975-7\n10.1148/ryct.2020200152\n10.1007/s13244-017-0588-8\n10.23736/s2723-9284.20.00026-4\n10.23736/s2723-9284.22.00189-1\n10.4329/wjr.v13.i10.327\n10.2307/30141996\n10.1016/j.jbi.2011.03.011\n10.1186/s41747-019-0118-1\n10.1186/s13244-023-01392-y\n10.1148/ryct.2021200596"}
{"title": "Optimized Xception Learning Model and XgBoost Classifier for Detection of Multiclass Chest Disease from X-ray Images.", "abstract": "Computed tomography (CT) scans, or radiographic images, were used to aid in the early diagnosis of patients and detect normal and abnormal lung function in the human chest. However, the diagnosis of lungs infected with coronavirus disease 2019 (COVID-19) was made more accurately from CT scan data than from a swab test. This study uses human chest radiography pictures to identify and categorize normal lungs, lung opacities, COVID-19-infected lungs, and viral pneumonia (often called pneumonia). In the past, several CAD systems using image processing, ML/DL, and other forms of machine learning have been developed. However, those CAD systems did not provide a general solution, required huge hyper-parameters, and were computationally inefficient to process huge datasets. Moreover, the DL models required high computational complexity, which requires a huge memory cost, and the complexity of the experimental materials' backgrounds, which makes it difficult to train an efficient model. To address these issues, we developed the Inception module, which was improved to recognize and detect four classes of Chest X-ray in this research by substituting the original convolutions with an architecture based on modified-Xception (m-Xception). In addition, the model incorporates depth-separable convolution layers within the convolution layer, interlinked by linear residuals. The model's training utilized a two-stage transfer learning process to produce an effective model. Finally, we used the XgBoost classifier to recognize multiple classes of chest X-rays. To evaluate the m-Xception model, the 1095 dataset was converted using a data augmentation technique into 48,000 X-ray images, including 12,000 normal, 12,000 pneumonia, 12,000 COVID-19 images, and 12,000 lung opacity images. To balance these classes, we used a data augmentation technique. Using public datasets with three distinct train-test divisions (80-20%, 70-30%, and 60-40%) to evaluate our work, we attained an average of 96.5% accuracy, 96% F1 score, 96% recall, and 96% precision. A comparative analysis demonstrates that the m-Xception method outperforms comparable existing methods. The results of the experiments indicate that the proposed approach is intended to assist radiologists in better diagnosing different lung diseases.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-08-12", "authors": ["KashifShaheed", "QaisarAbbas", "AyyazHussain", "ImranQureshi"], "doi": "10.3390/diagnostics13152583\n10.1002/jmv.25678\n10.3390/healthcare11060837\n10.1007/s10489-020-01888-w\n10.1007/s00530-021-00794-6\n10.1016/j.eswa.2022.119206\n10.1016/j.advengsoft.2022.103317\n10.1016/j.asoc.2022.109109\n10.1007/s10278-021-00431-8\n10.1016/j.patcog.2017.10.002\n10.1016/j.bbe.2022.11.003\n10.1016/j.eswa.2020.114054\n10.3389/fmed.2020.00427\n10.1007/s12652-021-03306-6\n10.1016/j.rineng.2023.101020\n10.1038/s41598-020-76550-z\n10.3390/app10093233\n10.1007/s11042-023-15097-3\n10.1007/s00521-023-08450-y\n10.1109/TII.2021.3057683\n10.1038/s41598-020-71294-2\n10.1155/2020/8889023\n10.1016/j.eswa.2020.113459\n10.1049/iet-ipr.2018.6235\n10.1007/s13246-020-00865-4\n10.1016/j.bspc.2022.103977\n10.1007/s00500-020-05424-3\n10.3390/s21051742\n10.1016/j.bspc.2022.104268\n10.1049/cje.2020.11.002"}
{"title": "Routine Brain MRI Findings on the Long-Term Effects of COVID-19: A Scoping Review.", "abstract": "Post-COVID condition (PCC) is associated with long-term neuropsychiatric symptoms. Magnetic resonance imaging (MRI) in PCC examines the brain metabolism, connectivity, and morphometry. Such techniques are not easily available in routine practice. We conducted a scoping review to determine what is known about the routine MRI findings in PCC patients.\nThe PubMed database was searched up to 11 April 2023. We included cohort, cross-sectional, and before-after studies in English. Articles with only advanced MRI sequences (DTI, fMRI, VBM, PWI, ASL), preprints, and case reports were excluded. The National Heart, Lung, and Blood Institute and PRISMA Extension tools were used for quality assurance.\nA total of 7 citations out of 167 were included. The total sample size was 451 patients (average age 51 \u00b1 8 years; 67% female). Five studies followed a single recovering cohort, while two studies compared findings between two severity groups. The MRI findings were perivascular spaces (47%), microbleeds (27%) and white matter lesions (10%). All the studies agreed that PCC manifestations are not associated with specific MRI findings.\nThe results of the included studies are heterogeneous due to the low agreement on the types of MRI abnormalities in PCC. Our findings indicate that the routine brain MRI protocol has little value for long COVID diagnostics.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-08-12", "authors": ["YuriyVasilev", "IvanBlokhin", "AnnaKhoruzhaya", "MariaKodenko", "VasiliyKolyshenkov", "OlgaNanova", "YuliyaShumskaya", "OlgaOmelyanskaya", "AntonVladzymyrskyy", "RomanReshetnikov"], "doi": "10.3390/diagnostics13152533\n10.1057/s41599-021-00900-z\n10.1038/s41579-022-00846-2\n10.1136/bmjgh-2021-005427\n10.1093/cid/ciad045\n10.1001/jamanetworkopen.2022.44486\n10.1038/s41586-022-04569-5\n10.1016/j.eclinm.2020.100484\n10.1007/s00415-018-9149-4\n10.1371/journal.pone.0273704\n10.1371/journal.pone.0214887\n10.1016/j.jclinepi.2019.02.003\n10.1002/jmri.26211\n10.7326/M18-0850\n10.1371/journal.pone.0110803\n10.1111/ene.15812\n10.1016/j.ensci.2022.100418\n10.1007/s13365-022-01079-y\n10.1136/jnnp-2021-327899\n10.2147/NDT.S387501\n10.1002/acn3.51496\n10.1136/bmjopen-2021-055164\n10.4467/20842627OZ.20.003.12657\n10.17816/DD51043\n10.3389/fpsyg.2019.01768\n10.1016/j.ejrad.2020.109393\n10.1161/STROKEAHA.116.016289\n10.1001/jamaneurol.2015.0174\n10.3174/ajnr.A4956\n10.1148/radiol.2021203071\n10.1007/s10072-020-04733-7\n10.1007/s12031-021-01899-3\n10.1038/s41582-020-0312-z\n10.2967/jnumed.121.263085"}
{"title": "Analysis of emerging trends and hot spots in respiratory biomechanics from 2003 to 2022 based on CiteSpace.", "abstract": "", "journal": "Frontiers in physiology", "date": "2023-08-07", "authors": ["XiaofeiHuang", "JiaqiZheng", "YeMa", "MeijinHou", "XiangbinWang"], "doi": "10.3389/fphys.2023.1190155\n10.3390/polym11091518\n10.1109/access.2019.2912956\n10.1038/s41591-021-01433-3\n10.1016/j.ijrobp.2015.11.049\n10.1088/0031-9155/54/7/001\n10.1109/RBME.2017.2763681\n10.1371/journal.pone.0223994\n10.3390/molecules25245823\n10.1039/d1tc04180k\n10.1109/TMI.2010.2076299\n10.1038/s41467-020-19057-5\n10.1063/5.0106594\n10.1001/jamanetworkopen.2021.28568\n10.1002/mp.15036\n10.1016/j.media.2004.11.007\n10.1016/s0022-5223(19)42529-4\n10.1152/jappl.1972.32.4.535\n10.1016/S0140-6736(21)01755-4\n10.1016/j.snb.2023.133396\n10.1186/s40779-020-0233-6\n10.1016/S0006-3495(66)86694-8\n10.1021/acs.analchem.2c02760\n10.1007/s10877-022-00876-4\n10.1118/1.2349696\n10.2147/COPD.S153525\n10.1097/MNM.0000000000001368\n10.2147/JPR.S132808\n10.1002/advs.202004023\n10.3389/fbioe.2021.684778\n10.1088/0031-9155/61/17/6515\n10.1159/000462916\n10.1016/j.media.2012.09.005\n10.4209/aaqr.2020.04.0185\n10.1098/rsif.2022.0062\n10.1053/j.semnuclmed.2008.01.002\n10.1002/mp.15620\n10.3390/cancers14174308\n10.1088/0031-9155/61/17/6485\n10.1109/JSEN.2021.3077530\n10.1007/s11845-018-1936-5\n10.1118/1.3182340\n10.1088/0031-9155/54/15/009\n10.1148/radiol.2502071998\n10.1016/s0360-3016(02)02803-1\n10.1002/mp.15275\n10.1021/acsnano.0c01804\n10.1088/0031-9155/48/1/304\n10.1155/2013/872739\n10.3390/s17081840\n10.1016/j.bios.2021.113329\n10.1016/j.nanoen.2017.05.056\n10.2147/COPD.S339195\n10.1016/j.jinf.2020.06.003\n10.1016/j.snb.2021.131300\n10.1088/2752-5724/ac44ab\n10.3389/fpubh.2022.904855"}
{"title": "A predictive decision support system for coronavirus disease 2019 response management and medical logistic planning.", "abstract": "Coronavirus disease 2019 demonstrated the inconsistencies in adequately responding to biological threats on a global scale due to a lack of powerful tools for assessing various factors in the formation of the epidemic situation and its forecasting. Decision support systems have a role in overcoming the challenges in health monitoring systems in light of current or future epidemic outbreaks. This paper focuses on some applied examples of logistic planning, a key service of the Earth Cognitive System for Coronavirus Disease 2019 project, here presented, evidencing the added value of artificial intelligence algorithms towards predictive hypotheses in tackling health emergencies.\nEarth Cognitive System for Coronavirus Disease 2019 is a decision support system designed to support healthcare institutions in monitoring, management and forecasting activities through artificial intelligence, social media analytics, geospatial analysis and satellite imaging. The monitoring, management and prediction of medical equipment logistic needs rely on machine learning to predict the regional risk classification colour codes, the emergency rooms attendances, and the forecast of regional medical supplies, synergically enhancing geospatial and temporal dimensions.\nThe overall performance of the regional risk colour code classifier yielded a high value of the macro-average F1-score (0.82) and an accuracy of 85%. The prediction of the emergency rooms attendances for the Lazio region yielded a very low root mean square error (<11 patients) and a high positive correlation with the actual values for the major hospitals of the Lazio region which admit about 90% of the region's patients. The prediction of the medicinal purchases for the regions of Lazio and Piemonte has yielded a low root mean squared percentage error of 16%.\nAccurate forecasting of the evolution of new cases and drug utilisation enables the resulting excess demand throughout the supply chain to be managed more effectively. Forecasting during a pandemic becomes essential for effective government decision-making, managing supply chain resources, and for informing tough policy decisions.", "journal": "Digital health", "date": "2023-08-07", "authors": ["SofianeAtek", "FilippoBianchini", "CorradoDe Vito", "VincenzoCardinale", "SimoneNovelli", "CristianoPesaresi", "MarcoEugeni", "MassimoMecella", "AntonelloRescio", "LucaPetronzio", "AldoVincenzi", "PasqualePistillo", "GianfrancoGiusto", "GiorgioPasquali", "DomenicoAlvaro", "PaoloVillari", "MarcoMancini", "PaoloGaudenzi"], "doi": "10.1177/20552076231185475\n10.1016/j.actaastro.2022.05.013\n10.1109/IGARSS39084.2020.9323827\n10.1016/B978-0-12-824536-1.00029-0\n10.1088/1742-6596/1797/1/012009"}
{"title": "How healthcare workers reacted to the different COVID-19 waves: An Italian survey.", "abstract": "The COVID-19 pandemic had a huge impact on radiology departments all over the world, affecting both management and healthcare workers (HCWs). Therefore, it became challenging to guarantee high standards of diagnosis while keeping up with the workload.\nThe study was approved by the institutional review board. Its aim was to assess the impact of the COVID-19 pandemic over the radiology departments and HCWs through a survey. The questionnaire was available online from January to March 2022. Twelve areas of interest (sessions) were highlighted in the survey.\nThe number of total responders was 1376 and 73.7% of participants worked in public healthcare facilities. Comparisons between participants working in public versus private healthcare facilities were carried out using chi-square tests and Fisher tests. Within public healthcare workers, 82% affirmed having operating instruction protocols regarding confirmed or suspected COVID-19 patient CT management (p< 0.001). Private healthcare facilities had fewer CT scanners available in general (p< 0.001); in fact, only 18% of them affirmed having two or more CT scanners, and did not have CT scanners dedicated to confirmed or suspected COVID-19 patients (p< 0.001). Finally, public facilities strongly reduced (by 88%) the number of examinations booked during the first wave, compared to private healthcare facilities (p< 0.001).\nThis survey showed that public facilities appeared to be better prepared from an organizational point of view than private facilities. Rescheduling the examinations booked during the first COVID-19 wave was challenging and not always possible.", "journal": "Journal of medical imaging and radiation sciences", "date": "2023-08-07", "authors": ["MarcoNicol\u00f2", "AltinAdraman", "CamillaRisoli", "AngieDevetti", "AlessandroTombolesi", "Irene GertrudRigott", "MatteoMigliorini", "LisaConversi", "Daniele DiFeo", "Angelo DiNaro", "ElisaVetti", "OscarBrazzo", "ChiaraMartini"], "doi": "10.1016/j.jmir.2023.07.004"}
{"title": "Chest CT scan predictors of intensive care unit admission in hospitalized pregnant women with COVID-19: a case-control study.", "abstract": "To investigate the role of chest computed tomography (CT) scan in the prediction of admission of pregnant women with COVID-19 into intensive care unit (ICU).\nThis was a single-center retrospective case-control study. We included pregnant women diagnosed with COVID-19 by reverse transcriptase polymerase chain reaction between February 2020 and July 2021, requiring hospital admission due to symptoms, who also had a CT chest scan at presentation. Patients admitted to the ICU (case group) were compared with patients who did not require ICU admission (control group). The CT scans were reported by an experienced radiologist, blinded to the patient's course and outcome, aided by an artificial intelligence software. Total CT scan score, chest CT severity score (CT-SS), total lung volume (TLV), infected lung volume (ILV), and infected-to-total lung volume ratio (ILV/TLV) were calculated. Receiver operating characteristic curves were constructed to test the sensitivity and specificity of each parameter.\n8/28 patients (28.6%) required ICU admission. These also had lower TLV, higher ILV, and ILV/TLV. The area under the curve (AUC) for these three parameters was 0.789, 0.775, and 0.763, respectively. TLV, ILV, and ILV/TLV had good sensitivity (62.5%, 87.5%, and 87.5%, respectively) and specificity (84.2%, 70%, and 73.7%, respectively) for predicting ICU admission at the following selected thresholds: 2255\u2009mL, 319\u2009mL, and 14%, respectively. The performance of CT-SS, CT scan score, and ILV/TLV in predicting ICU admission was comparable.\nTLV, ILV, and ILV/TLV as measured by an artificial intelligence software on chest CT, may predict ICU admission in hospitalized pregnant women, symptomatic for COVID-19.", "journal": "The journal of maternal-fetal & neonatal medicine : the official journal of the European Association of Perinatal Medicine, the Federation of Asia and Oceania Perinatal Societies, the International Society of Perinatal Obstetricians", "date": "2023-08-07", "authors": ["Dominique ABadr", "FedericoDe Lucia", "AndrewCarlin", "Jacques CJani", "Mieke MCannie"], "doi": "10.1080/14767058.2023.2241107"}
{"title": "Training certified detectives to track down the intrinsic shortcuts in COVID-19 chest x-ray data sets.", "abstract": "Deep learning faces a significant challenge wherein the trained models often underperform when used with external test data sets. This issue has been attributed to spurious correlations between irrelevant features in the input data and corresponding labels. This study uses the classification of COVID-19 from chest x-ray radiographs as an example to demonstrate that the image contrast and sharpness, which are characteristics of a chest radiograph dependent on data acquisition systems and imaging parameters, can be intrinsic shortcuts that impair the model's generalizability. The study proposes training certified shortcut detective models that meet a set of qualification criteria which can then identify these intrinsic shortcuts in a curated data set.", "journal": "Scientific reports", "date": "2023-08-05", "authors": ["RanZhang", "DaltonGriner", "John WGarrett", "ZhihuaQi", "Guang-HongChen"], "doi": "10.1038/s41598-023-39855-3\n10.1371/journal.pmed.1002683\n10.1038/s42256-021-00307-0\n10.1016/j.patter.2021.100269\n10.1038/s42256-020-00257-z\n10.1038/s42256-021-00338-7\n10.1109/TMI.2020.2993291\n10.3390/s21217116\n10.1038/s42256-021-00343-w\n10.1038/s41597-019-0322-0"}
{"title": "Brain MRI findings in neurologically symptomatic COVID-19 patients: a systematic review and meta-analysis.", "abstract": "Coronavirus disease 2019 (COVID-19) has been associated with nervous system involvement, with more than one-third of COVID-19 patients experiencing neurological manifestations. Utilizing a systematic review, this study aims to summarize brain MRI findings in COVID-19 patients presenting with neurological symptoms.\nSystematic review was conducted in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-analysis (PRISMA) checklist. The electronic databases of PubMed/MEDLINE, Embase, Scopus, and Web of Science were systematically searched for literature addressing brain MRI findings in COVID-19 patients with neurological symptoms.\n25 publications containing a total number of 3118 COVID-19 patients with neurological symptoms who underwent MRI were included. The most common MRI findings and the respective pooled incidences in decreasing order were acute/subacute infarct (22%), olfactory bulb abnormalities (22%), white matter abnormalities (20%), cerebral microbleeds (17%), grey matter abnormalities (12%), leptomeningeal enhancement (10%), ADEM (Acute Disseminated Encephalomyelitis) or ADEM-like lesions (10%), non-traumatic ICH (10%), cranial neuropathy (8%), cortical gray matter signal changes compatible with encephalitis (8%), basal ganglia abnormalities (5%), PRES (Posterior Reversible Encephalopathy Syndrome) (3%), hypoxic-ischemic lesions (4%), venous thrombosis (2%), and cytotoxic lesions of the corpus callosum (2%).\nThe present study revealed that a considerable proportion of patients with COVID-19 might harbor neurological abnormalities detectable by MRI. Among various findings, the most common MRI alterations are acute/subacute infarction, olfactory bulb abnormalities, white matter abnormalities, and cerebral microbleeds.", "journal": "Journal of neurology", "date": "2023-08-03", "authors": ["Amir MasoudAfsahi", "Alexander MNorbash", "Shahla FSyed", "MayaSedaghat", "GhazalehAfsahi", "RaminShahidi", "ZohrehTajabadi", "MahsaBagherzadeh-Fard", "ShaghayeghKarami", "PouryaYarahmadi", "ShabnamShirdel", "AliAsgarzadeh", "MansourehBaradaran", "FattanehKhalaj", "HamidrezaSadeghsalehi", "MaryamFotouhi", "Mohammad AminHabibi", "HyungseokJang", "AbassAlavi", "SamSedaghat"], "doi": "10.1007/s00415-023-11914-9\n10.1080/10408363.2020.1783198\n10.1111/tmi.13383\n10.1002/jmv.26157\n10.3389/fimmu.2020.621735\n10.1212/WNL.0000000000012930\n10.1002/jcla.24403\n10.3389/fnins.2020.00072\n10.1148/radiol.2020202222\n10.3348/kjr.2021.0127\n10.1186/s13643-021-01626-4\n10.1016/j.jclinepi.2012.09.016\n10.1016/j.ejrad.2020.109393\n10.1515/revneuro-2021-0130\n10.1016/j.jstrokecerebrovasdis.2020.105552\n10.1002/jmv.26105\n10.1007/s00401-020-02166-2\n10.1002/lary.29286\n10.1007/s11882-020-00972-y\n10.1016/j.bjorl.2021.04.001\n10.1021/acschemneuro.0c00447\n10.1002/lary.30078\n10.1111/micc.12749\n10.1016/j.bbi.2020.03.031\n10.1111/ene.14913\n10.1515/revneuro-2021-0082\n10.1016/j.amsu.2021.103080\n10.1111/jon.12880\n10.1016/j.nicl.2022.102939\n10.1212/WNL.0000000000001587\n10.17305/bjbms.2021.6341\n10.1148/radiol.2020202422\n10.1016/j.amjoto.2021.102999\n10.3174/ajnr.A6713\n10.1007/s00415-020-10164-3\n10.1177/19418744211039374\n10.1148/rg.2017160085\n10.7759/cureus.35191\n10.55994/ejcc.1156561\n10.3390/ijms21155475\n10.1016/j.bbih.2020.100094\n10.1038/s41582-020-0398-3\n10.1016/j.autrev.2020.102591\n10.1016/j.intimp.2022.109183\n10.3906/sag-2105-138\n10.3174/ajnr.A6793\n10.3389/fneur.2022.884449\n10.1016/j.neurad.2021.07.004\n10.1038/s41598-021-00064-5\n10.1016/j.neurad.2020.11.004\n10.1007/s00415-020-10313-8\n10.1016/j.heliyon.2021.e07879\n10.1186/s43055-021-00630-x\n10.1148/radiol.2020201697\n10.1161/STROKEAHA.120.030940\n10.1212/WNL.0000000000010112\n10.1016/j.crad.2020.09.002\n10.1001/jamanetworkopen.2021.1489\n10.3174/ajnr.A7072\n10.3390/v13050845\n10.3174/ajnr.A7158\n10.1148/radiol.2020202791\n10.1177/08465371211002815\n10.1177/1941874420980622\n10.1371/journal.pone.0283614\n10.1186/s43055-022-00911-z"}
{"title": "Deep-learning-enabled protein-protein interaction analysis for prediction of SARS-CoV-2 infectivity and variant evolution.", "abstract": "Host-pathogen interactions and pathogen evolution are underpinned by protein-protein interactions between viral and host proteins. An understanding of how viral variants affect protein-protein binding is important for predicting viral-host interactions, such as the emergence of new pathogenic SARS-CoV-2 variants. Here we propose an artificial intelligence-based framework called UniBind, in which proteins are represented as a graph at the residue and atom levels. UniBind integrates protein three-dimensional structure and binding affinity and is capable of multi-task learning for heterogeneous biological data integration. In systematic tests on benchmark datasets and further experimental validation, UniBind effectively and scalably predicted the effects of SARS-CoV-2 spike protein variants on their binding affinities to the human ACE2 receptor, as well as to SARS-CoV-2 neutralizing monoclonal antibodies. Furthermore, in a cross-species analysis, UniBind could be applied to predict host susceptibility to SARS-CoV-2 variants and to predict future viral variant evolutionary trends. This in silico approach has the potential to serve as an early warning system for problematic emerging SARS-CoV-2 variants, as well as to facilitate research on protein-protein interactions in general.", "journal": "Nature medicine", "date": "2023-08-01", "authors": ["GuangyuWang", "XiaohongLiu", "KaiWang", "YuanxuGao", "GenLi", "Daniel TBaptista-Hon", "Xiaohong HelenaYang", "KanminXue", "Wa HouTai", "ZeyuJiang", "LinlingCheng", "MansonFok", "Johnson Yiu-NamLau", "ShengyongYang", "LigongLu", "PingZhang", "KangZhang"], "doi": "10.1038/s41591-023-02483-5"}
{"title": "Deep learning classifiers for computer-aided diagnosis of multiple lungs disease.", "abstract": "Computer aided diagnosis has gained momentum in the recent past. The advances in deep learning and availability of huge volumes of data along with increased computational capabilities has reshaped the diagnosis and prognosis procedures.\nThese methods are proven to be relatively less expensive and safer alternatives of the otherwise traditional approaches. This study is focused on efficient diagnosis of three very common diseases: lung cancer, pneumonia and Covid-19 using X-ray images.\nThree different deep learning models are designed and developed to perform 4-way classification. Inception V3, Convolutional Neural Networks (CNN) and Long Short Term Memory models (LSTM) are used as building blocks. The performance of these models is evaluated using three publicly available datasets, the first dataset contains images for Lung cancer, second contains images for Covid-19 and third dataset contains images for Pneumonia and normal subjects. Combining three datasets creates a class imbalance problem which is resolved using pre-processing and data augmentation techniques. After data augmentation 1386 subjects are randomly chosen for each class.\nIt is observed that CNN when combined with LSTM (CNN-LSTM) produces significantly improved results (accuracy of 94.5 %) which is better than CNN and InceptionV3-LSTM. 3,5, and 10 fold cross validation is performed to verify all results calculated using three different classifiersConclusions:This research concludes that a single computer-aided diagnosis system can be developed for diagnosing multiple diseases.", "journal": "Journal of X-ray science and technology", "date": "2023-07-31", "authors": ["AzizUr Rehman", "AsmaNaseer", "SairaKarim", "MariaTamoor", "SaminaNaz"], "doi": "10.3233/XST-230113"}
{"title": "An evaluation of lightweight deep learning techniques in medical imaging for high precision COVID-19 diagnostics.", "abstract": "Timely and rapid diagnoses are core to informing on optimum interventions that curb the spread of COVID-19. The use of medical images such as chest X-rays and CTs has been advocated to supplement the Reverse-Transcription Polymerase Chain Reaction (RT-PCR) test, which in turn has stimulated the application of deep learning techniques in the development of automated systems for the detection of infections. Decision support systems relax the challenges inherent to the physical examination of images, which is both time consuming and requires interpretation by highly trained clinicians. A review of relevant reported studies to date shows that most deep learning algorithms utilised approaches are not amenable to implementation on resource-constrained devices. Given the rate of infections is increasing, rapid, trusted diagnoses are a central tool in the management of the spread, mandating a need for a low-cost and mobile point-of-care detection systems, especially for middle- and low-income nations. The paper presents the development and evaluation of the performance of lightweight deep learning technique for the detection of COVID-19 using the MobileNetV2 model. Results demonstrate that the performance of the lightweight deep learning model is competitive with respect to heavyweight models but delivers a significant increase in the efficiency of deployment, notably in the lowering of the cost and memory requirements of computing resources.", "journal": "Healthcare analytics (New York, N.Y.)", "date": "2023-07-31", "authors": ["OgechukwuUkwandu", "HananHindy", "ElochukwuUkwandu"], "doi": "10.1016/j.health.2022.100096\n10.1148/radiol.2020200432\n10.1148/radiol.2020200330\n10.1148/radiol.2020201160\n10.1001/jama.2020.3786\n10.1016/j.compbiomed.2019.02.017\n10.1016/j.jacr.2017.12.028\n10.1118/1.597177\n10.1117/12.2293498\n10.1002/mp.13361\n10.1007/s11604-019-00831-5\n10.1016/j.artmed.2019.101779\n10.1007/s00034-019-01246-3\n10.4236/jbise.2020.136010\n10.1016/j.bbe.2020.08.005\n10.1109/ICEIEC49280.2020.9152329\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105581\n10.1016/j.mehy.2020.109761\n10.1016/j.compbiomed.2020.103792\n10.1007/s10489-020-01943-6\n10.1007/s10439-022-02958-5\n10.1016/j.bspc.2021.103182\n10.1016/j.clbc.2021.04.015\n10.4018/IJSIR.287544\n10.21203/rs.3.rs-24305/v1\n10.1007/s10489-020-01978-9\n10.1016/j.compbiomed.2020.104181\n10.1016/j.compbiomed.2022.105604\n10.1007/s10489-020-01867-1\n10.1016/j.bspc.2020.102365\n10.1016/j.ijmedinf.2020.104284\n10.1109/TKDE.2009.191\n10.1016/j.asoc.2020.106859"}
{"title": "Optical Biosensors for the Diagnosis of COVID-19 and Other Viruses-A Review.", "abstract": "The sudden outbreak of the COVID-19 pandemic led to a huge concern globally because of the astounding increase in mortality rates worldwide. The medical imaging computed tomography technique, whole-genome sequencing, and electron microscopy are the methods generally used for the screening and identification of the SARS-CoV-2 virus. The main aim of this review is to emphasize the capabilities of various optical techniques to facilitate not only the timely and effective diagnosis of the virus but also to apply its potential toward therapy in the field of virology. This review paper categorizes the potential optical biosensors into the three main categories, spectroscopic-, nanomaterial-, and interferometry-based approaches, used for detecting various types of viruses, including SARS-CoV-2. Various classifications of spectroscopic techniques such as Raman spectroscopy, near-infrared spectroscopy, and fluorescence spectroscopy are discussed in the first part. The second aspect highlights advances related to nanomaterial-based optical biosensors, while the third part describes various optical interferometric biosensors used for the detection of viruses. The tremendous progress made by lab-on-a-chip technology in conjunction with smartphones for improving the point-of-care and portability features of the optical biosensors is also discussed. Finally, the review discusses the emergence of artificial intelligence and its applications in the field of bio-photonics and medical imaging for the diagnosis of COVID-19. The review concludes by providing insights into the future perspectives of optical techniques in the effective diagnosis of viruses.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-07-29", "authors": ["PaulineJohn", "Nilesh JVasa", "AzharZam"], "doi": "10.3390/diagnostics13142418\n10.1038/nrmicro.2016.81\n10.1016/S0140-6736(03)14630-2\n10.1056/NEJMoa1211721\n10.1016/S0140-6736(20)30183-5\n10.3389/fimmu.2019.00549\n10.1353/bhm.2002.0022\n10.3390/pathogens5040066\n10.1038/35400\n10.1126/science.6189183\n10.1056/NEJMoa031349\n10.1016/S1473-3099(12)70121-4\n10.1016/j.bj.2020.04.007\n10.1016/j.bbadis.2020.165878\n10.1038/s41579-018-0118-9\n10.1016/j.coi.2005.05.009\n10.1016/j.envres.2020.110603\n10.1038/s41563-020-00906-z\n10.1016/j.scitotenv.2020.142289\n10.1038/s41368-020-0080-z\n10.1111/jam.14278\n10.1080/08830185.2021.1872566\n10.1016/j.ijnurstu.2020.103629\n10.3390/jcm9103372\n10.1016/j.prp.2021.153443\n10.1007/s12038-020-00114-6\n10.1016/j.talanta.2021.122609\n10.1021/acsnano.0c02624\n10.1002/cbic.202000744\n10.1016/j.trac.2017.09.015\n10.1016/j.clispe.2022.100022\n10.1117/1.JBO.19.6.067003\n10.1038/s41598-020-70811-7\n10.1016/j.bpc.2006.06.010\n10.1016/j.cca.2011.10.035\n10.3748/wjg.v22.i1.417\n10.1517/17530059.2011.537653\n10.1186/1743-422X-3-51\n10.1016/j.ymeth.2014.02.022\n10.1021/pr101067u\n10.1039/C5CS00828J\n10.1155/2008/419783\n10.1039/C7RA03361C\n10.3233/BSI-200203\n10.1038/s41598-021-95568-5\n10.1002/jbio.202000189\n10.1021/acs.analchem.0c04608\n10.1126/science.1247390\n10.1007/s00216-007-1768-z\n10.1016/j.bios.2012.08.048\n10.1126/science.297.5586.1536\n10.1002/chem.200801812\n10.1039/C2AN36099C\n10.1016/j.bios.2018.09.024\n10.1016/j.bios.2010.08.060\n10.1016/j.colsurfb.2011.12.026\n10.1021/nn3034056\n10.1016/j.aca.2020.02.039\n10.7150/thno.23856\n10.1016/j.biomaterials.2007.01.047\n10.1016/0250-6874(83)85036-7\n10.1364/BOE.3.003119\n10.1371/journal.ppat.1002627\n10.1364/BOE.2.001115\n10.1007/s11517-015-1262-2\n10.1016/j.snb.2020.128182\n10.1039/C4AN01077A\n10.1007/s00604-019-3420-y\n10.1021/acs.jpcc.9b09253\n10.1021/acs.analchem.5b02661\n10.1021/acsinfecdis.7b00110\n10.1021/acsabm.1c00102\n10.1016/j.watres.2021.117243\n10.1016/j.heliyon.2018.e00766\n10.1016/j.snb.2017.06.085\n10.1016/j.bios.2010.07.121\n10.1016/j.bios.2020.112496\n10.1021/acs.analchem.8b03052\n10.1039/C4CC00569D\n10.1039/c2cc16271g\n10.1016/j.talanta.2012.08.041\n10.1021/acsnano.0c02439\n10.1016/j.xcrp.2020.100288\n10.1016/j.aca.2008.05.022\n10.1364/AO.56.008257\n10.1016/j.sbsr.2019.100277\n10.4236/opj.2016.65011\n10.1016/j.colsurfb.2009.10.041\n10.1109/LPT.2008.916947\n10.1364/BOE.7.001672\n10.1007/s00216-007-1525-3\n10.1021/nl062595n\n10.1586/17434440.4.4.447\n10.1364/OE.20.020934\n10.1088/2040-8978/18/6/063003\n10.3390/s150717649\n10.1093/clinchem/43.9.1757\n10.1063/1.2387112\n10.1039/b716834a\n10.1021/nl103025u\n10.1007/BF01318246\n10.1016/j.bios.2022.114570\n10.1364/BOE.465136\n10.1016/j.bios.2011.01.019\n10.1038/srep14494\n10.1002/jbio.202200172\n10.3390/s140815458\n10.1016/j.bios.2020.112041\n10.1021/acs.analchem.6b00278\n10.1080/13102818.2018.1561211\n10.1002/jmv.24806\n10.1039/C7CS00837F\n10.1002/adma.201806739\n10.1016/j.bios.2016.07.015\n10.1016/j.bios.2014.02.077\n10.1039/c3lc40991k\n10.3390/bios12040200\n10.3390/s17112449\n10.1117/1.JBO.25.10.102703\n10.1080/07391102.2020.1767212\n10.1109/RBME.2020.2987975\n10.1016/j.eng.2020.04.010\n10.1109/ACCESS.2020.3001973\n10.1364/BOE.466005\n10.1038/s41746-021-00453-0\n10.1016/j.future.2021.05.019\n10.1021/acsabm.0c01004\n10.1117/1.JBO.27.2.025002\n10.1016/S1473-3099(20)30196-1\n10.1039/C3NR06132A\n10.1364/BOE.389342\n10.1038/lsa.2013.60\n10.1186/s12977-018-0424-3\n10.1016/j.tcb.2015.10.004\n10.1136/thoraxjnl-2020-215705\n10.1088/1752-7163/abb99b\n10.1371/journal.pone.0002691\n10.1016/j.pdpdt.2020.101765\n10.3109/1040841X.2013.879849\n10.1021/acsami.9b15032\n10.1016/S0924-8579(03)00035-9\n10.1002/lsm.21080\n10.3322/caac.20114\n10.1089/photob.2020.4868"}
{"title": "A pipeline to further enhance quality, integrity and reusability of the NCCID clinical data.", "abstract": "The National COVID-19 Chest Imaging Database (NCCID) is a centralized UK database of thoracic imaging and corresponding clinical data. It is made available by the National Health Service Artificial Intelligence (NHS AI) Lab to support the development of machine learning tools focused on Coronavirus Disease 2019 (COVID-19). A bespoke cleaning pipeline for NCCID, developed by the NHSx, was introduced in 2021. We present an extension to the original cleaning pipeline for the clinical data of the database. It has been adjusted to correct additional systematic inconsistencies in the raw data such as patient sex, oxygen levels and date values. The most important changes will be discussed in this paper, whilst the code and further explanations are made publicly available on GitLab. The suggested cleaning will allow global users to work with more consistent data for the development of machine learning tools without being an expert. In addition, it highlights some of the challenges when working with clinical multi-center data and includes recommendations for similar future initiatives.", "journal": "Scientific data", "date": "2023-07-28", "authors": ["AnnaBreger", "IanSelby", "MichaelRoberts", "JudithBabar", "EffrossyniGkrania-Klotsas", "JacobusPreller", "LorenaEscudero S\u00e1nchez", "NoneNone", "James H FRudd", "John A DAston", "Jonathan RWeir-McCall", "EvisSala", "Carola-BibianeSch\u00f6nlieb"], "doi": "10.1038/s41597-023-02340-7\n10.1148/RADIOL.2019191586\n10.1038/s42256-021-00307-0\n10.1038/S41591-022-01847-7\n10.1016/s0140-6736(21)01326-x\n10.1186/S12874-022-01578-w\n10.1038/s41591-021-01595-0\n10.1073/pnas.1919012117\n10.1136/BMJ.M1328\n10.1016/j.ajem.2022.04.036\n10.1016/J.SEMRADONC.2019.05.010\n10.1016/S2589-7500(22)00151-0\n10.1016/J.IJMEDINF.2021.104510\n10.48550/ARXIV.2206.08478\n10.1088/1361-6579/AB5154\n10.1177/0310057X1804600307\n10.1097/CCE.0000000000000758\n10.1136/THORAX-2022-BTSABSTRACTS.298\n10.5281/zenodo.3509134\n10.5694/j.1326-5377.2005.tb06958.x\n10.1111/jth.14956\n10.1136/THORAXJNL-2016-209729"}
{"title": "A Multiclass Radiomics Method-Based WHO Severity Scale for Improving COVID-19 Patient Assessment and Disease Characterization From CT Scans.", "abstract": "The aim of this study was to evaluate the severity of COVID-19 patients' disease by comparing a multiclass lung lesion model to a single-class lung lesion model and radiologists' assessments in chest computed tomography scans.\nThe proposed method, AssessNet-19, was developed in 2 stages in this retrospective study. Four COVID-19-induced tissue lesions were manually segmented to train a 2D-U-Net network for a multiclass segmentation task followed by extensive extraction of radiomic features from the lung lesions. LASSO regression was used to reduce the feature set, and the XGBoost algorithm was trained to classify disease severity based on the World Health Organization Clinical Progression Scale. The model was evaluated using 2 multicenter cohorts: a development cohort of 145 COVID-19-positive patients from 3 centers to train and test the severity prediction model using manually segmented lung lesions. In addition, an evaluation set of 90 COVID-19-positive patients was collected from 2 centers to evaluate AssessNet-19 in a fully automated fashion.\nAssessNet-19 achieved an F1-score of 0.76 \u00b1 0.02 for severity classification in the evaluation set, which was superior to the 3 expert thoracic radiologists (F1 = 0.63 \u00b1 0.02) and the single-class lesion segmentation model (F1 = 0.64 \u00b1 0.02). In addition, AssessNet-19 automated multiclass lesion segmentation obtained a mean Dice score of 0.70 for ground-glass opacity, 0.68 for consolidation, 0.65 for pleural effusion, and 0.30 for band-like structures compared with ground truth. Moreover, it achieved a high agreement with radiologists for quantifying disease extent with Cohen \u03ba of 0.94, 0.92, and 0.95.\nA novel artificial intelligence multiclass radiomics model including 4 lung lesions to assess disease severity based on the World Health Organization Clinical Progression Scale more accurately determines the severity of COVID-19 patients than a single-class model and radiologists' assessment.", "journal": "Investigative radiology", "date": "2023-07-26", "authors": ["John Anderson GarciaHenao", "ArnoDepotter", "Danielle VBower", "HerkusBajercius", "Plamena TeodosievaTodorova", "HugoSaint-James", "Aur\u00e9lie Pahudde Mortanges", "Maria CeciliaBarroso", "JianchunHe", "JunlinYang", "ChenyuYou", "Lawrence HStaib", "ChristopherGange", "Roberta EufrasiaLedda", "CaterinaCaminiti", "MarioSilva", "Isabel OlivaCortopassi", "Charles SDela Cruz", "WolfHautz", "Harald MBonel", "NicolaSverzellati", "James SDuncan", "MauricioReyes", "AlexanderPoellinger"], "doi": "10.1097/RLI.0000000000001005\n10.1007/978-3-319-24574-4_28\n10.1145/2939672.2939785"}
{"title": "Quantitative evaluation of CT scan images to determinate the prognosis of COVID-19 patient using deep learning.", "abstract": "The purpose of this research is to evaluate the accuracy of AI-assisted quantification in comparison to conventional CT parameters reviewed by a radiologist in predicting the severity, progression, and clinical outcome of disease. The current study is a cross-sectional study that was conducted on patients with the diagnosis of COVID-19 and underwent a pulmonary CT scan between August 23th, 2021 to December 21th, 2022. The initial CT scan on admission was used for imaging analysis. The presence of ground glass opacity (GGO), and consolidation were visually evaluated. CT severity score was calculated according to a semi-quantitative method. In addition, AI based quantification of GGO and consolidation volume were also performed. 291 patients (mean age: 64.7\u2009\u00b1\u20097; 129 males) were included. GGO\u2009+\u2009consolidation was more frequently revealed in progress-to-severe group whereas pure GGO was more likely to be found in non-severe group. Compared to non-severe group, patients in progress-to-severe group had larger GGO volume percentage (40.6%\u00b1 11.9%versus 21.7%\u00b1 8.8%, p \u02c20.001) as well as consolidation volume percentage (4.8%\u2009\u00b1\u20092% versus 1.9% \u00b1\u20091%, p\u2009<\u20090.001). Among imaging parameters, consolidation volume percentage and the largest area under curve (AUC) in discriminating non-severe from progress-to-severe group (AUC\u2009=\u20090.91, p\u2009<\u20090.001). According to multivariate regression, consolidation volume was the strongest predictor for disease progression. In conclusion, the consolidation volume measured on the initial chest CT was the most accurate predictor of disease progression, and a larger consolidation volume was associated with a poor clinical outcome. In patients with COVID-19, AI-assisted lesion quantification was useful for risk stratification and prognosis evaluation.", "journal": "European journal of translational myology", "date": "2023-07-26", "authors": ["Saeid SadeghiJoni", "RezaGerami", "FakherehPashaei", "HojatEbrahiminik", "MahmoodKarimi"], "doi": "10.4081/ejtm.2023.11571\n10.1007/978-3-319-24574-4\n10.48550/arXiv.1809.10486"}
{"title": "A Novel Classification Model Using Optimal Long Short-Term Memory for Classification of COVID-19 from CT Images.", "abstract": "The human respiratory system is affected when an individual is infected with COVID-19, which became a global pandemic in 2020 and affected millions of people worldwide. However, accurate diagnosis of COVID-19 can be challenging due to small variations in typical and COVID-19 pneumonia, as well as the complexities involved in classifying infection regions. Currently, various deep learning (DL)-based methods are being introduced for the automatic detection of COVID-19 using computerized tomography (CT) scan images. In this paper, we propose the pelican optimization algorithm-based long short-term memory (POA-LSTM) method for classifying coronavirus using CT scan images. The data preprocessing technique is used to convert raw image data into a suitable format for subsequent steps. Here, we develop a general framework called no new U-Net (nnU-Net) for region of interest (ROI) segmentation in medical images. We apply a set of heuristic guidelines derived from the domain to systematically optimize the ROI segmentation task, which represents the dataset's key properties. Furthermore, high-resolution net (HRNet) is a standard neural network design developed for feature extraction. HRNet chooses the top-down strategy over the bottom-up method after considering the two options. It first detects the subject, generates a bounding box around the object and then estimates the relevant feature. The POA is used to minimize the subjective influence of manually selected parameters and enhance the LSTM's parameters. Thus, the POA-LSTM is used for the classification process, achieving higher performance for each performance metric such as accuracy, sensitivity, F1-score, precision, and specificity of 99%, 98.67%, 98.88%, 98.72%, and 98.43%, respectively.", "journal": "Journal of digital imaging", "date": "2023-07-26", "authors": ["RVinothini", "GNiranjana", "FitriYakub"], "doi": "10.1007/s10278-023-00852-7\n10.1016/j.compbiomed.2021.105127\n10.1016/j.patcog.2021.108499\n10.1016/j.clinimag.2021.02.003\n10.1080/02286203.2021.1934797\n10.1016/j.asoc.2020.106885\n10.1016/j.chaos.2020.110190\n10.1109/TCBB.2021.3065361\n10.1002/ima.22627\n10.1016/j.eswa.2022.116554\n10.1016/j.imu.2020.100412\n10.1016/j.bspc.2021.103076\n10.1016/j.bspc.2021.102920\n10.1016/j.asoc.2022.109683\n10.1007/s42979-021-00690-w\n10.3390/s22030855"}
{"title": "A super-resolution network for medical imaging via transformation analysis of wavelet multi-resolution.", "abstract": "In recent years, deep learning super-resolution models for progressive reconstruction have achieved great success. However, these models which refer to multi-resolution analysis basically ignore the information contained in the lower subspaces and do not explore the correlation between features in the wavelet and spatial domain, resulting in not fully utilizing the auxiliary information brought by multi-resolution analysis with multiple domains. Therefore, we propose a super-resolution network based on the wavelet multi-resolution framework (WMRSR) to capture the auxiliary information contained in multiple subspaces and to be aware of the interdependencies between spatial domain and wavelet domain features. Initially, the wavelet multi-resolution input (WMRI) is generated by combining wavelet sub-bands obtained from each subspace through wavelet multi-resolution analysis and the corresponding spatial domain image content, which serves as input to the network. Then, the WMRSR captures the corresponding features from the WMRI in the wavelet domain and spatial domain, respectively, and fuses them adaptively, thus learning fully explored features in multi-resolution and multi-domain. Finally, the high-resolution images are gradually reconstructed in the wavelet multi-resolution framework by our convolution-based wavelet transform module which is suitable for deep neural networks. Extensive experiments conducted on two public datasets demonstrate that our method outperforms other state-of-the-art methods in terms of objective and visual qualities.", "journal": "Neural networks : the official journal of the International Neural Network Society", "date": "2023-07-25", "authors": ["YueYu", "KunShe", "JinhuaLiu", "XiaoCai", "KaiboShi", "O MKwon"], "doi": "10.1016/j.neunet.2023.07.005"}
{"title": "Machine learning with multimodal data for COVID-19.", "abstract": "In response to the unprecedented global healthcare crisis of the COVID-19 pandemic, the scientific community has joined forces to tackle the challenges and prepare for future pandemics. Multiple modalities of data have been investigated to understand the nature of COVID-19. In this paper, MIDRC investigators present an overview of the state-of-the-art development of multimodal machine learning for COVID-19 and model assessment considerations for future studies. We begin with a discussion of the lessons learned from radiogenomic studies for cancer diagnosis. We then summarize the multi-modality COVID-19 data investigated in the literature including symptoms and other clinical data, laboratory tests, imaging, pathology, physiology, and other omics data. Publicly available multimodal COVID-19 data provided by MIDRC and other sources are summarized. After an overview of machine learning developments using multimodal data for COVID-19, we present our perspectives on the future development of multimodal machine learning models for COVID-19.", "journal": "Heliyon", "date": "2023-07-24", "authors": ["WeijieChen", "Rui CS\u00e1", "YuntongBai", "SandyNapel", "OlivierGevaert", "Diane SLauderdale", "Maryellen LGiger"], "doi": "10.1016/j.heliyon.2023.e17934"}
{"title": "Longitudinal assessment of demographic representativeness in the Medical Imaging and Data Resource Center open data commons.", "abstract": "The Medical Imaging and Data Resource Center (MIDRC) open data commons was launched to accelerate the development of artificial intelligence (AI) algorithms to help address the COVID-19 pandemic. The purpose of this study was to quantify longitudinal representativeness of the demographic characteristics of the primary MIDRC dataset compared to the United States general population (US Census) and COVID-19 positive case counts from the Centers for Disease Control and Prevention (CDC).\nThe Jensen-Shannon distance (JSD), a measure of similarity of two distributions, was used to longitudinally measure the representativeness of the distribution of (1)\u00a0all unique patients in the MIDRC data to the 2020 US Census and (2)\u00a0all unique COVID-19 positive patients in the MIDRC data to the case counts reported by the CDC. The distributions were evaluated in the demographic categories of age at index, sex, race, ethnicity, and the combination of race and ethnicity.\nRepresentativeness of the MIDRC data by ethnicity and the combination of race and ethnicity was impacted by the percentage of CDC case counts for which this was not reported. The distributions by sex and race have retained their level of representativeness over time.\nThe representativeness of the open medical imaging datasets in the curated public data commons at MIDRC has evolved over time as the number of contributing institutions and overall number of subjects have grown. The use of metrics, such as the JSD support measurement of representativeness, is one step needed for fair and generalizable AI algorithm development.", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2023-07-20", "authors": ["Heather MWhitney", "NatalieBaughan", "Kyle JMyers", "KarenDrukker", "JudyGichoya", "BradBower", "WeijieChen", "NicholasGruszauskas", "JayashreeKalpathy-Cramer", "SanmiKoyejo", "Rui CS\u00e1", "BerkmanSahiner", "ZiZhang", "Maryellen LGiger"], "doi": "10.1117/1.JMI.10.6.061105\n10.15585/mmwr.mm6942e1\n10.1073/pnas.2006392117\n10.1186/s12979-020-00183-z\n10.1056/NEJMsa2011686\n10.1371/journal.pmed.1003321\n10.1371/journal.pone.0254066\n10.1371/journal.pone.0249133\n10.1371/journal.pone.0254809\n10.1186/s13293-021-00407-x\n10.1089/jwh.2020.8974\n10.1007/s11606-020-06527-1\n10.1371/journal.pone.0268317\n10.18632/aging.203863\n10.1186/s13293-022-00417-3\n10.1016/j.socscimed.2022.114716\n10.1016/j.jacr.2020.10.016\n10.1016/j.jacr.2020.12.009\n10.1007/s00247-022-05357-z\n10.1117/12.2610239\n10.1109/TIT.2003.813506\n10.3390/e22020218\n10.14736/kyb-2021-6-0879\n10.1109/18.61115\n10.1136/bmjopen-2020-048008\n10.1007/s00134-020-06316-8\n10.1148/ryai.210290\n10.1148/ryai.220010\n10.1148/ryai.220061\n10.1093/jamia/ocac070\n10.1038/s41467-022-32186-3\n10.1117/1.JMI.10.6.061104\n10.7326/M18-1990\n10.1093/jalm/jfac085\n10.1613/jair.953\n10.1109/TNNLS.2021.3136503\n10.1007/s10916-020-01562-1\n10.1038/s41467-020-17971-2\n10.1186/s40537-019-0192-5\n10.1016/S0001-2998(78)80014-2\n10.1049/iet-syb.2016.0052\n10.1515/crll.1909.136.210\n10.1080/01621459.2014.929522\n10.1089/jwh.2012.3753\n10.15585/mmwr.mm7219e1\n10.15585/mmwr.mm7032a2"}
{"title": "Artificial intelligence, machine learning and deep learning: Potential resources for the infection clinician.", "abstract": "Artificial intelligence (AI), machine learning and deep learning (including generative AI) are increasingly being investigated in the context of research and management of human infection.\nWe summarise recent and potential future applications of AI and its relevance to clinical infection practice.\n1617 PubMed results were screened, with priority given to clinical trials, systematic reviews and meta-analyses. This narrative review focusses on studies using prospectively collected real-world data with clinical validation, and on research with translational potential, such as novel drug discovery and microbiome-based interventions.\nThere is some evidence of clinical utility of AI applied to laboratory diagnostics (e.g. digital culture plate reading, malaria diagnosis, antimicrobial resistance profiling), clinical imaging analysis (e.g. pulmonary tuberculosis diagnosis), clinical decision support tools (e.g. sepsis prediction, antimicrobial prescribing) and public health outbreak management (e.g. COVID-19). Most studies to date lack any real-world validation or clinical utility metrics. Significant heterogeneity in study design and reporting limits comparability. Many practical and ethical issues exist, including algorithm transparency and risk of bias.\nInterest in and development of AI-based tools for infection research and management are undoubtedly gaining pace, although the real-world clinical utility to date appears much more modest.", "journal": "The Journal of infection", "date": "2023-07-20", "authors": ["Anastasia ATheodosiou", "Robert CRead"], "doi": "10.1016/j.jinf.2023.07.006"}
{"title": "Evolution from Medical Imaging to Visualized Medicine.", "abstract": "The discovery of X-ray in 1895 and the first X-ray image of Mrs. R\u00f6ntgen's hand opened up a new era of radiology and the research of medical imaging. The evolution of traditional medical imaging has been lasting for over 100\u00a0years, serving the detection, diagnosis, and treatments of human diseases with a clear view of the anatomy information. In late 1990s, the concept of molecular imaging was proposed as the science and technology of molecular biology and bio-engineering rapidly developed, and it directly gave birth to the emergence of precision medicine for clinical lesion-targeted treatments against various cancers and cardiocerebrovascular diseases. Physiological and pathological changes in live bodies from zebrafish to human beings can be imaged to ensure an efficient image-guided therapy. Nowadays, the philosophy of medical and molecular imaging has been a powerful tool and indispensable modality for doctors to make their decisions and give patients reliable advices. With the ever-emerging developments of advanced intelligent technologies such as flexible sensors, medical meta-data analysis, brain sciences, surgical robots, VR/AR, etc., modern medicine has been evolving from traditional medical and molecular imaging to visualized medicine, which has created novel accessible approaches along with cutting-edge techniques for the revolutionized diagnostic and therapeutic paradigms. In this context, the history and milestones from medical imaging to visualized medicine will be elucidated. And in particular, representative visualized medicine advances including its application to COVID-19 epidemics will be discussed in order to look for its important contributions and a future perspective to modern medicine.", "journal": "Advances in experimental medicine and biology", "date": "2023-07-18", "authors": ["YuShi", "ZheLiu"], "doi": "10.1007/978-981-32-9902-3_1\n10.1088/0031-9155/51/13/R02\n10.1148/radiol.2381041602\n10.2967/jnumed.108.045880\n10.1103/PhysRev.51.652\n10.1103/PhysRev.53.318\n10.1088/0022-3719/6/22/007\n10.1038/242190a0\n10.1088/0022-3719/10/3/004\n10.1038/133201a0\n10.1172/JCI101129\n10.1001/jama.1946.02870190005002\n10.1001/jama.1946.02870490016004\n10.1007/BF02877929\n10.1136/bmj.b4428\n10.1016/S0140-6736(58)91905-6\n10.1016/S1076-6332(03)80220-9\n10.1148/radiology.219.2.r01ma19316\n10.1161/01.CIR.0000059651.17045.77\n10.1148/radiology.212.3.r99se18609\n10.1126/science.8303295\n10.1016/j.biomaterials.2022.121636\n10.1021/bc970058b\n10.1097/00006231-199509000-00002\n10.1109/MEMB.2008.923962\n10.1002/anie.199707261\n10.1002/mrm.1910080111\n10.1016/S0730-725X(99)00085-5\n10.3348/kjr.2003.4.4.201\n10.1088/0031-9155/53/4/R01\n10.1161/01.STR.0000143214.22567.cb\n10.1118/1.596606\n10.1109/23.531891\n10.1118/1.598392\n10.1097/00003072-200011000-00005\n10.3390/jcm11030578\n10.1007/s11912-021-01020-2\n10.1007/s11307-015-0855-3\n10.1155/2021/6678958\n10.7861/futurehosp.6-2-94\n10.1007/s10278-018-0122-7\n10.1177/089431849300600103\n10.14326/abe.11.87\n10.1016/j.ejmp.2020.11.012\n10.3389/fonc.2020.580919\n10.1016/j.actbio.2021.01.005\n10.1016/j.ejmp.2021.04.016\n10.21037/tcr.2018.05.02\n10.1007/s00779-020-01492-2\n10.1007/s11655-019-3169-5\n10.1007/s11548-015-1169-2\n10.1016/j.dsx.2020.05.011\n10.1109/ACCESS.2020.3045792\n10.1016/j.jep.2021.113957"}
{"title": "Chatbot vs Medical Student Performance on Free-Response Clinical Reasoning Examinations.", "abstract": "This study compares performance on free-response clinical reasoning examinations of first- and second-year medical students vs 2 models of a popular chatbot.", "journal": "JAMA internal medicine", "date": "2023-07-17", "authors": ["EricStrong", "AliciaDiGiammarino", "YingjieWeng", "AndreKumar", "PoonamHosamani", "JasonHom", "Jonathan HChen"], "doi": "10.1001/jamainternmed.2023.2909\n10.48550/arXiv.2303.13375\n10.1056/NEJMsr2214184"}
{"title": "Advancing Dermatological Care: A Comprehensive Narrative Review of Tele-Dermatology and mHealth for Bridging Gaps and Expanding Opportunities beyond the COVID-19 Pandemic.", "abstract": "Mobile health (mHealth) has recently had significant advances in tele-dermatology (TD) thanks to the developments following the COVID-19 pandemic. This topic is very important, as telemedicine and mHealth, when applied to dermatology, could improve both the quality of healthcare for citizens and the workflow in the ", "journal": "Healthcare (Basel, Switzerland)", "date": "2023-07-14", "authors": ["DanieleGiansanti"], "doi": "10.3390/healthcare11131911\n10.1007/s13671-023-00382-z\n10.1055/a-1999-7523\n10.3390/healthcare10030509\n10.3390/healthcare9070858\n10.1016/j.det.2017.06.014\n10.12788/j.sder.2016.013\n10.1007/s13555-022-00833-8\n10.1016/j.jaad.2019.01.067\n10.5070/D32510045811\n10.1371/journal.pone.0215379\n10.22605/RRH4895\n10.1159/000493728\n10.1136/bmjopen-2018-022218\n10.1007/s00403-018-1862-4\n10.3390/healthcare11111646\n10.1001/jamadermatol.2021.4724\n10.5070/D32610050455\n10.1089/tmj.2020.0500\n10.3390/jcm11061511\n10.3389/falgy.2022.919746\n10.1111/hex.13229\n10.1371/journal.pone.0232131\n10.2147/PTT.S323471\n10.25259/IJDVL_118_2022\n10.7417/CT.2022.2467\n10.2196/34017\n10.1016/j.glohj.2022.03.001\n10.1111/srt.13163\n10.2196/38792\n10.25259/IJDVL_608_2021\n10.1007/s00105-020-04664-6\n10.2196/28149\n10.1007/s00105-020-04658-4\n10.1080/09546634.2020.1789046\n10.1111/srt.13195\n10.1089/tmj.2019.0228\n10.1111/dth.15022\n10.1111/ajd.13330\n10.1111/ced.13995\n10.2196/39867\n10.1177/1357633X20930453\n10.1016/j.ad.2019.10.003\n10.1109/JBHI.2021.3123936\n10.2196/26149\n10.1016/j.cmpb.2020.105649\n10.1089/tmj.2020.0057\n10.1089/tmj.2022.0009\n10.1002/ski2.141\n10.3390/diagnostics12061371\n10.2196/44932\n10.3390/diagnostics11030451\n10.1007/s13555-020-00445-0\n10.1111/jdv.16275\n10.1007/s00105-020-04660-w\n10.1002/jbio.202200381\n10.1055/s-0041-1735181\n10.3390/healthcare10030415\n10.1111/jdv.15676\n10.1177/1357633X18780562\n10.1016/j.clindermatol.2018.09.007\n10.1016/j.jisp.2018.09.003\n10.1177/1461444818797082\n10.1080/21642850.2021.1926256\n10.3390/healthcare9040371\n10.3390/healthcare9010030\n10.3390/ijerph20105810\n10.3390/ijerph20053940\n10.1111/jocd.15222\n10.2196/30082\n10.1080/03091902.2020.1838641\n10.1111/jocd.13797\n10.5070/D3261047183\n10.1016/j.jaad.2019.07.008\n10.2196/mhealth.8671\n10.1021/acsnano.7b04898\n10.1002/adhm.202202021\n10.3390/s19153426\n10.1186/s12917-018-1428-x\n10.1016/j.medengphy.2006.07.006\n10.1016/j.medengphy.2007.08.002\n10.1089/tmj.2008.0105"}
{"title": "Comparing machine learning algorithms to predict COVID\u201119 mortality using a dataset including chest computed tomography severity score data.", "abstract": "Since the beginning of the COVID-19 pandemic, new and non-invasive digital technologies such as artificial intelligence (AI) had been introduced for mortality prediction of COVID-19 patients. The prognostic performances of the machine learning (ML)-based models for predicting clinical outcomes of COVID-19 patients had been mainly evaluated using demographics, risk factors, clinical manifestations, and laboratory results. There is a lack of information about the prognostic role of imaging manifestations in combination with demographics, clinical manifestations, and laboratory predictors. The purpose of the present study is to develop an efficient ML prognostic model based on a more comprehensive dataset including chest CT severity score (CT-SS). Fifty-five primary features in six main classes were retrospectively reviewed for 6854 suspected cases. The independence test of Chi-square was used to determine the most important features in the mortality prediction of COVID-19 patients. The most relevant predictors were used to train and test ML algorithms. The predictive models were developed using eight ML algorithms including the J48 decision tree (J48), support vector machine (SVM), multi-layer perceptron (MLP), k-nearest neighbourhood (k-NN), Na\u00efve Bayes (NB), logistic regression (LR), random forest (RF), and eXtreme gradient boosting (XGBoost). The performances of the predictive models were evaluated using accuracy, precision, sensitivity, specificity, and area under the ROC curve (AUC) metrics. After applying the exclusion criteria, a total of 815 positive RT-PCR patients were the final sample size, where 54.85% of the patients were male and the mean age of the study population was 57.22\u2009\u00b1\u200916.76\u00a0years. The RF algorithm with an accuracy of 97.2%, the sensitivity of 100%, a precision of 94.8%, specificity of 94.5%, F1-score of 97.3%, and AUC of 99.9% had the best performance. Other ML algorithms with AUC ranging from 81.2 to 93.9% had also good prediction performances in predicting COVID-19 mortality. Results showed that timely and accurate risk stratification of COVID-19 patients could be performed using ML-based predictive models fed by routine data. The proposed algorithm with the more comprehensive dataset including CT-SS could efficiently predict the mortality of COVID-19 patients. This could lead to promptly targeting high-risk patients on admission, the optimal use of hospital resources, and an increased probability of survival of patients.", "journal": "Scientific reports", "date": "2023-07-14", "authors": ["Seyed SalmanZakariaee", "NegarNaderi", "MahdiEbrahimi", "HadiKazemi-Arpanahi"], "doi": "10.1038/s41598-023-38133-6\n10.1186/s43055-022-00741-z\n10.1186/s13000-020-01017-8\n10.1002/rmv.2107\n10.1177/1084822320974956\n10.1371/journal.pdig.0000136\n10.1371/journal.pone.0243262\n10.1016/j.asoc.2020.106879\n10.1016/j.idh.2018.10.002\n10.1016/j.imu.2022.100908\n10.1038/s41467-020-18684-2\n10.1016/S2589-7500(20)30217-X\n10.31661/jbpe.v0i0.2104-1300\n10.1186/s12911-021-01742-0\n10.1038/s41598-020-79183-4\n10.7717/peerj-cs.604\n10.7717/peerj-cs.670\n10.1002/cpe.7393\n10.1145/3136625\n10.1093/bioinformatics/btm344\n10.1111/acem.13992\n10.7717/peerj.10083\n10.1007/s11739-020-02475-0\n10.1186/s13049-020-00795-w\n10.1371/journal.pone.0236618\n10.1038/s42256-020-0180-7\n10.1038/s41379-020-00700-x\n10.2196/23128\n10.1016/j.amsu.2020.09.044\n10.1186/s12938-017-0416-x\n10.1007/s10654-020-00669-6\n10.1038/s41467-020-18685-1\n10.1016/j.compbiomed.2022.106331\n10.1016/j.compbiomed.2022.105587\n10.1038/s41591-020-0931-3\n10.1016/j.clim.2022.109218\n10.1002/jmv.26218"}
{"title": "Prediction of oxygen supplementation by a deep-learning model integrating clinical parameters and chest CT images in COVID-19.", "abstract": "As of March 2023, the number of patients with COVID-19 worldwide is declining, but the early diagnosis of patients requiring inpatient treatment and the appropriate allocation of limited healthcare resources remain unresolved issues. In this study we constructed a deep-learning (DL) model to predict the need for oxygen supplementation using clinical information and chest CT images of patients with COVID-19.\nWe retrospectively enrolled 738 patients with COVID-19 for whom clinical information (patient background, clinical symptoms, and blood test findings) was available and chest CT imaging was performed. The initial data set was divided into 591 training and 147 evaluation data. We developed a DL model that predicted oxygen supplementation by integrating clinical information and CT images. The model was validated at two other facilities (n\u2009=\u2009191 and n\u2009=\u2009230). In addition, the importance of clinical information for prediction was assessed.\nThe proposed DL model showed an area under the curve (AUC) of 89.9% for predicting oxygen supplementation. Validation from the two other facilities showed an AUC\u2009>\u200980%. With respect to interpretation of the model, the contribution of dyspnea and the lactate dehydrogenase level was higher in the model.\nThe DL model integrating clinical information and chest CT images had high predictive accuracy. DL-based prediction of disease severity might be helpful in the clinical management of patients with COVID-19.", "journal": "Japanese journal of radiology", "date": "2023-07-13", "authors": ["NaokoKawata", "YumaIwao", "YukikoMatsuura", "MasakiSuzuki", "RyogoEma", "YukiSekiguchi", "HirotakaSato", "AkiraNishiyama", "MasaruNagayoshi", "YasuoTakiguchi", "TakujiSuzuki", "HideakiHaneishi"], "doi": "10.1007/s11604-023-01466-3\n10.1056/NEJMp2006141\n10.1002/jmv.25727\n10.1080/22221751.2020.1811161\n10.1186/s12931-021-01717-9\n10.1007/s00259-020-04735-9\n10.1148/radiol.2020200432\n10.1148/radiol.2020200463\n10.1148/radiol.2020200642\n10.1148/radiol.2020200343\n10.1111/resp.14101\n10.1259/bjro.20200024\n10.3348/kjr.2020.0567\n10.1007/s00259-020-05075-4\n10.1001/jamainternmed.2020.3539\n10.1111/all.14657\n10.1016/j.jinf.2020.04.021\n10.1016/s0140-6736(20)30566-3\n10.2147/vhrm.S280962\n10.7150/ijbs.58825\n10.1109/jbhi.2020.3034296\n10.1183/13993003.00775-2020\n10.1038/s41467-020-17280-8\n10.1016/s2589-7500(21)00039-x\n10.1016/j.acra.2021.05.002\n10.1109/tpami.2019.2918284\n10.1186/s40537-016-0043-6\n10.1109/ICCV.2017.74\n10.1109/ACCESS.2020.2976199\n10.1136/bmj.m3339\n10.1016/s2213-2600(20)30559-2\n10.21037/jtd-20-1363\n10.1148/radiol.2020200370\n10.1093/cid/ciaa1470\n10.1136/bmjopen-2020-047007\n10.1002/jmv.25871\n10.1155/2021/9917302\n10.1161/atvbaha.120.314872\n10.1038/s41467-020-17971-2\n10.1002/mp.15359\n10.12998/wjcc.v8.i22.5535\n10.3390/diagnostics12082028\n10.1073/pnas.2203437119\n10.1016/j.chest.2022.05.013\n10.3390/cancers12123532\n10.1016/j.cell.2020.08.029"}
{"title": "PhysVENeT: a physiologically-informed deep learning-based framework for the synthesis of 3D hyperpolarized gas MRI ventilation.", "abstract": "Functional lung imaging modalities such as hyperpolarized gas MRI ventilation enable visualization and quantification of regional lung ventilation; however, these techniques require specialized equipment and exogenous contrast, limiting clinical adoption. Physiologically-informed techniques to map proton (", "journal": "Scientific reports", "date": "2023-07-13", "authors": ["Joshua RAstley", "Alberto MBiancardi", "HelenMarshall", "Laurie JSmith", "Paul J CHughes", "Guilhem JCollier", "Laura CSaunders", "GrahamNorquay", "Malina-MariaTofan", "Matthew QHatton", "RodHughes", "Jim MWild", "Bilal ATahir"], "doi": "10.1038/s41598-023-38105-w\n10.2967/jnumed.109.073957\n10.2967/jnmt.121.262887\n10.1053/j.semnuclmed.2018.10.013\n10.1007/978-3-642-27994-2_22\n10.1259/bjr.20210207\n10.1002/mrm.25732\n10.1088/1361-6560/aa8074\n10.1016/j.clon.2016.08.005\n10.1148/radiol.2017160532\n10.1002/jmri.20290\n10.1136/thoraxjnl-2016-208948\n10.1016/j.ijrobp.2018.04.077\n10.1007/s00330-018-5888-y\n10.1016/j.ijrobp.2005.03.023\n10.1016/j.media.2008.03.007\n10.1002/mp.13346\n10.1016/j.radonc.2016.02.019\n10.1118/1.4856055\n10.1148/radiol.2019190395\n10.1186/1465-9921-7-106\n10.1002/mrm.22031\n10.1002/mrm.26893\n10.1016/j.zemedi.2016.07.005\n10.1148/radiol.2018171993\n10.1089/jam.2006.19.148\n10.1259/bjr.20201107\n10.3389/fonc.2022.883516\n10.1002/mp.14004\n10.1002/mp.13421\n10.1148/radiol.2020202861\n10.5194/esurf-2-67-2014\n10.1038/s41598-019-54176-0\n10.1002/jmri.25992\n10.1103/PhysRevLett.121.153201\n10.1088/0031-9155/59/23/7267\n10.1088/1361-6560/ab0145\n10.1016/j.cmpb.2018.01.025\n10.3390/app11041667\n10.1109/TIP.2003.819861\n10.1007/s00285-014-0792-9\n10.1016/j.ejrad.2016.09.027\n10.1152/japplphysiol.00464.2018\n10.1183/13993003.00441-2020"}
{"title": "Artificial Intelligence and Blockchain Enabled Smart Healthcare System for Monitoring and Detection of COVID-19 in Biomedical Images.", "abstract": "Millions of individuals around the world have been impacted by the ongoing coronavirus outbreak, known as the COVID-19 pandemic. Blockchain, Artificial Intelligence (AI), and other cutting-edge digital and innovative technologies have all offered promising solutions in such situations. AI provides advanced and innovative techniques for classifying and detecting symptoms caused by the coronavirus. Additionally, Blockchain may be utilized in healthcare in a variety of ways thanks to its highly open, secure standards, which permit a significant drop in healthcare costs and opens up new ways for patients to access medical services. Likewise, these techniques and solutions facilitate medical experts in the early diagnosis of diseases and later in treatments and sustaining pharmaceutical manufacturing. Therefore, in this work, a smart blockchain and AI-enabled system is presented for the healthcare sector that helps to combat the coronavirus pandemic. To further incorporate Blockchain technology, a new deep learning-based architecture is designed to identify the virus in radiological images. As a result, the developed system may offer reliable data-gathering platforms and promising security solutions, guaranteeing the high quality of COVID-19 data analytics. We created a multi-layer sequential deep learning architecture using a benchmark data set. In order to make the suggested deep learning architecture for the analysis of radiological images more understandable and interpretable, we also implemented the Gradient-weighted Class Activation Mapping (Grad-CAM) based colour visualization approach to all of the tests. As a result, the architecture achieves a classification accuracy rate of 0.96, thus producing excellent results.", "journal": "IEEE/ACM transactions on computational biology and bioinformatics", "date": "2023-07-12", "authors": ["ImranAhmed", "AbdellahChehri", "GwanggilJeon"], "doi": "10.1109/TCBB.2023.3294333"}
{"title": "A computationally-inexpensive strategy in CT image data augmentation for robust deep learning classification in the early stages of an outbreak.", "abstract": "Coronavirus disease 2019 (COVID-19) has spread globally for over three years, and chest computed tomography (CT) has been used to diagnose COVID-19 and identify lung damage in COVID-19 patients. Given its widespread, CT will remain a common diagnostic tool in future pandemics, but its effectiveness at the beginning of any pandemic will depend strongly on the ability to classify CT scans quickly and correctly when only limited resources are available, as it will happen inevitably again in future pandemics. Here, we resort into the transfer learning procedure and limited hyperparameters to use as few computing resources as possible for COVID-19 CT images classification. Advanced Normalisation Tools (ANTs) are used to synthesise images as augmented/independent data and trained on EfficientNet to investigate the effect of synthetic images. On the COVID-CT dataset, classification accuracy increases from 91.15% to 95.50% and Area Under the Receiver Operating Characteristic (AUC) from 96.40% to 98.54%. We also customise a small dataset to simulate data collected in the early stages of the outbreak and report an improvement in accuracy from 85.95% to 94.32% and AUC from 93.21% to 98.61%. This study provides a feasible Low-Threshold, Easy-To-Deploy and Ready-To-Use solution with a relatively low computational cost for medical image classification at an early stage of an outbreak in which scarce data are available and traditional data augmentation may fail. Hence, it would be most suitable for low-resource settings.", "journal": "Biomedical physics & engineering express", "date": "2023-07-07", "authors": ["YikunHou", "MiguelNavarro-C\u00eda"], "doi": "10.1088/2057-1976/ace4cf"}
{"title": "StrainNet: Improved Myocardial Strain Analysis of Cine MRI by Deep Learning from DENSE.", "abstract": "To develop a three-dimensional (two dimensions + time) convolutional neural network trained with displacement encoding with stimulated echoes (DENSE) data for displacement and strain analysis of cine MRI.\nIn this retrospective multicenter study, a deep learning model (StrainNet) was developed to predict intramyocardial displacement from contour motion. Patients with various heart diseases and healthy controls underwent cardiac MRI examinations with DENSE between August 2008 and January 2022. Network training inputs were a time series of myocardial contours from DENSE magnitude images, and ground truth data were DENSE displacement measurements. Model performance was evaluated using pixelwise end-point error (EPE). For testing, StrainNet was applied to contour motion from cine MRI. Global and segmental circumferential strain (E\nThe study included 161 patients (110 men; mean age, 61 years \u00b1 14 [SD]), 99 healthy adults (44 men; mean age, 35 years \u00b1 15), and 45 healthy children and adolescents (21 males; mean age, 12 years \u00b1 3). StrainNet showed good agreement with DENSE for intramyocardial displacement, with an average EPE of 0.75 mm \u00b1 0.35. The ICCs between StrainNet and DENSE and FT and DENSE were 0.87 and 0.72, respectively, for global E\nStrainNet outperformed FT for global and segmental E", "journal": "Radiology. Cardiothoracic imaging", "date": "2023-07-05", "authors": ["YuWang", "ChangyuSun", "SonaGhadimi", "Daniel CAuger", "PierreCroisille", "MagalieViallon", "KennethMangion", "ColinBerry", "Christopher MHaggerty", "LinyuanJing", "Brandon KFornwalt", "J JaneCao", "JoshuaCheng", "Andrew DScott", "Pedro FFerreira", "John NOshinski", "Daniel BEnnis", "Kenneth CBilchick", "Frederick HEpstein"], "doi": "10.1148/ryct.220196"}
{"title": "", "abstract": "Since its inaugural issue in 2019, ", "journal": "Radiology. Cardiothoracic imaging", "date": "2023-07-05", "authors": ["DomenicoMastrodicasa", "Gilberto JAquino", "Karen GOrdovas", "DanielVargas", "DominikFleischmann", "SuhnyAbbara", "KateHanneman"], "doi": "10.1148/ryct.230042"}
{"title": "Computed Tomography scanning in patients with COVID-19: artificial intelligence analysis of lesions volume and outcome.", "abstract": "The aim of this study was to summarize the computed tomography (CT) chest scanning results of COVID-19 patients, and to assess the value of artificial intelligence (AI) dynamics and quantitative analysis of lesion volume change for the evaluation of the disease outcome.\nFirst chest CT and reexamination imaging data of 84 patients diagnosed with COVID-19 who were treated at Jiangshan Hospital of Guiyang, Guizhou Province from February 4, 2020, to February 22, 2020, were retrospectively analyzed. Distribution, location, and nature of lesions were analyzed according to the characteristics of CT imaging and COVID-19 diagnosis and treatment guidelines. Based on the results of the analysis, patients were divided into the group without abnormal pulmonary imaging, the early group, the rapid progression group, and the dissipation group. AI software was used to dynamically measure the lesion volume in the first examination and in the cases with more than two reexaminations.\nThere were statistically significant differences in the age of patients between the groups (p<0.01). The first chest CT examination of the lung without abnormal imaging findings mainly occurred in young adults. Early and rapid progression was more common in the elderly, with a median age of 56 years. The ratio of the lesion to the total lung volume was 3.7 (1.4, 5.3) ml 0.1%, 15.4 (4.5, 36.8) ml 0.3%, 115.0 (44.5, 183.3) ml 3.33%, 32.6 (8.7, 98.0) ml 1.22% in the non-imaging group, early group, rapid progression group, and dissipation group, respectively. Pairwise comparison between the four groups was statistically significant (p<0.001). AI measured the total volume of pneumonia lesions and the proportion of the total volume of pneumonia lesions to predict the receiver operating characteristic (ROC) curve from early development to rapid progression, with a sensitivity of 92.10%, 96.83%, specificity of 100%, 80.56%, and the area under the curve of 0.789.\nAccurate measurement of lesion volume and volume changes by AI technology is helpful in assessing the severity and development trend of the disease. The increase in the lesion volume proportion indicates that the disease has entered a rapid progression period and is aggravated.", "journal": "European review for medical and pharmacological sciences", "date": "2023-07-04", "authors": ["Y-HZuo", "YChen", "L-HChen", "QZhang", "BQiu"], "doi": "10.26355/eurrev_202306_32826"}
{"title": "Machine and Deep Learning for Tuberculosis Detection on Chest X-Rays: Systematic Literature Review.", "abstract": "Tuberculosis (TB) was the leading infectious cause of mortality globally prior to COVID-19 and chest radiography has an important role in the detection, and subsequent diagnosis, of patients with this disease. The conventional experts reading has substantial within- and between-observer variability, indicating poor reliability of human readers. Substantial efforts have been made in utilizing various artificial intelligence-based algorithms to address the limitations of human reading of chest radiographs for diagnosing TB.\nThis systematic literature review (SLR) aims to assess the performance of machine learning (ML) and deep learning (DL) in the detection of TB using chest radiography (chest x-ray [CXR]).\nIn conducting and reporting the SLR, we followed the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. A total of 309 records were identified from Scopus, PubMed, and IEEE (Institute of Electrical and Electronics Engineers) databases. We independently screened, reviewed, and assessed all available records and included 47 studies that met the inclusion criteria in this SLR. We also performed the risk of bias assessment using Quality Assessment of Diagnostic Accuracy Studies version 2 (QUADAS-2) and meta-analysis of 10 included studies that provided confusion matrix results.\nVarious CXR data sets have been used in the included studies, with 2 of the most popular ones being Montgomery County (n=29) and Shenzhen (n=36) data sets. DL (n=34) was more commonly used than ML (n=7) in the included studies. Most studies used human radiologist's report as the reference standard. Support vector machine (n=5), k-nearest neighbors (n=3), and random forest (n=2) were the most popular ML approaches. Meanwhile, convolutional neural networks were the most commonly used DL techniques, with the 4 most popular applications being ResNet-50 (n=11), VGG-16 (n=8), VGG-19 (n=7), and AlexNet (n=6). Four performance metrics were popularly used, namely, accuracy (n=35), area under the curve (AUC; n=34), sensitivity (n=27), and specificity (n=23). In terms of the performance results, ML showed higher accuracy (mean ~93.71%) and sensitivity (mean ~92.55%), while on average DL models achieved better AUC (mean ~92.12%) and specificity (mean ~91.54%). Based on data from 10 studies that provided confusion matrix results, we estimated the pooled sensitivity and specificity of ML and DL methods to be 0.9857 (95% CI 0.9477-1.00) and 0.9805 (95% CI 0.9255-1.00), respectively. From the risk of bias assessment, 17 studies were regarded as having unclear risks for the reference standard aspect and 6 studies were regarded as having unclear risks for the flow and timing aspect. Only 2 included studies had built applications based on the proposed solutions.\nFindings from this SLR confirm the high potential of both ML and DL for TB detection using CXR. Future studies need to pay a close attention on 2 aspects of risk of bias, namely, the reference standard and the flow and timing aspects.\nPROSPERO CRD42021277155; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=277155.", "journal": "Journal of medical Internet research", "date": "2023-07-03", "authors": ["SengHansun", "AhmadrezaArgha", "Siaw-TengLiaw", "Branko GCeller", "Guy BMarks"], "doi": "10.2196/43154\n10.1016/j.ebiom.2019.07.056\n10.1016/s2214-109x(18)30520-5\n10.5588/ijtld.11.0581\n10.1016/j.neunet.2014.09.003\n10.1007/s12178-020-09600-8\n10.30744/brjac.2179-3425.AR-38-2021\n10.18421/tem101-10\n10.3389/fmicb.2019.00395\n10.1016/j.eswa.2020.113514\n10.1109/JBHI.2017.2767063\n10.1148/radiol.2017162326\n10.1007/s11042-019-07984-5\n10.3390/ijerph16020250\n10.1016/j.heliyon.2020.e04614\n10.3390/electronics11172634\n10.1371/journal.pone.0221339\n10.1371/journal.pone.0221339\n10.1007/s10916-022-01870-8\n10.1371/journal.pmed.1000097\n10.1371/journal.pmed.1000097\n10.1186/2046-4053-4-1\n10.1186/2046-4053-4-1\n10.1136/bmj.g7647\n10.1186/s13643-018-0699-4\n10.1186/s13643-018-0699-4\n10.1002/jrsm.1378\n10.3348/kjr.2015.16.6.1188\n10.7326/0003-4819-155-8-201110180-00009?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.7326/0003-4819-155-8-201110180-00009\n10.3978/j.issn.2223-4292.2014.11.20\n10.1109/cvpr.2017.369\n10.1109/icaict51780.2020.9333464\n10.1117/12.2216198\n10.1007/s11277-020-07075-x\n10.1109/tmi.2014.2350539\n10.1109/EMBC.2018.8512337\n10.1109/EMBC44109.2020.9175919\n10.1038/srep25265\n10.1038/srep25265\n10.1109/eit.2019.8833768\n10.1155/2020/8889023\n10.1155/2020/8889023\n10.1109/42.993132\n10.1145/3297156.3297251\n10.1109/elnano.2018.8477564\n10.1038/s41746-020-00322-2\n10.1038/s41746-020-00322-2\n10.1117/12.2293140\n10.2196/21790\n10.1155/2020/9205082\n10.1155/2020/9205082\n10.1088/1742-6596/1490/1/012020\n10.1109/cbms49503.2020.00103\n10.1109/EMBC.2019.8856729\n10.1109/icsipa.2017.8120663\n10.1080/21681163.2020.1808532\n10.1109/ict.2019.8798798\n10.35940/ijeat.a2632.109119\n10.5588/ijtld.17.0520\n10.1093/cid/ciy967\n10.1038/s41598-019-42557-4\n10.1038/s41598-019-42557-4\n10.11591/ijeecs.v17.i2.pp1014-1020\n10.11591/ijai.v8.i4.pp429-435\n10.1007/s13246-020-00966-0\n10.1016/j.cmpb.2021.106058\n10.1109/cibec.2018.8641816\n10.1109/access.2020.3041867\n10.1109/icces.2018.8639200\n10.1109/tmi.2015.2505672\n10.1007/s13246-021-00980-w\n10.21037/jtd.2019.08.34\n10.1109/access.2020.3031384\n10.1088/1748-0221/15/10/p10011\n10.3389/frai.2020.583427\n10.1109/access.2020.2970023\n10.1080/03610926.2019.1568485\n10.3389/fpubh.2017.00307\n10.1093/bjaceaccp/mkn041\n10.1007/s11042-019-08260-2\n10.1148/radiol.2019191293\n10.1038/s41566-020-0604-2\n10.1109/TKDE.2005.50\n10.1016/j.csda.2017.01.005\n10.1109/RBME.2020.3040715\n10.1109/access.2019.2933498\n10.1186/s40537-019-0197-0\n10.1016/j.aap.2020.105950\n10.3390/en13174291\n10.1111/1754-9485.13261\n10.1016/j.neucom.2020.04.045\n10.1109/jproc.2020.3004555\n10.1016/j.compbiomed.2021.105175\n10.1016/j.compbiomed.2021.105096\n10.1109/msp.2018.2885359\n10.1038/s41598-021-92799-4\n10.1038/s41598-021-92799-4\n10.1109/iccsai53272.2021.9609717\n10.1038/srep12215\n10.1038/srep12215\n10.1371/journal.pone.0093757\n10.1371/journal.pone.0093757\n10.1038/s41598-020-62148-y\n10.1038/s41598-020-62148-y\n10.1038/s41598-018-30810-1\n10.1038/s41598-018-30810-1\n10.1371/journal.pone.0106381\n10.1371/journal.pone.0106381\n10.1183/13993003.02159-2016\n10.1038/s41598-019-51503-3\n10.1038/s41598-019-51503-3\n10.1038/s41598-019-56589-3\n10.1038/s41598-019-56589-3"}
{"title": "AI in drug discovery and its clinical relevance.", "abstract": "The COVID-19 pandemic has emphasized the need for novel drug discovery process. However, the journey from conceptualizing a drug to its eventual implementation in clinical settings is a long, complex, and expensive process, with many potential points of failure. Over the past decade, a vast growth in medical information has coincided with advances in computational hardware (cloud computing, GPUs, and TPUs) and the rise of deep learning. Medical data generated from large molecular screening profiles, personal health or pathology records, and public health organizations could benefit from analysis by Artificial Intelligence (AI) approaches to speed up and prevent failures in the drug discovery pipeline. We present applications of AI at various stages of drug discovery pipelines, including the inherently computational approaches of ", "journal": "Heliyon", "date": "2023-07-03", "authors": ["RizwanQureshi", "MuhammadIrfan", "Taimoor MuzaffarGondal", "SheheryarKhan", "JiaWu", "Muhammad UsmanHadi", "JohnHeymach", "XiuningLe", "HongYan", "TanvirAlam"], "doi": "10.1016/j.heliyon.2023.e17575"}
{"title": "Application of Topic Modeling on Artificial Intelligence Studies as a Foundation to Develop Ethical Guidelines in African American Dementia Caregiving.", "abstract": "We applied natural language processing and topic modeling to publicly available abstracts and titles of 263 papers in the scientific literature mentioning AI and demographics (corpus 1 before Covid-19, corpus 2 after Covid-19) extracted from the MEDLINE database. We found exponential growth of AI studies mentioning demographics since the pandemic (Before Covid-19: N= 40 vs. After Covid-19: N= 223) [forecast model equation: ln(Number of Records) = 250.543*ln(Year) + -1904.38, p = 0.0005229]. Topics related to diagnostic imaging, quality of life, Covid, psychology, and smartphone increased during the pandemic, while cancer-related topics decreased. The application of topic modeling to the scientific literature on AI and demographics provides a foundation for the next steps regarding developing guidelines for the ethical use of AI for African American dementia caregivers.", "journal": "Studies in health technology and informatics", "date": "2023-06-30", "authors": ["SunmooYoon", "PeterBroadwell", "Frederick FSun", "MariaDe Planell-Saguer", "NicoleDavis"], "doi": "10.3233/SHTI230553"}
{"title": "Non-contrast CT synthesis using patch-based cycle-consistent generative adversarial network (Cycle-GAN) for radiomics and deep learning in the era of COVID-19.", "abstract": "Handcrafted and deep learning (DL) radiomics are popular techniques used to develop computed tomography (CT) imaging-based artificial intelligence models for COVID-19 research. However, contrast heterogeneity from real-world datasets may impair model performance. Contrast-homogenous datasets present a potential solution. We developed a 3D patch-based cycle-consistent generative adversarial network (cycle-GAN) to synthesize non-contrast images from contrast CTs, as a data homogenization tool. We used a multi-centre dataset of 2078 scans from 1,650 patients with COVID-19. Few studies have previously evaluated GAN-generated images with handcrafted radiomics, DL and human assessment tasks. We evaluated the performance of our cycle-GAN with these three approaches. In a modified Turing-test, human experts identified synthetic vs acquired images, with a false positive rate of 67% and Fleiss' Kappa 0.06, attesting to the photorealism of the synthetic images. However, on testing performance of machine learning classifiers with radiomic features, performance decreased with use of synthetic images. Marked percentage difference was noted in feature values between pre- and post-GAN non-contrast images. With DL classification, deterioration in performance was observed with synthetic images. Our results show that whilst GANs can produce images sufficient to pass human assessment, caution is advised before GAN-synthesized images are used in medical imaging applications.", "journal": "Scientific reports", "date": "2023-06-30", "authors": ["RezaKalantar", "SumeetHindocha", "BenjaminHunter", "BhupinderSharma", "NasirKhan", "Dow-MuKoh", "MerinaAhmed", "Eric OAboagye", "Richard WLee", "Matthew DBlackledge"], "doi": "10.1038/s41598-023-36712-1"}
{"title": "Explainable COVID-19 Detection Based on Chest X-rays Using an End-to-End RegNet Architecture.", "abstract": "COVID-19,which is caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), is one of the worst pandemics in recent history. The identification of patients suspected to be infected with COVID-19 is becoming crucial to reduce its spread. We aimed to validate and test a deep learning model to detect COVID-19 based on chest X-rays. The recent deep convolutional neural network (CNN) RegNetX032 was adapted for detecting COVID-19 from chest X-ray (CXR) images using polymerase chain reaction (RT-PCR) as a reference. The model was customized and trained on five datasets containing more than 15,000 CXR images (including 4148COVID-19-positive cases) and then tested on 321 images (150 COVID-19-positive) from Montfort Hospital. Twenty percent of the data from the five datasets were used as validation data for hyperparameter optimization. Each CXR image was processed by the model to detect COVID-19. Multi-binary classifications were proposed, such as: COVID-19 vs. normal, COVID-19 + pneumonia vs. normal, and pneumonia vs. normal. The performance results were based on the area under the curve (AUC), sensitivity, and specificity. In addition, an explainability model was developed that demonstrated the high performance and high generalization degree of the proposed model in detecting and highlighting the signs of the disease. The fine-tuned RegNetX032 model achieved an overall accuracy score of 96.0%, with an AUC score of 99.1%. The model showed a superior sensitivity of 98.0% in detecting signs from CXR images of COVID-19 patients, and a specificity of 93.0% in detecting healthy CXR images. A second scenario compared COVID-19 + pneumonia vs. normal (healthy X-ray) patients. The model achieved an overall score of 99.1% (AUC) with a sensitivity of 96.0% and specificity of 93.0% on the Montfort dataset. For the validation set, the model achieved an average accuracy of 98.6%, an AUC score of 98.0%, a sensitivity of 98.0%, and a specificity of 96.0% for detection (COVID-19 patients vs. healthy patients). The second scenario compared COVID-19 + pneumonia vs. normal patients. The model achieved an overall score of 98.8% (AUC) with a sensitivity of 97.0% and a specificity of 96.0%. This robust deep learning model demonstrated excellent performance in detecting COVID-19 from chest X-rays. This model could be used to automate the detection of COVID-19 and improve decision making for patient triage and isolation in hospital settings. This could also be used as a complementary aid for radiologists or clinicians when differentiating to make smart decisions.", "journal": "Viruses", "date": "2023-06-28", "authors": ["MohamedChetoui", "Moulay AAkhloufi", "El MostafaBouattane", "JosephAbdulnour", "StephaneRoux", "Chantal D'AoustBernard"], "doi": "10.3390/v15061327\n10.1148/radiol.2020200432\n10.7326/M20-1495\n10.1186/s12938-018-0544-y\n10.1148/81.2.185\n10.12988/ams.2015.54348\n10.1371/journal.pone.0247954\n10.3390/bioengineering8060084\n10.1080/14737167.2020.1823221\n10.2147/RMHP.S341500\n10.3390/jcm11113013\n10.3389/frai.2022.919672\n10.1371/journal.pone.0259179\n10.3390/electronics11223836\n10.1016/j.imu.2022.100945\n10.1080/23311916.2022.2079221\n10.1038/s41598-020-71294-2\n10.59275/j.melba.2020-48g7\n10.1109/ACCESS.2020.3010287\n10.1002/ima.22770\n10.1016/j.bspc.2022.103530\n10.3389/fgene.2022.980338\n10.18280/mmep.090615\n10.1016/j.compbiomed.2022.106065\n10.1007/s10522-021-09946-7\n10.1016/j.patcog.2021.108243\n10.17605/OSF.IO/NH7G8"}
{"title": "A Review of the Role of Artificial Intelligence in Healthcare.", "abstract": "Artificial intelligence (AI) applications have transformed healthcare. This study is based on a general literature review uncovering the role of AI in healthcare and focuses on the following key aspects: (i) medical imaging and diagnostics, (ii) virtual patient care, (iii) medical research and drug discovery, (iv) patient engagement and compliance, (v) rehabilitation, and (vi) other administrative applications. The impact of AI is observed in detecting clinical conditions in medical imaging and diagnostic services, controlling the outbreak of coronavirus disease 2019 (COVID-19) with early diagnosis, providing virtual patient care using AI-powered tools, managing electronic health records, augmenting patient engagement and compliance with the treatment plan, reducing the administrative workload of healthcare professionals (HCPs), discovering new drugs and vaccines, spotting medical prescription errors, extensive data storage and analysis, and technology-assisted rehabilitation. Nevertheless, this science pitch meets several technical, ethical, and social challenges, including privacy, safety, the right to decide and try, costs, information and consent, access, and efficacy, while integrating AI into healthcare. The governance of AI applications is crucial for patient safety and accountability and for raising HCPs' belief in enhancing acceptance and boosting significant health consequences. Effective governance is a prerequisite to precisely address regulatory, ethical, and trust issues while advancing the acceptance and implementation of AI. Since COVID-19 hit the global health system, the concept of AI has created a revolution in healthcare, and such an uprising could be another step forward to meet future healthcare needs.", "journal": "Journal of personalized medicine", "date": "2023-06-28", "authors": ["AhmedAl Kuwaiti", "KhalidNazer", "AbdullahAl-Reedy", "ShaherAl-Shehri", "AfnanAl-Muhanna", "Arun VijaySubbarayalu", "DhohaAl Muhanna", "Fahad AAl-Muhanna"], "doi": "10.3390/jpm13060951\n10.1057/s41301-020-00273-x\n10.22037/SDH.V3I4.20919\n10.1016/j.icte.2020.10.002\n10.1007/s40747-022-00767-w\n10.1016/j.techfore.2021.120712\n10.1109/icc42927.2021.9500397\n10.3390/ijerph18168700\n10.7861/fhj.2021-0095\n10.1038/s41746-020-0303-x\n10.3390/jcm8030360\n10.1136/svn-2017-000101\n10.1016/j.ijin.2022.05.002\n10.1016/j.hitech.2021.100405\n10.7717/peerj-cs.323\n10.31695/ijasre.2018.32708\n10.5815/ijitcs.2015.08.07\n10.3390/virtualworlds1020007\n10.1109/ACCESS.2021.3140175\n10.3390/jfmk7030063\n10.3390/bdcc7020062\n10.3390/healthcare11060887\n10.2196/27850\n10.1016/j.aiopen.2022.10.001\n10.1038/s41598-020-62922-y\n10.3390/ijerph182111086\n10.1016/j.imed.2022.07.002\n10.1109/JBHI.2022.3224727\n10.1016/j.media.2023.102802\n10.4018/JOEUC.308814\n10.7861/fhj.2021-0165\n10.34172/apb.2021.049\n10.1136/jclinpath-2017-204734\n10.3389/fmed.2020.00027\n10.1186/s40537-019-0217-0\n10.1109/ACCESS.2019.2939158\n10.1038/nature21056\n10.1038/npjschz.2015.30\n10.3390/jpm13030406\n10.3390/s21238045\n10.1007/s13721-022-00367-1\n10.48550/arXiv.2110.04458\n10.2991/ijcis.d.200828.001\n10.48550/arXiv.2110.14731\n10.1148/rg.2021200151\n10.7861/fhj.2022-0013\n10.1038/s41591-023-02289-5\n10.1001/jama.2013.629\n10.1038/s42256-022-00549-6\n10.1186/s12911-021-01488-9\n10.1016/S2589-7500(20)30160-6\n10.1007/s10916-017-0760-1\n10.1016/j.talanta.2017.08.077\n10.1016/j.cvdhj.2021.01.001\n10.2196/24465\n10.1038/s41746-020-00363-7\n10.1101/2021.01.08.21249474\n10.1111/ajt.16062\n10.1136/bmj-2021-068197\n10.1016/j.imu.2022.101029\n10.4103/0028-3886.288994\n10.1021/acsnano.0c08494\n10.2139/ssrn.3998068\n10.1109/ACCESS.2023.3241628\n10.1002/widm.1485\n10.3389/fdgth.2021.721044\n10.1016/j.tbench.2023.100105\n10.1186/2046-4053-4-5\n10.1186/s13063-021-05489-x\n10.3390/virtualworlds2020007\n10.12669/pjms.39.2.7653\n10.26717/BJSTR.2018.12.002189\n10.1098/rsif.2014.1289\n10.1016/j.drudis.2018.11.014\n10.4155/fmc-2018-0212\n10.1097/JCMA.0000000000000900\n10.3390/bdcc7010035\n10.1016/j.drudis.2020.10.010\n10.1080/17460441.2019.1621284\n10.3390/molecules23092384\n10.1155/2022/7205241\n10.1016/j.jiph.2022.01.011\n10.7861/futurehosp.6-2-94\n10.1097/PHM.0000000000001171\n10.1037/0000129-031\n10.7326/0003-4819-159-10-201311190-00006\n10.1370/afm.1531\n10.7326/M16-0961\n10.1136/bmjstel-2015-000054\n10.1109/LRA.2018.2810544\n10.4103/pjiap.pjiap_3_22\n10.1038/s42256-022-00593-2\n10.24313/jpbl.2018.5.1.29\n10.1177/2055207619871808\n10.3390/ijerph20021578\n10.1371/journal.pone.0211466\n10.3389/frobt.2021.612415\n10.1186/s12984-021-00819-1\n10.3390/s22186920\n10.1136/bmjhci-2020-100289\n10.1186/s12889-019-7920-9\n10.1108/IMDS-03-2015-0087\n10.1038/s41746-019-0132-y\n10.3390/healthcare10040608\n10.1093/jamia/ocaa154\n10.1016/j.ijmedinf.2022.104983\n10.3390/make5010006\n10.55041/IJSREM17592\n10.48550/arXiv.1806.07538\n10.1109/RBME.2022.3185953\n10.1109/TVCG.2021.3114851\n10.1007/s11886-013-0441-8\n10.1016/j.gaceta.2020.12.019\n10.18502/ijph.v50i11.7600\n10.1093/jamia/ocz192\n10.1038/s41746-022-00557-1\n10.3389/fdgth.2022.931439\n10.1093/jlb/lsz013\n10.3389/fpubh.2022.921226\n10.1186/s12916-019-1426-2\n10.1016/j.hfh.2022.100021\n10.1007/s44174-023-00063-2\n10.1093/jamia/ocy142\n10.1109/MIS.2013.51\n10.1016/j.artmed.2019.101762\n10.2196/32215\n10.2196/16866\n10.1108/JHOM-03-2020-0074"}
{"title": "COVID-19 Severity Prediction from Chest X-ray Images Using an Anatomy-Aware Deep Learning Model.", "abstract": "The COVID-19 pandemic has been adversely affecting the patient management systems in hospitals around the world. Radiological imaging, especially chest x-ray and lung Computed Tomography (CT) scans, plays a vital role in the severity analysis of hospitalized COVID-19 patients. However, with an increasing number of patients and a lack of skilled radiologists, automated assessment of COVID-19 severity using medical image analysis has become increasingly important. Chest x-ray (CXR) imaging plays a significant role in assessing the severity of pneumonia, especially in low-resource hospitals, and is the most frequently used diagnostic imaging in the world. Previous methods that automatically predict the severity of COVID-19 pneumonia mainly focus on feature pooling from pre-trained CXR models without explicitly considering the underlying human anatomical attributes. This paper proposes an anatomy-aware (AA) deep learning model that learns the generic features from x-ray images considering the underlying anatomical information. Utilizing a pre-trained model and lung segmentation masks, the model generates a feature vector including disease-level features and lung involvement scores. We have used four different open-source datasets, along with an in-house annotated test set for training and evaluation of the proposed method. The proposed method improves the geographical extent score by 11% in terms of mean squared error (MSE) while preserving the benchmark result in lung opacity score. The results demonstrate the effectiveness of the proposed AA model in COVID-19 severity prediction from chest X-ray images. The algorithm can be used in low-resource setting hospitals for COVID-19 severity prediction, especially where there is a lack of skilled radiologists.", "journal": "Journal of digital imaging", "date": "2023-06-28", "authors": ["Nusrat BintaNizam", "Sadi MohammadSiddiquee", "MahbubaShirin", "Mohammed Imamul HassanBhuiyan", "TaufiqHasan"], "doi": "10.1007/s10278-023-00861-6\n10.3390/s23010426\n10.1007/s00330-020-07033-y\n10.1001/jamanetworkopen.2020.22779\n10.1016/j.chaos.2020.110495\n10.1007/s10489-020-01829-7\n10.3390/sym13010113\n10.1148/rg.2018170048\n10.1016/j.eswa.2020.114054\n10.1007/s00500-020-05424-3\n10.1016/j.compbiomed.2022.106331\n10.1007/s10489-020-01867-1\n10.1038/s41598-022-27266-9\n10.1016/j.eswa.2020.113909\n10.26599/BDMA.2020.9020012\n10.3390/s23020743\n10.1007/s11042-022-12156-z\n10.1016/j.compbiomed.2020.103792\n10.1038/s41598-021-93543-8\n10.1016/j.asoc.2021.107645\n10.1016/j.asoc.2022.109851\n10.1007/s10489-020-01943-6\n10.1016/j.compmedimag.2019.05.005\n10.1109/JBHI.2022.3199594\n10.1007/s10278-021-00434-5\n10.1371/journal.pone.0236621\n10.1016/S2589-7500(21)00039-X\n10.1016/j.media.2021.102046\n10.2214/ajr.174.1.1740071\n10.1016/j.media.2005.02.002"}
{"title": "In vivo inhibition of nuclear ACE2 translocation protects against SARS-CoV-2 replication and lung damage through epigenetic imprinting.", "abstract": "In vitro, ACE2 translocates to the nucleus to induce SARS-CoV-2 replication. Here, using digital spatial profiling of lung tissues from SARS-CoV-2-infected golden Syrian hamsters, we show that a specific and selective peptide inhibitor of nuclear ACE2 (NACE2i) inhibits viral replication two days after SARS-CoV-2 infection. Moreover, the peptide also prevents inflammation and macrophage infiltration, and increases NK cell infiltration in bronchioles. NACE2i treatment increases the levels of the active histone mark, H3K27ac, restores host translation in infected hamster bronchiolar cells, and leads to an enrichment in methylated ACE2 in hamster bronchioles and lung macrophages, a signature associated with virus protection. In addition, ACE2 methylation is increased in myeloid cells from vaccinated patients and associated with reduced SARS-CoV-2 spike protein expression in monocytes from individuals who have recovered from infection. This protective epigenetic scarring of ACE2 is associated with a reduced latent viral reservoir in monocytes/macrophages and enhanced immune protection against SARS-CoV-2. Nuclear ACE2 may represent a therapeutic target independent of the variant and strain of viruses that use the ACE2 receptor for host cell entry.", "journal": "Nature communications", "date": "2023-06-28", "authors": ["Wen JuanTu", "MichelleMelino", "JennyDunn", "Robert DMcCuaig", "HelleBielefeldt-Ohmann", "SofiyaTsimbalyuk", "Jade KForwood", "TaniyaAhuja", "JohnVandermeide", "XiaoTan", "MinhTran", "QuanNguyen", "LiangZhang", "AndyNam", "LiuliuPan", "YanLiang", "CoreySmith", "KatieLineburg", "Tam HNguyen", "Julian D JSng", "Zhen Wei MarcusTong", "Keng YihChew", "Kirsty RShort", "RogerLe Grand", "NabilaSeddiki", "SudhaRao"], "doi": "10.1038/s41467-023-39341-4\n10.1038/nature06092\n10.1038/cr.2008.15\n10.1073/pnas.1016147107\n10.1038/s41421-021-00279-w\n10.1016/j.antiviral.2012.06.008\n10.1016/j.ceb.2019.01.001\n10.1128/JVI.75.1.506-512.2001\n10.1128/JVI.79.17.11507-11512.2005\n10.1016/j.virusres.2005.05.007\n10.3389/fmicb.2015.00553\n10.1128/JVI.75.19.9345-9356.2001\n10.1038/s41586-020-2342-5\n10.1080/10799890801941889\n10.1016/j.bbrc.2011.07.100\n10.1074/jbc.M109.075838\n10.1158/0008-5472.CAN-10-2384\n10.1083/jcb.201706118\n10.1016/j.tem.2004.03.001\n10.1038/s41541-021-00321-8\n10.1172/JCI138554\n10.1177/03009858211057197\n10.1155/2021/8874339\n10.1016/j.cell.2020.10.004\n10.15252/emmm.202114150\n10.1016/j.cell.2021.11.033\n10.1038/s41423-021-00825-2\n10.1038/s41423-021-00728-2\n10.1038/s41556-021-00685-y\n10.1016/j.immuni.2020.07.026\n10.1016/j.cell.2020.02.052\n10.1038/s41598-021-84882-7\n10.1038/s41422-020-0367-9\n10.1038/s41577-020-0353-y\n10.3389/fimmu.2021.768695\n10.3389/fimmu.2021.746021\n10.1136/annrheumdis-2018-214295\n10.1016/j.chom.2017.12.010\n10.3389/fimmu.2014.00461\n10.1002/jmv.25987\n10.3346/jkms.2020.35.e343\n10.1016/j.heliyon.2021.e06187\n10.1038/s41591-020-1051-9\n10.1016/j.imbio.2006.12.001\n10.3389/fimmu.2021.641360\n10.1038/s41586-020-2497-0\n10.1016/j.cell.2022.05.004\n10.1016/j.cmet.2022.04.009\n10.1038/s41586-021-03791-x\n10.1016/j.lfs.2020.118102\n10.1002/JLB.4HI0720-470R\n10.3389/fimmu.2021.745332\n10.1016/j.celrep.2020.107548\n10.1016/j.chom.2012.06.006\n10.1038/s41421-022-00453-8\n10.1186/s12977-017-0344-7\n10.4049/jimmunol.178.10.6581\n10.1038/s41564-018-0335-z\n10.1073/pnas.98.2.658\n10.1038/s41591-021-01283-z\n10.1016/S2213-2600(21)00031-X\n10.1038/s41586-022-04702-4\n10.1038/s41598-017-17204-5\n10.1038/s41467-021-23779-5\n10.1038/s41586-020-2286-9\n10.1038/s41467-022-28020-5\n10.3389/fninf.2013.00045"}
{"title": "Deep learning-based technique for lesions segmentation in CT scan images for COVID-19 prediction.", "abstract": "Since 2019, COVID-19 disease caused significant damage and it has become a serious health issue in the worldwide. The number of infected and confirmed cases is increasing day by day. Different hospitals and countries around the world to this day are not equipped enough to treat these cases and stop this pandemic evolution. Lung and chest X-ray images (e.g., radiography images) and chest CT images are the most effective imaging techniques to analyze and diagnose the COVID-19 related problems. Deep learning-based techniques have recently shown good performance in computer vision and healthcare fields. We propose developing a new deep learning-based application for COVID-19 segmentation and analysis in this work. The proposed system is developed based on the context aggregation neural network. This network consists of three main modules: the context fuse model (CFM), attention mix module (AMM) and a residual convolutional module (RCM). The developed system can detect two main COVID-19-related regions: ground glass opacity and consolidation area in CT images. Generally, these lesions are often related to common pneumonia and COVID 19 cases. Training and testing experiments have been conducted using the COVID-x-CT dataset. Based on the obtained results, the developed system demonstrated better and more competitive results compared to state-of-the-art performances. The numerical findings demonstrate the effectiveness of the proposed work by outperforming other works in terms of accuracy by a factor of over 96.23%.", "journal": "Multimedia tools and applications", "date": "2023-06-26", "authors": ["MounaAfif", "RiadhAyachi", "YahiaSaid", "MohamedAtri"], "doi": "10.1007/s11042-023-14941-w\n10.1007/s11042-022-12577-w\n10.1142/S0218001421500245\n10.1109/TPAMI.2016.2644615\n10.3390/rs11101158\n10.1109/TMM.2014.2373812\n10.1148/radiol.2017162326\n10.1148/radiol.2020200905\n10.1186/s12880-020-00529-5\n10.1007/s11042-022-12326-z\n10.1016/j.patcog.2021.108498\n10.1109/TGRS.2022.3170493\n10.2214/AJR.20.22976"}
{"title": "MediNet: transfer learning approach with MediNet medical visual database.", "abstract": "The rapid development of machine learning has increased interest in the use of deep learning methods in medical research. Deep learning in the medical field is used in disease detection and classification problems in the clinical decision-making process. Large amounts of labeled datasets are often required to train deep neural networks; however, in the medical field, the lack of a sufficient number of images in datasets and the difficulties encountered during data collection are among the main problems. In this study, we propose MediNet, a new 10-class visual dataset consisting of Rontgen (X-ray), Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Ultrasound, and Histopathological images such as calcaneal normal, calcaneal tumor, colon benign colon adenocarcinoma, brain normal, brain tumor, breast benign, breast malignant, chest normal, chest pneumonia. AlexNet, VGG19-BN, Inception V3, DenseNet 121, ResNet 101, EfficientNet B0, Nested-LSTM + CNN, and proposed RdiNet deep learning algorithms are used in the transfer learning for pre-training and classification application. Transfer learning aims to apply previously learned knowledge in a new task. Seven algorithms were trained with the MediNet dataset, and the models obtained from these algorithms, namely feature vectors, were recorded. Pre-training models were used for classification studies on chest X-ray images, diabetic retinopathy, and Covid-19 datasets with the transfer learning technique. In performance measurement, an accuracy of 94.84% was obtained in the traditional classification study for the InceptionV3 model in the classification study performed on the Chest X-Ray Images dataset, and the accuracy was increased 98.71% after the transfer learning technique was applied. In the Covid-19 dataset, the classification success of the DenseNet121 model before pre-trained was 88%, while the performance after the transfer application with MediNet was 92%. In the Diabetic retinopathy dataset, the classification success of the Nested-LSTM + CNN model before pre-trained was 79.35%, while the classification success was 81.52% after the transfer application with MediNet. The comparison of results obtained from experimental studies observed that the proposed method produced more successful results.", "journal": "Multimedia tools and applications", "date": "2023-06-26", "authors": ["Hatice CatalReis", "VeyselTurk", "KouroshKhoshelham", "SerhatKaya"], "doi": "10.1007/s11042-023-14831-1\n10.1109/ACCESS.2020.2989273\n10.1016/j.dib.2019.104863\n10.1038/s41598-021-83503-7\n10.3390/cancers14051280\n10.1186/s43057-021-00053-4\n10.1016/j.compbiomed.2019.103345\n10.1109/ACCESS.2019.2891970\n10.1038/nature21056\n10.1162/neco.1997.9.8.1735\n10.3390/diagnostics12020274\n10.1016/j.measurement.2020.108046\n10.1109/ACCESS.2017.2788044\n10.3934/mbe.2020328\n10.1038/s41598-020-78129-0\n10.1155/2022/7672196\n10.1016/j.ins.2019.06.011\n10.1016/j.imavis.2019.103853\n10.3390/ai1040032\n10.1117/1.JMI.7.3.034501\n10.1109/ACCESS.2020.2978629\n10.1038/s41598-020-61055-6\n10.1007/s00034-019-01246-3\n10.1038/s41598-019-52737-x\n10.1109/TIP.2021.3058783"}
{"title": "COVID-19 Detection using adopted convolutional neural networks and high-performance computing.", "abstract": "The COVID 19 pandemic is highly contagious disease is wreaking havoc on people's health and well-being around the world. Radiological imaging with chest radiography is one among the key screening procedure. This disease contaminates the respiratory system and impacts the alveoli, which are small air sacs in the lungs. Several artificial intelligence (AI)-based method to detect\u00a0COVID-19 have been\u00a0introduced. The recognition of disease patients using features and variation in chest radiography images was demonstrated using this model. In proposed paper presents a model, a deep convolutional neural network (CNN) with ResNet50 configuration, that really is freely-available and accessible to the common people for detecting this infection from chest radiography\u00a0scans. The introduced\u00a0model is capable of recognizing coronavirus diseases from CT scan images that identifies the real time condition of covid-19 patients. Furthermore, the database is capable of tracking detected patients and maintaining their database for increasing accuracy of the training model. The proposed model gives approximately 97% accuracy in determining the above-mentioned results related to covid-19 disease by employing the combination of adopted-CNN and ResNet50 algorithms.", "journal": "Multimedia tools and applications", "date": "2023-06-26", "authors": ["Anil KumarSingh", "AnkitKumar", "VinayKumar", "ShivPrakash"], "doi": "10.1007/s11042-023-15640-2\n10.1016/j.sjbs.2016.02.019\n10.1186/s40248-017-0101-8\n10.1109/ACCESS.2021.3085418\n10.1007/s10044-021-00970-4\n10.1039/c5cs00217f\n10.1038/s41467-019-10192-2\n10.1039/c3tb21526a\n10.1021/acsnano.5b03300\n10.1021/ja107583h\n10.1109/MITP.2020.3036820\n10.1021/acsnano.6b04858\n10.30604/well.95212020"}
{"title": "COVID-19 prediction based on hybrid Inception V3 with VGG16 using chest X-ray images.", "abstract": "The Corona\u00a0Virus was first started in the Wuhan city, China in December of 2019. It belongs to the Coronaviridae family, which can infect both animals and humans. The diagnosis of coronavirus disease-2019 (COVID-19) is typically detected by Serology, Genetic Real-Time reverse transcription-Polymerase Chain Reaction (RT-PCR), and Antigen testing. These testing methods have limitations like limited sensitivity, high cost, and long turn-around time. It is necessary to develop an automatic detection system for COVID-19 prediction. Chest X-ray is a lower-cost process in comparison to chest Computed tomography\u00a0(CT). Deep learning is the best fruitful technique of machine learning, which provides useful investigation for learning and screening a large amount of chest X-ray images with COVID-19 and normal. There are many deep learning methods for prediction, but these methods have a few limitations like overfitting, misclassification, and false predictions for poor-quality chest X-rays. In order to overcome these limitations, the novel hybrid model called \"Inception V3 with VGG16 (Visual Geometry Group)\" is proposed for the prediction of COVID-19 using chest X-rays. It is a combination of two deep learning models, Inception V3 and VGG16 (IV3-VGG). To build the hybrid model, collected 243 images from the COVID-19 Radiography Database. Out of 243 X-rays, 121 are COVID-19 positive and 122 are normal images. The hybrid model is divided into two modules namely pre-processing and the IV3-VGG. In the dataset, some of the images with different sizes and different color intensities are identified and pre-processed. The second module i.e., IV3-VGG consists of four blocks.\u00a0The first block is considered for VGG-16 and blocks 2 and 3 are considered for Inception V3 networks and final block 4 consists of four layers namely Avg pooling, dropout, fully connected, and Softmax layers. The experimental results show that the IV3-VGG model achieves the highest accuracy of 98% compared to the existing five prominent deep learning models such as Inception V3, VGG16, ResNet50, DenseNet121, and MobileNet.", "journal": "Multimedia tools and applications", "date": "2023-06-26", "authors": ["KSrinivas", "RGagana Sri", "KPravallika", "KNishitha", "Subba RaoPolamuri"], "doi": "10.1007/s11042-023-15903-y\n10.1016/j.asoc.2019.04.031\n10.1016/j.cell.2020.06.035\n10.1007/s10489-020-01941-8\n10.1186/s12985-015-0422-1\n10.1016/j.chaos.2020.110495\n10.1016/j.eswa.2020.114054\n10.3390/sym12040651\n10.1016/j.media.2020.101794\n10.1016/j.compbiomed.2020.103792\n10.1016/j.jinf.2020.03.051\n10.1016/j.ijid.2020.05.098\n10.1007/s10489-020-01900-3\n10.1128/CVI.00355-10\n10.1016/j.jinf.2020.04.022\n10.1016/j.compbiomed.2020.103805\n10.3201/eid2007.140296\n10.3201/eid1608.100208\n10.1007/s10489-020-02019-1"}
{"title": "Deep efficient-nets with transfer learning assisted detection of COVID-19 using chest X-ray radiology imaging.", "abstract": "Corona Virus (COVID-19) could be considered as one of the most devastating pandemics of the twenty-first century. The effective and the rapid screening of infected patients could reduce the mortality and even the contagion rate. Chest X-ray radiology could be designed as one of the effective screening techniques for COVID-19 exploration. In this paper, we propose an advanced approach based on deep learning architecture to automatic and effective screening techniques dedicated to the COVID-19 exploration through chest X-ray (CXR) imaging. Despite the success of state-of-the-art deep learning-based models for COVID-19 detection, they might suffer from several problems such as the huge memory and the computational requirement, the overfitting effect, and the high variance. To alleviate these issues, we investigate the Transfer Learning to the Efficient-Nets models. Next, we fine-tuned the whole network to select the optimal hyperparameters. Furthermore, in the preprocessing step, we consider an intensity-normalization method succeeded by some data augmentation techniques to solve the imbalanced dataset classes' issues. The proposed approach has presented a good performance in detecting patients attained by COVID-19 achieving an accuracy rate of 99.0% and 98% respectively using training and testing datasets. A comparative study over a publicly available dataset with the recently published deep-learning-based architectures could attest the proposed approach's performance.", "journal": "Multimedia tools and applications", "date": "2023-06-26", "authors": ["HibaMzoughi", "InesNjeh", "Mohamed BenSlima", "AhmedBenHamida"], "doi": "10.1007/s11042-023-15097-3\n10.1007/s42979-021-00981-2\n10.1007/s10278-021-00431-8\n10.1016/j.eswa.2020.114054\n10.1007/s42600-021-00151-6\n10.1007/s10044-021-00984-y\n10.1016/j.ijsu.2020.02.034\n10.1142/S0218001409007326\n10.1109/TII.2021.3057683\n10.1038/s41598-020-76550-z\n10.1016/j.patcog.2017.10.002"}
{"title": "Bio-medical imaging (X-ray, CT, ultrasound, ECG), genome sequences applications of deep neural network and machine learning in diagnosis, detection, classification, and segmentation of COVID-19: a Meta-analysis & systematic review.", "abstract": "This review investigates how Deep Machine Learning (DML) has dealt with the Covid-19 epidemic and provides recommendations for future Covid-19 research. Despite the fact that vaccines for this epidemic have been developed, DL methods have proven to be a valuable asset in radiologists' arsenals for the automated assessment of Covid-19. This detailed review debates the techniques and applications developed for Covid-19 findings using DL systems. It also provides insights into notable datasets used to train neural networks, data partitioning, and various performance measurement metrics. The PRISMA taxonomy has been formed based on pretrained(45 systems) and hybrid/custom(17 systems) models with radiography modalities. A total of 62 systems with respect to X-ray(32), CT(19), ultrasound(7), ECG(2), and genome sequence(2) based modalities as taxonomy are selected from the studied articles. We originate by valuing the present phase of DL and conclude with significant limitations. The restrictions contain incomprehensibility, simplification measures, learning from incomplete labeled data, and data secrecy. Moreover, DML can be utilized to detect and classify Covid-19 from other COPD illnesses. The proposed literature review has found many DL-based systems to fight against Covid19. We expect this article will assist in speeding up the procedure of DL for Covid-19 researchers, including medical, radiology technicians, and data engineers.", "journal": "Multimedia tools and applications", "date": "2023-06-26", "authors": ["Yogesh HBhosale", "K SridharPatnaik"], "doi": "10.1007/s11042-023-15029-1\n10.1109/ACCESS.2021.3058066\n10.1016/j.compbiomed.2017.09.017\n10.1016/j.scs.2020.102571\n10.1016/j.chaos.2020.110120\n10.1007/s40846-020-00529-4\n10.1016/j.compbiomed.2020.103795\n10.1016/j.asoc.2020.106912\n10.1016/j.bspc.2022.104445\n10.3390/app11020672\n10.1016/j.berh.2010.05.003\n10.1016/j.ibmed.2020.100013\n10.1002/pbc.20783\n10.1109/RBME.2020.2990959\n10.1038/nature21056\n10.1097/RLI.0000000000000748\n10.1016/j.bspc.2019.101678\n10.1109/TMI.2020.2996256\n10.1016/j.jiph.2020.03.019\n10.1007/s11517-022-02591-3\n10.1016/j.ibmed.2021.100027\n10.1109/ACCESS.2020.3016780\n10.1007/s13246-022-01102-w\n10.1016/j.imu.2020.100412\n10.1109/ACCESS.2021.3058537\n10.1016/j.bbe.2020.08.008\n10.1007/s10489-020-01902-1\n10.1504/IJCAT.2021.120462\n10.1007/s00521-021-05719-y\n10.1109/JBHI.2020.3042523\n10.1016/j.bbe.2021.01.002\n10.1016/j.bspc.2021.102518\n10.1016/j.asoc.2020.106744\n10.1016/j.cmpb.2020.105581\n10.3390/s22031211\n10.1109/ACCESS.2021.3058854\n10.1097/RTI.0000000000000404\n10.1016/j.jhep.2008.02.005\n10.1148/radiol.2020200905\n10.1145/3465220\n10.1016/j.measurement.2020.108288\n10.1007/s42600-021-00151-6\n10.1002/jmri.26534\n10.1016/j.inffus.2021.02.013\n10.1016/j.bspc.2020.102365\n10.1007/s00330-020-07044-9\n10.1016/j.asoc.2020.106580\n10.1016/j.compbiomed.2020.103792\n10.1109/TNNLS.2021.3054746\n10.1016/j.radi.2020.10.018\n10.1016/j.chaos.2020.109944\n10.1016/j.imu.2020.100360\n10.1109/ACCESS.2020.3003810\n10.1016/j.eswa.2022.116554\n10.1016/j.engappai.2021.104210\n10.1016/j.artmed.2021.102228\n10.1109/TMI.2020.2994459\n10.9781/ijimai.2020.04.003\n10.1007/s00521-020-05410-8\n10.1016/j.ajem.2012.08.041\n10.33889/IJMEMS.2020.5.4.052\n10.1016/j.imu.2020.100427\n10.1016/j.compbiomed.2021.104650\n10.1016/j.asoc.2022.108883\n10.3390/ijms16059749\n10.1016/j.compbiomed.2022.105233\n10.1016/j.compmedimag.2019.101673\n10.1016/j.compbiomed.2020.103805\n10.1038/ncpcardio1246\n10.1038/s41591-020-0823-6\n10.1109/TMI.2020.2995965\n10.1016/j.eng.2020.04.010\n10.1109/TBDATA.2021.3056564\n10.1007/s11227-020-03535-0\n10.1007/s10489-020-01867-1\n10.1007/s00259-020-04953-1"}
{"title": "A non-entropy-based optimal multilevel threshold selection technique for COVID-19 X-ray images using chance-based birds' intelligence.", "abstract": "Recently, image thresholding methods based on various entropy functions have been found popularity. Nonetheless, entropic-based methods depend on the spatial distribution of the grey level values in an image. Hence, the accuracy of these methods is limited due to the non-uniform distribution of the grey values. Further, the analysis of the COVID-19 X-ray images is evolved as an important area of research. Therefore, it is needed to develop an efficient method for the segmentation of the COVID-19 X-ray images. To address these issues, an efficient non-entropy-based thresholding method is suggested. A novel fitness function in terms of the segmentation score (SS) is introduced, which is used to reduce the segmentation error. A soft computing approach is suggested. An efficient optimizer using the chance-based birds' intelligence is introduced to maximize the fitness values. The new optimizer is validated utilizing the benchmark test functions. The statistical parameters reveal that the suggested optimizer is efficient. It shows a quite significant improvement over its counterparts-optimization based on seagull/cuckoo birds. Precisely, the paper includes three novel contributions-(i) fitness function, (ii) chance-based birds' intelligence for optimization, (iii) multiclass segmentation. The COVID-19 X-ray images are taken from the Kaggle Radiography database, to the experiment. Its results are compared with three different state-of-the-art entropy-based techniques-Tsallis, Kapur's, and Masi. For providing a statistical analysis, Friedman's mean rank test is conducted and our method Ranked one. Its superiority is claimed in terms of Peak Signal to Noise Ratio (PSNR), Feature Similarity Index (FSIM) and Structure Similarity Index (SSIM). On the whole, an improvement of about 11% in PSNR values is achieved using the proposed method. This method would be helpful for medical image analysis.", "journal": "Soft computing", "date": "2023-06-26", "authors": ["GyaneshDas", "MonoramaSwain", "RutuparnaPanda", "Manoj KNaik", "SanjayAgrawal"], "doi": "10.1007/s00500-023-08135-7\n10.1016/j.swevo.2013.02.001\n10.1016/j.jksuci.2018.04.007\n10.1016/j.compbiomed.2021.104984\n10.1109/ACCESS.2020.3010287\n10.1016/j.eswa.2017.04.023\n10.1016/0734-189X(85)90125-2\n10.3390/s21217286\n10.1016/j.knosys.2022.110192\n10.1109/TIP.2011.2109730\n10.1016/j.physleta.2005.01.094\n10.1371/journal.pone.0280006\n10.1016/j.swevo.2021.100907\n10.1016/j.jksuci.2020.10.030\n10.1109/TSMC.1979.4310076\n10.3139/120.111529\n10.1016/j.eswa.2022.118822\n10.1117/1.1631315\n10.1142/S1469026820500157\n10.1080/09720502.2020.1731976\n10.1016/j.compbiomed.2022.105618\n10.1109/4235.585893\n10.1016/j.asoc.2020.106157\n10.1016/j.procs.2015.09.027\n10.1016/j.asoc.2022.108538\n10.1109/TIP.2003.819861"}
{"title": "Deep Learning Based COVID-19 Detection via Hard Voting Ensemble Method.", "abstract": "Healthcare systems throughout the world are under a great deal of strain because to the continuing COVID-19 epidemic, making early and precise diagnosis critical for limiting the virus's propagation and efficiently treating victims. The utilization of medical imaging methods like X-rays can help to speed up the diagnosis procedure. Which can offer valuable insights into the virus's existence in the lungs. We present a unique ensemble approach to identify COVID-19 using X-ray pictures (X-ray-PIC) in this paper. The suggested approach, based on hard voting, combines the confidence scores of three classic deep learning models: CNN, VGG16, and DenseNet. We also apply transfer learning to enhance performance on small medical image datasets. Experiments indicate that the suggested strategy outperforms current techniques with a 97% accuracy, a 96% precision, a 100% recall, and a 98% F1-score.These results demonstrate the effectiveness of using ensemble approaches and COVID-19 transfer-learning diagnosis using X-ray-PIC, which could greatly aid in early detection and reducing the burden on global health systems.", "journal": "Wireless personal communications", "date": "2023-06-26", "authors": ["Asaad QasimShareef", "SeferKurnaz"], "doi": "10.1007/s11277-023-10485-2\n10.1007/s13204-021-02100-2\n10.1148/radiol.2020200642\n10.1016/j.ijleo.2022.170396\n10.3390/sym15010123\n10.1080/07391102.2020.1767212\n10.1155/2021/6677314\n10.1016/j.rinp.2020.103811\n10.1016/j.chaos.2021.110708\n10.1016/S0140-6736(20)30183-5\n10.1016/j.rinp.2021.103813\n10.1016/j.heliyon.2022.e11185\n10.1038/nature14539\n10.1056/NEJMOa2001316\n10.1016/j.xinn.2020.04.001\n10.1515/cclm-2020-0285\n10.1007/s10044-021-00984-y\n10.1016/j.ipm.2022.103025\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2020.103792\n10.1016/j.imu.2020.100506\n10.1016/j.chaos.2020.109944\n10.1109/RBME.2020.2987975\n10.32604/cmc.2022.020140\n10.1007/s10096-020-03901-z\n10.22270/jddt.v11i2-S.4644\n10.1016/j.compbiomed.2022.105233\n10.1038/s41586-020-2008-3\n10.1016/j.ejrad.2020.109041\n10.1093/cid/ciaa725"}
{"title": "ResNetFed: Federated Deep Learning Architecture for Privacy-Preserving Pneumonia Detection from COVID-19 Chest Radiographs.", "abstract": "Personal health data is subject to privacy regulations, making it challenging to apply centralized data-driven methods in healthcare, where personalized training data is frequently used. Federated Learning (FL) promises to provide a decentralized solution to this problem. In FL, siloed data is used for the model training to ensure data privacy. In this paper, we investigate the viability of the federated approach using the detection of COVID-19 pneumonia as a use case. 1411 individual chest radiographs, sourced from the public data repository COVIDx8 are used. The dataset contains radiographs of 753 normal lung findings and 658 COVID-19 related pneumonias. We partition the data unevenly across five separate data silos in order to reflect a typical FL scenario. For the binary image classification analysis of these radiographs, we propose ", "journal": "Journal of healthcare informatics research", "date": "2023-06-26", "authors": ["PascalRiedel", "Reinholdvon Schwerin", "DanielSchaudt", "AlexanderHafner", "ChristianSp\u00e4te"], "doi": "10.1007/s41666-023-00132-7\n10.1038/s42256-022-00601-5\n10.48550/ARXIV.2007.09339\n10.1001/jama.2018.5630\n10.17265/1548-6605/2021.05\n10.1093/jlb/lsz013\n10.1111/rego.12349\n10.13052/jmbmit2245-456X.434\n10.1109/TII.2022.3144016\n10.1111/tmi.13383\n10.1007/s10096-022-04417-4\n10.1016/j.acra.2022.06.002\n10.1007/s10489-020-02010-w\n10.1038/s41598-020-76550-z\n10.1109/ACCESS.2018.2885997\n10.1109/ACCESS.2020.3010287\n10.1109/ICCWAMTIP.2017.8301487\n10.1016/j.asoc.2021.107330\n10.1016/j.chaos.2021.110749\n10.1016/j.eswa.2020.114054\n10.1016/j.bspc.2022.103848\n10.1016/j.compbiomed.2022.105979\n10.1016/j.asoc.2022.109872\n10.1016/j.asoc.2021.107160\n10.1371/journal.pone.0242958\n10.1148/radiol.2021204522\n10.1609/aaai.v33i01.33019808\n10.1109/TIFS.2020.2988575"}
{"title": "Pathological changes or technical artefacts? The problem of the heterogenous databases in COVID-19 CXR image analysis.", "abstract": "When the COVID-19 pandemic commenced in 2020, scientists assisted medical specialists with diagnostic algorithm development. One scientific research area related to COVID-19 diagnosis was medical imaging and its potential to support molecular tests. Unfortunately, several systems reported high accuracy in development but did not fare well in clinical application. The reason was poor generalization, a long-standing issue in AI development. Researchers found many causes of this issue and decided to refer to them as confounders, meaning a set of artefacts and methodological errors associated with the method. We aim to contribute to this steed by highlighting an undiscussed confounder related to image resolution.\n20 216 chest X-ray images (CXR) from worldwide centres were analyzed. The CXRs were bijectively projected into the 2D domain by performing Uniform Manifold Approximation and Projection (UMAP) embedding on the radiomic features (rUMAP) or CNN-based neural features (nUMAP) from the pre-last layer of the pre-trained classification neural network. Additional 44 339 thorax CXRs were used for validation. The comprehensive analysis of the multimodality of the density distribution in rUMAP/nUMAP domains and its relation to the original image properties was used to identify the main confounders.\nnUMAP revealed a hidden bias of neural networks towards the image resolution, which the regular up-sampling procedure cannot compensate for. The issue appears regardless of the network architecture and is not observed in a high-resolution dataset. The impact of the resolution heterogeneity can be partially diminished by applying advanced deep-learning-based super-resolution networks.\nrUMAP and nUMAP are great tools for image homogeneity analysis and bias discovery, as demonstrated by applying them to COVID-19 image data. Nonetheless, nUMAP could be applied to any type of data for which a deep neural network could be constructed. Advanced image super-resolution solutions are needed to reduce the impact of the resolution diversity on the classification network decision.", "journal": "Computer methods and programs in biomedicine", "date": "2023-06-26", "authors": ["MarekSocha", "WojciechPra\u017cuch", "AleksandraSuwalska", "Pawe\u0142Foszner", "JoannaTobiasz", "JerzyJaroszewicz", "KatarzynaGruszczynska", "MagdalenaSliwinska", "MateuszNowak", "BarbaraGizycka", "GabrielaZapolska", "TadeuszPopiela", "GrzegorzPrzybylski", "PiotrFiedor", "MalgorzataPawlowska", "RobertFlisiak", "KrzysztofSimon", "JerzyWalecki", "AndrzejCieszanowski", "EdytaSzurowska", "MichalMarczyk", "JoannaPolanska", "NoneNone"], "doi": "10.1016/j.cmpb.2023.107684\n10.7189/jogh.09.020318\n10.2217/fvl-2020-0130\n10.1016/j.cmpb.2020.105581\n10.1007/s10489-020-01829-7\n10.1016/j.compbiomed.2020.103792\n10.1038/s41598-020-76550-z\n10.1038/s42256-021-00307-0\n10.1109/IMCET53404.2021.9665574\n10.1007/s12553-021-00520-2\n10.1148/ryai.220028\n10.1371/journal.pone.0235187\n10.1109/TMI.2020.3040950\n10.1109/TMI.2020.2993291\n10.1371/journal.pone.0100335\n10.1186/s12916-019-1426-2\n10.1371/journal.pmed.1002683\n10.3390/s21217116\n10.1016/j.media.2021.102225\n10.1148/ryai.2021210011\n10.1200/CCI.19.00068\n10.1117/1.JMI.6.2.027501\n10.1038/s42256-021-00338-7\n10.1109/ACCESS.2020.3010287\n10.1038/s41597-023-02229-5\n10.21227/w3aw-rv39\n10.1007/978-3-319-24574-4_28\n10.5555/3294771.3294864\n10.1109/CVPR.2017.369\n10.3978/j.issn.2223-4292.2014.11.20\n10.1016/j.cell.2018.02.010\n10.1016/j.ejca.2011.11.036\n10.1158/0008-5472.CAN-17-0339\n10.1007/978-3-319-46493-0_38\n10.1109/CVPR.2017.195\n10.1109/CVPR.2016.308\n10.1109/CVPR.2017.243\n10.48550/arxiv.1409.1556\n10.21105/joss.00861\n10.1109/CVPRW.2017.151\n10.1145/3422622\n10.1007/978-3-030-11021-5_5\n10.1109/TMI.2017.2715284\n10.1109/TMI.2018.2827462\n10.3390/DIAGNOSTICS12030741\n10.1007/S11004-018-9743-0/TABLES/1\n10.1109/PRAI53619.2021.9551043\n10.1109/ACCESS.2019.2918926\n10.1148/ryai.2019190015"}
{"title": "LayNet-A multi-layer architecture to handle imbalance in medical imaging data.", "abstract": "In an imbalanced dataset, a machine learning classifier using traditional imbalance handling methods may achieve good accuracy, but in highly imbalanced datasets, it may over-predict the majority class and ignore the minority class. In the medical domain, failing to correctly estimate the minority class might lead to a false negative, which is concerning in cases of life-threatening illnesses and infectious diseases like Covid-19. Currently, classification in deep learning has a single layered architecture where a neural network is employed. This paper proposes a multilayer design entitled LayNet to address this issue. LayNet aims to lessen the class imbalance by dividing the classes among layers and achieving a balanced class distribution at each layer. To ensure that all the classes are being classified, minor classes are combined to form a single new 'hybrid' class at higher layers. The final layer has no hybrid class and only singleton(distinct) classes. Each layer of the architecture includes a separate model that determines if an input belongs to one class or a hybrid class. If it fits into the hybrid class, it advances to the following layer, which is further categorized within the hybrid class. The method to divide the classes into various architectural levels is also introduced in this paper. The Ocular Disease Intelligent Recognition Dataset, Covid-19 Radiography Dataset, and Retinal OCT Dataset are used to evaluate this methodology. The LayNet architecture performs better on these datasets when the results of the traditional single-layer architecture and the proposed multilayered architecture are compared.", "journal": "Computers in biology and medicine", "date": "2023-06-25", "authors": ["JayJani", "JayDoshi", "IshitaKheria", "KarishniMehta", "ChetashriBhadane", "RuhinaKarani"], "doi": "10.1016/j.compbiomed.2023.107179"}
{"title": "ConvCoroNet: a deep convolutional neural network optimized with iterative thresholding algorithm for Covid-19 detection using chest X-ray images.", "abstract": "Covid-19 is a global pandemic. Early and accurate detection of positive cases prevent the further spread of this epidemic and help to treat rapidly the infected patients. During the peak of this epidemic, there was an insufficiency of Covid-19 test kits. In addition, this technique takes a considerable time in the diagnosis. Hence the need to find fast, accurate and low-cost method to replace or supplement RT PCR-based methods. Covid-19 is a respiratory disease, chest X-ray images are often used to diagnose pneumonia. From this perspective, these images can play an important role in the Covid-19 detection. In this article, we propose ConvCoroNet, a deep convolutional neural network model optimized with new method based on iterative thresholding algorithm to detect coronavirus automatically from chest X-ray images. ConvCoroNet is trained on a dataset prepared by collecting chest X-ray images of Covid-19, pneumonia and normal cases from publically datasets. The experimental results of our proposed model show a high accuracy of 99.50%, sensitivity of 98.80% and specificity of 99.85% when detecting Covid-19 from chest X-ray images. ConvCoroNet achieves promising results in the automatic detection of Covid-19 from chest X-ray images. It may be able to help radiologists in the Covid-19 detection by reducing the examination time of X-ray images.Communicated by Ramaswamy H. Sarma.", "journal": "Journal of biomolecular structure & dynamics", "date": "2023-06-24", "authors": ["MMerrouchi", "YBenyoussef", "MSkittou", "KAtifi", "TGadi"], "doi": "10.1080/07391102.2023.2227726"}
{"title": "Automatic diagnosis of COVID-19 from CT images using CycleGAN and transfer learning.", "abstract": "The outbreak of the corona virus disease (COVID-19) has changed the lives of most people on Earth. Given the high prevalence of this disease, its correct diagnosis in order to quarantine patients is of the utmost importance in the steps of fighting this pandemic. Among the various modalities used for diagnosis, medical imaging, especially computed tomography (CT) imaging, has been the focus of many previous studies due to its accuracy and availability. In addition, automation of diagnostic methods can be of great help to physicians. In this paper, a method based on pre-trained deep neural networks is presented, which, by taking advantage of a cyclic generative adversarial net (CycleGAN) model for data augmentation, has reached state-of-the-art performance for the task at hand, i.e., 99.60% accuracy. Also, in order to evaluate the method, a dataset containing 3163 images from 189 patients has been collected and labeled by physicians. Unlike prior datasets, normal data have been collected from people suspected of having COVID-19 disease and not from data from other diseases, and this database is made available publicly. Moreover, the method's reliability is further evaluated by calibration metrics, and its decision is interpreted by Grad-CAM also to find suspicious regions as another output of the method and make its decisions trustworthy and explainable.", "journal": "Applied soft computing", "date": "2023-06-22", "authors": ["NavidGhassemi", "AfshinShoeibi", "MarjaneKhodatars", "JonathanHeras", "AlirezaRahimi", "AssefZare", "Yu-DongZhang", "Ram BilasPachori", "J ManuelGorriz"], "doi": "10.1016/j.asoc.2023.110511\n10.1109/ACCESS.2020.3001973\n10.1007/s00264-020-04609-7\n10.1016/j.jcv.2020.104438\n10.1038/s41598-020-80363-5\n10.1016/j.dsx.2020.05.008\n10.22038/IJMP.2016.8453\n10.1016/j.jiph.2020.06.028\n10.1056/NEJMoa1306742\n10.1016/j.molliq.2020.114706\n10.1016/j.chaos.2020.110059\n10.1128/JCM.02959-20\n10.1016/j.clinimag.2021.01.019\n10.1016/j.bspc.2021.102622\n10.2196/23811\n10.1109/RBME.2020.2990959\n10.1007/s10916-020-01582-x\n10.1080/14737159.2020.1757437\n10.1001/jama.2020.2783\n10.1148/radiol.2020200432\n10.1016/j.compbiomed.2020.103792\n10.1007/s10140-020-01886-y\n10.1016/j.bspc.2021.103182\n10.1016/j.compbiomed.2021.104454\n10.1016/j.bspc.2020.102365\n10.1016/j.compbiomed.2020.104181\n10.5152/dir.2020.20205\n10.1016/j.chaos.2020.110495\n10.1038/s41467-020-20657-4\n10.1016/j.bspc.2021.103076\n10.1007/s00330-021-07715-1\n10.1109/ACCESS.2020.3005510\n10.1109/ACCESS.2021.3058537\n10.1016/j.matpr.2020.06.245\n10.1109/BIBM49941.2020.9313252\n10.1016/j.chaos.2020.109947\n10.1109/RBME.2020.2987975\n10.1117/12.2582162\n10.1109/CVPR.2016.90\n10.1016/j.neucom.2020.05.078\n10.1016/j.compbiomed.2021.104697\n10.1016/j.bspc.2021.103417\n10.1007/978-3-030-55258-9_17\n10.1016/j.neucom.2020.07.144\n10.1016/j.media.2020.101836\n10.1109/TMI.2020.2995508\n10.1007/s00521-020-05437-x\n10.1007/s12559-020-09785-7\n10.1016/j.irbm.2020.05.003\n10.1016/j.compbiomed.2020.104037\n10.1080/09720502.2020.1857905\n10.1016/j.patrec.2020.10.001\n10.1016/j.asoc.2020.106885\n10.1007/s10489-020-02149-6\n10.1007/s00259-020-04929-1\n10.1016/j.irbm.2021.01.004\n10.1016/j.inffus.2020.10.004\n10.1007/s00330-021-07715-1\n10.3389/fmed.2020.608525\n10.1007/s10489-020-01826-w\n10.1007/s00330-020-06956-w\n10.1007/s12539-020-00408-1\n10.1016/j.compbiomed.2020.103795\n10.1109/ACCESS.2020.3005510\n10.1007/978-3-030-55258-9_5\n10.4236/jbise.2020.137014\n10.32604/cmes.2020.011920\n10.1109/ISCC50000.2020.9219726\n10.1016/j.bspc.2019.101678\n10.3390/computers10010006\n10.1155/2021/4832864\n10.1016/j.eswa.2020.113788\n10.1109/IV48863.2021.9575841\n10.1145/3377325.3377519\n10.1109/ICCV.2017.74\n10.1016/j.patrec.2021.11.020"}
{"title": "To segment or not to segment: COVID-19 detection for chest X-rays.", "abstract": "Artificial intelligence (AI) has been integrated into most technologies we use. One of the most promising applications in AI is medical imaging. Research demonstrates that AI has improved the performance of most medical imaging analysis systems. Consequently, AI has become a fundamental element of the state of the art with improved outcomes across a variety of medical imaging applications. Moreover, it is believed that computer vision (CV) algorithms are highly effective for image analysis. Recent advances in CV facilitate the recognition of patterns in medical images. In this manner, we investigate CV segmentation techniques for COVID-19 analysis. We use different segmentation techniques, such as k-means, U-net, and flood fill, to extract the lung region from CXRs. Afterwards, we compare the effectiveness of these three segmentation approaches when applied to CXRs. Then, we use machine learning (ML) and deep learning (DL) models to identify COVID-19 lesion molecules in both healthy and pathological lung x-rays. We evaluate our ML and DL findings in the context of CV techniques. Our results indicate that the segmentation-related CV techniques do not exhibit comparable performance to DL and ML techniques. The most optimal AI algorithm yields an accuracy range of 0.92-0.94, whereas the addition of CV algorithms leads to a reduction in accuracy to approximately the range of 0.81-0.88. In addition, we test the performance of DL models under real-world noise, such as salt and pepper noise, which negatively impacts the overall performance.", "journal": "Informatics in medicine unlocked", "date": "2023-06-22", "authors": ["SaraAl Hajj Ibrahim", "KhalilEl-Khatib"], "doi": "10.1016/j.imu.2023.101280"}
{"title": "Osteoporosis and Covid-19: Detected similarities in bone lacunar-level alterations via combined AI and advanced synchrotron testing.", "abstract": "While advanced imaging strategies have improved the diagnosis of bone-related pathologies, early signs of bone alterations remain difficult to detect. The Covid-19 pandemic has brought attention to the need for a better understanding of bone micro-scale toughening and weakening phenomena. This study used an artificial intelligence-based tool to automatically investigate and validate four clinical hypotheses by examining osteocyte lacunae on a large scale with synchrotron image-guided failure assessment. The findings indicate that trabecular bone features exhibit intrinsic variability related to external loading, micro-scale bone characteristics affect fracture initiation and propagation, osteoporosis signs can be detected at the micro-scale through changes in osteocyte lacunar features, and Covid-19 worsens micro-scale porosities in a statistically significant manner similar to the osteoporotic condition. Incorporating these findings with existing clinical and diagnostic tools could prevent micro-scale damages from progressing into critical fractures.", "journal": "Materials & design", "date": "2023-06-16", "authors": ["FedericaBuccino", "LuigiZagra", "ElenaLongo", "LorenzoD'Amico", "GiuseppeBanfi", "FilippoBerto", "GiulianaTromba", "Laura MariaVergani"], "doi": "10.1016/j.matdes.2023.112087"}
{"title": "CT medical image segmentation algorithm based on deep learning technology.", "abstract": "For the problems of blurred edges, uneven background distribution, and many noise interferences in medical image segmentation, we proposed a medical image segmentation algorithm based on deep neural network technology, which adopts a similar U-Net backbone structure and includes two parts: encoding and decoding. Firstly, the images are passed through the encoder path with residual and convolutional structures for image feature information extraction. We added the attention mechanism module to the network jump connection to address the problems of redundant network channel dimensions and low spatial perception of complex lesions. Finally, the medical image segmentation results are obtained using the decoder path with residual and convolutional structures. To verify the validity of the model in this paper, we conducted the corresponding comparative experimental analysis, and the experimental results show that the DICE and IOU of the proposed model are 0.7826, 0.9683, 0.8904, 0.8069, and 0.9462, 0.9537 for DRIVE, ISIC2018 and COVID-19 CT datasets, respectively. The segmentation accuracy is effectively improved for medical images with complex shapes and adhesions between lesions and normal tissues.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2023-06-16", "authors": ["TongpingShen", "FangliangHuang", "XusongZhang"], "doi": "10.3934/mbe.2023485"}
{"title": "COV-MobNets: a mobile networks ensemble model for diagnosis of COVID-19 based on chest X-ray images.", "abstract": "The medical profession is facing an excessive workload, which has led to the development of various Computer-Aided Diagnosis (CAD) systems as well as Mobile-Aid Diagnosis (MAD) systems. These technologies enhance the speed and accuracy of diagnoses, particularly in areas with limited resources or remote regions during the pandemic. The primary purpose of this research is to predict and diagnose COVID-19 infection from chest X-ray images by developing a mobile-friendly deep learning framework, which has the potential for deployment in portable devices such as mobile or tablet, especially in situations where the workload of radiology specialists may be high. Moreover, this could improve the accuracy and transparency of population screening to assist radiologists during the pandemic.\nIn this study, the Mobile Networks ensemble model called COV-MobNets is proposed to classify positive COVID-19 X-ray images from negative ones and can have an assistant role in diagnosing COVID-19. The proposed model is an ensemble model, combining two lightweight and mobile-friendly models: MobileViT based on transformer structure and MobileNetV3 based on Convolutional Neural Network. Hence, COV-MobNets can extract the features of chest X-ray images in two different methods to achieve better and more accurate results. In addition, data augmentation techniques were applied to the dataset to avoid overfitting during the training process. The COVIDx-CXR-3 benchmark dataset was used for training and evaluation.\nThe classification accuracy of the improved MobileViT and MobileNetV3 models on the test set has reached 92.5% and 97%, respectively, while the accuracy of the proposed model (COV-MobNets) has reached 97.75%. The sensitivity and specificity of the proposed model have also reached 98.5% and 97%, respectively. Experimental comparison proves the result is more accurate and balanced than other methods.\nThe proposed method can distinguish between positive and negative COVID-19 cases more accurately and quickly. The proposed method proves that utilizing two automatic feature extractors with different structures as an overall framework of COVID-19 diagnosis can lead to improved performance, enhanced accuracy, and better generalization to new or unseen data. As a result, the proposed framework in this study can be used as an effective method for computer-aided diagnosis and mobile-aided diagnosis of COVID-19. The code is available publicly for open access at https://github.com/MAmirEshraghi/COV-MobNets .", "journal": "BMC medical imaging", "date": "2023-06-16", "authors": ["Mohammad AmirEshraghi", "AhmadAyatollahi", "Shahriar BaradaranShokouhi"], "doi": "10.1186/s12880-023-01039-w\n10.3390/app10165683\n10.7717/PEERJ-CS.364\n10.1016/j.patrec.2020.09.010\n10.1038/s41598-020-76550-z\n10.1109/ACCESS.2020.2994762\n10.48550/arxiv.2010.11929\n10.32604/CMC.2022.031147\n10.48550/arxiv.2206.03671\n10.3390/e23111383\n10.48550/arxiv.1704.04861\n10.48550/arxiv.2110.02178\n10.3390/electronics12010223"}
{"title": "A genetic programming-based convolutional deep learning algorithm for identifying COVID-19 cases via X-ray images.", "abstract": "Evolutionary algorithms have been successfully employed to find the best structure for many learning algorithms including neural networks. Due to their flexibility and promising results, Convolutional Neural Networks (CNNs) have found their application in many image processing applications. The structure of CNNs greatly affects the performance of these algorithms both in terms of accuracy and computational cost, thus, finding the best architecture for these networks is a crucial task before they are employed. In this paper, we develop a genetic programming approach for the optimization of CNN structure in diagnosing COVID-19 cases via X-ray images. A graph representation for CNN architecture is proposed and evolutionary operators including crossover and mutation are specifically designed for the proposed representation. The proposed architecture of CNNs is defined by two sets of parameters, one is the skeleton which determines the arrangement of the convolutional and pooling operators and their connections and one is the numerical parameters of the operators which determine the properties of these operators like filter size and kernel size. The proposed algorithm in this paper optimizes the skeleton and the numerical parameters of the CNN architectures in a co-evolutionary scheme. The proposed algorithm is used to identify covid-19 cases via X-ray images.", "journal": "Artificial intelligence in medicine", "date": "2023-06-15", "authors": ["Mohammad Hassan TayaraniNajaran"], "doi": "10.1016/j.artmed.2023.102571\n10.1109/TIP.2015.2475625\n10.1109/TEVC.2011.2163638"}
{"title": "An amalgamation of vision transformer with convolutional neural network for automatic lung tumor segmentation.", "abstract": "Lung cancer has the highest mortality rate. Its diagnosis and treatment analysis depends upon the accurate segmentation of the tumor. It becomes tedious if done manually as radiologists are overburdened with numerous medical imaging tests due to the increase in cancer patients and the COVID pandemic. Automatic segmentation techniques play an essential role in assisting medical experts. The segmentation approaches based on convolutional neural networks have provided state-of-the-art performances. However, they cannot capture long-range relations due to the region-based convolutional operator. Vision Transformers can resolve this issue by capturing global multi-contextual features. To explore this advantageous feature of the vision transformer, we propose an approach for lung tumor segmentation using an amalgamation of the vision transformer and convolutional neural network. We design the network as an encoder-decoder structure with convolution blocks deployed in the initial layers of the encoder to capture the features carrying essential information and the corresponding blocks in the final layers of the decoder. The deeper layers utilize the transformer blocks with a self-attention mechanism to capture more detailed global feature maps. We use a recently proposed unified loss function that combines cross-entropy and dice-based losses for network optimization. We trained our network on a publicly available NSCLC-Radiomics dataset and tested its generalizability on our dataset collected from a local hospital. We could achieve average dice coefficients of 0.7468 and 0.6847 and Hausdorff distances of 15.336 and 17.435 on public and local test data, respectively.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2023-06-15", "authors": ["ShwetaTyagi", "Devidas TKushnure", "Sanjay NTalbar"], "doi": "10.1016/j.compmedimag.2023.102258"}
{"title": "Artificial intelligence-assisted quantification of COVID-19 pneumonia burden from computed tomography improves prediction of adverse outcomes over visual scoring systems.", "abstract": "We aimed to evaluate the effectiveness of utilizing artificial intelligence (AI) to quantify the extent of pneumonia from chest CT scans, and to determine its ability to predict clinical deterioration or mortality in patients admitted to the hospital with COVID-19 in comparison to semi-quantitative visual scoring systems.\nA deep-learning algorithm was utilized to quantify the pneumonia burden, while semi-quantitative pneumonia severity scores were estimated through visual means. The primary outcome was clinical deterioration, the composite end point including admission to the intensive care unit, need for invasive mechanical ventilation, or vasopressor therapy, as well as in-hospital death.\nThe final population comprised 743 patients (mean age 65 \u202f\u00b1\u202f 17 years, 55% men), of whom 175 (23.5%) experienced clinical deterioration or death. The area under the receiver operating characteristic curve (AUC) for predicting the primary outcome was significantly higher for AI-assisted quantitative pneumonia burden (0.739, \nUtilizing AI-assisted quantification of pneumonia burden from chest CT scans offers a more accurate prediction of clinical deterioration in patients with COVID-19 compared to semi-quantitative severity scores, while requiring only a fraction of the analysis time.\nQuantitative pneumonia burden assessed using AI demonstrated higher performance for predicting clinical deterioration compared to current semi-quantitative scoring systems. Such an AI system has the potential to be applied for image-based triage of COVID-19 patients in clinical practice.", "journal": "The British journal of radiology", "date": "2023-06-13", "authors": ["KajetanGrodecki", "AdityaKillekar", "JuditSimon", "AndrewLin", "SebastienCadet", "PriscillaMcElhinney", "CatoChan", "Michelle CWilliams", "Barry DPressman", "PeterJulien", "DebiaoLi", "PeterChen", "NicolaGaibazzi", "UditThakur", "ElisabettaMancini", "CeciliaAgalbato", "JiroMunechika", "HidenariMatsumoto", "RobertoMen\u00e8", "GianfrancoParati", "FrancoCernigliaro", "NiteshNerlekar", "CamillaTorlasco", "GianlucaPontone", "PalMaurovich-Horvat", "Piotr JSlomka", "DaminiDey"], "doi": "10.1259/bjr.20220180\n10.1038/s41598-020-80061-2\n10.1016/j.jcct.2020.08.013\n10.1007/s00330-020-06817-6\n10.1148/ryct.2020200047\n10.1038/s41598-021-84561-7\n10.1148/rg.2017170077\n10.1016/j.cell.2020.04.045\n10.1148/ryct.2020200389\n10.1148/radiol.2462070712\n10.1002/sim.4085\n10.2307/2529310\n10.1148/radiol.2020201365\n10.1016/j.jcct.2020.03.002\n10.1016/j.jcmg.2020.04.012\n10.2214/ajr.169.4.9308447\n10.1148/radiol.2363040958\n10.1016/j.rmed.2020.106271\n10.1148/radiol.2020201433\n10.1109/TMI.2018.2804799\n10.1148/ryct.2020200441\n10.1016/j.acra.2011.01.011\n10.1002/0470011815\n10.1016/j.metabol.2020.154436\n10.1371/journal.pone.0242400\n10.1016/j.ajem.2020.05.073\n10.5152/dir.2020.20232"}
{"title": "Evolution of the newest diagnostic methods for COVID-19: a Chinese perspective.", "abstract": "Coronavirus disease 2019 (COVID-19) has continued to spread globally since late 2019, representing a formidable challenge to the world's healthcare systems, wreaking havoc, and spreading rapidly through human contact. With fever, fatigue, and a persistent dry cough being the hallmark symptoms, this disease threatened to destabilize the delicate balance of our global community. Rapid and accurate diagnosis of COVID-19 is a prerequisite for understanding the number of confirmed cases in the world or a region, and an important factor in epidemic assessment and the development of control measures. It also plays a crucial role in ensuring that patients receive the appropriate medical treatment, leading to optimal patient care. Reverse transcription-polymerase chain reaction (RT-PCR) technology is currently the most mature method for detecting viral nucleic acids, but it has many drawbacks. Meanwhile, a variety of COVID-19 detection methods, including molecular biological diagnostic, immunodiagnostic, imaging, and artificial intelligence methods have been developed and applied in clinical practice to meet diverse scenarios and needs. These methods can help clinicians diagnose and treat COVID-19 patients. This review describes the variety of such methods used in China, providing an important reference in the field of the clinical diagnosis of COVID-19.\n\u81ea2019\u5e74\u5e95\u4ee5\u6765\uff0c\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u611f\u67d3\uff08COVID-19\uff09\u7ee7\u7eed\u5728\u5168\u7403\u8513\u5ef6\uff0c\u5bf9\u4e16\u754c\u536b\u751f\u4fdd\u5065\u7cfb\u7edf\u6784\u6210\u4e25\u5cfb\u6311\u6218\uff0c\u9020\u6210\u4e25\u91cd\u7834\u574f\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u63a5\u89e6\u8fc5\u901f\u4f20\u64ad\u3002\u8fd9\u79cd\u75be\u75c5\u7684\u4e3b\u8981\u75c7\u72b6\u662f\u53d1\u70e7\u3001\u75b2\u52b3\u548c\u6301\u7eed\u7684\u5e72\u54b3\uff0c\u5b83\u5a01\u80c1\u5230\u6211\u4eec\u5168\u7403\u793e\u4f1a\u5065\u5eb7\u7cfb\u7edf\u7684\u5e73\u8861\u3002\u5feb\u901f\u51c6\u786e\u8bca\u65ad\u65b0\u51a0\u80ba\u708e\u662f\u638c\u63e1\u5168\u7403\u6216\u5730\u533a\u786e\u8bca\u75c5\u4f8b\u6570\u91cf\u7684\u524d\u63d0\uff0c\u4e5f\u662f\u75ab\u60c5\u8bc4\u4f30\u548c\u5236\u5b9a\u63a7\u5236\u63aa\u65bd\u7684\u91cd\u8981\u56e0\u7d20\u3002\u540c\u65f6\uff0c\u8fd9\u4e5f\u786e\u4fdd\u60a3\u8005\u83b7\u5f97\u5408\u7406\u6cbb\u7597\uff0c\u4e3a\u60a3\u8005\u83b7\u5f97\u6700\u4f73\u7684\u62a4\u7406\u65b9\u6848\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u9006\u8f6c\u5f55\u805a\u5408\u9176\u94fe\u53cd\u5e94\uff08RT-PCR\uff09\u6280\u672f\u662f\u76ee\u524d\u68c0\u6d4b\u75c5\u6bd2\u6838\u9178\u6700\u6210\u719f\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u5b58\u5728\u5f88\u591a\u7f3a\u70b9\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u5206\u5b50\u751f\u7269\u5b66\u8bca\u65ad\u3001\u514d\u75ab\u8bca\u65ad\u3001\u5f71\u50cf\u5b66\u3001\u4eba\u5de5\u667a\u80fd\u7b49\u591a\u79cd\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u68c0\u6d4b\u65b9\u6cd5\u5df2\u7ecf\u5f00\u53d1\u5e76\u5e94\u7528\u4e8e\u4e34\u5e8a\uff0c\u9002\u5e94\u4e0d\u540c\u573a\u666f\u548c\u9700\u6c42\u3002\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u8bca\u65ad\u548c\u6cbb\u7597COVID-19\u60a3\u8005\u3002\u672c\u6587\u603b\u7ed3\u4e86\u76ee\u524d\u56fd\u5185\u5e38\u7528\u4e8e\u65b0\u51a0\u80ba\u708e\u8bca\u65ad\u7684\u591a\u79cd\u6700\u65b0\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u65b0\u51a0\u80ba\u708e\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002.\n\u81ea2019\u5e74\u5e95\u4ee5\u6765\uff0c\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u611f\u67d3\uff08COVID-19\uff09\u7ee7\u7eed\u5728\u5168\u7403\u8513\u5ef6\uff0c\u5bf9\u4e16\u754c\u536b\u751f\u4fdd\u5065\u7cfb\u7edf\u6784\u6210\u4e25\u5cfb\u6311\u6218\uff0c\u9020\u6210\u4e25\u91cd\u7834\u574f\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u63a5\u89e6\u8fc5\u901f\u4f20\u64ad\u3002\u8fd9\u79cd\u75be\u75c5\u7684\u4e3b\u8981\u75c7\u72b6\u662f\u53d1\u70e7\u3001\u75b2\u52b3\u548c\u6301\u7eed\u7684\u5e72\u54b3\uff0c\u5b83\u5a01\u80c1\u5230\u6211\u4eec\u5168\u7403\u793e\u4f1a\u5065\u5eb7\u7cfb\u7edf\u7684\u5e73\u8861\u3002\u5feb\u901f\u51c6\u786e\u8bca\u65ad\u65b0\u51a0\u80ba\u708e\u662f\u638c\u63e1\u5168\u7403\u6216\u5730\u533a\u786e\u8bca\u75c5\u4f8b\u6570\u91cf\u7684\u524d\u63d0\uff0c\u4e5f\u662f\u75ab\u60c5\u8bc4\u4f30\u548c\u5236\u5b9a\u63a7\u5236\u63aa\u65bd\u7684\u91cd\u8981\u56e0\u7d20\u3002\u540c\u65f6\uff0c\u8fd9\u4e5f\u786e\u4fdd\u60a3\u8005\u83b7\u5f97\u5408\u7406\u6cbb\u7597\uff0c\u4e3a\u60a3\u8005\u83b7\u5f97\u6700\u4f73\u7684\u62a4\u7406\u65b9\u6848\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u9006\u8f6c\u5f55\u805a\u5408\u9176\u94fe\u53cd\u5e94\uff08RT-PCR\uff09\u6280\u672f\u662f\u76ee\u524d\u68c0\u6d4b\u75c5\u6bd2\u6838\u9178\u6700\u6210\u719f\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u5b58\u5728\u5f88\u591a\u7f3a\u70b9\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u5206\u5b50\u751f\u7269\u5b66\u8bca\u65ad\u3001\u514d\u75ab\u8bca\u65ad\u3001\u5f71\u50cf\u5b66\u3001\u4eba\u5de5\u667a\u80fd\u7b49\u591a\u79cd\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u68c0\u6d4b\u65b9\u6cd5\u5df2\u7ecf\u5f00\u53d1\u5e76\u5e94\u7528\u4e8e\u4e34\u5e8a\uff0c\u9002\u5e94\u4e0d\u540c\u573a\u666f\u548c\u9700\u6c42\u3002\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u8bca\u65ad\u548c\u6cbb\u7597COVID-19\u60a3\u8005\u3002\u672c\u6587\u603b\u7ed3\u4e86\u76ee\u524d\u56fd\u5185\u5e38\u7528\u4e8e\u65b0\u51a0\u80ba\u708e\u8bca\u65ad\u7684\u591a\u79cd\u6700\u65b0\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u65b0\u51a0\u80ba\u708e\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "journal": "Journal of Zhejiang University. Science. B", "date": "2023-06-13", "authors": ["MingtaoLiu", "JialiLyu", "XianhuiZheng", "ZhimanLiang", "BaoyingLei", "HuihuangChen", "YiyinMai", "HuiminHuang", "BaoqingSun"], "doi": "10.1631/jzus.B2200625\n10.1016/j.jadohealth.2020.06.025\n10.3390/diagnostics11050855\n10.1016/j.tmaid.2020.101608\n10.1148/radiol.2020200823\n10.1631/jzus.B2000479\n10.1016/j.cmi.2020.03.020\n10.1148/radiol.2020200463\n10.1016/j.ebiom.2021.103502\n10.1016/j.jcv.2021.104900\n10.1038/s41591-020-01202-8\n10.1038/s41586-022-05644-7\n10.1016/j.ijsu.2022.106882\n10.1016/S0140-6736(20)30154-9\n10.18006/2022.10(4).679.688\n10.1002/phar.2439\n10.1111/jch.14308\n10.1016/j.bios.2020.112642\n10.1007/s15010-020-01401-y\n10.1007/s12016-021-08900-2\n10.3390/ijms22137135\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1016/S2666-5247(21)00056-2\n10.1021/acsinfecdis.0c00274\n10.1126/science.abb4489\n10.1111/resp.12022\n10.1007/s12098-020-03322-y\n10.1039/D2CC01297A\n10.1148/radiol.2020200432\n10.1002/jmv.26829\n10.1016/S0140-6736(20)32635-0\n10.1016/j.bbi.2021.12.007\n10.1016/j.talanta.2021.123197\n10.1016/j.foodchem.2018.12.013\n10.1039/D2AN00556E\n10.1126/science.abi6680\n10.1002/dta.2946\n10.1016/j.ijid.2021.02.098\n10.1126/science.1179504\n10.1038/s41564-018-0296-2\n10.1146/annurev-pathmechdis-012418-012751\n10.1038/s41575-021-00416-6\n10.3201/eid2606.200516\n10.1016/j.ijid.2020.08.029\n10.1016/j.ijid.2021.02.005\n10.1016/j.ijid.2021.05.067\n10.1128/JVI.00711-20\n10.1128/mSphere.00808-20\n10.1039/C9AN02485A\n10.1038/nbt729\n10.3390/ijerph192315638\n10.3389/fpubh.2022.985445\n10.22207/JPAM.16.SPL1.18\n10.3389/fimmu.2022.837290\n10.22207/JPAM.16.SPL1.16\n10.1016/j.scitotenv.2022.159350\n10.3389/fitd.2022.1068364\n10.1016/j.coesh.2022.100396\n10.1016/j.envpol.2022.119679\n10.1038/s41598-020-72533-2\n10.1038/s41598-020-76913-6\n10.3978/j.issn.2218-6751.2013.09.03\n10.1042/EBC20150012\n10.1016/j.jviromet.2020.114024\n10.3390/diagnostics10060434\n10.7326/M20-1495\n10.1021/acsabm.1c00943\n10.1038/s41591-022-02162-x\n10.1016/j.rbmo.2020.06.001\n10.1128/jcm.30.3.545-551.1992\n10.1016/j.ijid.2021.05.063\n10.1016/j.aca.2021.338248\n10.1007/s00109-017-1600-y\n10.1177/2472630317727519\n10.1002/jmv.25727\n10.1371/journal.pone.0158199\n10.1016/j.cca.2020.03.032\n10.1111/j.1198-743X.2004.00722.x\n10.1002/iid3.387\n10.1002/jmv.25986\n10.1371/journal.pone.0237833\n10.1515/cclm-2021-0182\n10.1038/s41576-021-00360-w\n10.1038/nrg2626\n10.1038/s41598-022-09699-4\n10.1056/NEJMp2025631\n10.55705/cmbr.2022.345025.1047\n10.1016/j.jcv.2020.104413\n10.3201/eid2612.203309\n10.1016/j.ijid.2020.11.190\n10.1016/j.lanwpc.2021.100115\n10.1016/j.jcv.2020.104511\n10.1080/22221751.2020.1743767\n10.7326/M20-3012\n10.1038/s41591-020-0997-y\n10.1148/radiol.2020200370\n10.1021/acs.analchem.0c04047\n10.1016/S1473-3099(21)00048-7\n10.1136/bmj.m1163\n10.1016/j.bios.2020.112674\n10.12122/j.issn.1673-4254.2020.02.16\n10.1186/1471-2164-13-341\n10.1016/j.chaos.2021.110749\n10.1128/CMR.00228-20\n10.1016/j.compbiomed.2021.104759\n10.1002/cbic.202000250\n10.1016/j.scitotenv.2020.138890\n10.1038/s41586-020-2381-y\n10.2807/1560-7917.ES.2017.22.13.30494\n10.1016/j.foodchem.2014.08.092\n10.1038/s41467-020-18611-5\n10.7326/M22-0760\n10.3390/s23031533\n10.1073/pnas.2012358117\n10.1111/ene.14227\n10.1016/j.bios.2020.112830\n10.1038/s41587-020-0631-z\n10.1016/S1473-3099(21)00146-8\n10.1186/s13062-020-00282-3\n10.1631/jzus.B2200054\n10.1016/j.bios.2020.112766\n10.13488/j.smhx.20200280\n10.1148/radiol.2020200843\n10.3390/molecules27010172\n10.1016/j.ijid.2020.09.015\n10.1038/s41586-020-2008-3\n10.1016/S2213-2600(20)30076-X\n10.1021/acsabm.1c00102\n10.1016/j.cmi.2020.04.001\n10.1016/j.bios.2021.113535\n10.1016/j.jinf.2020.02.016\n10.1631/jzus.B2000608\n10.1016/j.bios.2020.112752\n10.1016/S1473-3099(23)00010-5\n10.1002/jmv.27334\n10.4155/bio-2020-0289\n10.1016/j.talanta.2021.123173\n10.1007/s11032-019-1092-2\n10.1056/NEJMoa2001017"}
{"title": "A transformer-based representation-learning model with unified processing of multimodal input for clinical diagnostics.", "abstract": "During the diagnostic process, clinicians leverage multimodal information, such as the chief complaint, medical images and laboratory test results. Deep-learning models for aiding diagnosis have yet to meet this requirement of leveraging multimodal information. Here we report a transformer-based representation-learning model as a clinical diagnostic aid that processes multimodal input in a unified manner. Rather than learning modality-specific features, the model leverages embedding layers to convert images and unstructured and structured text into visual tokens and text tokens, and uses bidirectional blocks with intramodal and intermodal attention to learn holistic representations of radiographs, the unstructured chief complaint and clinical history, and structured clinical information such as laboratory test results and patient demographic information. The unified model outperformed an image-only model and non-unified multimodal diagnosis models in the identification of pulmonary disease (by 12% and 9%, respectively) and in the prediction of adverse clinical outcomes in patients with COVID-19 (by 29% and 7%, respectively). Unified multimodal transformer-based models may help streamline the triaging of patients and facilitate the clinical decision-making process.", "journal": "Nature biomedical engineering", "date": "2023-06-13", "authors": ["Hong-YuZhou", "YizhouYu", "ChengdiWang", "ShuZhang", "YuanxuGao", "JiaPan", "JunShao", "GuangmingLu", "KangZhang", "WeiminLi"], "doi": "10.1038/s41551-023-01045-x\n10.1038/s41591-018-0307-0\n10.1038/s41591-018-0335-9\n10.1038/s41568-021-00408-3\n10.1093/pcmedi/pbaa017\n10.1111/ijd.12330\n10.3390/cancers14194823\n10.1038/s41746-020-00341-z\n10.3389/fimmu.2022.828560\n10.1016/j.cell.2020.04.045\n10.1016/j.cell.2018.02.010\n10.1038/s41591-021-01614-0\n10.1038/nature14539\n10.1016/j.neunet.2014.09.003\n10.1038/s41551-021-00704-1\n10.1038/s42256-021-00425-9\n10.1038/s41746-020-0273-z\n10.1038/s41746-022-00648-z\n10.1038/s41591-020-0931-3\n10.1148/radiol.2019182716\n10.1038/s41551-021-00745-6\n10.1038/s41746-021-00446-z\n10.1148/radiol.2019182622\n10.1164/ajrccm.163.5.2101039\n10.1038/s41598-020-62922-y\n10.1002/mef2.38\n10.1002/mef2.43\n10.1038/s41586-023-05881-4\n10.1007/s00330-020-07044-9"}
{"title": "GIONet: Global information optimized network for multi-center COVID-19 diagnosis via COVID-GAN and domain adversarial strategy.", "abstract": "The outbreak of coronavirus disease (COVID-19) in 2019 has highlighted the need for automatic diagnosis of the disease, which can develop rapidly into a severe condition. Nevertheless, distinguishing between COVID-19 pneumonia and community-acquired pneumonia (CAP) through computed tomography scans can be challenging due to their similar characteristics. The existing methods often perform poorly in the 3-class classification task of healthy, CAP, and COVID-19 pneumonia, and they have poor ability to handle the heterogeneity of multi-centers data. To address these challenges, we design a COVID-19 classification model using global information optimized network (GIONet) and cross-centers domain adversarial learning strategy. Our approach includes proposing a 3D convolutional neural network with graph enhanced aggregation unit and multi-scale self-attention fusion unit to improve the global feature extraction capability. We also verified that domain adversarial training can effectively reduce feature distance between different centers to address the heterogeneity of multi-center data, and used specialized generative adversarial networks to balance data distribution and improve diagnostic performance. Our experiments demonstrate satisfying diagnosis results, with a mixed dataset accuracy of 99.17% and cross-centers task accuracies of 86.73% and 89.61%.", "journal": "Computers in biology and medicine", "date": "2023-06-12", "authors": ["JingZhang", "YiyaoLiu", "BaiyingLei", "DandanSun", "SiqiWang", "ChangningZhou", "XingDing", "YangChen", "FenChen", "TianfuWang", "RuidongHuang", "KuntaoChen"], "doi": "10.1016/j.compbiomed.2023.107113"}
{"title": "Remora Namib Beetle Optimization Enabled Deep Learning for Severity of COVID-19 Lung Infection Identification and Classification Using CT Images.", "abstract": "Coronavirus disease 2019 (COVID-19) has seen a crucial outburst for both females and males worldwide. Automatic lung infection detection from medical imaging modalities provides high potential for increasing the treatment for patients to tackle COVID-19 disease. COVID-19 detection from lung CT images is a rapid way of diagnosing patients. However, identifying the occurrence of infectious tissues and segmenting this from CT images implies several challenges. Therefore, efficient techniques termed as Remora Namib Beetle Optimization_ Deep Quantum Neural Network (RNBO_DQNN) and RNBO_Deep Neuro Fuzzy Network (RNBO_DNFN) are introduced for the identification as well as classification of COVID-19 lung infection. Here, the pre-processing of lung CT images is performed utilizing an adaptive Wiener filter, whereas lung lobe segmentation is performed employing the Pyramid Scene Parsing Network (PSP-Net). Afterwards, feature extraction is carried out wherein features are extracted for the classification phase. In the first level of classification, DQNN is utilized, tuned by RNBO. Furthermore, RNBO is designed by merging the Remora Optimization Algorithm (ROA) and Namib Beetle Optimization (NBO). If a classified output is COVID-19, then the second-level classification is executed using DNFN for further classification. Additionally, DNFN is also trained by employing the newly proposed RNBO. Furthermore, the devised RNBO_DNFN achieved maximum testing accuracy, with TNR and TPR obtaining values of 89.4%, 89.5% and 87.5%.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-06-10", "authors": ["AmgothuShanthi", "SrinivasKoppu"], "doi": "10.3390/s23115316\n10.1148/radiol.2020200343\n10.1148/ryct.2020200034\n10.1186/s12938-020-00831-x\n10.1001/jama.2020.1097\n10.1016/j.media.2021.102205\n10.1056/NEJMoa2001017\n10.1016/S1473-3099(20)30120-1\n10.1109/ACCESS.2020.3027738\n10.1155/2021/5544742\n10.1109/TMI.2020.2995965\n10.1109/ACCESS.2020.2994762\n10.1007/s00330-021-07715-1\n10.1007/s42979-021-00874-4\n10.1109/TMI.2020.3000314\n10.1016/j.image.2022.116835\n10.1016/j.eng.2020.04.010\n10.1101/2020.03.12.20027185\n10.1016/j.eswa.2021.115665\n10.1002/cpe.6524\n10.3390/electronics11244137\n10.1109/MCE.2022.3211455\n10.1109/TMI.2020.2996645\n10.3390/electronics9101634"}
{"title": "Role of Imaging in the Management of Patients with SARS-CoV-2 Lung Involvement Admitted to the Emergency Department: A Systematic Review.", "abstract": "During the waves of the coronavirus disease (COVID-19) pandemic, emergency departments were overflowing with patients suffering with suspected medical or surgical issues. In these settings, healthcare staff should be able to deal with different medical and surgical scenarios while protecting themselves against the risk of contamination. Various strategies were used to overcome the most critical issues and guarantee quick and efficient diagnostic and therapeutic charts. The use of saliva and nasopharyngeal swab Nucleic Acid Amplification Tests (NAAT) in the diagnosis of COVID-19 was one of the most adopted worldwide. However, NAAT results were slow to report and could sometimes create significant delays in patient management, especially during pandemic peaks. On these bases, radiology has played and continues to play an essential role in detecting COVID-19 patients and solving differential diagnosis between different medical conditions. This systematic review aims to summarize the role of radiology in the management of COVID-19 patients admitted to emergency departments by using chest X-rays (CXR), computed tomography (CT), lung ultrasounds (LUS), and artificial intelligence (AI).", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-06-10", "authors": ["CesareMaino", "Paolo Niccol\u00f2Franco", "CammilloTalei Franzesi", "TeresaGiandola", "MariaRagusi", "RoccoCorso", "DavideIppolito"], "doi": "10.3390/diagnostics13111856\n10.1001/jamainternmed.2020.8876\n10.1016/j.ajem.2022.01.028\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200463\n10.4329/wjr.v12.i11.247\n10.1148/radiol.2020200370\n10.1148/radiol.2020201365\n10.3348/kjr.2020.0146\n10.1016/j.cmi.2022.05.036\n10.1016/j.ejrad.2020.109092\n10.1148/ryct.2020200280\n10.1016/j.clinimag.2020.04.001\n10.1016/j.jmir.2021.03.036\n10.1183/23120541.00357-2020\n10.12669/pjms.38.6.5279\n10.7759/cureus.18114\n10.4103/ijri.IJRI_967_20\n10.4187/respcare.09761\n10.1007/s11547-020-01327-3\n10.1183/23120541.00359-2020\n10.1007/s11547-020-01271-2\n10.1097/MD.0000000000026692\n10.1007/s11547-020-01179-x\n10.1148/ryct.2020200196\n10.1007/s00330-021-08003-8\n10.1148/rg.2020200159\n10.1186/s12931-020-01611-w\n10.5152/dir.2020.20350\n10.1007/s11547-020-01302-y\n10.1148/radiol.2020201473\n10.1007/s00330-022-08576-y\n10.1007/s11547-021-01335-x\n10.1016/j.chest.2021.06.016\n10.1097/RTI.0000000000000649\n10.1016/j.rmed.2020.106271\n10.3389/fmed.2022.914098\n10.1002/jmv.25965\n10.1007/s11547-020-01231-w\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020201160\n10.1148/radiol.2020200642\n10.1007/s00330-020-07273-y\n10.1016/j.clinimag.2020.11.022\n10.1148/ryct.2020200034\n10.1055/a-1217-1603\n10.1002/jum.15755\n10.1007/s11547-020-01236-5\n10.1016/j.jemermed.2020.06.032\n10.23736/S0031-0808.18.03577-2\n10.1016/j.ejrad.2022.110156\n10.1111/echo.14768\n10.1002/emp2.12194\n10.1213/ANE.0000000000004920\n10.1007/s10140-020-01849-3\n10.3390/jcm11216420\n10.1007/s00134-021-06373-7\n10.1016/j.annemergmed.2020.10.008\n10.1007/s11739-020-02524-8\n10.1016/j.ultrasmedbio.2020.07.005\n10.1097/CM9.0000000000001921\n10.1016/j.annemergmed.2021.05.011\n10.1016/j.ejrad.2020.109344\n10.5811/westjem.2020.5.47743\n10.3390/diagnostics11020373\n10.1016/S2589-7500(20)30199-0\n10.1038/s41591-020-0931-3\n10.21037/atm-21-5571\n10.1016/j.asoc.2020.106897\n10.1002/emp2.12297\n10.1016/j.chaos.2020.109944\n10.1038/s41598-021-99986-3"}
{"title": "A Review Paper about Deep Learning for Medical Image Analysis.", "abstract": "Medical imaging refers to the process of obtaining images of internal organs for therapeutic purposes such as discovering or studying diseases. The primary objective of medical image analysis is to improve the efficacy of clinical research and treatment options. Deep learning has revamped medical image analysis, yielding excellent results in image processing tasks such as registration, segmentation, feature extraction, and classification. The prime motivations for this are the availability of computational resources and the resurgence of deep convolutional neural networks. Deep learning techniques are good at observing hidden patterns in images and supporting clinicians in achieving diagnostic perfection. It has proven to be the most effective method for organ segmentation, cancer detection, disease categorization, and computer-assisted diagnosis. Many deep learning approaches have been published to analyze medical images for various diagnostic purposes. In this paper, we review the work exploiting current state-of-the-art deep learning approaches in medical image processing. We begin the survey by providing a synopsis of research works in medical imaging based on convolutional neural networks. Second, we discuss popular pretrained models and general adversarial networks that aid in improving convolutional networks' performance. Finally, to ease direct evaluation, we compile the performance metrics of deep learning models focusing on COVID-19 detection and child bone age prediction.", "journal": "Computational and mathematical methods in medicine", "date": "2023-06-07", "authors": ["BagherSistaninejhad", "HabibRasi", "ParisaNayeri"], "doi": "10.1155/2023/7091301\n10.1002/mp.13764\n10.1109/MPUL.2011.942929\n10.1016/j.neucom.2022.04.065\n10.1016/j.patcog.2018.05.014\n10.3390/su13031224\n10.1109/TMI.2016.2528162\n10.1016/j.artmed.2020.101938\n10.1016/j.ejmp.2021.05.003\n10.5772/intechopen.69792\n10.3390/app10061999\n10.1109/TPAMI.2016.2572683\n10.1016/j.neucom.2020.10.031\n10.1007/978-3-319-24574-4_28\n10.1007/s13369-020-05309-5\n10.1016/j.cmpb.2020.105876\n10.1186/s12880-021-00728-8\n10.1109/TPAMI.2018.2844175\n10.1016/j.cmpb.2021.106141\n10.1007/s13244-018-0639-9\n10.1109/5.726791\n10.1145/3065386\n10.1109/ACCESS.2021.3131741\n10.1007/s10278-020-00371-9\n10.3390/s21175704\n10.3390/s20164373\n10.1016/j.eswa.2020.113274\n10.24018/ejece.2021.5.1.268\n10.3390/sym12111787\n10.1145/3422622\n10.1016/j.clinimag.2020.10.014\n10.1155/2021/9956983\n10.1155/2021/5536903\n10.1016/j.bspc.2021.102901\n10.1016/j.mehy.2020.109684\n10.3390/app10020559\n10.3390/electronics9071066\n10.1109/ACCESS.2021.3079204\n10.1007/978-3-030-73689-7_52\n10.1155/2021/6296811\n10.1016/j.aej.2021.03.048\n10.1007/s13534-020-00168-3\n10.3390/diagnostics11112147\n10.3390/s20205736\n10.1016/j.cmpb.2021.106018\n10.3390/biomedicines10020223\n10.1007/s13369-020-04480-z\n10.1016/j.compbiomed.2020.103884\n10.1109/TPAMI.2017.2699184\n10.1038/s41598-020-76550-z\n10.1148/radiol.2018180736\n10.1007/s42600-021-00151-6\n10.1109/ACCESS.2020.2994762\n10.1109/TMI.2020.2993291\n10.3390/s21041480\n10.1155/2021/5528441\n10.1109/TCBB.2020.3009859\n10.1109/ACCESS.2020.3016780\n10.1155/2020/8460493\n10.3390/app10207233\n10.1007/s11548-020-02266-0\n10.1007/s11042-021-10935-8"}
{"title": "Additional value of chest CT AI-based quantification of lung involvement in predicting death and ICU admission for COVID-19 patients.", "abstract": "We evaluated the contribution of lung lesion quantification on chest CT using a clinical Artificial Intelligence (AI) software in predicting death and intensive care units (ICU) admission for COVID-19 patients.\nFor 349 patients with positive COVID-19-PCR test that underwent a chest CT scan at admittance or during hospitalization, we applied the AI for lung and lung lesion segmentation to obtain lesion volume (LV), and LV/Total Lung Volume (TLV) ratio. ROC analysis was used to extract the best CT criterion in predicting death and ICU admission. Two prognostic models using multivariate logistic regressions were constructed to predict each outcome and were compared using AUC values. The first model (\"Clinical\") was based on patients' characteristics and clinical symptoms only. The second model (\"Clinical+LV/TLV\") included also the best CT criterion.\nLV/TLV ratio demonstrated best performance for both outcomes; AUC of 67.8% (95% CI: 59.5 - 76.1) and 81.1% (95% CI: 75.7 - 86.5) respectively. Regarding death prediction, AUC values were 76.2% (95% CI: 69.9 - 82.6) and 79.9% (95%IC: 74.4 - 85.5) for the \"Clinical\" and the \"Clinical+LV/TLV\" models respectively, showing significant performance increase (+ 3.7%; p-value<0.001) when adding LV/TLV ratio. Similarly, for ICU admission prediction, AUC values were 74.9% (IC 95%: 69.2 - 80.6) and 84.8% (IC 95%: 80.4 - 89.2) respectively corresponding to significant performance increase (+ 10%: p-value<0.001).\nUsing a clinical AI software to quantify the COVID-19 lung involvement on chest CT, combined with clinical variables, allows better prediction of death and ICU admission.", "journal": "Research in diagnostic and interventional imaging", "date": "2023-06-07", "authors": ["EloiseGalzin", "LaurentRoche", "AnnaVlachomitrou", "OlivierNempont", "HeikeCarolus", "AlexanderSchmidt-Richberg", "PengJin", "PedroRodrigues", "TobiasKlinder", "Jean-ChristopheRichard", "KarimTazarourte", "MarionDouplat", "AlainSigal", "MaudeBouscambert-Duchamp", "Salim AymericSi-Mohamed", "SylvainGouttard", "AdelineMansuy", "Fran\u00e7oisTalbot", "Jean-BaptistePialat", "OlivierRouvi\u00e8re", "LaurentMilot", "Fran\u00e7oisCotton", "PhilippeDouek", "AntoineDuclos", "MurielRabilloud", "LoicBoussel"], "doi": "10.1016/j.redii.2022.100018\n10.1056/NEJMoa2001017\n10.1186/s12916-020-01533-w\n10.3390/molecules26010039\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30183-5\n10.1016/S2213-2600(20)30079-5\n10.1016/S2213-2600(20)30161-2\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1148/radiol.2020200330\n10.1128/JCM.00512-20\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020200343\n10.1016/j.ajem.2020.09.056\n10.1007/s00330-020-07033-y\n10.2214/AJR.20.22954\n10.1007/s00330-020-06801-0\n10.1038/s41467-020-18786-x\n10.2214/AJR.20.22976\n10.1109/JBHI.2020.3034296\n10.1183/13993003.00775-2020\n10.1148/radiol.2020200905\n10.1148/radiol.2020202439\n10.1148/ryai.2020200098\n10.1148/ryct.2020200441\n10.1101/2020.04.22.20075416\n10.1001/jamainternmed.2020.2033\n10.1093/cid/ciaa414\n10.1136/bmj.m3339\n10.1136/bmj.m1328\n10.1515/cclm-2020-0198\n10.1016/j.media.2020.101860\n10.1016/j.redii.2022.100003\n10.1118/1.3528204\n10.1002/sim.5727\n10.1016/s0895-4356(03)00047-7\n10.1186/1471-2105-12-77\n10.1038/s41467-020-20657-4\n10.1016/j.compbiomed.2021.104304\n10.1016/j.cell.2020.04.045\n10.1007/s00134-020-05991-x\n10.1097/RLI.0000000000000672\n10.18632/aging.103000\n10.1016/j.dsx.2020.03.002\n10.1016/j.acra.2020.03.003\n10.1148/radiol.2020200370\n10.1148/ryai.2020200029\n10.1109/TMI.2020.2996645\n10.1109/RBME.2020.2987975\n10.1097/RTI.0000000000000524\n10.1148/radiol.2020201365\n10.3969/j.issn.1007-8134.2020.04.001\n10.1148/radiol.2020203173"}
{"title": "Expression profile of HERVs and inflammatory mediators detected in nasal mucosa as a predictive biomarker of COVID-19 severity.", "abstract": "Our research group and others demonstrated the implication of the human endogenous retroviruses (HERVs) in SARS-CoV-2 infection and their association with disease progression, suggesting HERVs as contributing factors in COVID-19 immunopathology. To identify early predictive biomarkers of the COVID-19 severity, we analyzed the expression of HERVs and inflammatory mediators in SARS-CoV-2-positive and -negative nasopharyngeal/oropharyngeal swabs with respect to biochemical parameters and clinical outcome.\nResiduals of swab samples (20 SARS-CoV-2-negative and 43 SARS-CoV-2-positive) were collected during the first wave of the pandemic and expression levels of HERVs and inflammatory mediators were analyzed by qRT-Real time PCR.\nThe results obtained show that infection with SARS-CoV-2 resulted in a general increase in the expression of HERVs and mediators of the immune response. In particular, SARS-CoV-2 infection is associated with increased expression of HERV-K and HERV-W, IL-1\u03b2, IL-6, IL-17, TNF-\u03b1, MCP-1, INF-\u03b3, TLR-3, and TLR-7, while lower levels of IL-10, IFN-\u03b1, IFN-\u03b2, and TLR-4 were found in individuals who underwent hospitalization. Moreover, higher expression of HERV-W, IL-1\u03b2, IL-6, IFN-\u03b1, and IFN-\u03b2 reflected the respiratory outcome of patients during hospitalization. Interestingly, a machine learning model was able to classify hospitalized \nOverall, the present results suggest HERVs as contributing elements in COVID-19 and early genomic biomarkers to predict COVID-19 severity and disease outcome.", "journal": "Frontiers in microbiology", "date": "2023-06-07", "authors": ["VitaPetrone", "MarialauraFanelli", "MartinaGiudice", "NicolaToschi", "AllegraConti", "ChristianMaracchioni", "MarcoIannetta", "ClaudiaResta", "ChiaraCipriani", "Martino TonyMiele", "FrancescaAmati", "MassimoAndreoni", "LoredanaSarmati", "PaolaRogliani", "GiuseppeNovelli", "EnricoGaraci", "GuidoRasi", "PaolaSinibaldi-Vallebona", "AntonellaMinutolo", "ClaudiaMatteucci", "EmanuelaBalestrieri", "SandroGrelli"], "doi": "10.3389/fmicb.2023.1155624\n10.1016/j.heliyon.2020.e05143\n10.1136/bmj.m1996\n10.3389/fmicb.2018.01448\n10.3389/fimmu.2019.02244\n10.1016/j.ebiom.2021.103341\n10.1126/sciimmunol.abl4340\n10.1126/science.abd4585\n10.1016/j.cell.2022.07.004\n10.1007/s00281-017-0629-x\n10.1016/j.isci.2023.106604\n10.3389/fimmu.2018.02803\n10.1016/j.intimp.2007.05.016\n10.1016/j.cell.2015.07.011\n10.1503/cmaj.1040398\n10.3390/ijms19113286\n10.3389/fimmu.2021.648004\n10.1038/s41388-018-0282-4\n10.1038/s41385-020-00359-2\n10.3390/pathogens8030106\n10.3389/fimmu.2018.02039\n10.3389/fmicb.2018.00265\n10.3390/v14050996\n10.1016/S0140-6736(20)30183-5\n10.1099/vir.0.000017\n10.1098/rstb.2016.0277\n10.7554/eLife.68563\n10.1128/Spectrum.01260-21\n10.1002/eji.200939422\n10.1038/s41586-020-2180-5\n10.1038/mi.2014.135\n10.1101/gr.233585.117\n10.1007/s11892-019-1256-9\n10.1128/JVI.03628-13\n10.1038/s41591-020-0901-9\n10.1016/j.jneuroim.2015.12.006\n10.1186/1742-4690-10-16\n10.1128/JVI.01503-16\n10.1073/pnas.2200413119\n10.1016/j.antiviral.2020.104811\n10.1128/JVI.00919-14\n10.1172/jci.insight.147170\n10.1038/s41598-021-02489-4\n10.1016/j.semcancer.2018.10.001\n10.1093/ofid/ofaa588\n10.1016/S0140-6736(20)30628-0\n10.3390/pathogens10121639\n10.1093/cid/ciaa248\n10.4049/jimmunol.176.12.7636\n10.1016/j.heliyon.2021.e06187\n10.1128/spectrum.01280-22\n10.1007/s10067-020-05190-5\n10.1038/s41577-020-0311-8\n10.1186/s40168-022-01260-9\n10.1002/ajh.25829\n10.1186/s13100-018-0142-3\n10.1089/aid.2006.22.979\n10.1097/QAD.0000000000000477\n10.3389/fphys.2020.00699\n10.1097/MIB.0000000000000344\n10.1016/j.bbi.2017.09.009\n10.1073/pnas.1602336113\n10.1038/s41586-020-2008-3\n10.1080/22221751.2020.1780953\n10.1038/s41586-022-04447-0\n10.1126/science.abd4570\n10.1038/s41586-020-2355-0\n10.1038/s41422-021-00495-9\n10.1038/s41422-021-00501-0\n10.1016/j.cell.2020.04.035"}
{"title": "Deep learning framework for rapid and accurate respiratory COVID-19 prediction using chest X-ray images.", "abstract": "COVID-19 is a contagious disease that affects the human respiratory system. Infected individuals may develop serious illnesses, and complications may result in death. Using medical images to detect COVID-19 from essentially identical thoracic anomalies is challenging because it is time-consuming, laborious, and prone to human error. This study proposes an end-to-end deep-learning framework based on deep feature concatenation and a Multi-head Self-attention network. Feature concatenation involves fine-tuning the pre-trained backbone models of DenseNet, VGG-16, and InceptionV3, which are trained on a large-scale ImageNet, whereas a Multi-head Self-attention network is adopted for performance gain. End-to-end training and evaluation procedures are conducted using the COVID-19_Radiography_Dataset for binary and multi-classification scenarios. The proposed model achieved overall accuracies (96.33% and 98.67%) and F1_scores (92.68% and 98.67%) for multi and binary classification scenarios, respectively. In addition, this study highlights the difference in accuracy (98.0% vs. 96.33%) and F_1 score (97.34% vs. 95.10%) when compared with feature concatenation against the highest individual model performance. Furthermore, a virtual representation of the saliency maps of the employed attention mechanism focusing on the abnormal regions is presented using explainable artificial intelligence (XAI) technology. The proposed framework provided better COVID-19 prediction results outperforming other recent deep learning models using the same dataset.", "journal": "Journal of King Saud University. Computer and information sciences", "date": "2023-06-05", "authors": ["Chiagoziem CUkwuoma", "DongshengCai", "Md Belal BinHeyat", "OlusolaBamisile", "HumphreyAdun", "ZaidAl-Huda", "Mugahed AAl-Antari"], "doi": "10.1016/j.jksuci.2023.101596"}
{"title": "Comparison of artificial intelligence versus real-time physician assessment of pulmonary edema with lung ultrasound.", "abstract": "Lung ultrasound can evaluate for pulmonary edema, but data suggest moderate inter-rater reliability among users. Artificial intelligence (AI) has been proposed as a model to increase the accuracy of B line interpretation. Early data suggest a benefit among more novice users, but data are limited among average residency-trained physicians. The objective of this study was to compare the accuracy of AI versus real-time physician assessment for B lines.\nThis was a prospective, observational study of adult Emergency Department patients presenting with suspected pulmonary edema. We excluded patients with active COVID-19 or interstitial lung disease. A physician performed thoracic ultrasound using the 12-zone technique. The physician recorded a video clip in each zone and provided an interpretation of positive (\u22653 B lines or a wide, dense B line) or negative (<3 B lines and the absence of a wide, dense B line) for pulmonary edema based upon the real-time assessment. A research assistant then utilized the AI program to analyze the same saved clip to determine if it was positive versus negative for pulmonary edema. The physician sonographer was blinded to this assessment. The video clips were then reviewed independently by two expert physician sonographers (ultrasound leaders with >10,000 prior ultrasound image reviews) who were blinded to the AI and initial determinations. The experts reviewed all discordant values and reached consensus on whether the field (i.e., the area of lung between two adjacent ribs) was positive or negative using the same criteria as defined above, which served as the gold standard.\n71 patients were included in the study (56.3% female; mean BMI: 33.4 [95% CI 30.6-36.2]), with 88.3% (752/852) of lung fields being of adequate quality for assessment. Overall, 36.1% of lung fields were positive for pulmonary edema. The physician was 96.7% (95% CI 93.8%-98.5%) sensitive and 79.1% (95% CI 75.1%-82.6%) specific. The AI software was 95.6% (95% CI 92.4%-97.7%) sensitive and 64.1% (95% CI 59.8%-68.5%) specific.\nBoth the physician and AI software were highly sensitive, though the physician was more specific. Future research should identify which factors are associated with increased diagnostic accuracy.", "journal": "The American journal of emergency medicine", "date": "2023-06-04", "authors": ["MichaelGottlieb", "DavenPatel", "MirandaViars", "JackTsintolas", "Gary DPeksa", "JohnBailitz"], "doi": "10.1016/j.ajem.2023.05.029"}
{"title": "POLCOVID: a multicenter multiclass chest X-ray database (Poland, 2020-2021).", "abstract": "The outbreak of the SARS-CoV-2 pandemic has put healthcare systems worldwide to their limits, resulting in increased waiting time for diagnosis and required medical assistance. With chest radiographs (CXR) being one of the most common COVID-19 diagnosis methods, many artificial intelligence tools for image-based COVID-19 detection have been developed, often trained on a small number of images from COVID-19-positive patients. Thus, the need for high-quality and well-annotated CXR image databases increased. This paper introduces POLCOVID dataset, containing chest X-ray (CXR) images of patients with COVID-19 or other-type pneumonia, and healthy individuals gathered from 15 Polish hospitals. The original radiographs are accompanied by the preprocessed images limited to the lung area and the corresponding lung masks obtained with the segmentation model. Moreover, the manually created lung masks are provided for a part of POLCOVID dataset and the other four publicly available CXR image collections. POLCOVID dataset can help in pneumonia or COVID-19 diagnosis, while the set of matched images and lung masks may serve for the development of lung segmentation solutions.", "journal": "Scientific data", "date": "2023-06-03", "authors": ["AleksandraSuwalska", "JoannaTobiasz", "WojciechPrazuch", "MarekSocha", "PawelFoszner", "DamianPiotrowski", "KatarzynaGruszczynska", "MagdalenaSliwinska", "JerzyWalecki", "TadeuszPopiela", "GrzegorzPrzybylski", "MateuszNowak", "PiotrFiedor", "MalgorzataPawlowska", "RobertFlisiak", "KrzysztofSimon", "GabrielaZapolska", "BarbaraGizycka", "EdytaSzurowska", "NoneNone", "MichalMarczyk", "AndrzejCieszanowski", "JoannaPolanska"], "doi": "10.1038/s41597-023-02229-5\n10.1038/s41591-021-01381-y\n10.1038/s41579-020-00461-z\n10.1136/bmj.m2426\n10.1148/radiol.2020201160\n10.1038/s41598-020-76550-z\n10.1016/j.media.2020.101794\n10.1016/j.eswa.2020.114054\n10.1016/j.media.2021.102225\n10.1016/j.cell.2018.02.010\n10.1109/ACCESS.2020.3010287\n10.1016/j.media.2021.102216\n10.1109/TNB.2017.2676725\n10.7303/syn50877085\n10.1002/cem.1123"}
{"title": "Harnessing Machine Learning in Early COVID-19 Detection and Prognosis: A Comprehensive Systematic Review.", "abstract": "During the early phase of the COVID-19 pandemic, reverse transcriptase-polymerase chain reaction (RT-PCR) testing faced limitations, prompting the exploration of machine learning (ML) alternatives for diagnosis and prognosis.\u00a0Providing a comprehensive appraisal of such decision support systems and their use in\u00a0COVID-19 management can aid the medical community in making informed decisions during the risk assessment of their patients, especially in low-resource settings. Therefore, the objective of this study was to systematically review the studies that predicted the diagnosis of COVID-19 or the severity of the disease using ML. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA), we conducted a literature search of MEDLINE (OVID), Scopus, EMBASE, and IEEE Xplore\u00a0from January 1 to June 31, 2020. The outcomes were COVID-19 diagnosis or prognostic measures such as death, need for mechanical ventilation, admission, and acute respiratory distress syndrome. We included peer-reviewed observational studies, clinical trials, research letters, case series, and reports. We extracted data about the study's country, setting, sample size, data source, dataset, diagnostic or prognostic outcomes, prediction measures, type of ML model, and measures of diagnostic accuracy. Bias was assessed using the Prediction model Risk Of Bias ASsessment\u00a0Tool (PROBAST). This study was registered in the International Prospective Register of Systematic Reviews (PROSPERO), with the number CRD42020197109. The final records included for data extraction were 66. Forty-three (64%) studies used secondary data. The majority of studies were from Chinese authors (30%).\u00a0Most of the literature (79%) relied on chest imaging for prediction, while the remainder used various laboratory indicators, including hematological, biochemical, and immunological markers. Thirteen studies explored predicting COVID-19 severity, while the rest predicted diagnosis. Seventy percent of the articles used deep learning models, while 30% used traditional ML algorithms. Most studies reported high sensitivity, specificity, and accuracy for the ML models (exceeding 90%). The overall concern about the risk of bias was \"unclear\" in 56% of the studies. This was mainly due to concerns about selection bias. ML may help identify COVID-19 patients in the early phase of the pandemic, particularly in the context of chest imaging. Although these studies reflect that these ML models exhibit high accuracy, the novelty of these models and the biases in dataset selection make using them as a replacement for the clinicians' cognitive decision-making questionable.\u00a0Continued research is needed to enhance the robustness and reliability of ML systems in COVID-19 diagnosis and prognosis.", "journal": "Cureus", "date": "2023-06-02", "authors": ["RufaidahDabbagh", "AmrJamal", "Jakir HossainBhuiyan Masud", "Maher ATiti", "Yasser SAmer", "AfnanKhayat", "Taha SAlhazmi", "LayalHneiny", "Fatmah ABaothman", "MetabAlkubeyyer", "Samina AKhan", "Mohamad-HaniTemsah"], "doi": "10.7759/cureus.38373"}
{"title": "A multimodal AI-based non-invasive COVID-19 grading framework powered by deep learning, manta ray, and fuzzy inference system from multimedia vital signs.", "abstract": "The COVID-19 pandemic has presented unprecedented challenges to healthcare systems worldwide. One of the key challenges in controlling and managing the pandemic is accurate and rapid diagnosis of COVID-19 cases. Traditional diagnostic methods such as RT-PCR tests are time-consuming and require specialized equipment and trained personnel. Computer-aided diagnosis systems and artificial intelligence (AI) have emerged as promising tools for developing cost-effective and accurate diagnostic approaches. Most studies in this area have focused on diagnosing COVID-19 based on a single modality, such as chest X-rays or cough sounds. However, relying on a single modality may not accurately detect the virus, especially in its early stages. In this research, we propose a non-invasive diagnostic framework consisting of four cascaded layers that work together to accurately detect COVID-19 in patients. The first layer of the framework performs basic diagnostics such as patient temperature, blood oxygen level, and breathing profile, providing initial insights into the patient's condition. The second layer analyzes the coughing profile, while the third layer evaluates chest imaging data such as X-ray and CT scans. Finally, the fourth layer utilizes a fuzzy logic inference system based on the previous three layers to generate a reliable and accurate diagnosis. To evaluate the effectiveness of the proposed framework, we used two datasets: the Cough Dataset and the COVID-19 Radiography Database. The experimental results demonstrate that the proposed framework is effective and trustworthy in terms of accuracy, precision, sensitivity, specificity, F1-score, and balanced accuracy. The audio-based classification achieved an accuracy of 96.55%, while the CXR-based classification achieved an accuracy of 98.55%. The proposed framework has the potential to significantly improve the accuracy and speed of COVID-19 diagnosis, allowing for more effective control and management of the pandemic. Furthermore, the framework's non-invasive nature makes it a more attractive option for patients, reducing the risk of infection and discomfort associated with traditional diagnostic methods.", "journal": "Heliyon", "date": "2023-05-30", "authors": ["Saleh AteeqAlmutairi"], "doi": "10.1016/j.heliyon.2023.e16552"}
{"title": "Automated screening of computed tomography using weakly supervised anomaly detection.", "abstract": "Current artificial intelligence studies for supporting CT screening tasks depend on either supervised learning or detecting anomalies. However, the former involves a heavy annotation workload owing to requiring many slice-wise annotations (ground truth labels); the latter is promising, but while it reduces the annotation workload, it often suffers from lower performance. This study presents a novel weakly supervised anomaly detection (WSAD) algorithm trained based on scan-wise normal and anomalous annotations to provide better performance than conventional methods while reducing annotation workload.\nBased on surveillance video anomaly detection methodology, feature vectors representing each CT slice were trained on an AR-Net-based convolutional network using a dynamic multiple-instance learning loss and a center loss function. The following two publicly available CT datasets were retrospectively analyzed: the RSNA brain hemorrhage dataset (normal scans: 12,862; scans with intracranial hematoma: 8882) and COVID-CT set (normal scans: 282; scans with COVID-19: 95).\nAnomaly scores of each slice were successfully predicted despite inaccessibility to any slice-wise annotations. Slice-level area under the curve (AUC), sensitivity, specificity, and accuracy from the brain CT dataset were 0.89, 0.85, 0.78, and 0.79, respectively. The proposed method reduced the number of annotations in the brain dataset by 97.1% compared to an ordinary slice-level supervised learning method.\nThis study demonstrated a significant annotation reduction in identifying anomalous CT slices compared to a supervised learning approach. The effectiveness of the proposed WSAD algorithm was verified through higher AUC than existing anomaly detection techniques.", "journal": "International journal of computer assisted radiology and surgery", "date": "2023-05-29", "authors": ["AtsuhiroHibi", "Michael DCusimano", "AlexanderBilbily", "Rahul GKrishnan", "Pascal NTyrrell"], "doi": "10.1007/s11548-023-02965-4\n10.1001/jama.2021.0377\n10.1177/0846537120913033\n10.1186/s13244-020-00925-z\n10.1109/ACCESS.2021.3102740\n10.1007/s40042-021-00202-2\n10.1016/j.neunet.2021.12.008\n10.1007/s11604-022-01249-2\n10.1148/ryai.2020190211\n10.1016/j.bspc.2021.102588\n10.1016/j.neucom.2018.12.085\n10.1007/s10278-020-00388-0\n10.1007/s10489-021-02782-9\n10.3390/s20195611\n10.1007/s00330-020-06915-5\n10.1016/j.cmpb.2022.106731\n10.1016/j.media.2020.101952\n10.1016/j.media.2022.102475\n10.1016/j.nicl.2021.102785"}
{"title": "Fusion-Extracted Features by Deep Networks for Improved COVID-19 Classification with Chest X-ray Radiography.", "abstract": "Convolutional neural networks (CNNs) have shown promise in accurately diagnosing coronavirus disease 2019 (COVID-19) and bacterial pneumonia using chest X-ray images. However, determining the optimal feature extraction approach is challenging. This study investigates the use of fusion-extracted features by deep networks to improve the accuracy of COVID-19 and bacterial pneumonia classification with chest X-ray radiography. A Fusion CNN method was developed using five different deep learning models after transferred learning to extract image features (Fusion CNN). The combined features were used to build a support vector machine (SVM) classifier with a RBF kernel. The performance of the model was evaluated using accuracy, Kappa values, recall rate, and precision scores. The Fusion CNN model achieved an accuracy and Kappa value of 0.994 and 0.991, with precision scores for normal, COVID-19, and bacterial groups of 0.991, 0.998, and 0.994, respectively. The results indicate that the Fusion CNN models with the SVM classifier provided reliable and accurate classification performance, with Kappa values no less than 0.990. Using a Fusion CNN approach could be a possible solution to enhance accuracy further. Therefore, the study demonstrates the potential of deep learning and fusion-extracted features for accurate COVID-19 and bacterial pneumonia classification with chest X-ray radiography.", "journal": "Healthcare (Basel, Switzerland)", "date": "2023-05-27", "authors": ["Kuo-HsuanLin", "Nan-HanLu", "TakahideOkamoto", "Yung-HuiHuang", "Kuo-YingLiu", "AkariMatsushima", "Che-ChengChang", "Tai-BeenChen"], "doi": "10.3390/healthcare11101367\n10.1177/1063293X211021435\n10.1016/j.bspc.2022.104297\n10.1155/2023/4310418\n10.1016/j.cmpb.2020.105581\n10.1016/j.compbiomed.2021.104319\n10.1038/s41598-020-76550-z\n10.21203/rs.3.rs-70158/v1\n10.1016/j.asoc.2020.106580\n10.1371/journal.pone.0242535\n10.3389/fpubh.2022.1046296\n10.1007/s11042-022-13739-6\n10.1007/s11042-022-13843-7\n10.1016/j.patrec.2020.12.010\n10.1145/3065386\n10.48550/arXiv.1409.1556\n10.48550/arXiv.1512.03385\n10.1109/CVPR.2017.243\n10.48550/arXiv.1804.02767\n10.1007/BF00994018\n10.1155/2022/4254631\n10.1007/s40747-020-00199-4\n10.3390/ijerph20032035\n10.1007/s10522-021-09946-7\n10.1016/j.chemolab.2022.104750\n10.1016/j.asoc.2022.109401\n10.1177/20552076221092543\n10.1145/3551690.3551695\n10.1007/s40747-020-00216-6\n10.7717/peerj-cs.306\n10.1016/j.cell.2018.02.010"}
{"title": "A Novel Deep Learning-Based Classification Framework for COVID-19 Assisted with Weighted Average Ensemble Modeling.", "abstract": "COVID-19 is an infectious disease caused by the deadly virus SARS-CoV-2 that affects the lung of the patient. Different symptoms, including fever, muscle pain and respiratory syndrome, can be identified in COVID-19-affected patients. The disease needs to be diagnosed in a timely manner, otherwise the lung infection can turn into a severe form and the patient's life may be in danger. In this work, an ensemble deep learning-based technique is proposed for COVID-19 detection that can classify the disease with high accuracy, efficiency, and reliability. A weighted average ensemble (WAE) prediction was performed by combining three CNN models, namely Xception, VGG19 and ResNet50V2, where 97.25% and 94.10% accuracy was achieved for binary and multiclass classification, respectively. To accurately detect the disease, different test methods have been proposed and developed, some of which are even being used in real-time situations. RT-PCR is one of the most successful COVID-19 detection methods, and is being used worldwide with high accuracy and sensitivity. However, complexity and time-consuming manual processes are limitations of this method. To make the detection process automated, researchers across the world have started to use deep learning to detect COVID-19 applied on medical imaging. Although most of the existing systems offer high accuracy, different limitations, including high variance, overfitting and generalization errors, can be found that can degrade the system performance. Some of the reasons behind those limitations are a lack of reliable data resources, missing preprocessing techniques, a lack of proper model selection, etc., which eventually create reliability issues. Reliability is an important factor for any healthcare system. Here, transfer learning with better preprocessing techniques applied on two benchmark datasets makes the work more reliable. The weighted average ensemble technique with hyperparameter tuning ensures better accuracy than using a randomly selected single CNN model.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-05-27", "authors": ["Gouri ShankarChakraborty", "SalilBatra", "AmanSingh", "GhulamMuhammad", "Vanessa YelamosTorres", "MakulMahajan"], "doi": "10.3390/diagnostics13101806\n10.1016/j.knosys.2022.108207\n10.1016/j.radi.2022.03.011\n10.1109/ACCESS.2021.3086229\n10.1016/j.bspc.2022.103772\n10.1101/2020.08.20.20178913\n10.1007/s00530-021-00794-6\n10.1117/1.JEI.31.4.041212\n10.36548/jtcsst.2021.4.004\n10.1109/ACCESS.2020.3028012\n10.3390/diagnostics12081805\n10.1007/s10489-021-02292-8\n10.1016/j.cmpbup.2022.100054\n10.3390/electronics11233999\n10.31224/osf.io/wx89s\n10.1007/s10489-020-01904-z\n10.1016/j.asoc.2021.107947\n10.1145/3453170"}
{"title": "CRV-NET: Robust Intensity Recognition of Coronavirus in Lung Computerized Tomography Scan Images.", "abstract": "The early diagnosis of infectious diseases is demanded by digital healthcare systems. Currently, the detection of the new coronavirus disease (COVID-19) is a major clinical requirement. For COVID-19 detection, deep learning models are used in various studies, but the robustness is still compromised. In recent years, deep learning models have increased in popularity in almost every area, particularly in medical image processing and analysis. The visualization of the human body's internal structure is critical in medical analysis; many imaging techniques are in use to perform this job. A computerized tomography (CT) scan is one of them, and it has been generally used for the non-invasive observation of the human body. The development of an automatic segmentation method for lung CT scans showing COVID-19 can save experts time and can reduce human error. In this article, the CRV-NET is proposed for the robust detection of COVID-19 in lung CT scan images. A public dataset (SARS-CoV-2 CT Scan dataset), is used for the experimental work and customized according to the scenario of the proposed model. The proposed modified deep-learning-based U-Net model is trained on a custom dataset with 221 training images and their ground truth, which was labeled by an expert. The proposed model is tested on 100 test images, and the results show that the model segments COVID-19 with a satisfactory level of accuracy. Moreover, the comparison of the proposed CRV-NET with different state-of-the-art convolutional neural network models (CNNs), including the U-Net Model, shows better results in terms of accuracy (96.67%) and robustness (low epoch value in detection and the smallest training data size).", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-05-27", "authors": ["UzairIqbal", "RomilImtiaz", "Abdul Khader JilaniSaudagar", "Khubaib AmjadAlam"], "doi": "10.3390/diagnostics13101783\n10.1056/NEJMoa2002032\n10.1007/s00330-020-06801-0\n10.1101/2020.04.06.20054890\n10.1016/S1473-3099(20)30086-4\n10.1016/j.procs.2020.03.295\n10.1016/j.eswa.2019.112855\n10.1016/j.imu.2020.100357\n10.1016/j.procs.2018.01.104\n10.13053/cys-22-3-2526\n10.3390/s20051516\n10.1109/icip.2018.8451295\n10.1016/j.neucom.2018.12.085\n10.1016/j.neunet.2020.01.005\n10.1109/ACCESS.2021.3131216\n10.1109/ACCESS.2021.3056516\n10.3390/app9010069\n10.1117/12.2540176\n10.3390/app10165683\n10.3390/diagnostics11050893\n10.1016/j.compbiomed.2021.104526\n10.1109/RBME.2020.2987975\n10.1117/12.2581496\n10.1016/j.compbiomed.2020.104037\n10.1016/j.compbiomed.2020.103792\n10.1016/j.imu.2020.100360\n10.1016/j.eng.2020.04.010\n10.1007/s00330-021-07715-1\n10.1101/2020.03.12.20027185\n10.1007/s13246-020-00865-4\n10.1007/s10044-021-00984-y\n10.1101/2020.05.08.20094664\n10.1109/TMI.2020.2996645\n10.1016/j.jstrokecerebrovasdis.2020.105089\n10.3390/sym12091526\n10.1016/j.chaos.2020.110059\n10.1016/j.eswa.2020.114054\n10.1101/2020.04.24.20078584\n10.1186/s12880-020-00529-5\n10.1109/ACCESS.2021.3086020\n10.1002/ima.22527\n10.1038/s41598-020-76550-z\n10.3390/info12110471\n10.7717/peerj-cs.655\n10.1109/TMI.2018.2837502\n10.1080/02564602.2021.1955760\n10.1007/978-3-030-00889-5\n10.1109/MIS.2020.2988604\n10.3390/s18061714\n10.1007/s12652-021-03612-z\n10.3390/sym14020194\n10.32604/cmc.2021.018472\n10.3390/s21165571\n10.1007/s11280-022-01046-x"}
{"title": "A Survey of COVID-19 Diagnosis Using Routine Blood Tests with the Aid of Artificial Intelligence Techniques.", "abstract": "Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), causing a disease called COVID-19, is a class of acute respiratory syndrome that has considerably affected the global economy and healthcare system. This virus is diagnosed using a traditional technique known as the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. However, RT-PCR customarily outputs a lot of false-negative and incorrect results. Current works indicate that COVID-19 can also be diagnosed using imaging resolutions, including CT scans, X-rays, and blood tests. Nevertheless, X-rays and CT scans cannot always be used for patient screening because of high costs, radiation doses, and an insufficient number of devices. Therefore, there is a requirement for a less expensive and faster diagnostic model to recognize the positive and negative cases of COVID-19. Blood tests are easily performed and cost less than RT-PCR and imaging tests. Since biochemical parameters in routine blood tests vary during the COVID-19 infection, they may supply physicians with exact information about the diagnosis of COVID-19. This study reviewed some newly emerging artificial intelligence (AI)-based methods to diagnose COVID-19 using routine blood tests. We gathered information about research resources and inspected 92 articles that were carefully chosen from a variety of publishers, such as IEEE, Springer, Elsevier, and MDPI. Then, these 92 studies are classified into two tables which contain articles that use machine Learning and deep Learning models to diagnose COVID-19 while using routine blood test datasets. In these studies, for diagnosing COVID-19, Random Forest and logistic regression are the most widely used machine learning methods and the most widely used performance metrics are accuracy, sensitivity, specificity, and AUC. Finally, we conclude by discussing and analyzing these studies which use machine learning and deep learning models and routine blood test datasets for COVID-19 detection. This survey can be the starting point for a novice-/beginner-level researcher to perform on COVID-19 classification.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-05-27", "authors": ["SoheilaAbbasi Habashi", "MuratKoyuncu", "RoohallahAlizadehsani"], "doi": "10.3390/diagnostics13101749\n10.1007/s12652-022-04199-9\n10.1145/3529395\n10.1620/tjem.255.127\n10.1109/TCBB.2021.3065361\n10.3389/fpubh.2022.949482\n10.1080/14737159.2020.1757437\n10.1371/journal.pone.0242958\n10.1155/2022/6184170\n10.1515/cclm-2020-1294\n10.1016/j.radi.2020.09.010\n10.1016/j.bspc.2021.102518\n10.1016/j.phrs.2021.105920\n10.1016/j.celrep.2021.109821\n10.22074/IJFS.2021.137035.1018\n10.1007/s11042-020-10340-7\n10.18502/fbt.v8i2.6517\n10.1007/s40544-022-0710-x\n10.3389/fbioe.2022.940511\n10.1007/s10916-020-01597-4\n10.1002/mabi.202200554\n10.1038/s41467-022-31997-8\n10.1109/TITS.2021.3113787\n10.18502/acta.v59i10.7771\n10.7150/thno.63177\n10.1016/j.injury.2023.01.041\n10.1002/ijgo.14052\n10.1109/IPRIA53572.2021.9483458\n10.1109/JSEN.2022.3201015\n10.1016/j.neucom.2020.10.038\n10.1016/j.ymssp.2022.109821\n10.3389/fnbot.2022.840594\n10.1109/ICEE52715.2021.9544399\n10.3390/electronics11132012\n10.1155/2022/8733632\n10.3390/app12062828\n10.1016/j.bspc.2022.103658\n10.3390/ijerph18031117\n10.1007/s12034-015-1001-1\n10.1017/S0022109018001564\n10.1016/j.imu.2021.100564\n10.1007/978-1-4899-7641-3\n10.17849/insm-47-01-31-39.1\n10.4249/scholarpedia.1883\n10.1145/2939672.2939785\n10.1007/s41870-022-00864-6\n10.1016/j.cclet.2021.03.076\n10.1016/j.cclet.2021.09.062\n10.1016/j.cclet.2021.01.001\n10.1109/CINTI-MACRo57952.2022.10029403\n10.3390/jcm12020400\n10.1109/TPAMI.2023.3237740\n10.1109/CEC55065.2022.9870280\n10.22061/jecei.2021.8051.475\n10.48550/arXiv.2110.11870\n10.1109/ICEE52715.2021.9544258\n10.1007/978-3-030-92238-2_57\n10.5120/1462-1976\n10.1101/2020.04.02.20051136\n10.2196/25884\n10.1007/978-3-030-82529-4_38\n10.2196/24048\n10.2196/27293\n10.1038/s41598-021-86735-9\n10.3390/s22062224\n10.1007/s12539-021-00499-4\n10.1038/s41598-021-90265-9\n10.1038/s41598-021-82885-y\n10.1016/j.compbiomed.2021.104335\n10.1007/s42600-020-00112-5\n10.1016/j.bspc.2021.103263\n10.1101/2020.06.03.20120881\n10.1186/s12911-020-01266-z\n10.1101/2020.03.18.20035816\n10.1038/s41598-021-82492-x\n10.1109/JIOT.2021.3050775\n10.1038/s41598-022-07307-z\n10.1016/j.intimp.2020.106705\n10.1109/IEMTRONICS52119.2021.9422534\n10.1093/labmed/lmaa111\n10.1021/acs.analchem.0c04497\n10.1038/s41551-020-00633-5\n10.1016/S2589-7500(20)30274-0\n10.30699/fhi.v9i1.234\n10.1016/j.imu.2020.100449\n10.1101/2021.04.06.21254997\n10.1093/clinchem/hvaa200\n10.1016/j.jcv.2020.104502\n10.2196/21439\n10.1101/2020.04.04.20052092\n10.1101/2020.04.10.20061036\n10.32604/cmc.2020.010691\n10.1186/s12911-020-01316-6\n10.48550/arXiv.2011.10657\n10.3389/fcell.2020.00683\n10.48550/arXiv.2011.12247\n10.1038/s41746-020-00343-x\n10.1145/3388440.3412463\n10.1371/journal.pone.0239474\n10.1016/j.media.2020.101844\n10.2196/29514\n10.2196/24018\n10.1136/bmjspcare-2020-002602\n10.3390/ijerph17228386\n10.1183/13993003.01104-2020\n10.1038/s41467-020-18684-2\n10.7717/peerj.10083\n10.1101/2020.12.04.20244137\n10.1109/ACCESS.2020.3034032\n10.1101/2020.09.18.20197319\n10.1101/2020.07.07.20148361\n10.7717/peerj.9482\n10.1101/2020.08.18.20176776\n10.3389/frai.2021.579931\n10.2139/ssrn.3638427\n10.1007/s13755-021-00164-6\n10.1038/s41746-021-00456-x\n10.1038/s41598-021-93719-2\n10.1016/j.meegid.2021.104737\n10.1016/j.patter.2020.100092\n10.48550/arXiv.2005.06546\n10.2139/ssrn.3551355\n10.1016/j.cmpb.2021.106444\n10.1038/s41598-021-04509-9\n10.1016/j.compbiomed.2021.105166\n10.1016/j.compbiomed.2022.105284\n10.1038/s41598-021-03632-x\n10.1038/s42256-020-0180-7\n10.1155/2021/4733167\n10.1101/2020.02.27.20028027\n10.2139/ssrn.3594614\n10.21203/rs.3.rs-38576/v1\n10.1109/TIM.2021.3130675\n10.1016/j.asoc.2021.107329\n10.1016/j.chaos.2020.110120\n10.1002/emp2.12205\n10.35414/akufemubid.788898\n10.1109/IBITeC53045.2021.9649254\n10.1038/s41591-020-0931-3\n10.1038/s41598-021-95957-w\n10.4018/JGIM.302890\n10.1109/WiDSTaif52235.2021.9430233\n10.3390/app12105137\n10.1364/OE.442321\n10.1097/MD.0000000000026503\n10.1002/jmv.26797\n10.1002/rmv.2264\n10.2196/23390\n10.3390/diagnostics10090618\n10.1002/JLB.5COVBCR0720-310RR\n10.1002/jmv.27093\n10.1093/infdis/jiaa591\n10.1002/jmv.26506\n10.1161/CIRCRESAHA.120.318218\n10.2147/IDR.S258639\n10.1016/j.lfs.2020.119010\n10.1002/adma.202103646\n10.3389/fmolb.2022.845179"}
{"title": "COVID-ConvNet: A Convolutional Neural Network Classifier for Diagnosing COVID-19 Infection.", "abstract": "The novel coronavirus (COVID-19) pandemic still has a significant impact on the worldwide population's health and well-being. Effective patient screening, including radiological examination employing chest radiography as one of the main screening modalities, is an important step in the battle against the disease. Indeed, the earliest studies on COVID-19 found that patients infected with COVID-19 present with characteristic anomalies in chest radiography. In this paper, we introduce COVID-ConvNet, a deep convolutional neural network (DCNN) design suitable for detecting COVID-19 symptoms from chest X-ray (CXR) scans. The proposed deep learning (DL) model was trained and evaluated using 21,165 CXR images from the COVID-19 Database, a publicly available dataset. The experimental results demonstrate that our COVID-ConvNet model has a high prediction accuracy at 97.43% and outperforms recent related works by up to 5.9% in terms of prediction accuracy.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-05-27", "authors": ["Ibtihal A LAlablani", "Mohammed J FAlenazi"], "doi": "10.3390/diagnostics13101675\n10.3390/s22031211\n10.1007/s13204-021-01868-7\n10.1080/14760584.2023.2157817\n10.1111/risa.13500\n10.1111/phn.12809\n10.1007/s40258-020-00580-x\n10.1016/j.scitotenv.2020.140561\n10.3390/ijerph18063056\n10.1016/j.radi.2020.05.012\n10.1016/j.compbiomed.2020.103792\n10.1016/j.procbio.2020.08.016\n10.1109/ACCESS.2020.3016780\n10.1109/JAS.2020.1003393\n10.1109/JBHI.2020.3037127\n10.1016/j.asoc.2022.109109\n10.1038/s41598-020-76550-z\n10.1109/ACCESS.2020.3044858\n10.1109/TMI.2020.2994908\n10.1007/s13755-021-00166-4\n10.1016/j.eswa.2020.114054\n10.1007/s10044-021-00984-y\n10.1007/s10489-020-01829-7\n10.1109/TMI.2013.2290491\n10.1007/s10489-020-01902-1\n10.1007/s10439-022-02958-5\n10.1016/j.bspc.2022.103772\n10.1016/j.eswa.2022.118029\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1016/j.chest.2020.04.010\n10.1101/2020.02.10.20021584\n10.1186/s12967-020-02505-7\n10.1186/s40560-020-00453-4\n10.1007/s40815-022-01399-5\n10.1109/ACCESS.2021.3068614\n10.1109/IACC48062.2019.8971494\n10.1007/s11042-021-11100-x\n10.1016/j.patrec.2019.04.019\n10.1109/ACCESS.2021.3136129\n10.1111/2041-210X.13373\n10.1109/ACCESS.2023.3237851\n10.1109/TMC.2019.2930506\n10.3390/s21196361"}
{"title": "Level Set Image Feature Detection and Application in COVID-19 Image Feature Knowledge Detection.", "abstract": "Artificial intelligence (AI) scholars and mediciners have reported AI systems that accurately detect medical imaging and COVID-19 in chest images. However, the robustness of these models remains unclear for the segmentation of images with nonuniform density distribution or the multiphase target. The most representative one is the Chan-Vese (CV) image segmentation model. In this paper, we demonstrate that the recent level set (LV) model has excellent performance on the detection of target characteristics from medical imaging relying on the filtering variational method based on the global medical pathology facture. We observe that the capability of the filtering variational method to obtain image feature quality is better than other LV models. This research reveals a far-reaching problem in medical-imaging AI knowledge detection. In addition, from the analysis of experimental results, the algorithm proposed in this paper has a good effect on detecting the lung region feature information of COVID-19 images and also proves that the algorithm has good adaptability in processing different images. These findings demonstrate that the proposed LV method should be seen as an effective clinically adjunctive method using machine-learning healthcare models.", "journal": "BioMed research international", "date": "2023-05-26", "authors": ["DongshengJi", "YafengLiu", "QingyiZhang", "WenjunZheng"], "doi": "10.1155/2023/1632992\n10.1007/s10851-007-0002-0\n10.1109/ACCESS.2018.2826924\n10.1007/BF00133570\n10.1109/TIP.2007.908073\n10.1007/s11760-017-1234-0\n10.1016/j.ejmp.2017.09.123\n10.1109/TIP.2017.2666042\n10.1109/TIP.2015.2389619\n10.1007/s11042-018-6035-0\n10.1016/j.patcog.2009.10.010\n10.1016/j.patcog.2013.11.014\n10.1109/TIP.2018.2806201\n10.21917/ijivp.2017.0231\n10.1007/s11042-018-7087-x\n10.1007/s00500-018-3491-4\n10.1109/CVPR.2005.212\n10.1109/LGRS.2019.2933149\n10.1016/j.jvcir.2016.05.011\n10.1109/CVPR.2007.383012\n10.1007/978-3-642-34732-0_23\n10.1109/TPAMI.2006.97\n10.1109/ICPR.2002.1044788\n10.1007/s11760-020-01850-w"}
{"title": "Learning without forgetting by leveraging transfer learning for detecting COVID-19 infection from CT images.", "abstract": "COVID-19, a global pandemic, has killed thousands in the last three years. Pathogenic laboratory testing is the gold standard but has a high false-negative rate, making alternate diagnostic procedures necessary to fight against it. Computer Tomography (CT) scans help diagnose and monitor COVID-19, especially in severe cases. But, visual inspection of CT images takes time and effort. In this study, we employ Convolution Neural Network (CNN) to detect coronavirus infection from CT images. The proposed study utilized transfer learning on the three pre-trained deep CNN models, namely VGG-16, ResNet, and wide ResNet, to diagnose and detect COVID-19 infection from the CT images. However, when the pre-trained models are retrained, the model suffers the generalization capability to categorize the data in the original datasets. The novel aspect of this work is the integration of deep CNN architectures with Learning without Forgetting (LwF) to enhance the model's generalization capabilities on both trained and new data samples. The LwF makes the network use its learning capabilities in training on the new dataset while preserving the original competencies. The deep CNN models with the LwF model are evaluated on original images and CT scans of individuals infected with Delta-variant of the SARS-CoV-2 virus. The experimental results show that of the three fine-tuned CNN models with the LwF method, the wide ResNet model's performance is superior and effective in classifying original and delta-variant datasets with an accuracy of 93.08% and 92.32%, respectively.", "journal": "Scientific reports", "date": "2023-05-26", "authors": ["MalligaSubramanian", "Veerappampalayam EaswaramoorthySathishkumar", "JaehyukCho", "KogilavaniShanmugavadivel"], "doi": "10.1038/s41598-023-34908-z\n10.1148/radiol.2020200236\n10.1148/radiol.2020200269\n10.1145/3065386\n10.1016/j.ecoinf.2020.101093\n10.1007/s42979-021-00782-7\n10.1089/big.2021.0218\n10.1155/2020/8843664\n10.1002/mp.14609\n10.1007/s00330-021-07715-1\n10.1109/TCBB.2021.3065361\n10.1007/s10044-021-00984-y\n10.1007/s10096-020-03901-z\n10.1186/s43055-021-00524-y\n10.1038/s42256-021-00307-0\n10.1016/j.bspc.2021.103441\n10.1016/j.compbiomed.2022.105604\n10.1016/j.bbe.2021.12.001\n10.1007/s00521-022-06918-x\n10.1145/3551647\n10.7717/peerj-cs.364\n10.7717/peerj-cs.358\n10.1016/j.compbiomed.2023.106646\n10.1109/ACCESS.2023.3244952\n10.1038/nature14539\n10.1007/s10462-020-09825-6\n10.1016/j.compag.2020.105393\n10.1109/TPAMI.2017.2773081\n10.1016/j.catena.2019.104249\n10.3390/s23042085"}
{"title": "A Systematic Literature Review and Future Perspectives for Handling Big Data Analytics in COVID-19 Diagnosis.", "abstract": "In today's digital world, information is growing along with the expansion of Internet usage worldwide. As a consequence, bulk of data is generated constantly which is known to be \"Big Data\". One of the most evolving technologies in twenty-first century is Big Data analytics, it is promising field for extracting knowledge from very large datasets and enhancing benefits while lowering costs. Due to the enormous success of big data analytics, the healthcare sector is increasingly shifting toward adopting these approaches to diagnose diseases. Due to the recent boom in medical big data and the development of computational methods, researchers and practitioners have gained the ability to mine and visualize medical big data on a larger scale. Thus, with the aid of integration of big data analytics in healthcare sectors, precise medical data analysis is now feasible with early sickness detection, health status monitoring, patient treatment, and community services is now achievable. With all these improvements, a deadly disease COVID is considered in this comprehensive review with the intention of offering remedies utilizing big data analytics. The use of big data applications is vital to managing pandemic conditions, such as predicting outbreaks of COVID-19 and identifying cases and patterns of spread of COVID-19. Research is still being done on leveraging big data analytics to forecast COVID-19. But precise and early identification of COVID disease is still lacking due to the volume of medical records like dissimilar medical imaging modalities. Meanwhile, Digital imaging has now become essential to COVID diagnosis, but the main challenge is the storage of massive volumes of data. Taking these limitations into account, a comprehensive analysis is presented in the systematic literature review (SLR) to provide a deeper understanding of big data in the field of COVID-19.", "journal": "New generation computing", "date": "2023-05-25", "authors": ["NagamaniTenali", "Gatram Rama MohanBabu"], "doi": "10.1007/s00354-023-00211-8\n10.1016/j.ijinfomgt.2020.102231\n10.1007/s10462-019-09685-9\n10.1016/j.jbusres.2020.09.012\n10.1080/0960085X.2020.1740618\n10.1016/j.jbusres.2020.01.022\n10.1016/j.idh.2018.10.002\n10.1109/TFUZZ.2020.3016346\n10.1007/s11036-020-01700-6\n10.1016/j.jbusres.2020.03.028\n10.1016/j.ijinfomgt.2018.12.011\n10.1007/s10916-019-1419-x\n10.1108/BPMJ-11-2020-0527\n10.22381/JSME8320202\n10.3390/ijerph17176161\n10.1109/ACCESS.2019.2948949\n10.1109/TBDATA.2018.2847629\n10.1109/ACCESS.2020.3033279\n10.1109/ACCESS.2021.3062467\n10.1109/ACCESS.2020.3023344\n10.1109/TSMC.2019.2961378\n10.1109/TII.2019.2927349\n10.1186/s40537-017-0111-6\n10.1109/ACCESS.2019.2955983\n10.1007/s12652-018-1095-6\n10.26599/BDMA.2020.9020024\n10.1111/poms.12737\n10.1007/s10796-020-10022-7\n10.1016/j.asoc.2021.107447\n10.1016/j.cose.2021.102435\n10.1016/j.neucom.2021.08.086\n10.1080/09720529.2021.1969733\n10.1111/anec.12839\n10.1016/j.jpdc.2021.03.002\n10.1002/cpe.6316\n10.1016/j.bdr.2021.100285\n10.26599/BDMA.2021.9020011\n10.1007/s00521-020-05076-2\n10.1007/s10619-021-07375-6\n10.1007/s10586-020-03146-7\n10.1007/s11633-021-1312-1\n10.1186/s40537-021-00519-6\n10.1093/comjnl/bxab135\n10.4018/IJSKD.297043\n10.1007/s10619-020-07285-z\n10.1007/s12652-020-02181-x\n10.1016/j.compeleceng.2017.03.009\n10.1109/TSMC.2017.2700889\n10.1016/j.inffus.2017.09.005\n10.1007/s10766-017-0513-2\n10.26599/BDMA.2018.9020005\n10.1007/s11227-020-03190-5\n10.1007/s11227-018-2362-1\n10.1016/j.displa.2021.102061\n10.1016/j.eswa.2021.115076\n10.1016/j.knosys.2021.107417\n10.1109/ACCESS.2020.3001973\n10.1109/TMI.2020.2993291\n10.1016/j.chaos.2020.110120\n10.1016/j.ijhm.2020.102849\n10.1016/j.chaos.2020.110336\n10.1007/s11227-020-03586-3\n10.1016/j.chaos.2020.109944\n10.1007/s10796-021-10135-7\n10.3233/JIFS-189284\n10.3390/ijerph181910147\n10.1016/j.bspc.2020.102257\n10.1109/TUFFC.2021.3068190\n10.1007/s42979-021-00782-7\n10.1109/ACCESS.2020.3040245\n10.1109/TCYB.2020.3042837\n10.1109/TII.2021.3057683\n10.1016/j.aej.2021.03.052\n10.1007/s11042-020-10000-w\n10.1007/s11042-023-14605-9\n10.1007/s00354-014-0407-4"}
{"title": "Deep Convolutional Neural Networks for Detecting COVID-19 Using Medical Images: A Survey.", "abstract": "Coronavirus Disease 2019 (COVID-19), which is caused by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2), surprised the world in December 2019 and has threatened the lives of millions of people. Countries all over the world closed worship places and shops, prevented gatherings, and implemented curfews to stand against the spread of COVID-19. Deep Learning (DL) and Artificial Intelligence (AI) can have a great role in detecting and fighting this disease. Deep learning can be used to detect COVID-19 symptoms and signs from different imaging modalities, such as X-Ray, Computed Tomography (CT), and Ultrasound Images (US). This could help in identifying COVID-19 cases as a first step to curing them. In this paper, we reviewed the research studies conducted from January 2020 to September 2022 about deep learning models that were used in COVID-19 detection. This paper clarified the three most common imaging modalities (X-Ray, CT, and US) in addition to the DL approaches that are used in this detection and compared these approaches. This paper also provided the future directions of this field to fight COVID-19 disease.", "journal": "New generation computing", "date": "2023-05-25", "authors": ["RanaKhattab", "Islam RAbdelmaksoud", "SamirAbdelrazek"], "doi": "10.1007/s00354-023-00213-6\n10.1016/j.jiph.2020.03.019\n10.1016/j.ijid.2020.01.009\n10.1001/jama.2020.2633\n10.1016/S2213-2600(20)30056-4\n10.1016/j.tmrv.2020.02.003\n10.1016/j.jare.2020.03.005\n10.1038/s41423-020-0400-4\n10.1038/s41579-020-0336-9\n10.3390/pathogens9030231\n10.1016/S0140-6736(20)30211-7\n10.3390/v12020135\n10.1038/s41579-020-00459-7\n10.3389/fpubh.2016.00159\n10.1001/jama.2020.2131\n10.46234/ccdcw2020.032\n10.1016/j.jinf.2020.02.018\n10.1093/cid/ciaa198\n10.1016/S0140-6736(10)61459-6\n10.1001/jama.2020.1585\n10.1016/j.rmed.2020.105980\n10.1118/1.2208736\n10.1148/radiol.2020200642\n10.1038/s41467-020-18685-1\n10.1148/ryct.2020200026\n10.1016/S1473-3099(20)30086-4\n10.2214/AJR.20.22975\n10.1007/s10489-020-01943-6\n10.3390/app9081526\n10.3390/diagnostics10010027\n10.1016/j.media.2017.07.005\n10.1159/000455809\n10.1109/TNNLS.2020.2995800\n10.26599/BDMA.2018.9020001\n10.1109/TMI.2019.2919951\n10.1109/5.726791\n10.1167/9.8.1037\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1016/j.media.2020.101794\n10.1007/s10044-021-00984-y\n10.1142/S0218001421510046\n10.1016/j.ins.2016.01.068\n10.1016/j.radi.2020.10.018\n10.1109/TMI.2020.3040950\n10.5455/jjee.204-1585312246\n10.1016/j.imu.2022.100916\n10.1016/j.cmpb.2020.105581\n10.1016/j.cmpb.2020.105532\n10.1038/s41598-020-74539-2\n10.3390/sym12040651\n10.1016/j.patrec.2020.09.010\n10.1016/j.mehy.2020.109761\n10.1007/s10489-020-01900-3\n10.1007/s10489-020-02076-6\n10.3390/ai1040032\n10.1016/j.imu.2020.100412\n10.1016/j.imu.2020.100360\n10.1016/j.compbiomed.2021.105188\n10.32604/iasc.2022.022179\n10.3390/electronics11010103\n10.1016/j.inffus.2021.04.008\n10.3390/electronics10212701\n10.1007/s10140-020-01886-y\n10.1007/s00330-021-07715-1\n10.1016/j.compbiomed.2020.104037\n10.1109/TMI.2020.2995965\n10.1148/radiol.2020200905\n10.7717/peerj.10086\n10.1007/s00330-020-07087-y\n10.18280/isi.250505\n10.1016/j.compbiomed.2020.103795\n10.1016/j.ipm.2022.103025\n10.1016/j.eng.2020.04.010\n10.1109/TCBB.2021.3065361\n10.1007/s10096-020-03901-z\n10.1007/s11356-020-10133-3\n10.1016/j.cell.2020.04.045\n10.1007/s00259-020-04953-1\n10.3390/diagnostics11020158\n10.1109/TMI.2020.2994459\n10.1016/j.radi.2020.04.005\n10.1016/j.acra.2020.04.032\n10.1109/ACCESS.2020.3016780\n10.1016/j.chaos.2020.110190\n10.1080/07391102.2020.1767212\n10.1016/j.bbe.2021.12.001\n10.1109/ACCESS.2020.3010287\n10.1186/s13104-021-05592-x\n10.1002/jum.15285\n10.1016/j.cell.2018.02.010"}
{"title": "Artificial neural network based prediction of the lung tissue involvement as an independent in-hospital mortality and mechanical ventilation risk factor in COVID-19.", "abstract": "During COVID-19 pandemic, artificial neural network (ANN) systems have been providing aid for clinical decisions. However, to achieve optimal results, these models should link multiple clinical data points to simple models. This study aimed to model the in-hospital mortality and mechanical ventilation risk using a two step approach combining clinical variables and ANN-analyzed lung inflammation data.\nA data set of 4317 COVID-19 hospitalized patients, including 266 patients requiring mechanical ventilation, was analyzed. Demographic and clinical data (including the length of hospital stay and mortality) and chest computed tomography (CT) data were collected. Lung involvement was analyzed using a trained ANN. The combined data were then analyzed using unadjusted and multivariate Cox proportional hazards models.\nOverall in-hospital mortality associated with ANN-assigned percentage of the lung involvement (hazard ratio [HR]: 5.72, 95% confidence interval [CI]: 4.4-7.43, p\u2009<\u20090.001 for the patients with >50% of lung tissue affected by COVID-19 pneumonia), age category (HR: 5.34, 95% CI: 3.32-8.59 for cases >80 years, p\u2009<\u20090.001), procalcitonin (HR: 2.1, 95% CI: 1.59-2.76, p\u2009<\u20090.001, C-reactive protein level (CRP) (HR: 2.11, 95% CI: 1.25-3.56, p\u2009=\u20090.004), glomerular filtration rate (eGFR) (HR: 1.82, 95% CI: 1.37-2.42, p\u2009<\u20090.001) and troponin (HR: 2.14, 95% CI: 1.69-2.72, p\u2009<\u20090.001). Furthermore, the risk of mechanical ventilation is also associated with ANN-based percentage of lung inflammation (HR: 13.2, 95% CI: 8.65-20.4, p\u2009<\u20090.001 for patients with >50% involvement), age, procalcitonin (HR: 1.91, 95% CI: 1.14-3.2, p\u2009=\u20090.14, eGFR (HR: 1.82, 95% CI: 1.2-2.74, p\u2009=\u20090.004) and clinical variables, including diabetes (HR: 2.5, 95% CI: 1.91-3.27, p\u2009<\u20090.001), cardiovascular and cerebrovascular disease (HR: 3.16, 95% CI: 2.38-4.2, p\u2009<\u20090.001) and chronic pulmonary disease (HR: 2.31, 95% CI: 1.44-3.7, p\u2009<\u20090.001).\nANN-based lung tissue involvement is the strongest predictor of unfavorable outcomes in COVID-19 and represents a valuable support tool for clinical decisions.", "journal": "Journal of medical virology", "date": "2023-05-23", "authors": ["Mi\u0142oszParczewski", "JakubKufel", "BoguszAksak-W\u0105s", "JoannaPiwnik", "DanielChober", "TomaszPuzio", "LauraLesiewska", "SebastianBia\u0142kowski", "MilenaRafalska-Kosior", "JacekWydra", "KrystianAwgul", "MilenaGrobelna", "AdamMajchrzak", "KosmaDunikowski", "KrzysztofJurczyk", "MarekPodyma", "KarolSerwin", "JakubMusia\u0142ek"], "doi": "10.1002/jmv.28787"}
{"title": "Generative adversarial network for automatic quantification of Coronavirus disease 2019 pneumonia on chest radiographs.", "abstract": "To develop a generative adversarial network (GAN) to quantify COVID-19 pneumonia on chest radiographs automatically.\nThis retrospective study included 50,000 consecutive non-COVID-19 chest CT scans in 2015-2017 for training. Anteroposterior virtual chest, lung, and pneumonia radiographs were generated from whole, segmented lung, and pneumonia pixels from each CT scan. Two GANs were sequentially trained to generate lung images from radiographs and to generate pneumonia images from lung images. GAN-driven pneumonia extent (pneumonia area/lung area) was expressed from 0% to 100%. We examined the correlation of GAN-driven pneumonia extent with semi-quantitative Brixia X-ray severity score (one dataset, n\u00a0=\u00a04707) and quantitative CT-driven pneumonia extent (four datasets, n\u00a0=\u00a054-375), along with analyzing a measurement difference between the GAN and CT extents. Three datasets (n\u00a0=\u00a0243-1481), where unfavorable outcomes (respiratory failure, intensive care unit admission, and death) occurred in 10%, 38%, and 78%, respectively, were used to examine the predictive power of GAN-driven pneumonia extent.\nGAN-driven radiographic pneumonia was correlated with the severity score (0.611) and CT-driven extent (0.640). 95% limits of agreements between GAN and CT-driven extents were -27.1% to 17.4%. GAN-driven pneumonia extent provided odds ratios of 1.05-1.18 per percent for unfavorable outcomes in the three datasets, with areas under the receiver operating characteristic curve (AUCs) of 0.614-0.842. When combined with demographic information only and with both demographic and laboratory information, the prediction models yielded AUCs of 0.643-0.841 and 0.688-0.877, respectively.\nThe generative adversarial network automatically quantified COVID-19 pneumonia on chest radiographs and identified patients with unfavorable outcomes.", "journal": "European journal of radiology", "date": "2023-05-21", "authors": ["Seung-JinYoo", "HyungjinKim", "Joseph NathanaelWitanto", "ShoheiInui", "Jeong-HwaYoon", "Ki-DeokLee", "Yo WonChoi", "Jin MoGoo", "Soon HoYoon"], "doi": "10.1016/j.ejrad.2023.110858"}
{"title": "Fuzzy Attention Neural Network to Tackle Discontinuity in Airway Segmentation.", "abstract": "Airway segmentation is crucial for the examination, diagnosis, and prognosis of lung diseases, while its manual delineation is unduly burdensome. To alleviate this time-consuming and potentially subjective manual procedure, researchers have proposed methods to automatically segment airways from computerized tomography (CT) images. However, some small-sized airway branches (e.g., bronchus and terminal bronchioles) significantly aggravate the difficulty of automatic segmentation by machine learning models. In particular, the variance of voxel values and the severe data imbalance in airway branches make the computational module prone to discontinuous and false-negative predictions, especially for cohorts with different lung diseases. The attention mechanism has shown the capacity to segment complex structures, while fuzzy logic can reduce the uncertainty in feature representations. Therefore, the integration of deep attention networks and fuzzy theory, given by the fuzzy attention layer, should be an escalated solution for better generalization and robustness. This article presents an efficient method for airway segmentation, comprising a novel fuzzy attention neural network (FANN) and a comprehensive loss function to enhance the spatial continuity of airway segmentation. The deep fuzzy set is formulated by a set of voxels in the feature map and a learnable Gaussian membership function. Different from the existing attention mechanism, the proposed channel-specific fuzzy attention addresses the issue of heterogeneous features in different channels. Furthermore, a novel evaluation metric is proposed to assess both the continuity and completeness of airway structures. The efficiency, generalization, and robustness of the proposed method have been proved by training on normal lung disease while testing on datasets of lung cancer, COVID-19, and pulmonary fibrosis.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2023-05-19", "authors": ["YangNan", "Javier DelSer", "ZeyuTang", "PengTang", "XiaodanXing", "YingyingFang", "FranciscoHerrera", "WitoldPedrycz", "SimonWalsh", "GuangYang"], "doi": "10.1109/TNNLS.2023.3269223"}
{"title": "Profiling post-COVID-19 condition across different variants of SARS-CoV-2: a prospective longitudinal study in unvaccinated wild-type, unvaccinated alpha-variant, and vaccinated delta-variant populations.", "abstract": "Self-reported symptom studies rapidly increased understanding of SARS-CoV-2 during the COVID-19 pandemic and enabled monitoring of long-term effects of COVID-19 outside hospital settings. Post-COVID-19 condition presents as heterogeneous profiles, which need characterisation to enable personalised patient care. We aimed to describe post-COVID-19 condition profiles by viral variant and vaccination status.\nIn this prospective longitudinal cohort study, we analysed data from UK-based adults (aged 18-100 years) who regularly provided health reports via the Covid Symptom Study smartphone app between March 24, 2020, and Dec 8, 2021. We included participants who reported feeling physically normal for at least 30 days before testing positive for SARS-CoV-2 who subsequently developed long COVID (ie, symptoms lasting longer than 28 days from the date of the initial positive test). We separately defined post-COVID-19 condition as symptoms that persisted for at least 84 days after the initial positive test. We did unsupervised clustering analysis of time-series data to identify distinct symptom profiles for vaccinated and unvaccinated people with post-COVID-19 condition after infection with the wild-type, alpha (B.1.1.7), or delta (B.1.617.2 and AY.x) variants of SARS-CoV-2. Clusters were then characterised on the basis of symptom prevalence, duration, demography, and previous comorbidities. We also used an additional testing sample with additional data from the Covid Symptom Study Biobank (collected between October, 2020, and April, 2021) to investigate the effects of the identified symptom clusters of post-COVID-19 condition on the lives of affected people.\nWe included 9804 people from the COVID Symptom Study with long COVID, 1513 (15%) of whom developed post-COVID-19 condition. Sample sizes were sufficient only for analyses of the unvaccinated wild-type, unvaccinated alpha variant, and vaccinated delta variant groups. We identified distinct profiles of symptoms for post-COVID-19 condition within and across variants: four endotypes were identified for infections due to the wild-type variant (in unvaccinated people), seven for the alpha variant (in unvaccinated people), and five for the delta variant (in vaccinated people). Across all variants, we identified a cardiorespiratory cluster of symptoms, a central neurological cluster, and a multi-organ systemic inflammatory cluster. These three main clusers were confirmed in a testing sample. Gastrointestinal symptoms clustered in no more than two specific phenotypes per viral variant.\nOur unsupervised analysis identified different profiles of post-COVID-19 condition, characterised by differing symptom combinations, durations, and functional outcomes. Our classification could be useful for understanding the distinct mechanisms of post-COVID-19 condition, as well as for identification of subgroups of individuals who might be at risk of prolonged debilitation.\nUK Government Department of Health and Social Care, Chronic Disease Research Foundation, The Wellcome Trust, UK Engineering and Physical Sciences Research Council, UK Research and Innovation London Medical Imaging & Artificial Intelligence Centre for Value-Based Healthcare, UK National Institute for Health Research, UK Medical Research Council, British Heart Foundation, UK Alzheimer's Society, and ZOE.", "journal": "The Lancet. Digital health", "date": "2023-05-19", "authors": ["Liane SCanas", "ErikaMolteni", "JieDeng", "Carole HSudre", "BenjaminMurray", "EricKerfoot", "MichelaAntonelli", "KhaledRjoob", "JoanCapdevila Pujol", "LorenzoPolidori", "AnnaMay", "Marc F\u00d6sterdahl", "RonanWhiston", "Nathan JCheetham", "VickyBowyer", "Tim DSpector", "AlexanderHammers", "Emma LDuncan", "SebastienOurselin", "Claire JSteves", "MarcModat"], "doi": "10.1016/S2589-7500(23)00056-0"}
{"title": "Collective and Individual Assessment of the Risk of Death from COVID-19 for the Elderly, 2020-2022.", "abstract": "Coronavirus disease 2019 (COVID-19) has had profound disruptions worldwide. For a population or individual, it is critical to assess the risk of death for making preventative decisions.\nIn this study, clinical data from approximately 100 million cases were statistically analyzed. A software and an online assessment tool were developed in Python to evaluate the risk of mortality.\nOur analysis revealed that 76.51% of COVID-19-related fatalities occurred among individuals aged over 65 years, with frailty-associated deaths accounting for more than 80% of these cases. Furthermore, over 80% of the reported deaths involved unvaccinated individuals. A notable overlap was observed between aging and frailty-associated deaths, both of which were connected to underlying health conditions. For those with at least two comorbidities, the proportion of frailty and the proportion of COVID-19-related death were both close to 75 percent. Subsequently, we established a formula to calculate the number of deaths, which was validated using data from twenty countries and regions. Using this formula, we developed and verified an intelligent software designed to predict the death risk for a given population. To facilitate rapid risk screening on an individual level, we also introduced a six-question online assessment tool.\nThis study examined the impact of underlying diseases, frailty, age, and vaccination history on COVID-19-related mortality, resulting in a sophisticated software and a user-friendly online scale to assess mortality risk. These tools offer valuable assistance in informed decision-making.", "journal": "China CDC weekly", "date": "2023-05-18", "authors": ["ChaobaoZhang", "HongzhiWang", "ZiluWen", "ZhijunBao", "XiangqiLi"], "doi": "10.46234/ccdcw2023.077\n10.3389/fpubh.2022.808471\n10.3389/fcimb.2022.836409\n10.1016/j.jiph.2021.01.002\n10.1093/cid/ciab493\n10.1016/S0140-6736(21)02249-2\n10.1093/ageing/afaa184\n10.1016/j.csbj.2021.04.004\n10.1016/S0140-6736(12)62167-9\n10.1093/ageing/afaa219\n10.1016/S2666-7568(21)00006-4\n10.1016/S0140-6736(19)31785-4"}
{"title": "Artificial intelligence for reducing the radiation burden of medical imaging for the diagnosis of coronavirus disease.", "abstract": "Medical imaging has been intensively employed in screening, diagnosis and monitoring during the COVID-19 pandemic. With the improvement of RT-PCR and rapid inspection technologies, the diagnostic references have shifted. Current recommendations tend to limit the application of medical imaging in the acute setting. Nevertheless, efficient and complementary values of medical imaging have been recognized at the beginning of the pandemic when facing unknown infectious diseases and a lack of sufficient diagnostic tools. Optimizing medical imaging for pandemics may still have encouraging implications for future public health, especially for long-lasting post-COVID-19 syndrome theranostics. A critical concern for the application of medical imaging is the increased radiation burden, particularly when medical imaging is used for screening and rapid containment purposes. Emerging artificial intelligence (AI) technology provides the opportunity to reduce the radiation burden while maintaining diagnostic quality. This review summarizes the current AI research on dose reduction for medical imaging, and the retrospective identification of their potential in COVID-19 may still have positive implications for future public health.", "journal": "European physical journal plus", "date": "2023-05-17", "authors": ["JiaxiHu", "StavroulaMougiakakou", "SongXue", "AliAfshar-Oromieh", "WolfHautz", "AndreasChriste", "RaphaelSznitman", "AxelRominger", "LukasEbner", "KuangyuShi"], "doi": "10.1140/epjp/s13360-023-03745-4\n10.1016/j.jaut.2020.102433\n10.1056/NEJMoa2002032\n10.1148/radiol.2020200823\n10.1148/radiol.2020202708\n10.1016/j.crad.2020.03.003\n10.1016/j.crad.2020.03.003\n10.1007/s00330-020-07347-x\n10.1016/j.radi.2020.05.012\n10.11604/pamj.supp.2020.35.24258\n10.1016/j.radi.2020.09.010\n10.1148/radiol.2020200642\n10.1016/j.ijid.2020.04.023\n10.1016/j.xinn.2020.100061\n10.2807/1560-7917.ES.2020.25.9.2000173\n10.1001/jama.2020.3786\n10.1155/2021/6627207\n10.1016/j.tmaid.2020.101627\n10.1007/s11604-020-00967-9\n10.1148/radiol.2020200343\n10.1148/radiol.2020200432\n10.1148/radiol.2020200463\n10.1183/13993003.01809-2020\n10.1016/j.clinimag.2020.08.014\n10.1016/j.jinf.2021.01.004\n10.1148/radiol.2021211396\n10.1038/s41591-020-01177-6\n10.1007/s00259-020-04734-w\n10.1001/jamanetworkopen.2021.11417\n10.1038/s41587-021-00984-7\n10.1136/thoraxjnl-2020-215818\n10.3233/XST-200685\n10.1097/RLU.0000000000002779\n10.2967/jnumed.117.199075\n10.1038/nature14539\n10.1073/pnas.1604850113\n10.1016/j.jalz.2015.01.010\n10.1126/science.aam6960\n10.1038/nature21056\n10.1016/j.cell.2018.02.010\n10.1016/j.neuroimage.2018.03.045\n10.1002/mp.13187\n10.2967/jnumed.117.199414\n10.1109/TCBB.2021.3065361\n10.1007/s00330-020-07044-9\n10.1016/j.ejmp.2020.11.027\n10.1007/s00330-021-07715-1\n10.1148/radiol.2020201491\n10.1016/j.eng.2020.04.010\n10.1186/s40001-020-00450-1\n10.1148/radiol.2020200905\n10.1148/radiol.2020201473\n10.1148/radiol.2020202439\n10.1038/s42256-021-00307-0\n10.1117/1.JMI.9.3.034003\n10.1109/RBME.2020.2987975\n10.1007/s12559-020-09787-5\n10.1148/radiol.2020200527\n10.1054/bjoc.2000.1531\n10.1007/s00330-020-06809-6\n10.3390/diagnostics12010225\n10.1016/j.jmir.2020.09.001\n10.1002/mp.13627\n10.1016/j.ejmp.2020.11.012\n10.2214/AJR.12.9424\n10.1109/TMI.2019.2963248\n10.1016/j.ejro.2016.04.001\n10.2214/ajr.183.3.1830809\n10.1007/s10140-020-01838-6\n10.3390/jcm9123860\n10.1097/MNM.0000000000001299\n10.1007/s00259-020-04767-1\n10.21037/qims-20-1078\n10.1016/j.neucom.2017.06.048\n10.1002/mp.13626"}
{"title": "COVID-19 Diagnosis in Computerized Tomography (CT) and X-ray Scans Using Capsule Neural Network.", "abstract": "This study proposes a deep-learning-based solution (named CapsNetCovid) for COVID-19 diagnosis using a capsule neural network (CapsNet). CapsNets are robust for image rotations and affine transformations, which is advantageous when processing medical imaging datasets. This study presents a performance analysis of CapsNets on standard images and their augmented variants for binary and multi-class classification. CapsNetCovid was trained and evaluated on two COVID-19 datasets of CT images and X-ray images. It was also evaluated on eight augmented datasets. The results show that the proposed model achieved classification accuracy, precision, sensitivity, and F1-score of 99.929%, 99.887%, 100%, and 99.319%, respectively, for the CT images. It also achieved a classification accuracy, precision, sensitivity, and F1-score of 94.721%, 93.864%, 92.947%, and 93.386%, respectively, for the X-ray images. This study presents a comparative analysis between CapsNetCovid, CNN, DenseNet121, and ResNet50 in terms of their ability to correctly identify randomly transformed and rotated CT and X-ray images without the use of data augmentation techniques. The analysis shows that CapsNetCovid outperforms CNN, DenseNet121, and ResNet50 when trained and evaluated on CT and X-ray images without data augmentation. We hope that this research will aid in improving decision making and diagnostic accuracy of medical professionals when diagnosing COVID-19.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-05-16", "authors": ["Andronicus AAkinyelu", "BubacarrBah"], "doi": "10.3390/diagnostics13081484\n10.3389/frai.2022.919672\n10.1007/s10586-022-03703-2\n10.1613/jair.953\n10.1109/TAI.2021.3104791\n10.1016/j.chaos.2020.110122\n10.1002/ima.22566\n10.1016/j.patrec.2020.09.010\n10.3389/frai.2021.598932\n10.1016/j.compbiomed.2021.104399\n10.1016/j.compbiomed.2021.105182\n10.1016/j.chemolab.2022.104750\n10.1038/s41598-023-27697-y\n10.1007/s10140-020-01886-y\n10.1016/j.asoc.2022.109401\n10.1177/20552076221092543\n10.1038/s41598-021-93832-2\n10.1007/s40747-020-00216-6\n10.3390/ijerph20032035\n10.1007/s42979-020-00209-9\n10.1016/j.cell.2020.04.045\n10.1016/j.bspc.2021.102588\n10.1038/s41551-020-00633-5\n10.3389/fmed.2021.729287\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1002/ima.22706\n10.1016/j.imu.2020.100505\n10.1007/s13246-020-00865-4\n10.1007/s10044-021-00984-y\n10.1007/s42979-021-00881-5\n10.1007/s11042-023-14353-w\n10.1016/j.imu.2020.100360\n10.1007/s10140-020-01808-y\n10.1038/s41598-020-76550-z"}
{"title": "Predicting the negative conversion time of nonsevere COVID-19 patients using machine learning methods.", "abstract": "Based on the patient's clinical characteristics and laboratory indicators, different machine-learning methods were used to develop models for predicting the negative conversion time of nonsevere coronavirus disease 2019 (COVID-19) patients. A retrospective analysis was performed on 376 nonsevere COVID-19 patients admitted to Wuxi Fifth People's Hospital from May 2, 2022, to May 14, 2022. The patients were divided into training set (n\u2009=\u2009309) and test set (n\u2009=\u200967). The clinical features and laboratory parameters of the patients were collected. In the training set, the least absolute shrinkage and selection operator (LASSO) was used to select predictive features and train six machine learning models: multiple linear regression (MLR), K-Nearest Neighbors Regression (KNNR), random forest regression (RFR), support vector machine regression (SVR), XGBoost regression (XGBR), and multilayer perceptron regression (MLPR). Seven best predictive features selected by LASSO included: age, gender, vaccination status, IgG, lymphocyte ratio, monocyte ratio, and lymphocyte count. The predictive performance of the models in the test set was MLPR\u2009>\u2009SVR\u2009>\u2009MLR\u2009>\u2009KNNR\u2009>\u2009XGBR\u2009>\u2009RFR, and MLPR had the strongest generalization performance, which is significantly better than SVR and MLR. In the MLPR model, vaccination status, IgG, lymphocyte count, and lymphocyte ratio were protective factors for negative conversion time; male gender, age, and monocyte ratio were risk factors. The top three features with the highest weights were vaccination status, gender, and IgG. Machine learning methods (especially MLPR) can effectively predict the negative conversion time of non-severe COVID-19 patients. It can help to rationally allocate limited medical resources and prevent disease transmission, especially during the Omicron pandemic.", "journal": "Journal of medical virology", "date": "2023-05-15", "authors": ["JiruYe", "XiaonanShao", "YongYang", "FengZhu"], "doi": "10.1002/jmv.28747"}
{"title": "A Real Time Method for Distinguishing COVID-19 Utilizing 2D-CNN and Transfer Learning.", "abstract": "Rapid identification of COVID-19 can assist in making decisions for effective treatment and epidemic prevention. The PCR-based test is expert-dependent, is time-consuming, and has limited sensitivity. By inspecting Chest R-ray (CXR) images, COVID-19, pneumonia, and other lung infections can be detected in real time. The current, state-of-the-art literature suggests that deep learning (DL) is highly advantageous in automatic disease classification utilizing the CXR images. The goal of this study is to develop models by employing DL models for identifying COVID-19 and other lung disorders more efficiently. For this study, a dataset of 18,564 CXR images with seven disease categories was created from multiple publicly available sources. Four DL architectures including the proposed CNN model and pretrained VGG-16, VGG-19, and Inception-v3 models were applied to identify healthy and six lung diseases (fibrosis, lung opacity, viral pneumonia, bacterial pneumonia, COVID-19, and tuberculosis). Accuracy, precision, recall, f1 score, area under the curve (AUC), and testing time were used to evaluate the performance of these four models. The results demonstrated that the proposed CNN model outperformed all other DL models employed for a seven-class classification with an accuracy of 93.15% and average values for precision, recall, f1-score, and AUC of 0.9343, 0.9443, 0.9386, and 0.9939. The CNN model equally performed well when other multiclass classifications including normal and COVID-19 as the common classes were considered, yielding accuracy values of 98%, 97.49%, 97.81%, 96%, and 96.75% for two, three, four, five, and six classes, respectively. The proposed model can also identify COVID-19 with shorter training and testing times compared to other transfer learning models.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-05-13", "authors": ["AbidaSultana", "MdNahiduzzaman", "Sagor ChandroBakchy", "Saleh MohammedShahriar", "Hasibul IslamPeyal", "Muhammad E HChowdhury", "AmithKhandakar", "MohamedArselene Ayari", "MominulAhsan", "JulfikarHaider"], "doi": "10.3390/s23094458\n10.1038/s41586-020-2008-3\n10.1016/j.jaut.2020.102433\n10.1001/jama.2020.3786\n10.1002/jmv.25674\n10.1097/RTI.0000000000000404\n10.1016/S0140-6736(20)30211-7\n10.1016/j.neunet.2020.01.018\n10.1109/ACCESS.2021.3061621\n10.1109/ACCESS.2022.3182498\n10.1109/TMI.2015.2457891\n10.1038/nature21056\n10.1109/ACCESS.2020.3031384\n10.1016/j.chaos.2020.109944\n10.3390/s22031211\n10.3390/diagnostics13010131\n10.1016/j.eswa.2020.114054\n10.1109/JAS.2020.1003393\n10.1109/ACCESS.2020.3010287\n10.1109/ACCESS.2022.3221531\n10.1016/j.cmpb.2020.105608\n10.1016/j.compbiomed.2020.103792\n10.1016/j.eswa.2020.113909\n10.3390/s21051742\n10.1016/j.eswa.2022.118576\n10.3390/s22020669\n10.3390/s21175813\n10.3390/s21041480\n10.1016/j.eswa.2022.118029\n10.1016/j.compeleceng.2022.108405\n10.1186/s40537-020-00392-9\n10.1186/s40537-019-0192-5\n10.1186/s40537-019-0263-7\n10.3390/pathogens12010017\n10.1016/j.eng.2020.04.010\n10.1016/j.bspc.2022.103848\n10.1007/s13246-020-00865-4\n10.3389/fmed.2020.00427\n10.1016/j.chaos.2020.110495\n10.1016/j.cmpb.2020.105581"}
{"title": "A Lightweight AMResNet Architecture with an Attention Mechanism for Diagnosing COVID-19.", "abstract": "COVID-19 has become a worldwide epidemic disease and a new challenge for all mankind. The potential advantages of chest X-ray images on COVID-19 were discovered. We proposed a lightweight and effective Convolution Neural Network framework based on chest X-ray images for the diagnosis of COVID-19, named AMResNet.\nCOVID-19 has become a worldwide epidemic disease and a new challenge for all mankind. The potential advantages of chest X-ray images on COVID-19 were discovered.\nA lightweight and effective Convolution Neural Network framework based on chest X-ray images for the diagnosis of COVID-19.\nBy introducing the channel attention mechanism and image spatial information attention mechanism, a better level can be achieved without increasing the number of model parameters.\nIn the collected data sets, we achieved an average accuracy rate of more than 92%, and the sensitivity and specificity of specific disease categories were also above 90%.\nThe convolution neural network framework can be used as a novel method for artificial intelligence to diagnose COVID-19 or other diseases based on medical images.", "journal": "Current medical imaging", "date": "2023-05-12", "authors": ["QiZhou", "Jamal AlzobairHammad Kowah", "HuijunLi", "MingqingYuan", "LiheJiang", "XuLiu"], "doi": "10.2174/1573405620666230426121437"}
{"title": "3DFlex: determining structure and motion of flexible proteins from cryo-EM.", "abstract": "Modeling flexible macromolecules is one of the foremost challenges in single-particle cryogenic-electron microscopy (cryo-EM), with the potential to illuminate fundamental questions in structural biology. We introduce Three-Dimensional Flexible Refinement (3DFlex), a motion-based neural network model for continuous molecular heterogeneity for cryo-EM data. 3DFlex exploits knowledge that conformational variability of a protein is often the result of physical processes that transport density over space and tend to preserve local geometry. From two-dimensional image data, 3DFlex enables the determination of high-resolution 3D density, and provides an explicit model of a flexible protein's motion over its conformational landscape. Experimentally, for large molecular machines (tri-snRNP spliceosome complex, translocating ribosome) and small flexible proteins (TRPV1 ion channel, \u03b1V\u03b28 integrin, SARS-CoV-2 spike), 3DFlex learns nonrigid molecular motions while resolving details of moving secondary structure elements. 3DFlex can improve 3D density resolution beyond the limits of existing methods because particle images contribute coherent signal over the conformational landscape.", "journal": "Nature methods", "date": "2023-05-12", "authors": ["AliPunjani", "David JFleet"], "doi": "10.1038/s41592-023-01853-8\n10.1074/jbc.REV118.005602\n10.7554/eLife.36861\n10.1016/j.jsb.2015.05.007\n10.1016/j.jsb.2021.107702\n10.1016/j.ymeth.2016.02.007\n10.1038/s41592-020-01049-4\n10.1038/s41592-021-01220-5\n10.1038/nature16940\n10.1038/nature17964\n10.1107/S2052252520012725\n10.1016/j.cell.2019.12.030\n10.1038/s41467-021-26133-x\n10.1038/s41592-020-00990-8\n10.1038/nmeth.4169\n10.1016/j.jmb.2003.07.013\n10.7554/eLife.35383\n10.1016/bs.mie.2016.04.012\n10.1016/j.jsb.2012.09.006\n10.1038/nmeth992\n10.1038/nature24282\n10.1137/17M1153509\n10.1016/j.str.2011.10.003\n10.1038/s41467-020-18403-x\n10.1073/pnas.1419276111\n10.1088/1361-6420/ab4f55\n10.1038/nmeth.2115\n10.1111/cgf.14505\n10.1016/j.jsb.2006.05.004\n10.1016/j.jmb.2011.11.010\n10.1109/TPAMI.1987.4767965\n10.1016/j.jsb.2010.03.007"}
{"title": "Artificial intelligence guided HRCT assessment predicts the severity of COVID-19 pneumonia based on clinical parameters.", "abstract": "The purpose of the study was to compare the results of AI (artificial intelligence) analysis of the extent of pulmonary lesions on HRCT (high resolution computed tomography) images in COVID-19 pneumonia, with clinical data including laboratory markers of inflammation, to verify whether AI HRCT assessment can predict the clinical severity of COVID-19 pneumonia.\nThe analyzed group consisted of 388 patients with COVID-19 pneumonia, with automatically analyzed HRCT parameters of volume: AIV (absolute inflammation), AGV (absolute ground glass), ACV (absolute consolidation), PIV (percentage inflammation), PGV (percentage ground glass), PCV (percentage consolidation). Clinical data included: age, sex, admission parameters: respiratory rate, oxygen saturation, CRP (C-reactive protein), IL6 (interleukin 6), IG - immature granulocytes, WBC (white blood count), neutrophil count, lymphocyte count, serum ferritin, LDH (lactate dehydrogenase), NIH (National Institute of Health) severity score; parameters of clinical course: in-hospital death, transfer to the ICU (intensive care unit), length of hospital stay.\nThe highest correlation coefficients were found for PGV, PIV, with LDH (respectively 0.65, 0.64); PIV, PGV, with oxygen saturation (respectively -\u20090.53, -0.52); AIV, AGV, with CRP (respectively 0.48, 0.46); AGV, AIV, with ferritin (respectively 0.46, 0.45). Patients with critical pneumonia had significantly lower oxygen saturation, and higher levels of immune-inflammatory biomarkers on admission. The radiological parameters of lung involvement proved to be strong predictors of transfer to the ICU (in particular, PGV\u2009\u2265\u2009cut-off point 29% with Odds Ratio (OR): 7.53) and in-hospital death (in particular: AIV\u2009\u2265\u2009cut-off point 831 cm\nAutomatic analysis of HRCT images by AI may be a valuable method for predicting the severity of COVID-19 pneumonia. The radiological parameters of lung involvement correlate with laboratory markers of inflammation, and are strong predictors of transfer to the ICU and in-hospital death from COVID-19.\nNational Center for Research and Development CRACoV-HHS project, contract number SZPITALE-JEDNOIMIENNE/18/2020.", "journal": "BMC infectious diseases", "date": "2023-05-11", "authors": ["RobertChrzan", "BarbaraWizner", "WojciechSydor", "WiktoriaWojciechowska", "TadeuszPopiela", "MonikaBoci\u0105ga-Jasik", "AgnieszkaOlszanecka", "MagdalenaStrach"], "doi": "10.1186/s12879-023-08303-y\n10.1016/j.acra.2020.03.003\n10.1016/j.crad.2020.03.003\n10.33963/KP.15990\n10.1016/j.crad.2019.04.017\n10.2214/AJR.19.21572\n10.1148/radiol.2020200905\n10.1148/radiol.2020201491\n10.3390/jpm11050391\n10.1007/s00330-020-07156-2\n10.7150/thno.45985\n10.1038/s41598-020-80261-w\n10.1016/j.medmal.2020.03.007\n10.1631/jzus.B2000133\n10.1148/radiol.2020200230\n10.1007/s00330-020-07033-y\n10.1148/radiol.2020200370\n10.1038/s41598-021-83831-8\n10.3389/fmed.2021.689568"}
{"title": "Deep learning approach for early prediction of COVID-19 mortality using chest X-ray and electronic health records.", "abstract": "An artificial-intelligence (AI) model for predicting the prognosis or mortality of coronavirus disease 2019 (COVID-19) patients will allow efficient allocation of limited medical resources. We developed an early mortality prediction ensemble model for COVID-19 using AI models with initial chest X-ray and electronic health record (EHR) data.\nWe used convolutional neural network (CNN) models (Inception-ResNet-V2 and EfficientNet) for chest X-ray analysis and multilayer perceptron (MLP), Extreme Gradient Boosting (XGBoost), and random forest (RF) models for EHR data analysis. The Gradient-weighted Class Activation Mapping and Shapley Additive Explanations (SHAP) methods were used to determine the effects of these features on COVID-19. We developed an ensemble model (Area under the receiver operating characteristic curve of 0.8698) using a soft voting method with weight differences for CNN, XGBoost, MLP, and RF models. To resolve the data imbalance, we conducted F1-score optimization by adjusting the cutoff values to optimize the model performance (F1 score of 0.77).\nOur study is meaningful in that we developed an early mortality prediction model using only the initial chest X-ray and EHR data of COVID-19 patients. Early prediction of the clinical courses of patients is helpful for not only treatment but also bed management. Our results confirmed the performance improvement of the ensemble model achieved by combining AI models. Through the SHAP method, laboratory tests that indicate the factors affecting COVID-19 mortality were discovered, highlighting the importance of these tests in managing COVID-19 patients.", "journal": "BMC bioinformatics", "date": "2023-05-10", "authors": ["Seung MinBaik", "Kyung SookHong", "Dong JinPark"], "doi": "10.1186/s12859-023-05321-0\n10.1016/j.radi.2020.09.010\n10.3390/diagnostics12040920\n10.3390/diagnostics12040821\n10.1002/jmv.27352\n10.1371/journal.pone.0252384\n10.1371/journal.pone.0249285\n10.3389/fcvm.2021.638011\n10.1109/jbhi.2020.3012383\n10.1007/978-3-030-33128-3_1\n10.1111/cyt.12942\n10.1007/s13244-018-0639-9\n10.1007/s12559-020-09751-3\n10.1016/j.compbiomed.2020.103792\n10.1007/s00500-020-05424-3\n10.1007/s00330-021-08049-8\n10.1016/s2589-7500(21)00039-x\n10.3389/fpsyg.2021.651398\n10.1186/s40462-021-00245-x\n10.1016/j.jneumeth.2021.109098\n10.1016/j.compbiomed.2022.105550\n10.3389/fmed.2021.676343\n10.1002/jcla.24053\n10.1002/jmv.26082\n10.1016/j.mayocp.2020.04.006\n10.3389/fpubh.2022.857368\n10.3389/fcvm.2021.697737\n10.1109/tpami.2021.3083089\n10.1007/s10439-018-02116-w\n10.1038/s41598-021-87171-5\n10.1016/j.compbiomed.2021.104829\n10.3390/s21227475\n10.1007/s00521-021-06177-2\n10.2174/1573409914666180828105228\n10.1001/jamapsychiatry.2019.3671\n10.1016/j.envpol.2019.06.088"}
{"title": "COVID-19 disease identification network based on weakly supervised feature selection.", "abstract": "The coronavirus disease 2019 (COVID-19) outbreak has resulted in countless infections and deaths worldwide, posing increasing challenges for the health care system. The use of artificial intelligence to assist in diagnosis not only had a high accuracy rate but also saved time and effort in the sudden outbreak phase with the lack of doctors and medical equipment. This study aimed to propose a weakly supervised COVID-19 classification network (W-COVNet). This network was divided into three main modules: weakly supervised feature selection module (W-FS), deep learning bilinear feature fusion module (DBFF) and Grad-CAM++ based network visualization module (Grad-\u2164). The first module, W-FS, mainly removed redundant background features from computed tomography (CT) images, performed feature selection and retained core feature regions. The second module, DBFF, mainly used two symmetric networks to extract different features and thus obtain rich complementary features. The third module, Grad-\u2164, allowed the visualization of lesions in unlabeled images. A fivefold cross-validation experiment showed an average classification accuracy of 85.3%, and a comparison with seven advanced classification models showed that our proposed network had a better performance.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2023-05-10", "authors": ["JingyaoLiu", "QingheFeng", "YuMiao", "WeiHe", "WeiliShi", "ZhengangJiang"], "doi": "10.3934/mbe.2023409"}
{"title": "An efficient, lightweight MobileNetV2-based fine-tuned model for COVID-19 detection using chest X-ray images.", "abstract": "In recent years, deep learning's identification of cancer, lung disease and heart disease, among others, has contributed to its rising popularity. Deep learning has also contributed to the examination of COVID-19, which is a subject that is currently the focus of considerable scientific debate. COVID-19 detection based on chest X-ray (CXR) images primarily depends on convolutional neural network transfer learning techniques. Moreover, the majority of these methods are evaluated by using CXR data from a single source, which makes them prohibitively expensive. On a variety of datasets, current methods for COVID-19 detection may not perform as well. Moreover, most current approaches focus on COVID-19 detection. This study introduces a rapid and lightweight MobileNetV2-based model for accurate recognition of COVID-19 based on CXR images; this is done by using machine vision algorithms that focused largely on robust and potent feature-learning capabilities. The proposed model is assessed by using a dataset obtained from various sources. In addition to COVID-19, the dataset includes bacterial and viral pneumonia. This model is capable of identifying COVID-19, as well as other lung disorders, including bacterial and viral pneumonia, among others. Experiments with each model were thoroughly analyzed. According to the findings of this investigation, MobileNetv2, with its 92% and 93% training validity and 88% precision, was the most applicable and reliable model for this diagnosis. As a result, one may infer that this study has practical value in terms of giving a reliable reference to the radiologist and theoretical significance in terms of establishing strategies for developing robust features with great presentation ability.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2023-05-10", "authors": ["ShubashiniVelu"], "doi": "10.3934/mbe.2023368"}
{"title": "Data augmentation based semi-supervised method to improve COVID-19 CT classification.", "abstract": "The Coronavirus (COVID-19) outbreak of December 2019 has become a serious threat to people around the world, creating a health crisis that infected millions of lives, as well as destroying the global economy. Early detection and diagnosis are essential to prevent further transmission. The detection of COVID-19 computed tomography images is one of the important approaches to rapid diagnosis. Many different branches of deep learning methods have played an important role in this area, including transfer learning, contrastive learning, ensemble strategy, etc. However, these works require a large number of samples of expensive manual labels, so in order to save costs, scholars adopted semi-supervised learning that applies only a few labels to classify COVID-19 CT images. Nevertheless, the existing semi-supervised methods focus primarily on class imbalance and pseudo-label filtering rather than on pseudo-label generation. Accordingly, in this paper, we organized a semi-supervised classification framework based on data augmentation to classify the CT images of COVID-19. We revised the classic teacher-student framework and introduced the popular data augmentation method Mixup, which widened the distribution of high confidence to improve the accuracy of selected pseudo-labels and ultimately obtain a model with better performance. For the COVID-CT dataset, our method makes precision, F1 score, accuracy and specificity 21.04%, 12.95%, 17.13% and 38.29% higher than average values for other methods respectively, For the SARS-COV-2 dataset, these increases were 8.40%, 7.59%, 9.35% and 12.80% respectively. For the Harvard Dataverse dataset, growth was 17.64%, 18.89%, 19.81% and 20.20% respectively. The codes are available at https://github.com/YutingBai99/COVID-19-SSL.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2023-05-10", "authors": ["XiangtaoChen", "YutingBai", "PengWang", "JiaweiLuo"], "doi": "10.3934/mbe.2023294"}
{"title": "Recent development of surface-enhanced Raman scattering for biosensing.", "abstract": "Surface-Enhanced Raman Scattering (SERS) technology, as a powerful tool to identify molecular species by collecting molecular spectral signals at the single-molecule level, has achieved substantial progresses in the fields of environmental science, medical diagnosis, food safety, and biological analysis. As deepening research is delved into SERS sensing, more and more high-performance or multifunctional SERS substrate materials emerge, which are expected to push Raman sensing into more application fields. Especially in the field of biological analysis, intrinsic and extrinsic SERS sensing schemes have been widely used and explored due to their fast, sensitive and reliable advantages. Herein, recent developments of SERS substrates and their applications in biomolecular detection (SARS-CoV-2 virus, tumor etc.), biological imaging and pesticide detection are summarized. The SERS concepts (including its basic theory and sensing mechanism) and the important strategies (extending from nanomaterials with tunable shapes and nanostructures to surface bio-functionalization by modifying affinity groups or specific biomolecules) for improving SERS biosensing performance are comprehensively discussed. For data analysis and identification, the applications of machine learning methods and software acquisition sources in SERS biosensing and diagnosing are discussed in detail. In conclusion, the challenges and perspectives of SERS biosensing in the future are presented.", "journal": "Journal of nanobiotechnology", "date": "2023-05-07", "authors": ["ChenglongLin", "YanyanLi", "YusiPeng", "ShuaiZhao", "MeimeiXu", "LingxiaZhang", "ZhengrenHuang", "JianlinShi", "YongYang"], "doi": "10.1186/s12951-023-01890-7\n10.1016/0009-2614(74)85388-1\n10.1021/ja00457a071\n10.1016/S0022-0728(77)80224-6\n10.1021/es101228z\n10.1021/acs.analchem.5b02899\n10.1007/s40820-021-00620-8\n10.1002/smtd.202000993\n10.1002/smtd.201800182\n10.1021/acs.analchem.8b05533\n10.1038/s41467-018-05237-x\n10.1021/acs.analchem.8b00008\n10.1038/nature08907\n10.1039/C8NR05300F\n10.1021/la9508452\n10.1021/nl0515753\n10.1366/0003702934066460\n10.1103/PhysRevLett.78.1667\n10.1063/1.437095\n10.1126/science.275.5303.1102\n10.1021/ja104174m\n10.1039/C9NH00590K\n10.3390/nano7110398\n10.1021/acsanm.9b00400\n10.1038/nmat4957\n10.1021/nl404610c\n10.1039/C9NR02615K\n10.1007/s40820-020-00565-4\n10.1016/j.apsusc.2020.146325\n10.1063/1.3243982\n10.1039/C7NR09162A\n10.1038/nbt.2886\n10.1038/ncomms4591\n10.1021/acssensors.8b01047\n10.1021/acs.analchem.7b00911\n10.1039/C6AY00406G\n10.1016/j.bios.2019.111800\n10.1021/acssensors.9b02423\n10.1016/j.snb.2020.128826\n10.1021/j100099a038\n10.1021/ar800249y\n10.1039/C7CS00238F\n10.1039/a827241z\n10.1039/C7CS00155J\n10.1002/jrs.4874\n10.1063/1.450037\n10.1021/acsnano.9b04224\n10.1103/PhysRevLett.60.1085\n10.1021/jacs.8b02972\n10.1103/PhysRevE.62.4318\n10.1007/s11467-021-1047-z\n10.1021/ja401666p\n10.1039/C7CC06357A\n10.1002/anie.201907283\n10.1007/s12161-016-0533-3\n10.1088/0957-4484/23/9/095301\n10.1002/jrs.2880\n10.1021/ac902696y\n10.1021/acsnano.7b01536\n10.3390/bios10030025\n10.1371/journal.pone.0005381\n10.2174/1385272013374806\n10.1021/acsami.8b13180\n10.1021/nn300352b\n10.1016/j.tifs.2017.12.012\n10.1126/science.287.5454.820\n10.1016/S1084-8592(99)80014-9\n10.1021/cr030183i\n10.1002/anie.201908154\n10.1016/j.colsurfb.2020.110940\n10.1021/acsami.0c21493\n10.1002/chem.200701307\n10.1007/s12161-019-01453-3\n10.1016/j.bios.2022.114553\n10.1002/adom.202001214\n10.1039/C9NR08754K\n10.1002/anie.201902776\n10.1016/j.cca.2020.01.028\n10.1021/bc800469g\n10.1002/jrs.5234\n10.1158/0008-5472.CAN-10-3069\n10.1039/D0TB01815E\n10.1021/ja992128q\n10.1021/jp903664u\n10.1016/j.vibspec.2016.03.018\n10.3390/diagnostics11050825\n10.1128/Spectrum.00315-21\n10.1016/j.meegid.2021.104910\n10.1021/acssensors.1c02079\n10.1080/22221751.2022.2054368\n10.1038/s41467-021-21996-6\n10.1016/j.bios.2021.113857\n10.1021/acs.analchem.5b02661\n10.1038/srep03851\n10.1021/nn3036232\n10.1021/acsnano.5b05622\n10.1039/D0NA01007C\n10.1038/s42003-020-01615-8\n10.1016/j.snb.2020.129214\n10.1021/acssensors.1c00596\n10.1016/j.bios.2022.114008\n10.1021/acs.analchem.1c01061\n10.1021/acs.nanolett.1c04722\n10.3390/bios12100862\n10.1016/j.watres.2021.117104\n10.1002/jmv.27686\n10.1007/s00604-022-05272-y\n10.1016/j.talanta.2022.123356\n10.1016/j.bbrc.2021.05.074\n10.3390/md19120685\n10.3390/ph15020258\n10.1038/s41598-021-00844-z\n10.1016/j.matt.2021.11.028\n10.1039/D1RA03481B\n10.1007/s40820-021-00771-8\n10.1016/j.snb.2022.131974\n10.1016/j.watres.2021.117243\n10.1038/s41591-020-0820-9\n10.1021/acsnano.1c09371\n10.1021/acs.analchem.1c01657\n10.1016/j.bios.2021.113421\n10.1021/acssensors.1c01344\n10.1016/j.surfin.2021.101454\n10.3390/nano11061394\n10.1016/j.cej.2022.137085\n10.1039/C5NR07243C\n10.1021/acs.analchem.1c02305\n10.1016/j.bios.2010.09.021\n10.1039/c2lc40353f\n10.1021/acs.accounts.9b00192\n10.1006/scbi.1998.0119\n10.1073/pnas.64.1.161\n10.1021/ac802722c\n10.1038/s41467-018-07947-8\n10.1021/acs.analchem.7b00902\n10.1016/j.bios.2015.12.052\n10.1016/j.bios.2019.01.039\n10.1016/j.apmt.2019.04.016\n10.1038/sj.bjc.6603566\n10.1016/j.cej.2023.141502\n10.1038/nrc1958\n10.1038/nbt1377\n10.1083/jcb.201211138\n10.1126/science.aau6977\n10.1039/C8SC01611A\n10.1021/acssensors.1c00890\n10.1002/smll.201970091\n10.1021/acsami.9b21333\n10.1007/s40820-022-00803-x\n10.3402/jev.v5.31803\n10.1021/acs.analchem.8b01187\n10.1021/ac5023056\n10.1039/C7AN00672A\n10.1021/acsami.1c09388\n10.1039/C8AN01041B\n10.1016/j.bios.2022.114372\n10.1021/acsami.8b10590\n10.1021/acs.nanolett.9b04288\n10.1038/s41467-020-14774-3\n10.1002/adfm.201805710\n10.1016/j.bios.2014.10.082\n10.1142/S0217984915501973\n10.1021/jp5051035\n10.1016/j.apsusc.2018.08.216\n10.1021/acssensors.9b01436\n10.1002/jrs.1570\n10.1021/jp407647f\n10.1016/j.apsusc.2020.145376\n10.1002/advs.201900310\n10.1021/la502403q\n10.1007/s00604-020-04303-w\n10.1039/c3nr03460g\n10.1021/jacs.5b10144\n10.1016/j.apsusc.2019.144073\n10.1016/j.apsusc.2015.01.242\n10.1021/ja401428e\n10.1021/ja2057874\n10.1039/c3nr33502j\n10.1016/S1005-9040(06)60123-2\n10.1021/acssuschemeng.9b07840\n10.1021/acsphotonics.6b00213\n10.1002/smll.202107027\n10.1007/s40843-020-1283-8\n10.1021/jacs.8b02972\n10.1002/adfm.202200273\n10.1038/s41467-018-05237-x\n10.1016/j.nano.2008.04.001\n10.1038/nrc.2017.7\n10.1039/C8LC00991K\n10.1016/j.apsb.2018.03.007\n10.2967/jnumed.115.158196\n10.1073/pnas.0710575105\n10.1016/j.jconrel.2011.06.004\n10.1063/5.0047578\n10.1038/s41467-019-11829-y\n10.1039/D0TB00659A\n10.1021/acsnano.8b02681\n10.1021/acsami.6b15170\n10.1002/asia.202200014\n10.1021/ac034672u\n10.1557/mrs.2013.157\n10.1016/j.biomaterials.2010.10.058\n10.1038/nm.2721\n10.1126/scitranslmed.3010633\n10.1016/j.addr.2012.10.002\n10.1111/cas.12152\n10.2183/pjab.88.53\n10.1016/j.trac.2016.06.017\n10.1021/acs.jafc.6b04774\n10.1039/C5AY01058F\n10.1016/j.foodchem.2013.10.023\n10.1021/acs.analchem.6b04324\n10.1111/1750-3841.12759\n10.1007/s11947-011-0774-5\n10.1039/C6AY00513F\n10.1039/C8TC01741G\n10.1016/j.talanta.2019.03.021\n10.1039/C4NR06429A\n10.1016/j.talanta.2020.121188\n10.1002/0471140864.ps1708s71\n10.1016/0167-4838(92)90261-B\n10.1006/abio.1999.4320\n10.1016/S0065-3233(08)60528-8\n10.1063/1.4892667\n10.1080/00032719.2019.1710524\n10.1007/s11356-019-06407-0\n10.1063/1.5091477\n10.1016/j.aca.2017.05.013\n10.1016/j.foodcont.2021.107917\n10.1016/j.bios.2019.111718\n10.1021/acssensors.2c02194\n10.1002/jbio.202000456\n10.1155/2021/5572782\n10.1145/1961189.1961199\n10.1021/acs.langmuir.5b01473"}
{"title": "Use of machine learning to assess the prognostic utility of radiomic features for in-hospital COVID-19 mortality.", "abstract": "As portable chest X-rays are an efficient means of triaging emergent cases, their use has raised the question as to whether imaging carries additional prognostic utility for survival among patients with COVID-19. This study assessed the importance of known risk factors on in-hospital mortality and investigated the predictive utility of radiomic texture features using various machine learning approaches. We detected incremental improvements in survival prognostication utilizing texture features derived from emergent chest X-rays, particularly among older patients or those with\u00a0a higher comorbidity burden. Important features included age, oxygen saturation, blood pressure, and certain comorbid conditions, as well as image features related to the intensity and variability of pixel distribution. Thus, widely available chest X-rays, in conjunction with clinical information, may be predictive of survival outcomes of patients with COVID-19, especially older, sicker patients, and can aid in disease management by providing additional information.", "journal": "Scientific reports", "date": "2023-05-06", "authors": ["YumingSun", "StephenSalerno", "XinweiHe", "ZiyangPan", "EileenYang", "ChinakornSujimongkol", "JiyeonSong", "XinanWang", "PeisongHan", "JianKang", "Michael WSjoding", "ShrutiJolly", "David CChristiani", "YiLi"], "doi": "10.1038/s41598-023-34559-0\n10.1016/S0140-6736(21)02758-6\n10.1126/science.372.6549.1375\n10.1007/s00134-020-06313-x\n10.1136/bmjgh-2021-005427\n10.1016/j.clinimag.2020.04.001\n10.1007/s10140-020-01808-y\n10.1148/radiol.2020203173\n10.1007/s00330-020-07270-1\n10.1007/s00330-020-07504-2\n10.1186/s41747-020-00195-w\n10.1186/s12938-020-00831-x\n10.3390/jcm10071351\n10.1093/cid/ciaa560\n10.1542/peds.142.1MA2.105\n10.1007/s10489-020-01829-7\n10.1038/s41467-021-21553-1\n10.1214/08-AOAS169\n10.1093/biostatistics/kxj011\n10.1001/jama.1982.03320430047030\n10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4\n10.1016/j.tranon.2017.10.010\n10.1007/s12559-023-10118-7\n10.1038/s41598-020-78888-w\n10.1016/j.biopha.2020.111173\n10.1148/radiol.2021202553\n10.1016/j.ijid.2020.10.067\n10.7554/eLife.70640\n10.1148/radiol.2020202723\n10.1016/j.media.2021.102216\n10.1016/j.crad.2021.02.005\n10.18632/oncotarget.15399\n10.1002/ima.22613\n10.1158/0008-5472.CAN-17-0339\n10.1259/bjr.20210221\n10.3390/app12083903\n10.5152/dir.2020.20205\n10.1038/s41598-020-66895-w\n10.1186/s43055-020-00296-x\n10.1371/journal.pone.0264172\n10.1097/01.mlr.0000182534.19832.83\n10.1016/j.ejrad.2022.110164\n10.1038/s41591-021-01292-y\n10.1016/S2589-7500(21)00039-X\n10.1371/journal.pone.0233147\n10.1371/journal.pone.0236240\n10.1136/bmj.m1198\n10.7189/jogh.10.020503\n10.1093/cid/ciaa1012\n10.1007/s11606-020-05983-z\n10.11606/s1518-8787.2020054002481\n10.1371/journal.pone.0258278\n10.1371/journal.pone.0238215\n10.18632/aging.103000\n10.1038/s41586-020-2521-4\n10.1001/jama.2020.2648\n10.1016/j.ijid.2020.03.017\n10.1001/jamanetworkopen.2020.25197\n10.1111/joim.13213\n10.1001/jamanetworkopen.2020.17703\n10.1097/00005650-199801000-00004\n10.1097/MLR.0b013e31819432e5\n10.1088/0031-9155/61/13/R150\n10.1016/S0146-664X(75)80008-6\n10.1016/0167-8655(90)90112-F\n10.1109/TSMC.1973.4309314\n10.1142/S0218001413570024\n10.1002/sam.10103\n10.1198/016214507000000149\n10.1002/sim.4154\n10.1023/A:1010933404324"}
{"title": "The Impact of Artificial Intelligence on Radiology Education in the Wake of Coronavirus Disease 2019.", "abstract": null, "journal": "Korean journal of radiology", "date": "2023-05-03", "authors": ["IsmailMese"], "doi": "10.3348/kjr.2023.0278"}
{"title": "A deep learning-based application for COVID-19 diagnosis on CT: The Imaging COVID-19 AI initiative.", "abstract": "Recently, artificial intelligence (AI)-based applications for chest imaging have emerged as potential tools to assist clinicians in the diagnosis and management of patients with coronavirus disease 2019 (COVID-19).\nTo develop a deep learning-based clinical decision support system for automatic diagnosis of COVID-19 on chest CT scans. Secondarily, to develop a complementary segmentation tool to assess the extent of lung involvement and measure disease severity.\nThe Imaging COVID-19 AI initiative was formed to conduct a retrospective multicentre cohort study including 20 institutions from seven different European countries. Patients with suspected or known COVID-19 who underwent a chest CT were included. The dataset was split on the institution-level to allow external evaluation. Data annotation was performed by 34 radiologists/radiology residents and included quality control measures. A multi-class classification model was created using a custom 3D convolutional neural network. For the segmentation task, a UNET-like architecture with a backbone Residual Network (ResNet-34) was selected.\nA total of 2,802 CT scans were included (2,667 unique patients, mean [standard deviation] age = 64.6 [16.2] years, male/female ratio 1.3:1). The distribution of classes (COVID-19/Other type of pulmonary infection/No imaging signs of infection) was 1,490 (53.2%), 402 (14.3%), and 910 (32.5%), respectively. On the external test dataset, the diagnostic multiclassification model yielded high micro-average and macro-average AUC values (0.93 and 0.91, respectively). The model provided the likelihood of COVID-19 vs other cases with a sensitivity of 87% and a specificity of 94%. The segmentation performance was moderate with Dice similarity coefficient (DSC) of 0.59. An imaging analysis pipeline was developed that returned a quantitative report to the user.\nWe developed a deep learning-based clinical decision support system that could become an efficient concurrent reading tool to assist clinicians, utilising a newly created European dataset including more than 2,800 CT scans.", "journal": "PloS one", "date": "2023-05-02", "authors": ["LaurensTopff", "Jos\u00e9S\u00e1nchez-Garc\u00eda", "RafaelL\u00f3pez-Gonz\u00e1lez", "Ana Jim\u00e9nezPastor", "Jacob JVisser", "MerelHuisman", "JulienGuiot", "Regina G HBeets-Tan", "AngelAlberich-Bayarri", "AlmudenaFuster-Matanzo", "Erik RRanschaert", "NoneNone"], "doi": "10.1371/journal.pone.0285121\n10.1186/s12941-021-00438-7\n10.1038/s41576-021-00360-w\n10.1016/j.talanta.2022.123409\n10.1007/s15010-022-01819-6\n10.1148/radiol.2020201365\n10.1186/s13244-021-01096-1\n10.1016/j.diii.2020.11.008\n10.1148/radiol.2020200642\n10.1148/radiol.2020200343\n10.1259/bjr.20201039\n10.1183/13993003.00398-2020\n10.1183/13993003.00334-2020\n10.1016/S1473-3099(20)30134-1\n10.1016/S1473-3099(20)30086-4\n10.1016/j.radi.2020.09.010\n10.1016/j.ejrad.2020.108961\n10.1016/j.ejrad.2019.108774\n10.1148/radiol.2021203957\n10.1016/j.ejmp.2021.06.001\n10.1148/radiol.2020200370\n10.3389/fmed.2021.704256\n10.1148/radiol.2020201491\n10.1016/j.cell.2020.04.045\n10.1038/s41598-020-76282-0\n10.1007/s00330-021-07715-1\n10.1183/13993003.00775-2020\n10.1007/s00330-020-07033-y\n10.1148/ryct.2020200389\n10.1007/s00330-020-07013-2\n10.1148/ryct.2020200047\n10.3389/fmed.2022.930055\n10.1007/s11042-021-11153-y\n10.1038/s41598-022-06854-9\n10.1016/j.ejro.2020.100272"}
{"title": "Tools and factors predictive of the severity of COVID-19.", "abstract": "The outbreak of the novel coronavirus infection caused worldwide confusion. The problem with this infection is that it causes severe illness in some patients, resulting in a high rate of death if appropriate treatment is not given. If patients with severe illness that requires treatment are appropriately identified, treatment can be focused on these patients. However, in the early days of the COVID-19 outbreak, the inability to predict and diagnose the disease led to hospitals being overwhelmed. Therefore, various methods for the diagnosis of severe disease were developed early on, and various methods are still being investigated to predict high-risk patients. The currently available prediction methods are divided into those that predict the onset of severe disease and those used to determine the severity of the disease. Specifically, the main methods include genetic factors, serum humoral factors, laboratory tests, and diagnostic imaging. Since each of these factors has different features, using them in combination is likely to be advantageous.", "journal": "Global health & medicine", "date": "2023-05-02", "authors": ["MasayaSugiyama"], "doi": "10.35772/ghm.2022.01046"}
{"title": "Contemporary Concise Review 2022: Interstitial lung disease.", "abstract": "Novel genetic associations for idiopathic pulmonary fibrosis (IPF) risk have been identified. Common genetic variants associated with IPF are also associated with chronic hypersensitivity pneumonitis. The characterization of underlying mechanisms, such as pathways involved in myofibroblast differentiation, may reveal targets for future treatments. Newly identified circulating biomarkers are associated with disease progression and mortality. Deep learning and machine learning may increase accuracy in the interpretation of CT scans. Novel treatments have shown benefit in phase 2 clinical trials. Hospitalization with COVID-19 is associated with residual lung abnormalities in a substantial number of patients. Inequalities exist in delivering and accessing interstitial lung disease specialist care.", "journal": "Respirology (Carlton, Vic.)", "date": "2023-05-01", "authors": ["David J FSmith", "R GisliJenkins"], "doi": "10.1111/resp.14511"}
{"title": "Interpretable clinical phenotypes among patients hospitalized with COVID-19 using cluster analysis.", "abstract": "Multiple clinical phenotypes have been proposed for coronavirus disease (COVID-19), but few have used multimodal data. Using clinical and imaging data, we aimed to identify distinct clinical phenotypes in patients admitted with COVID-19 and to assess their clinical outcomes. Our secondary objective was to demonstrate the clinical applicability of this method by developing an interpretable model for phenotype assignment.\nWe analyzed data from 547 patients hospitalized with COVID-19 at a Canadian academic hospital. We processed the data by applying a factor analysis of mixed data (FAMD) and compared four clustering algorithms: k-means, partitioning around medoids (PAM), and divisive and agglomerative hierarchical clustering. We used imaging data and 34 clinical variables collected within the first 24\u2005h of admission to train our algorithm. We conducted a survival analysis to compare the clinical outcomes across phenotypes. With the data split into training and validation sets (75/25 ratio), we developed a decision-tree-based model to facilitate the interpretation and assignment of the observed phenotypes.\nAgglomerative hierarchical clustering was the most robust algorithm. We identified three clinical phenotypes: 79 patients (14%) in Cluster 1, 275 patients (50%) in Cluster 2, and 203 (37%) in Cluster 3. Cluster 2 and Cluster 3 were both characterized by a low-risk respiratory and inflammatory profile but differed in terms of demographics. Compared with Cluster 3, Cluster 2 comprised older patients with more comorbidities. Cluster 1 represented the group with the most severe clinical presentation, as inferred by the highest rate of hypoxemia and the highest radiological burden. Intensive care unit (ICU) admission and mechanical ventilation risks were the highest in Cluster 1. Using only two to four decision rules, the classification and regression tree (CART) phenotype assignment model achieved an AUC of 84% (81.5-86.5%, 95 CI) on the validation set.\nWe conducted a multidimensional phenotypic analysis of adult inpatients with COVID-19 and identified three distinct phenotypes associated with different clinical outcomes. We also demonstrated the clinical usability of this approach, as phenotypes can be accurately assigned using a simple decision tree. Further research is still needed to properly incorporate these phenotypes in the management of patients with COVID-19.", "journal": "Frontiers in digital health", "date": "2023-04-28", "authors": ["EricYamga", "LouisMullie", "MadeleineDurand", "AlexandreCadrin-Chenevert", "AnTang", "EmmanuelMontagnon", "CarlChartrand-Lefebvre", "Micha\u00eblChass\u00e9"], "doi": "10.3389/fdgth.2023.1142822\n10.1016/S2665-9913(20)30310-6\n10.1164/rccm.202003-0817LE\n10.1007/s00134-020-06033-2\n10.1055/s-0040-1710018\n10.1001/jamainternmed.2021.6203\n10.1111/jth.14768\n10.1183/13993003.02195-2020\n10.1007/s00134-020-06083-6\n10.1038/nmeth.4642\n10.1136/thoraxjnl-2016-209846\n10.1001/jama.2019.5791\n10.1016/S1473-3099(21)00019-0\n10.3389/fmed.2020.570614\n10.3389/fmed.2021.632933\n10.3390/jcm9113488\n10.1007/s00134-020-06120-4\n10.1371/journal.pone.0248956\n10.1186/s13054-021-03487-8\n10.1002/ctm2.648\n10.1038/s41467-022-33801-z\n10.1007/s00330-020-07270-1\n10.1007/s00521-019-04051-w\n10.2214/AJR.19.21512\n10.1148/ryai.2019180041\n10.1002/sam.11348\n10.1007/s00228-017-2333-0\n10.6026/97320630013101\n10.1016/j.ress.2015.05.018\n10.1016/j.intimp.2020.106504\n10.1016/j.ajem.2021.01.092\n10.1016/j.ajem.2020.12.053\n10.1001/jamainternmed.2020.8193\n10.21203/rs.3.rs-144684/v1\n10.1016/j.mayocp.2020.04.006\n10.1186/s40560-020-00453-4\n10.1055/s-0041-1727283\n10.1016/j.resp.2020.103455\n10.1002/cpt.2504\n10.1038/s41746-020-0254-2\n10.1146/annurev-biodatasci-092820-033938\n10.1001/jama.2021.5248\n10.1016/S2213-2600(21)00218-6\n10.1038/s41467-022-34638-2\n10.1136/bmj-2021-068197\n10.1038/s41746-020-00308-0\n10.1038/s41467-022-34638-2\n10.1055/s-0041-1735184\n10.3389/fpubh.2020.00418"}
{"title": "Progressive attention integration-based multi-scale efficient network for medical imaging analysis with application to COVID-19 diagnosis.", "abstract": "In this paper, a novel deep learning-based medical imaging analysis framework is developed, which aims to deal with the insufficient feature learning caused by the imperfect property of imaging data. Named as multi-scale efficient network (MEN), the proposed method integrates different attention mechanisms to realize sufficient extraction of both detailed features and semantic information in a progressive learning manner. In particular, a fused-attention block is designed to extract fine-grained details from the input, where the squeeze-excitation (SE) attention mechanism is applied to make the model focus on potential lesion areas. A multi-scale low information loss (MSLIL)-attention block is proposed to compensate for potential global information loss and enhance the semantic correlations among features, where the efficient channel attention (ECA) mechanism is adopted. The proposed MEN is comprehensively evaluated on two COVID-19 diagnostic tasks, and the results show that as compared with some other advanced deep learning models, the proposed method is competitive in accurate COVID-19 recognition, which yields the best accuracy of 98.68% and 98.85%, respectively, and exhibits satisfactory generalization ability as well.", "journal": "Computers in biology and medicine", "date": "2023-04-27", "authors": ["TingyiXie", "ZidongWang", "HanLi", "PeishuWu", "HuixiangHuang", "HongyiZhang", "Fuad EAlsaadi", "NianyinZeng"], "doi": "10.1016/j.compbiomed.2023.106947\n10.1080/00207721.2022.2083262"}
{"title": "Dual attention fusion UNet for COVID-19 lesion segmentation from CT images.", "abstract": "Chest CT scan is an effective way to detect and diagnose COVID-19 infection. However, features of COVID-19 infection in chest CT images are very complex and heterogeneous, which make segmentation of COVID-19 lesions from CT images quite challenging.\nTo overcome this challenge, this study proposes and tests an end-to-end deep learning method called dual attention fusion UNet (DAF-UNet).\nThe proposed DAF-UNet improves the typical UNet into an advanced architecture. The dense-connected convolution is adopted to replace the convolution operation. The mixture of average-pooling and max-pooling acts as the down-sampling in the encoder. Bridge-connected layers, including convolution, batch normalization, and leaky rectified linear unit (leaky ReLU) activation, serve as the skip connections between the encoder and decoder to bridge the semantic gap differences. A multiscale pyramid pooling module acts as the bottleneck to fit the features of COVID-19 lesion with complexity. Furthermore, dual attention feature (DAF) fusion containing channel and position attentions followed the improved UNet to learn the long-dependency contextual features of COVID-19 and further enhance the capacity of the proposed DAF-UNet. The proposed model is first pre-trained on the pseudo label dataset (generated by Inf-Net) containing many samples, then fine-tuned on the standard annotation dataset (provided by the Italian Society of Medical and Interventional Radiology) with high-quality but limited samples to improve performance of COVID-19 lesion segmentation on chest CT images.\nThe Dice coefficient and Sensitivity are 0.778 and 0.798 respectively. The proposed DAF-UNet has higher scores than the popular models (Att-UNet, Dense-UNet, Inf-Net, COPLE-Net) tested using the same dataset as our model.\nThe study demonstrates that the proposed DAF-UNet achieves superior performance for precisely segmenting COVID-19 lesions from chest CT scans compared with the state-of-the-art approaches. Thus, the DAF-UNet has promising potential for assisting COVID-19 disease screening and detection.", "journal": "Journal of X-ray science and technology", "date": "2023-04-24", "authors": ["YinjinMa", "YajuanZhang", "LinChen", "QiangJiang", "BiaoWei"], "doi": "10.3233/XST-230001"}
{"title": "A novel CT image de-noising and fusion based deep learning network to screen for disease (COVID-19).", "abstract": "A COVID-19, caused by SARS-CoV-2, has been declared a global pandemic by WHO. It first appeared in China at the end of 2019 and quickly spread throughout the world. During the third layer, it became more critical. COVID-19 spread is extremely difficult to control, and a huge number of suspected cases must be screened for a cure as soon as possible. COVID-19 laboratory testing takes time and can result in significant false negatives. To combat COVID-19, reliable, accurate and fast methods are urgently needed. The commonly used Reverse Transcription Polymerase Chain Reaction has a low sensitivity of approximately 60% to 70%, and sometimes even produces negative results. Computer Tomography (CT) has been observed to be a subtle approach to detecting COVID-19, and it may be the best screening method. The scanned image's quality, which is impacted by motion-induced Poisson or Impulse noise, is vital. In order to improve the quality of the acquired image for post segmentation, a novel Impulse and Poisson noise reduction method employing boundary division max/min intensities elimination along with an adaptive window size mechanism is proposed. In the second phase, a number of CNN techniques are explored for detecting COVID-19 from CT images and an Assessment Fusion Based model is proposed to predict the result. The AFM combines the results for cutting-edge CNN architectures and generates a final prediction based on choices. The empirical results demonstrate that our proposed method performs extensively and is extremely useful in actual diagnostic situations.", "journal": "Scientific reports", "date": "2023-04-24", "authors": ["Sajid UllahKhan", "ImdadUllah", "NajeebUllah", "SajidShah", "Mohammed ElAffendi", "BumshikLee"], "doi": "10.1038/s41598-023-33614-0\n10.1038/s41586-020-2008-3\n10.1016/S0140-6736(20)30183-5\n10.22207/JPAM.14.SPL1.40\n10.1007/s12098-020-03263-6\n10.1148/radiol.2020200490\n10.1148/radiol.2020200527\n10.1148/radiol.2020200343\n10.1016/S1473-3099(20)30134-1\n10.1016/j.media.2017.07.005\n10.1109/ACCESS.2017.2788044\n10.1146/annurev-bioeng-071516-044442\n10.1038/s41591-018-0268-3\n10.1016/j.compbiomed.2017.08.022\n10.1109/TIP.2005.871129\n10.1109/5.192071\n10.1145/358198.358222\n10.1109/31.83870\n10.1109/83.902289\n10.3390/app12147092\n10.32604/cmc.2022.029134\n10.1038/s41598-022-25539-x\n10.1038/s41598-021-99015-3\n10.3390/ijerph19042013"}
{"title": "Added value of chest CT in a machine learning-based prediction model to rule out COVID-19 before inpatient admission: A retrospective university network study.", "abstract": "During the coronavirus disease 2019 (COVID-19) pandemic, hospitals still face the challenge of timely identification of infected individuals before inpatient admission. An artificial intelligence approach based on an established clinical network may improve prospective pandemic preparedness.\nSupervised machine learning was used to construct diagnostic models to predict COVID-19. A pooled database was retrospectively generated from 4437 participant data that were collected between January 2017 and October 2020 at 12 German centers that belong to the radiological cooperative network of the COVID-19 (RACOON) consortium. A total of 692 (15.6\u00a0%) participants were COVID-19 positive according to the reference of the reverse transcription-polymerase chain reaction test. The diagnostic models included chest CT features (model R), clinical examination and laboratory test features (model CL), or all three feature categories (model RCL). Performance outcomes included accuracy, sensitivity, specificity, negative and positive predictive value, and area under the receiver operating curve (AUC).\nPerformance of predictive models improved significantly by adding chest CT features to clinical evaluation and laboratory test features. Without (model CL) and with inclusion of chest CT (model RCL), sensitivity was 0.82 and 0.89 (p\u00a0<\u00a00.0001), specificity was 0.84 and 0.89 (p\u00a0<\u00a00.0001), negative predictive value was 0.96 and 0.97 (p\u00a0<\u00a00.0001), AUC was 0.92 and 0.95 (p\u00a0<\u00a00.0001), and proportion of false negative classifications was 2.6\u00a0% and 1.7\u00a0% (p\u00a0<\u00a00.0001), respectively.\nAddition of chest CT features to machine learning-based predictive models improves the effectiveness in ruling out COVID-19 before inpatient admission to regular wards.", "journal": "European journal of radiology", "date": "2023-04-24", "authors": ["MartinKr\u00e4mer", "MajaIngwersen", "UlfTeichgr\u00e4ber", "FelixG\u00fcttler", "NoneNone"], "doi": "10.1016/j.ejrad.2023.110827"}
{"title": "Quo vadis Radiomics? Bibliometric analysis of 10-year Radiomics journey.", "abstract": "Radiomics is the high-throughput extraction of mineable and-possibly-reproducible quantitative imaging features from medical imaging. The aim of this work is to perform an unbiased bibliometric analysis on Radiomics 10\u00a0years after the first work became available, to highlight its status, pitfalls, and growing interest.\nScopus database was used to investigate all the available English manuscripts about Radiomics. R Bibliometrix package was used for data analysis: a cumulative analysis of document categories, authors affiliations, country scientific collaborations, institution collaboration networks, keyword analysis, comprehensive of co-occurrence network, thematic map analysis, and 2021 sub-analysis of trend topics was performed.\nA total of 5623 articles and 16,833 authors from 908 different sources have been identified. The first available document was published in March 2012, while the most recent included was released on the 31st of December 2021. China and USA were the most productive countries. Co-occurrence network analysis identified five words clusters based on top 50 authors' keywords: Radiomics, computed tomography, radiogenomics, deep learning, tomography. Trend topics analysis for 2021 showed an increased interest in artificial intelligence (n\u2009=\u2009286), nomogram (n\u2009=\u2009166), hepatocellular carcinoma (n\u2009=\u2009125), COVID-19 (n\u2009=\u200963), and X-ray computed (n\u2009=\u200960).\nOur work demonstrates the importance of bibliometrics in aggregating information that otherwise would not be available in a granular analysis, detecting unknown patterns in Radiomics publications, while highlighting potential developments to ensure knowledge dissemination in the field and its future real-life applications in the clinical practice.\nThis work aims to shed light on the state of the art in radiomics, which offers numerous tangible and intangible benefits, and to encourage its integration in the contemporary clinical practice for more precise imaging analysis.\n\u2022 ML-based bibliometric analysis is fundamental to detect unknown pattern of data in Radiomics publications. \u2022 A raising interest in the field, the most relevant collaborations, keywords co-occurrence network, and trending topics have been investigated. \u2022 Some pitfalls still exist, including the scarce standardization and the relative lack of homogeneity across studies.", "journal": "European radiology", "date": "2023-04-19", "authors": ["StefaniaVolpe", "FedericoMastroleo", "MarcoKrengli", "Barbara AlicjaJereczek-Fossa"], "doi": "10.1007/s00330-023-09645-6\n10.1016/j.ejca.2011.11.036\n10.1038/s41598-021-01470-5\n10.1007/s00330-021-08009-2\n10.1038/s42003-021-02894-5\n10.1080/0284186X.2021.1983207\n10.3390/diagnostics12040794\n10.3389/fonc.2018.00131\n10.3389/fonc.2018.00294\n10.1038/nrclinonc.2017.141\n10.1016/j.jbusres.2021.04.070\n10.1016/j.joi.2017.08.007\n10.3390/su14063643\n10.1038/s41598-020-69250-1\n10.1148/radiol.2015151169\n10.1038/ncomms5006\n10.1148/radiol.2020191145\n10.1002/asi.20317"}
{"title": "Analysis of Covid-19 CT chest image classification using Dl4jMlp classifier and Multilayer Perceptron in WEKA Environment.", "abstract": "In recent years, various deep learning algorithms have exhibited remarkable performance in various data-rich applications, like health care, medical imaging, as well as in computer vision. Covid-19, which is a rapidly spreading virus, has affected people of all ages both socially and economically. Early detection of this virus is therefore important in order to prevent its further spread.\nCovid-19 crisis has also galvanized researchers to adopt various machine learning as well as deep learning techniques in order to combat the pandemic. Lung images can be used in the diagnosis of Covid-19.\nIn this paper, we have analysed the Covid-19 chest CT image classification efficiency using multilayer perceptron with different imaging filters, like edge histogram filter, colour histogram equalization filter, color-layout filter, and Garbo filter in the WEKA environment.\nThe performance of CT image classification has also been compared comprehensively with the deep learning classifier Dl4jMlp. It was observed that the multilayer perceptron with edge histogram filter outperformed other classifiers compared in this paper with 89.6% of correctly classified instances.", "journal": "Current medical imaging", "date": "2023-04-19", "authors": ["NoneSreejith S", "NoneJ Ajayan", "NoneN V Uma Reddy", "BabuDevasenapati S", "ShashankRebelli"], "doi": "10.2174/1573405620666230417090246"}
{"title": "CCS-GAN: COVID-19 CT Scan Generation and Classification with Very Few Positive Training Images.", "abstract": "We present a novel algorithm that is able to generate deep synthetic COVID-19 pneumonia CT scan slices using a very small sample of positive training images in tandem with a larger number of normal images. This generative algorithm produces images of sufficient accuracy to enable a DNN classifier to achieve high classification accuracy using as few as 10 positive training slices (from 10 positive cases), which to the best of our knowledge is one order of magnitude fewer than the next closest published work at the time of writing. Deep learning with extremely small positive training volumes is a very difficult problem and has been an important topic during the COVID-19 pandemic, because for quite some time it was difficult to obtain large volumes of COVID-19-positive images for training. Algorithms that can learn to screen for diseases using few examples are an important area of research. Furthermore, algorithms to produce deep synthetic images with smaller data volumes have the added benefit of reducing the barriers of data sharing between healthcare institutions. We present the cycle-consistent segmentation-generative adversarial network (CCS-GAN). CCS-GAN combines style transfer with pulmonary segmentation and relevant transfer learning from negative images in order to create a larger volume of synthetic positive images for the purposes of improving diagnostic classification performance. The performance of a VGG-19 classifier plus CCS-GAN was trained using a small sample of positive image slices ranging from at most 50 down to as few as 10 COVID-19-positive CT scan images. CCS-GAN achieves high accuracy with few positive images and thereby greatly reduces the barrier of acquiring large training volumes in order to train a diagnostic classifier for COVID-19.", "journal": "Journal of digital imaging", "date": "2023-04-18", "authors": ["SumeetMenon", "JayalakshmiMangalagiri", "JoshGalita", "MichaelMorris", "BabakSaboury", "YaacovYesha", "YelenaYesha", "PhuongNguyen", "AryyaGangopadhyay", "DavidChapman"], "doi": "10.1007/s10278-023-00811-2\n10.1007/s10489-020-01862-6\n10.1109/CSCI51800.2020.00160\n10.1016/j.eswa.2021.114848\n10.1016/j.neucom.2018.09.013\n10.1016/S0031-3203(02)00060-2"}
{"title": "CTMLP: Can MLPs replace CNNs or transformers for COVID-19 diagnosis?", "abstract": "Convolutional Neural Networks (CNNs) and the hybrid models of CNNs and Vision Transformers (VITs) are the recent mainstream methods for COVID-19 medical image diagnosis. However, pure CNNs lack global modeling ability, and the hybrid models of CNNs and VITs have problems such as large parameters and computational complexity. These models are difficult to be used effectively for medical diagnosis in just-in-time applications.\nTherefore, a lightweight medical diagnosis network CTMLP based on convolutions and multi-layer perceptrons (MLPs) is proposed for the diagnosis of COVID-19. The previous self-supervised algorithms are based on CNNs and VITs, and the effectiveness of such algorithms for MLPs is not yet known. At the same time, due to the lack of ImageNet-scale datasets in the medical image domain for model pre-training. So, a pre-training scheme TL-DeCo based on transfer learning and self-supervised learning was constructed. In addition, TL-DeCo is too tedious and resource-consuming to build a new model each time. Therefore, a guided self-supervised pre-training scheme was constructed for the new lightweight model pre-training.\nThe proposed CTMLP achieves an accuracy of 97.51%, an f1-score of 97.43%, and a recall of 98.91% without pre-training, even with only 48% of the number of ResNet50 parameters. Furthermore, the proposed guided self-supervised learning scheme can improve the baseline of simple self-supervised learning by 1%-1.27%.\nThe final results show that the proposed CTMLP can replace CNNs or Transformers for a more efficient diagnosis of COVID-19. In addition, the additional pre-training framework was developed to make it more promising in clinical practice.", "journal": "Computers in biology and medicine", "date": "2023-04-18", "authors": ["JundingSun", "PengpengPi", "ChaoshengTang", "Shui-HuaWang", "Yu-DongZhang"], "doi": "10.1016/j.compbiomed.2023.106847"}
{"title": "A lightweight CORONA-NET for COVID-19 detection in X-ray images.", "abstract": "Since December 2019, COVID-19 has posed the most serious threat to living beings. With the advancement of vaccination programs around the globe, the need to quickly diagnose COVID-19 in general with little logistics is fore important. As a consequence, the fastest diagnostic option to stop COVID-19 from spreading, especially among senior patients, should be the development of an automated detection system. This study aims to provide a lightweight deep learning method that incorporates a convolutional neural network (CNN), discrete wavelet transform (DWT), and a long short-term memory (LSTM), called CORONA-NET for diagnosing COVID-19 from chest X-ray images. In this system, deep feature extraction is performed by CNN, the feature vector is reduced yet strengthened by DWT, and the extracted feature is detected by LSTM for prediction. The dataset included 3000 X-rays, 1000 of which were COVID-19 obtained locally. Within minutes of the test, the proposed test platform's prototype can accurately detect COVID-19 patients. The proposed method achieves state-of-the-art performance in comparison with the existing deep learning methods. We hope that the suggested method will hasten clinical diagnosis and may be used for patients in remote areas where clinical labs are not easily accessible due to a lack of resources, location, or other factors.", "journal": "Expert systems with applications", "date": "2023-04-18", "authors": ["Muhammad UsmanHadi", "RizwanQureshi", "AyeshaAhmed", "NadeemIftikhar"], "doi": "10.1016/j.eswa.2023.120023"}
{"title": "Abdominal imaging associates body composition with COVID-19 severity.", "abstract": "The main drivers of COVID-19 disease severity and the impact of COVID-19 on long-term health after recovery are yet to be fully understood. Medical imaging studies investigating COVID-19 to date have mostly been limited to small datasets and post-hoc analyses of severe cases. The UK Biobank recruited recovered SARS-CoV-2 positive individuals (n = 967) and matched controls (n = 913) who were extensively imaged prior to the pandemic and underwent follow-up scanning. In this study, we investigated longitudinal changes in body composition, as well as the associations of pre-pandemic image-derived phenotypes with COVID-19 severity. Our longitudinal analysis, in a population of mostly mild cases, associated a decrease in lung volume with SARS-CoV-2 positivity. We also observed that increased visceral adipose tissue and liver fat, and reduced muscle volume, prior to COVID-19, were associated with COVID-19 disease severity. Finally, we trained a machine classifier with demographic, anthropometric and imaging traits, and showed that visceral fat, liver fat and muscle volume have prognostic value for COVID-19 disease severity beyond the standard demographic and anthropometric measurements. This combination of image-derived phenotypes from abdominal MRI scans and ensemble learning to predict risk may have future clinical utility in identifying populations at-risk for a severe COVID-19 outcome.", "journal": "PloS one", "date": "2023-04-14", "authors": ["NicolasBasty", "Elena PSorokin", "MarjolaThanaj", "RamprakashSrinivasan", "BrandonWhitcher", "Jimmy DBell", "MadeleineCule", "E LouiseThomas"], "doi": "10.1371/journal.pone.0283506\n10.1038/s41598-021-95565-8\n10.1038/s41591-021-01283-z\n10.1038/s41586-021-03553-9\n10.1007/s10735-020-09915-3\n10.1002/oby.22818\n10.1016/s2213-8587(21)00089-9\n10.1038/s41574-020-00462-1\n10.1038/s41574-021-00608-9\n10.1016/j.jcmg.2020.05.004\n10.4329/wjr.v13.i5.122\n10.1016/j.eclinm.2020.100683\n10.1136/bmjopen-2020-048391\n10.5603/CJ.a2020.0101\n10.1016/j.jacc.2020.03.031\n10.1038/s41591-022-01689-3\n10.1016/j.acra.2020.08.009\n10.3389/fmed.2020.00423\n10.1016/j.kint.2020.03.005\n10.1136/bmj.m1963\n10.1681/ASN.2020040419\n10.1007/s00261-020-02669-2\n10.1002/jum.15778\n10.14309/ajg.0000000000001264\n10.1093/cid/ciaa460\n10.1101/2020.06.04.20122457v1\n10.1016/S2213-8587(22)00044-4\n10.1067/j.cpradiol.2021.06.015\n10.1007/s00256-021-03734-7\n10.3748/wjg.v27.i26.4143\n10.1101/2020.07.25.20161091\n10.1007/s40520-020-01653-6\n10.1101/2020.07.10.20147777\n10.1101/2020.06.09.20127092v2\n10.1016/j.dsx.2020.06.060\n10.1038/s41569-020-0413-9\n10.1101/2021.11.04.21265918\n10.1101/2021.06.11.21258690\n10.1371/journal.pmed.1001779\n10.7554/eLife.65554\n10.1038/s41598-020-77351-0\n10.1109/isbi45749.2020.9098650\n10.1111/j.2517-6161.1995.tb02031.x\n10.1016/j.mri.2012.05.001\n10.1159/000444418\n10.1152/japplphysiol.00217.2011\n10.3390/jcm10051021\n10.1007/s15010-020-01474-9\n10.1016/j.ijid.2020.10.095\n10.1016/j.accpm.2020.10.014\n10.1186/s12890-021-01594-4\n10.3389/fncel.2015.00028\n10.1016/j.metabol.2020.154319\n10.1186/s12879-021-06958-z\n10.1016/j.metabol.2020.154440\n10.1038/s41366-021-00907-1\n10.1111/liv.14575\n10.1016/j.dld.2020.09.007\n10.1186/s40001-021-00590-y\n10.3389/fmed.2021.626425\n10.3390/nu12113361\n10.1038/s41467-020-19741-6\n10.1038/oby.2011.142\n10.1101/2021.10.24.465626\n10.1016/j.cmet.2021.10.014\n10.1111/codi.12805\n10.1002/jcsm.12862\n10.1371/journal.pone.0253433\n10.1016/j.clnu.2021.08.004\n10.1148/radiol.2021204141\n10.1038/s41598-022-07556-y"}
{"title": "Effects of post-acute COVID-19 syndrome on the functional brain networks of non-hospitalized individuals.", "abstract": "The long-term impact of COVID-19 on brain function remains poorly understood, despite growing concern surrounding post-acute COVID-19 syndrome (PACS). The goal of this cross-sectional, observational study was to determine whether there are significant alterations in resting brain function among non-hospitalized individuals with PACS, compared to symptomatic individuals with non-COVID infection.\nData were collected for 51 individuals who tested positive for COVID-19 (mean age 41\u00b112 yrs., 34 female) and 15 controls who had cold and flu-like symptoms but tested negative for COVID-19 (mean age 41\u00b114 yrs., 9 female), with both groups assessed an average of 4-5 months after COVID testing. None of the participants had prior neurologic, psychiatric, or cardiovascular illness. Resting brain function was assessed \nIndividuals with COVID-19 had lower temporal and subcortical functional connectivity relative to controls. A greater number of ongoing post-COVID symptoms was also associated with altered functional connectivity between temporal, parietal, occipital and subcortical regions.\nThese results provide preliminary evidence that patterns of functional connectivity distinguish PACS from non-COVID infection and correlate with the severity of clinical outcome, providing novel insights into this highly prevalent disorder.", "journal": "Frontiers in neurology", "date": "2023-04-14", "authors": ["Nathan WChurchill", "EugenieRoudaia", "J JeanChen", "AsafGilboa", "AllisonSekuler", "XiangJi", "FuqiangGao", "ZhongminLin", "AravinthanJegatheesan", "MarioMasellis", "MagedGoubran", "Jennifer SRabin", "BenjaminLam", "IvyCheng", "RobertFowler", "ChrisHeyn", "Sandra EBlack", "Bradley JMacIntosh", "Simon JGraham", "Tom ASchweizer"], "doi": "10.3389/fneur.2023.1136408\n10.1056/NEJMoa2001017\n10.1016/j.ejim.2021.06.009\n10.3174/ajnr.A6805\n10.3389/fnagi.2021.646908\n10.1001/jamaneurol.2022.0154\n10.1007/s00401-020-02190-2\n10.1016/S1474-4422(20)30308-2\n10.5114/fn.2021.108829\n10.1016/j.celrep.2022.111573\n10.3390/v14040776\n10.1038/s41467-022-29440-z\n10.1016/S0140-6736(21)00847-3\n10.1038/s41591-021-01283-z\n10.9778/cmajo.20210023\n10.1002/jmri.28555\n10.1038/s41586-022-04569-5\n10.1016/j.eclinm.2020.100484\n10.3390/biomedicines9030287\n10.1093/brain/awab009\n10.1162/NECO_a_00877\n10.1093/arclin/acz007\n10.3233/JAD-160979\n10.3233/JAD-180131\n10.1080/03610926.2011.625486\n10.1002/hbm.25741\n10.3390/brainsci12040511\n10.1016/j.jad.2022.06.061\n10.1172/JCI147329\n10.1038/s41593-020-00758-5\n10.1007/s00259-021-05215-4\n10.1007/s00259-021-05294-3\n10.1073/pnas.2200960119\n10.1515/revneuro-2012-0067\n10.1146/annurev-neuro-080317-062144\n10.1016/j.yfrne.2012.09.003\n10.1007/s12264-017-0150-1\n10.1007/s42399-021-00964-7\n10.1016/j.jagp.2020.07.003\n10.1001/jamapsychiatry.2021.0109\n10.1038/nrn2651\n10.1002/hbm.22234\n10.1080/23279095.2022.2123739\n10.1080/87565640701190841\n10.3389/fdgth.2021.564906\n10.1002/hbm.21151\n10.1148/radiol.12120748\n10.1371/journal.pone.0065470\n10.1037/neu0000110\n10.1111/eci.13537"}
{"title": "COVID-19 Prediction Using Black-Box Based Pearson Correlation Approach.", "abstract": "The novel coronavirus (COVID-19), also known as SARS-CoV-2, is a highly contagious respiratory disease that first emerged in Wuhan, China in 2019 and has since become a global pandemic. The virus is spread through respiratory droplets produced when an infected person coughs or sneezes, and it can lead to a range of symptoms, from mild to severe. Some people may not have any symptoms at all and can still spread the virus to others. The best way to prevent the spread of COVID-19 is to practice good hygiene. It is also important to follow the guidelines set by local health authorities, such as physical distancing and quarantine measures. The World Health Organization (WHO), on the other hand, has classified this virus as a pandemic, and as a result, all nations are attempting to exert control and secure all public spaces. The current study aimed to (I) compare the weekly COVID-19 cases between Israel and Greece, (II) compare the monthly COVID-19 mortality cases between Israel and Greece, (III) evaluate and report the influence of the vaccination rate on COVID-19 mortality cases in Israel, and (IV) predict the number of COVID-19 cases in Israel. The advantage of completing these tasks is the minimization of the spread of the virus by deploying different mitigations. To attain our objective, a correlation analysis was carried out, and two distinct artificial intelligence (AI)-based models-specifically, an artificial neural network (ANN) and a classical multiple linear regression (MLR)-were developed for the prediction of COVID-19 cases in Greece and Israel by utilizing related variables as the input variables for the models. For the evaluation of the models, four evaluation metrics (determination coefficient (R2), mean square error (MSE), root mean square error (RMSE), and correlation coefficient (R)) were considered in order to determine the performance of the deployed models. From a variety of perspectives, the corresponding determination coefficient (R2) demonstrated the statistical advantages of MLR over the ANN model by following a linear pattern. The MLR predictive model was both efficient and accurate, with 98% accuracy, while ANN showed 94% accuracy in the effective prediction of COVID-19 cases.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-04-14", "authors": ["DilberUzun Ozsahin", "EfePrecious Onakpojeruo", "BasilBartholomew Duwa", "Abdullahi GarbaUsman", "SaniIsah Abba", "BernaUzun"], "doi": "10.3390/diagnostics13071264\n10.1155/2020/9756518\n10.2478/ebtj-2021-0017\n10.1002/jmv.27661\n10.1001/jama.2020.2648\n10.1007/s11356-021-13824-7\n10.1038/s41598-022-05052-x\n10.1038/s41598-022-20904-2\n10.1016/j.dsx.2020.07.045\n10.1016/j.chaos.2020.110050\n10.1016/j.scitotenv.2020.142810\n10.1016/j.chaos.2020.110086\n10.1016/j.chemolab.2022.104680\n10.1016/j.chaos.2020.109853\n10.1016/j.artmed.2022.102286\n10.1016/j.gr.2022.03.014\n10.1016/j.ebiom.2023.104482\n10.2196/18828\n10.3390/diagnostics12061326\n10.1109/aie57029.2022.00023\n10.3390/diagnostics13010081\n10.1007/s40808-022-01385-8\n10.1186/s12911-022-01861-2\n10.1016/j.sjbs.2021.09.055\n10.1016/j.procs.2017.11.212\n10.1080/02626667.2021.1937179\n10.1186/s42269-022-00756-6\n10.1016/j.scitotenv.2022.159697\n10.3390/diagnostics12112702\n10.1002/sscp.202200071\n10.1016/j.measurement.2021.110080\n10.1021/es070144e\n10.1016/j.scs.2022.103830\n10.1016/j.dajour.2021.100007\n10.1007/s11356-022-22373-6"}
{"title": "A multicenter evaluation of a deep learning software (LungQuant) for lung parenchyma characterization in COVID-19 pneumonia.", "abstract": "The role of computed tomography (CT) in the diagnosis and characterization of coronavirus disease 2019 (COVID-19) pneumonia has been widely recognized. We evaluated the performance of a software for quantitative analysis of chest CT, the LungQuant system, by comparing its results with independent visual evaluations by a group of 14 clinical experts. The aim of this work is to evaluate the ability of the automated tool to extract quantitative information from lung CT, relevant for the design of a diagnosis support model.\nLungQuant segments both the lungs and lesions associated with COVID-19 pneumonia (ground-glass opacities and consolidations) and computes derived quantities corresponding to qualitative characteristics used to clinically assess COVID-19 lesions. The comparison was carried out on 120 publicly available CT scans of patients affected by COVID-19 pneumonia. Scans were scored for four qualitative metrics: percentage of lung involvement, type of lesion, and two disease distribution scores. We evaluated the agreement between the LungQuant output and the visual assessments through receiver operating characteristics area under the curve (AUC) analysis and by fitting a nonlinear regression model.\nDespite the rather large heterogeneity in the qualitative labels assigned by the clinical experts for each metric, we found good agreement on the metrics compared to the LungQuant output. The AUC values obtained for the four qualitative metrics were 0.98, 0.85, 0.90, and 0.81.\nVisual clinical evaluation could be complemented and supported by computer-aided quantification, whose values match the average evaluation of several independent clinical experts.\nWe conducted a multicenter evaluation of the deep learning-based LungQuant automated software. We translated qualitative assessments into quantifiable metrics to characterize coronavirus disease 2019 (COVID-19) pneumonia lesions. Comparing the software output to the clinical evaluations, results were satisfactory despite heterogeneity of the clinical evaluations. An automatic quantification tool may contribute to improve the clinical workflow of COVID-19 pneumonia.", "journal": "European radiology experimental", "date": "2023-04-10", "authors": ["CamillaScapicchio", "AndreaChincarini", "ElenaBallante", "LucaBerta", "EleonoraBicci", "ChandraBortolotto", "FrancescaBrero", "Raffaella FiammaCabini", "GiuseppeCristofalo", "Salvatore ClaudioFanni", "Maria EvelinaFantacci", "SilviaFigini", "MassimoGalia", "PietroGemma", "EmanueleGrassedonio", "AlessandroLascialfari", "CristinaLenardi", "AliceLionetti", "FrancescaLizzi", "MaurizioMarrale", "MassimoMidiri", "CosimoNardi", "PiernicolaOliva", "NoemiPerillo", "IanPostuma", "LorenzoPreda", "VieriRastrelli", "FrancescoRizzetto", "NicolaSpina", "CinziaTalamonti", "AlbertoTorresin", "AngeloVanzulli", "FedericaVolpi", "EmanueleNeri", "AlessandraRetico"], "doi": "10.1186/s41747-023-00334-z\n10.1007/s00330-020-07347-x\n10.21037/atm-20-3311\n10.1093/rheumatology/keab615\n10.1016/j.ejrad.2021.109650\n10.1148/radiol.2020200527\n10.1007/s11547-020-01237-4\n10.1097/RLI.0000000000000689\n10.1016/j.ejmp.2021.01.004\n10.1007/s10278-019-00223-1\n10.1016/j.patcog.2021.108071\n10.1016/j.ejmp.2021.06.001\n10.1016/j.ejro.2020.100272\n10.1016/j.radonc.2020.09.045\n10.1038/srep23376\n10.1007/s10140-020-01867-1\n10.21037/qims-22-175\n10.1007/s11548-021-02501-2\n10.1148/ryai.2020200029\n10.1148/radiol.2462070712\n10.7150/ijms.50568\n10.1016/j.jcm.2016.02.012\n10.1016/j.nicl.2019.101846\n10.1007/s11547-020-01291-y\n10.1016/j.jinf.2020.02.017\n10.1016/j.acra.2020.03.003\n10.1016/j.ejrad.2020.109209\n10.1120/jacmp.v16i4.5001\n10.1016/j.chest.2021.06.063"}
{"title": "CMM: A CNN-MLP Model for COVID-19 Lesion Segmentation and Severity Grading.", "abstract": "In this paper, a CNN-MLP model (CMM) is proposed for COVID-19 lesion segmentation and severity grading in CT images. The CMM starts by lung segmentation using UNet, and then segmenting the lesion from the lung region using a multi-scale deep supervised UNet (MDS-UNet), finally implementing the severity grading by a multi-layer preceptor (MLP). In MDS-UNet, shape prior information is fused with the input CT image to reduce the searching space of the potential segmentation outputs. The multi-scale input compensates for the loss of edge contour information in convolution operations. In order to enhance the learning of multiscale features, the multi-scale deep supervision extracts supervision signals from different upsampling points on the network. In addition, it is empirical that the lesion which has a whiter and denser appearance tends to be more severe in the COVID-19 CT image. So, the weighted mean gray-scale value (WMG) is proposed to depict this appearance, and together with the lung and lesion area to serve as input features for the severity grading in MLP. To improve the precision of lesion segmentation, a label refinement method based on the Frangi vessel filter is also proposed. Comparative experiments on COVID-19 public datasets show that our proposed CMM achieves high accuracy on COVID-19 lesion segmentation and severity grading.", "journal": "IEEE/ACM transactions on computational biology and bioinformatics", "date": "2023-04-08", "authors": ["FangfangLu", "ZhihaoZhang", "ShuaiZhao", "XiantianLin", "ZhengyuZhang", "BeiJin", "WeiyanGu", "JingjingChen", "XiaoxinWu"], "doi": "10.1109/TCBB.2023.3253901"}
{"title": "Interpretable CNN-Multilevel Attention Transformer for Rapid Recognition of Pneumonia From Chest X-Ray Images.", "abstract": "Chest imaging plays an essential role in diagnosing and predicting patients with COVID-19 with evidence of worsening respiratory status. Many deep learning-based approaches for pneumonia recognition have been developed to enable computer-aided diagnosis. However, the long training and inference time makes them inflexible, and the lack of interpretability reduces their credibility in clinical medical practice. This paper aims to develop a pneumonia recognition framework with interpretability, which can understand the complex relationship between lung features and related diseases in chest X-ray (CXR) images to provide high-speed analytics support for medical practice. To reduce the computational complexity to accelerate the recognition process, a novel multi-level self-attention mechanism within Transformer has been proposed to accelerate convergence and emphasize the task-related feature regions. Moreover, a practical CXR image data augmentation has been adopted to address the scarcity of medical image data problems to boost the model's performance. The effectiveness of the proposed method has been demonstrated on the classic COVID-19 recognition task using the widespread pneumonia CXR image dataset. In addition, abundant ablation experiments validate the effectiveness and necessity of all of the components of the proposed method.", "journal": "IEEE journal of biomedical and health informatics", "date": "2023-04-08", "authors": ["ShengchaoChen", "SufenRen", "GuanjunWang", "MengxingHuang", "ChenyangXue"], "doi": "10.1109/JBHI.2023.3247949"}
{"title": "Monkeypox detection from skin lesion images using an amalgamation of CNN models aided with Beta function-based normalization scheme.", "abstract": "We have recently been witnessing that our society is starting to heal from the impacts of COVID-19. The economic, social and cultural impacts of a pandemic cannot be ignored and we should be properly equipped to deal with similar situations in future. Recently, Monkeypox has been concerning the international health community with its lethal impacts for a probable pandemic. In such situations, having appropriate protocols and methodologies to deal with the outbreak efficiently is of paramount interest to the world. Early diagnosis and treatment stand as the only viable option to tackle such problems. To this end, in this paper, we propose an ensemble learning-based framework to detect the presence of the Monkeypox virus from skin lesion images. We first consider three pre-trained base learners, namely Inception V3, Xception and DenseNet169 to fine-tune on a target Monkeypox dataset. Further, we extract probabilities from these deep models to feed into the ensemble framework. To combine the outcomes, we propose a Beta function-based normalization scheme of probabilities to learn an efficient aggregation of complementary information obtained from the base learners followed by the sum rule-based ensemble. The framework is extensively evaluated on a publicly available Monkeypox skin lesion dataset using a five-fold cross-validation setup to evaluate its effectiveness. The model achieves an average of 93.39%, 88.91%, 96.78% and 92.35% accuracy, precision, recall and F1 scores, respectively. The supporting source codes are presented in https://github.com/BihanBanerjee/MonkeyPox.", "journal": "PloS one", "date": "2023-04-08", "authors": ["RishavPramanik", "BihanBanerjee", "GeorgeEfimenko", "DmitriiKaplun", "RamSarkar"], "doi": "10.1371/journal.pone.0281815\n10.15585/mmwr.mm6710a5\n10.1038/d41586-022-01421-8\n10.1016/j.asoc.2022.109464\n10.1038/nature14539\n10.1007/s00521-021-06629-9\n10.1109/JPROC.2020.3004555\n10.1109/JPROC.2021.3054390\n10.1016/j.compbiomed.2022.105437\n10.1109/TMI.2020.2993291\n10.1109/TMI.2020.3040950\n10.1371/journal.pone.0177544\n10.1016/j.eswa.2021.116377\n10.1016/j.media.2021.102197\n10.1038/s41598-019-48995-4\n10.1038/s41598-022-18463-7\n10.1016/j.neucom.2019.07.080\n10.1016/j.cmpb.2022.106776\n10.1016/j.eswa.2021.116167\n10.3390/math10203845\n10.1007/s10916-022-01863-7\n10.1186/s40537-019-0197-0\n10.1109/ICCMA.2013.6506188\n10.1038/s41598-021-93658-y"}
{"title": "Redefining Lobe-Wise Ground-Glass Opacity in COVID-19 Through Deep Learning and its Correlation With Biochemical Parameters.", "abstract": "During COVID-19 pandemic qRT-PCR, CT scans and biochemical parameters were studied to understand the patients' physiological changes and disease progression. There is a lack of clear understanding of the correlation of lung inflammation with biochemical parameters available. Among the 1136 patients studied, C-reactive-protein (CRP) is the most critical parameter for classifying symptomatic and asymptomatic groups. Elevated CRP is corroborated with increased D-dimer, Gamma-glutamyl-transferase (GGT), and urea levels in COVID-19 patients. To overcome the limitations of manual chest CT scoring system, we segmented the lungs and detected ground-glass-opacity (GGO) in specific lobes from 2D CT images by 2D U-Net-based deep learning (DL) approach. Our method shows accuracy, compared to the manual method ( \u00a0\u223c\u00a080%), which is subjected to the radiologist's experience. We determined a positive correlation of GGO in the right upper-middle (0.34) and lower (0.26) lobe with D-dimer. However, a modest correlation was observed with CRP, ferritin and other studied parameters. The final Dice Coefficient (or the F1 score) and Intersection-Over-Union for testing accuracy are 95.44% and 91.95%, respectively. This study can help reduce the burden and manual bias besides increasing the accuracy of GGO scoring. Further study on geographically diverse large populations may help to understand the association of the biochemical parameters and pattern of GGO in lung lobes with different SARS-CoV-2 Variants of Concern's disease pathogenesis in these populations.", "journal": "IEEE journal of biomedical and health informatics", "date": "2023-04-07", "authors": ["BudhadevBaral", "KartikMuduli", "ShwetaJakhmola", "OmkarIndari", "JatinJangir", "Ashraf HaroonRashid", "SuchitaJain", "Amrut KumarMohapatra", "ShubhransuPatro", "PreetinandaParida", "NamrataMisra", "Ambika PrasadMohanty", "Bikash RSahu", "Ajay KumarJain", "SelvakumarElangovan", "Hamendra SinghParmar", "MTanveer", "Nirmal KumarMohakud", "Hem ChandraJha"], "doi": "10.1109/JBHI.2023.3263431"}
{"title": "Evaluating the Performance of a Commercially Available Artificial Intelligence Algorithm for Automated Detection of Pulmonary Embolism on Contrast-Enhanced Computed Tomography and Computed Tomography Pulmonary Angiography in Patients With Coronavirus Disease 2019.", "abstract": "To investigate the performance of a commercially available artificial intelligence (AI) algorithm for the detection of pulmonary embolism (PE) on contrast-enhanced computed tomography (CT) scans in patients hospitalized for coronavirus disease 2019 (COVID-19).\nRetrospective analysis was performed of all contrast-enhanced chest CT scans of patients admitted for COVID-19 between March 1, 2020 and December 31, 2021. Based on the original radiology reports, all PE-positive examinations were included (n=527). Using a reversed-flow single-gate diagnostic accuracy case-control model, a randomly selected cohort of PE-negative examinations (n=977) was included. Pulmonary parenchymal disease severity was assessed for all the included studies using a semiquantitative system, the total severity score. All included CT scans were sent for interpretation by the commercially available AI algorithm, Aidoc. Discrepancies between AI and original radiology reports were resolved by 3 blinded radiologists, who rendered a final determination of indeterminate, positive, or negative.\nA total of 78 studies were found to be discrepant, of which 13 (16.6%) were deemed indeterminate by readers and were excluded. The sensitivity and specificity of AI were 93.2% (95% CI, 90.6%-95.2%) and 99.6% (95% CI, 98.9%-99.9%), respectively. The accuracy of AI for all total severity score groups (mild, moderate, and severe) was high (98.4%, 96.7%, and 97.2%, respectively). Artificial intelligence was more accurate in PE detection on CT pulmonary angiography scans than on contrast-enhanced CT scans (\nThe AI algorithm demonstrated high sensitivity, specificity, and accuracy for PE on contrast-enhanced CT scans in patients with COVID-19 regardless of parenchymal disease. Accuracy was significantly affected by the mean attenuation of the pulmonary vasculature. How this affects the legitimacy of the binary outcomes reported by AI is not yet known.", "journal": "Mayo Clinic proceedings. Innovations, quality & outcomes", "date": "2023-04-07", "authors": ["Karim AZaazoue", "Mathew RMcCann", "Ahmed KAhmed", "Isabel OCortopassi", "Young MErben", "Brent PLittle", "Justin TStowell", "Beau BToskich", "Charles ARitchie"], "doi": "10.1016/j.mayocpiqo.2023.03.001"}
{"title": "Boosting automatic COVID-19 detection performance with self-supervised learning and batch knowledge ensembling.", "abstract": "Detecting COVID-19 from chest X-ray (CXR) images has become one of the fastest and easiest methods for detecting COVID-19. However, the existing methods usually use supervised transfer learning from natural images as a pretraining process. These methods do not consider the unique features of COVID-19 and the similar features between COVID-19 and other pneumonia.\nIn this paper, we want to design a novel high-accuracy COVID-19 detection method that uses CXR images, which can consider the unique features of COVID-19 and the similar features between COVID-19 and other pneumonia.\nOur method consists of two phases. One is self-supervised learning-based pertaining; the other is batch knowledge ensembling-based fine-tuning. Self-supervised learning-based pretraining can learn distinguished representations from CXR images without manually annotated labels. On the other hand, batch knowledge ensembling-based fine-tuning can utilize category knowledge of images in a batch according to their visual feature similarities to improve detection performance. Unlike our previous implementation, we introduce batch knowledge ensembling into the fine-tuning phase, reducing the memory used in self-supervised learning and improving COVID-19 detection accuracy.\nOn two public COVID-19 CXR datasets, namely, a large dataset and an unbalanced dataset, our method exhibited promising COVID-19 detection performance. Our method maintains high detection accuracy even when annotated CXR training images are reduced significantly (e.g., using only 10% of the original dataset). In addition, our method is insensitive to changes in hyperparameters.\nThe proposed method outperforms other state-of-the-art COVID-19 detection methods in different settings. Our method can reduce the workloads of healthcare providers and radiologists.", "journal": "Computers in biology and medicine", "date": "2023-04-06", "authors": ["GuangLi", "RenTogo", "TakahiroOgawa", "MikiHaseyama"], "doi": "10.1016/j.compbiomed.2023.106877"}
{"title": "Smart Artificial Intelligence techniques using embedded band for diagnosis and combating COVID-19.", "abstract": "Recently, COVID-19 virus spread to create a major impact in human body worldwide. The Corona virus, initiated by the SARS-CoV-2 virus, was known in China, December 2019 and affirmed a worldwide epidemic by the World Health Organization on 11 March 2020. The core aim of this research is to detect the spreading of COVID-19 virus and solve the problems in human lungs infection quickly. An Artificial Intelligence (AI) technique is a possibly controlling device in the battle against the corona virus epidemic. Recently, AI with computational techniques are utilized for COVID-19 virus with the building blocks of Deep Learning method using Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN) is used to classify and identify the lung images affected region. These two algorithms used to diagnose COVID-19 infections rapidly. The AI applications against COVID-19 are Medical Imaging for Diagnosis, Lung delineation, Lesion measurement, Non-Invasive Measurements for Disease Tracking, Patient Outcome Prediction, Molecular Scale: from Proteins to Drug Development and Societal Scale: Epidemiology and Infodemiology.", "journal": "Microprocessors and microsystems", "date": "2023-04-06", "authors": ["MAshwin", "Abdulrahman SaadAlqahtani", "AzathMubarakali"], "doi": "10.1016/j.micpro.2023.104819\n10.1109/rbme.2020.2987975\n10.1186/s40537-020-00392-9"}
{"title": "MiniSeg: An Extremely Minimum Network Based on Lightweight Multiscale Learning for Efficient COVID-19 Segmentation.", "abstract": "The rapid spread of the new pandemic, i.e., coronavirus disease 2019 (COVID-19), has severely threatened global health. Deep-learning-based computer-aided screening, e.g., COVID-19 infected area segmentation from computed tomography (CT) image, has attracted much attention by serving as an adjunct to increase the accuracy of COVID-19 screening and clinical diagnosis. Although lesion segmentation is a hot topic, traditional deep learning methods are usually data-hungry with millions of parameters, easy to overfit under limited available COVID-19 training data. On the other hand, fast training/testing and low computational cost are also necessary for quick deployment and development of COVID-19 screening systems, but traditional methods are usually computationally intensive. To address the above two problems, we propose MiniSeg, a lightweight model for efficient COVID-19 segmentation from CT images. Our efforts start with the design of an attentive hierarchical spatial pyramid (AHSP) module for lightweight, efficient, effective multiscale learning that is essential for image segmentation. Then, we build a two-path (TP) encoder for deep feature extraction, where one path uses AHSP modules for learning multiscale contextual features and the other is a shallow convolutional path for capturing fine details. The two paths interact with each other for learning effective representations. Based on the extracted features, a simple decoder is added for COVID-19 segmentation. For comparing MiniSeg to previous methods, we build a comprehensive COVID-19 segmentation benchmark. Extensive experiments demonstrate that the proposed MiniSeg achieves better accuracy because its only 83k parameters make it less prone to overfitting. Its high efficiency also makes it easy to deploy and develop. The code has been released at https://github.com/yun-liu/MiniSeg.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2023-04-05", "authors": ["YuQiu", "YunLiu", "ShijieLi", "JingXu"], "doi": "10.1109/TNNLS.2022.3230821"}
{"title": "Dual Multiscale Mean Teacher Network for Semi-Supervised Infection Segmentation in Chest CT Volume for COVID-19.", "abstract": "Automated detecting lung infections from computed tomography (CT) data plays an important role for combating coronavirus 2019 (COVID-19). However, there are still some challenges for developing AI system: 1) most current COVID-19 infection segmentation methods mainly relied on 2-D CT images, which lack 3-D sequential constraint; 2) existing 3-D CT segmentation methods focus on single-scale representations, which do not achieve the multiple level receptive field sizes on 3-D volume; and 3) the emergent breaking out of COVID-19 makes it hard to annotate sufficient CT volumes for training deep model. To address these issues, we first build a multiple dimensional-attention convolutional neural network (MDA-CNN) to aggregate multiscale information along different dimension of input feature maps and impose supervision on multiple predictions from different convolutional neural networks (CNNs) layers. Second, we assign this MDA-CNN as a basic network into a novel dual multiscale mean teacher network (DM [Formula: see text]-Net) for semi-supervised COVID-19 lung infection segmentation on CT volumes by leveraging unlabeled data and exploring the multiscale information. Our DM [Formula: see text]-Net encourages multiple predictions at different CNN layers from the student and teacher networks to be consistent for computing a multiscale consistency loss on unlabeled data, which is then added to the supervised loss on the labeled data from multiple predictions of MDA-CNN. Third, we collect two COVID-19 segmentation datasets to evaluate our method. The experimental results show that our network consistently outperforms the compared state-of-the-art methods.", "journal": "IEEE transactions on cybernetics", "date": "2023-04-05", "authors": ["LianshengWang", "JiachengWang", "LeiZhu", "HuazhuFu", "PingLi", "GaryCheng", "ZhipengFeng", "ShuoLi", "Pheng-AnnHeng"], "doi": "10.1109/TCYB.2022.3223528"}
{"title": "Federated Active Learning for Multicenter Collaborative Disease Diagnosis.", "abstract": "Current computer-aided diagnosis system with deep learning method plays an important role in the field of medical imaging. The collaborative diagnosis of diseases by multiple medical institutions has become a popular trend. However, large scale annotations put heavy burdens on medical experts. Furthermore, the centralized learning system has defects in privacy protection and model generalization. To meet these challenges, we propose two federated active learning methods for multicenter collaborative diagnosis of diseases: the Labeling Efficient Federated Active Learning (LEFAL) and the Training Efficient Federated Active Learning (TEFAL). The proposed LEFAL applies a task-agnostic hybrid sampling strategy considering data uncertainty and diversity simultaneously to improve data efficiency. The proposed TEFAL evaluates the client informativeness with a discriminator to improve client efficiency. On the Hyper-Kvasir dataset for gastrointestinal disease diagnosis, with only 65% of labeled data, the LEFAL achieves 95% performance on the segmentation task with whole labeled data. Moreover, on the CC-CCII dataset for COVID-19 diagnosis, with only 50 iterations, the accuracy and F1-score of TEFAL are 0.90 and 0.95, respectively on the classification task. Extensive experimental results demonstrate that the proposed federated active learning methods outperform state-of-the-art methods on segmentation and classification tasks for multicenter collaborative disease diagnosis.", "journal": "IEEE transactions on medical imaging", "date": "2023-04-05", "authors": ["XingWu", "JiePei", "ChengChen", "YiminZhu", "JianjiaWang", "QuanQian", "JianZhang", "QunSun", "YikeGuo"], "doi": "10.1109/TMI.2022.3227563"}
{"title": "2D medical image synthesis using transformer-based denoising diffusion probabilistic model.", "abstract": "", "journal": "Physics in medicine and biology", "date": "2023-04-05", "authors": ["ShaoyanPan", "TongheWang", "Richard L JQiu", "MarianAxente", "Chih-WeiChang", "JunboPeng", "Ashish BPatel", "JosephShelton", "Sagar APatel", "JustinRoper", "XiaofengYang"], "doi": "10.1088/1361-6560/acca5c\n10.1016/j.compmedimag.2022.102123\n10.1016/j.compmedimag.2019.101684\n10.1109/TMI.2018.2837502\n10.1088/1361-6560/ac6ebc\n10.1002/mp.14946\n10.1002/mp.15264\n10.1016/j.neunet.2017.12.012\n10.1145/3422622\n10.1088/1361-6579/ac3b3d\n10.1016/j.artmed.2020.101938\n10.1002/mp.13933\n10.1002/mp.13617\n10.1088/1361-6560/ab63bb\n10.1002/mp.14386\n10.1088/1361-6560/ac95f7\n10.1002/mp.16135\n10.1117/12.2582151\n10.1117/12.2611540\n10.1109/TMI.2018.2881415\n10.1007/s42979-021-00720-7\n10.3389/fonc.2021.744250\n10.1002/acm2.13121\n10.1016/j.media.2019.101552\n10.1007/s13278-021-00731-5"}
{"title": "Benchmark methodological approach for the application of artificial intelligence to lung ultrasound data from COVID-19 patients: From frame to prognostic-level.", "abstract": "Automated ultrasound imaging assessment of the effect of CoronaVirus disease 2019 (COVID-19) on lungs has been investigated in various studies using artificial intelligence-based (AI) methods. However, an extensive analysis of state-of-the-art Convolutional Neural Network-based (CNN) models for frame-level scoring, a comparative analysis of aggregation techniques for video-level scoring, together with a thorough evaluation of the capability of these methodologies to provide a clinically valuable prognostic-level score is yet missing within the literature. In addition to that, the impact on the analysis of the posterior probability assigned by the network to the predicted frames as well as the impact of temporal downsampling of LUS data are topics not yet extensively investigated. This paper takes on these challenges by providing a benchmark analysis of methods from frame to prognostic level. For frame-level scoring, state-of-the-art deep learning models are evaluated with additional analysis of best performing model in transfer-learning settings. A novel cross-correlation based aggregation technique is proposed for video and exam-level scoring. Results showed that ResNet-18, when trained from scratch, outperformed the existing methods with an F1-Score of 0.659. The proposed aggregation method resulted in 59.51%, 63.29%, and 84.90% agreement with clinicians at the video, exam, and prognostic levels, respectively; thus, demonstrating improved performances over the state of the art. It was also found that filtering frames based on the posterior probability shows higher impact on the LUS analysis in comparison to temporal downsampling. All of these analysis were conducted over the largest standardized and clinically validated LUS dataset from COVID-19 patients.", "journal": "Ultrasonics", "date": "2023-04-05", "authors": ["UmairKhan", "SajjadAfrakhteh", "FedericoMento", "NoreenFatima", "LauraDe Rosa", "Leonardo LucioCustode", "ZihadulAzam", "ElenaTorri", "GinoSoldati", "FrancescoTursi", "Veronica NarvenaMacioce", "AndreaSmargiassi", "RiccardoInchingolo", "TizianoPerrone", "GiovanniIacca", "LibertarioDemi"], "doi": "10.1016/j.ultras.2023.106994\n10.1038/d41586-022-00858-1"}
{"title": "Visual Perception and Convolutional Neural Network-Based Robotic Autonomous Lung Ultrasound Scanning Localization System.", "abstract": "Under the situation of severe COVID-19 epidemic, lung ultrasound (LUS) has been proved to be an effective and convenient method to diagnose and evaluate the extent of respiratory disease. However, the traditional clinical ultrasound (US) scanning requires doctors not only to be in close contact with patients but also to have rich experience. In order to alleviate the shortage of medical resources and reduce the work stress and risk of infection for doctors, we propose a visual perception and convolutional neural network (CNN)-based robotic autonomous LUS scanning localization system to realize scanned target recognition, probe pose solution and movement, and the acquisition of US images. The LUS scanned targets are identified through the target segmentation and localization algorithm based on the improved CNN, which is using the depth camera to collect the image information; furthermore, the method based on multiscale compensation normal vector is used to solve the attitude of the probe; finally, a position control strategy based on force feedback is designed to optimize the position and attitude of the probe, which can not only obtain high-quality US images but also ensure the safety of patients and the system. The results of human LUS scanning experiment verify the accuracy and feasibility of the system. The positioning accuracy of the scanned targets is 15.63 \u00b1 0.18 mm, and the distance accuracy and rotation angle accuracy of the probe position calculation are 6.38 \u00b1 0.25 mm and 8.60", "journal": "IEEE transactions on ultrasonics, ferroelectrics, and frequency control", "date": "2023-04-05", "authors": ["BohengZhang", "HaiboCong", "YiShen", "MingjianSun"], "doi": "10.1109/TUFFC.2023.3263514"}
{"title": "An enhanced ant colony optimizer with Cauchy-Gaussian fusion and novel movement strategy for multi-threshold COVID-19 X-ray image segmentation.", "abstract": "The novel coronavirus pneumonia (COVID-19) is a respiratory disease of great concern in terms of its dissemination and severity, for which X-ray imaging-based diagnosis is one of the effective complementary diagnostic methods. It is essential to be able to separate and identify lesions from their pathology images regardless of the computer-aided diagnosis techniques. Therefore, image segmentation in the pre-processing stage of COVID-19 pathology images would be more helpful for effective analysis. In this paper, to achieve highly effective pre-processing of COVID-19 pathological images by using multi-threshold image segmentation (MIS), an enhanced version of ant colony optimization for continuous domains (MGACO) is first proposed. In MGACO, not only a new move strategy is introduced, but also the Cauchy-Gaussian fusion strategy is incorporated. It has been accelerated in terms of convergence speed and has significantly enhanced its ability to jump out of the local optimum. Furthermore, an MIS method (MGACO-MIS) based on MGACO is developed, where it applies the non-local means, 2D histogram as the basis, and employs 2D Kapur's entropy as the fitness function. To demonstrate the performance of MGACO, we qualitatively analyze it in detail and compare it with other peers on 30 benchmark functions from IEEE CEC2014, which proves that it has a stronger capability of solving problems over the original ant colony optimization for continuous domains. To verify the segmentation effect of MGACO-MIS, we conducted a comparison experiment with eight other similar segmentation methods based on real pathology images of COVID-19 at different threshold levels. The final evaluation and analysis results fully demonstrate that the developed MGACO-MIS is sufficient to obtain high-quality segmentation results in the COVID-19 image segmentation and has stronger adaptability to different threshold levels than other methods. Therefore, it has been well-proven that MGACO is an excellent swarm intelligence optimization algorithm, and MGACO-MIS is also an excellent segmentation method.", "journal": "Frontiers in neuroinformatics", "date": "2023-04-04", "authors": ["XiuzhiZhao", "LeiLiu", "Ali AsgharHeidari", "YiChen", "Benedict JunMa", "HuilingChen", "ShichaoQuan"], "doi": "10.3389/fninf.2023.1126783\n10.1016/j.eswa.2017.07.043\n10.1016/j.engappai.2020.104105\n10.1016/j.asoc.2020.106642\n10.1016/j.eswa.2020.114243\n10.1016/j.bspc.2020.102259\n10.3390/app12168261\n10.1148/radiol.2020200463\n10.1109/CVPR.2005.38\n10.1016/j.eswa.2019.07.031\n10.1109/TSTE.2022.3217514\n10.1016/j.cmpb.2021.105971\n10.1007/s40815-018-0458-7\n10.1080/00207721.2022.2153635\n10.1016/j.apm.2019.02.004\n10.1016/j.neucom.2022.05.006\n10.1007/s00500-016-2307-7\n10.1016/j.patcog.2021.107826\n10.1016/j.neucom.2022.10.064\n10.1007/s10586-017-0793-8\n10.1016/j.eswa.2017.03.036\n10.48550/arXiv.2006.11988\n10.1109/TSMC.2020.3030792\n10.1016/j.swevo.2011.02.002\n10.1016/j.eswa.2021.114766\n10.1016/j.knosys.2021.107529\n10.1016/j.asoc.2020.106347\n10.1016/j.jocs.2018.12.005\n10.1007/s11721-017-0133-x\n10.1179/1432891714Z.0000000001100\n10.1016/j.ins.2009.12.010\n10.1016/j.future.2019.02.028\n10.1016/j.asoc.2019.105521\n10.1007/s00521-019-04015-0\n10.1016/j.comcom.2020.08.010\n10.1016/j.inffus.2021.02.016\n10.1016/j.aej.2020.06.054\n10.1016/j.ins.2022.11.019\n10.3991/ijoe.v12i12.6451\n10.1049/el:20080522\n10.1038/s41597-022-01564-3\n10.1016/j.eswa.2021.114848\n10.1109/TFUZZ.2013.2272480\n10.1016/j.jestch.2020.07.007\n10.1148/radiol.2020200241\n10.1016/0734-189X(85)90125-2\n10.1080/02522667.2017.1385162\n10.1109/ICNN.1995.488968\n10.1016/j.measurement.2018.08.007\n10.1007/s13198-017-0651-3\n10.1109/tec.2017.2669518\n10.1016/j.asoc.2017.03.018\n10.1109/TMI.2022.3151666\n10.1109/TPWRS.2018.2812711\n10.1109/TEVC.2005.857610\n10.1109/TCYB.2022.3163759\n10.1155/2014/428539\n10.1016/j.neucom.2022.06.075\n10.3390/electronics11203264\n10.1038/s41467-022-31339-8\n10.1016/j.apm.2019.03.046\n10.1016/j.apm.2018.07.044\n10.1016/j.compbiomed.2021.104504\n10.1016/j.measurement.2013.09.031\n10.1016/j.knosys.2015.07.006\n10.1016/j.knosys.2015.12.022\n10.1016/j.advengsoft.2016.01.008\n10.1007/s00521-015-1870-7\n10.1016/j.advengsoft.2013.12.007\n10.1016/j.mri.2019.05.009\n10.1016/0146-664X(81)90038-1\n10.3390/electronics11152321\n10.1155/2018/4231647\n10.1016/j.micpro.2020.103283\n10.1016/j.gltp.2021.01.011\n10.1016/j.eswa.2020.113428\n10.3233/idt-160278\n10.1016/j.jksuci.2019.04.006\n10.1016/j.patcog.2020.107700\n10.1016/j.ejor.2006.06.046\n10.1023/a:1008202821328\n10.1109/TSG.2022.3232545\n10.1016/j.eswa.2019.07.037\n10.1016/j.knosys.2020.106642\n10.1007/s10489-018-1334-8\n10.1016/j.knosys.2021.106859\n10.1016/j.eswa.2020.114121\n10.1007/s00521-021-06816-8\n10.1016/j.neucom.2022.12.048\n10.1016/j.knosys.2020.106437\n10.1016/j.ins.2020.05.033\n10.1007/s11047-015-9537-y\n10.1016/j.swevo.2017.07.004\n10.1016/j.media.2020.101913\n10.1016/j.knosys.2021.106952\n10.1007/s11280-020-00830-x\n10.1016/j.knosys.2020.105679\n10.1016/j.compbiomed.2022.105726\n10.1016/j.knosys.2020.105570\n10.1016/j.ins.2022.06.036\n10.1016/j.compbiomed.2022.105944\n10.3389/fenvs.2022.996513\n10.1109/TNNLS.2022.3153088\n10.1109/TEVC.2022.3193287\n10.2298/tsci131124023z\n10.1109/TIP.2011.2109730\n10.1016/j.neucom.2020.10.038\n10.1109/TCSVT.2022.3227348\n10.1016/j.knosys.2020.106510\n10.1016/j.eswa.2020.114122\n10.1109/TIP.2003.819861\n10.1109/jsee.2015.00037"}
{"title": "PCovNet+: A CNN-VAE anomaly detection framework with LSTM embeddings for smartwatch-based COVID-19 detection.", "abstract": "The world is slowly recovering from the Coronavirus disease 2019 (COVID-19) pandemic; however, humanity has experienced one of its According to work by Mishra et\u00a0al. (2020), the study's first phase included a cohort of 5,262 subjects, with 3,325 Fitbit users constituting the majority. However, among this large cohort of 5,262 subjects, most significant trials in modern times only to learn about its lack of preparedness in the face of a highly contagious pathogen. To better prepare the world for any new mutation of the same pathogen or the newer ones, technological development in the healthcare system is a must. Hence, in this work, PCovNet+, a deep learning framework, was proposed for smartwatches and fitness trackers to monitor the user's Resting Heart Rate (RHR) for the infection-induced anomaly. A convolutional neural network (CNN)-based variational autoencoder (VAE) architecture was used as the primary model along with a long short-term memory (LSTM) network to create latent space embeddings for the VAE. Moreover, the framework employed pre-training using normal data from healthy subjects to circumvent the data shortage problem in the personalized models. This framework was validated on a dataset of 68 COVID-19-infected subjects, resulting in anomalous RHR detection with precision, recall, F-beta, and F-1 score of 0.993, 0.534, 0.9849, and 0.6932, respectively, which is a significant improvement compared to the literature. Furthermore, the PCovNet+ framework successfully detected COVID-19 infection for 74% of the subjects (47% presymptomatic and 27% post-symptomatic detection). The results prove the usability of such a system as a secondary diagnostic tool enabling continuous health monitoring and contact tracing.", "journal": "Engineering applications of artificial intelligence", "date": "2023-04-04", "authors": ["Farhan FuadAbir", "Muhammad E HChowdhury", "Malisha IslamTapotee", "AdamMushtak", "AmithKhandakar", "SakibMahmud", "Md AnwarulHasan"], "doi": "10.1016/j.engappai.2023.106130\n10.48550/arXiv.1603.04467\n10.1016/j.compbiomed.2022.105682\n10.1038/s41591-021-01593-2\n10.1016/j.compbiomed.2022.106070\n10.1109/MPRV.2020.3021321\n10.3390/jcm9103372\n10.3390/biology9080182\n10.1016/S2666-5247(20)30172-5\n10.3390/s21175787\n10.1038/s41598-022-11329-y\n10.2217/pme-2018-0044\n10.1056/NEJMe2009758\n10.1371/journal.pone.0240123\n10.1038/s41586-020-2649-2\n10.48550/arXiv.1312.6114\n10.3390/jimaging4020036\n10.1056/NEJMoa2001316\n10.1109/ICASSP40776.2020.9053558\n10.48550/arXiv.2205.13607\n10.1016/S2589-7500(22)00019-X\n10.1117/1.JBO.25.10.102703\n10.3389/fdgth.2020.00008\n10.5281/zenodo.3509134"}
{"title": "A COVID-19 medical image classification algorithm based on Transformer.", "abstract": "Coronavirus 2019 (COVID-19) is a new acute respiratory disease that has spread rapidly throughout the world. This paper proposes a novel deep learning network based on ResNet-50 merged transformer named RMT-Net. On the backbone of ResNet-50, it uses Transformer to capture long-distance feature information, adopts convolutional neural networks and depth-wise convolution to obtain local features, reduce the computational cost and acceleration the detection process. The RMT-Net includes four stage blocks to realize the feature extraction of different receptive fields. In the first three stages, the global self-attention method is adopted to capture the important feature information and construct the relationship between tokens. In the fourth stage, the residual blocks are used to extract the details of feature. Finally, a global average pooling layer and a fully connected layer perform classification tasks. Training, verification and testing are carried out on self-built datasets. The RMT-Net model is compared with ResNet-50, VGGNet-16, i-CapsNet and MGMADS-3. The experimental results show that the RMT-Net model has a Test_ acc of 97.65% on the X-ray image dataset, 99.12% on the CT image dataset, which both higher than the other four models. The size of RMT-Net model is only 38.5 M, and the detection speed of X-ray image and CT image is 5.46 ms and 4.12 ms per image, respectively. It is proved that the model can detect and classify COVID-19 with higher accuracy and efficiency.", "journal": "Scientific reports", "date": "2023-04-04", "authors": ["KeyingRen", "GengHong", "XiaoyanChen", "ZichenWang"], "doi": "10.1038/s41598-023-32462-2\n10.1016/j.compbiomed.2021.105134\n10.1016/j.chaos.2020.110495\n10.1148/radiol.2020200343\n10.1148/radiol.2020200463\n10.1016/j.compbiomed.2021.105123\n10.1038/s41598-021-97428-8\n10.1109/TCBB.2021.3065361\n10.1016/j.patcog.2020.107747\n10.1016/j.bspc.2021.103371\n10.1016/j.irbm.2020.05.003\n10.3390/jpm12020310\n10.3390/jcm11113013\n10.1007/s13042-022-01676-7\n10.1016/j.cmpb.2022.107141\n10.1109/TMI.2020.2995965\n10.1007/s40846-020-00529-4\n10.1007/s12559-020-09775-9\n10.1007/s10489-020-01829-7\n10.1016/j.asoc.2022.108780\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2022.105244\n10.1016/j.compbiomed.2021.104399\n10.1016/j.cmpb.2020.105581\n10.1016/j.bspc.2021.102588\n10.3390/tomography8020071\n10.1007/s10096-020-03901-z\n10.3389/frai.2021.598932\n10.1148/radiol.2020200905\n10.1016/j.compbiomed.2020.104037"}
{"title": "The value of lung ultrasound in COVID-19 pneumonia, verified by high resolution computed tomography assessed by artificial intelligence.", "abstract": "Lung ultrasound (LUS) is an increasingly popular imaging method in clinical practice. It became particularly important during the COVID-19 pandemic due to its mobility and ease of use compared to high-resolution computed tomography (HRCT). The objective of this study was to assess the value of LUS in quantifying the degree of lung involvement and in discrimination of lesion types in the course of COVID-19 pneumonia as compared to HRCT analyzed by the artificial intelligence (AI).\nThis was a prospective observational study including adult patients hospitalized due to COVID-19 in whom initial HRCT and LUS were performed with an interval\u2009<\u200972\u00a0h. HRCT assessment was performed automatically by AI. We evaluated the correlations between the inflammation volume assessed both in LUS and HRCT, between LUS results and the HRCT structure of inflammation, and between LUS and the laboratory markers of inflammation. Additionally we compared the LUS results in subgroups depending on the respiratory failure throughout the hospitalization.\nStudy group comprised 65 patients, median 63\u00a0years old. For both lungs, the median LUS score was 19 (IQR-interquartile range 11-24) and the median CT score was 22 (IQR 16-26). Strong correlations were found between LUS and CT scores (for both lungs r\u2009=\u20090.75), and between LUS score and percentage inflammation volume (PIV) (r\u2009=\u20090.69). The correlations remained significant, if weakened, for individual lung lobes. The correlations between LUS score and the value of the percentage consolidation volume (PCV) divided by percentage ground glass volume (PGV), were weak or not significant. We found significant correlation between LUS score and C-reactive protein (r\u2009=\u20090.55), and between LUS score and interleukin 6 (r\u2009=\u20090.39). LUS score was significantly higher in subgroups with more severe respiratory failure.\nLUS can be regarded as an accurate method to evaluate the extent of COVID-19 pneumonia and as a promising tool to estimate its clinical severity. Evaluation of LUS in the assessment of the structure of inflammation, requires further studies in the course of the disease.\nThe study has been preregistered 13 Aug 2020 on clinicaltrials.gov with the number NCT04513210.", "journal": "BMC infectious diseases", "date": "2023-04-03", "authors": ["RobertChrzan", "KamilPolok", "JakubAntczak", "And\u017celikaSiwiec-Ko\u017alik", "WojciechJagie\u0142\u0142o", "TadeuszPopiela"], "doi": "10.1186/s12879-023-08173-4\n10.1136/postgradmedj-2020-138137\n10.1002/jcu.23184\n10.1016/j.annemergmed.2016.08.457\n10.5114/pjr.2022.112613\n10.1016/j.acra.2020.03.003\n10.1016/j.crad.2020.03.003\n10.1016/j.crad.2019.04.017\n10.2214/AJR.19.21572\n10.1148/radiol.2020200905\n10.1148/radiol.2020201491\n10.3390/jpm11050391\n10.1007/s00330-020-07156-2\n10.7150/thno.45985\n10.20452/pamw.16332\n10.1097/ALN.0000000000000558\n10.1038/s41598-020-80261-w\n10.1159/000518516\n10.1159/000509223\n10.3390/diagnostics11081351\n10.1016/j.ultrasmedbio.2020.07.018\n10.1016/j.jamda.2020.05.050\n10.1007/s42399-021-00986-1\n10.1016/j.ejrad.2021.109650\n10.1186/s40635-020-00367-3\n10.1007/s00134-020-06033-2\n10.1093/ehjci/jeaa163\n10.1002/jum.15444\n10.1016/j.ultrasmedbio.2020.07.005\n10.1093/cid/ciaa1408"}
{"title": "Measurement and Processing of Thermographic Data of Passing Persons for Epidemiological Purposes.", "abstract": "Non-contact temperature measurement of persons during an epidemic is the most preferred measurement option because of the safety of personnel and minimal possibility of spreading infection. The use of infrared (IR) sensors to monitor building entrances for infected persons has seen a major boom between 2020 and 2022 due to the COVID-19 epidemic, but with questionable results. This article does not deal with the precise determination of the temperature of an individual person but focuses on the possibility of using infrared cameras for monitoring the health of the population. The aim is to use large amounts of infrared data from many locations to provide information to epidemiologists so they can have better information about potential outbreaks. This paper focuses on the long-term monitoring of the temperature of passing persons inside public buildings and the search for the most appropriate tools for this purpose and is intended as the first step towards creating a useful tool for epidemiologists. As a classical approach, the identification of persons based on their characteristic temperature values over time throughout the day is used. These results are compared with the results of a method using artificial intelligence (AI) to evaluate temperature from simultaneously acquired infrared images. The advantages and disadvantages of both methods are discussed.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-03-31", "authors": ["Ji\u0159\u00edTesa\u0159", "Luk\u00e1\u0161Muzika", "Ji\u0159\u00edSk\u00e1la", "Tom\u00e1\u0161Kohlsch\u00fctter", "MilanHonner"], "doi": "10.3390/s23062945\n10.3390/s22218395\n10.3390/ijerph192416511\n10.3390/ijerph18063286\n10.1016/j.sbsr.2022.100513\n10.1016/j.jtherbio.2022.103444\n10.1117/1.jbo.25.9.097002\n10.1016/j.apergo.2021.103576\n10.1088/1742-6596/2112/1/012024\n10.1016/j.mvr.2004.05.003\n10.1038/s41467-021-25120-6\n10.2478/s11772-012-0037-7\n10.1177/101053950501700107\n10.1016/j.infrared.2018.12.017\n10.3390/s21020346"}
{"title": "Impact of the COVID-19 Pandemic on Clinical Findings in Medical Imaging Exams in a Nationwide Israeli Health Organization: Observational Study.", "abstract": "The outbreak of the COVID-19 pandemic had a major effect on the consumption of health care services. Changes in the use of routine diagnostic exams, increased incidences of postacute COVID-19 syndrome (PCS), and other pandemic-related factors may have influenced detected clinical conditions.\nThis study aimed to analyze the impact of COVID-19 on the use of outpatient medical imaging services and clinical findings therein, specifically focusing on the time period after the launch of the Israeli COVID-19 vaccination campaign. In addition, the study tested whether the observed gains in abnormal findings may be linked to PCS or COVID-19 vaccination.\nOur data set included 572,480 ambulatory medical imaging patients in a national health organization from January 1, 2019, to August 31, 2021. We compared different measures of medical imaging utilization and clinical findings therein before and after the surge of the pandemic to identify significant changes. We also inspected the changes in the rate of abnormal findings during the pandemic after adjusting for changes in medical imaging utilization. Finally, for imaging classes that showed increased rates of abnormal findings, we measured the causal associations between SARS-CoV-2 infection, COVID-19-related hospitalization (indicative of COVID-19 complications), and COVID-19 vaccination and future risk for abnormal findings. To adjust for a multitude of confounding factors, we used causal inference methodologies.\nAfter the initial drop in the utilization of routine medical imaging due to the first COVID-19 wave, the number of these exams has increased but with lower proportions of older patients, patients with comorbidities, women, and vaccine-hesitant patients. Furthermore, we observed significant gains in the rate of abnormal findings, specifically in musculoskeletal magnetic resonance (MR-MSK) and brain computed tomography (CT-brain) exams. These results also persisted after adjusting for the changes in medical imaging utilization. Demonstrated causal associations included the following: SARS-CoV-2 infection increasing the risk for an abnormal finding in a CT-brain exam (odds ratio [OR] 1.4, 95% CI 1.1-1.7) and COVID-19-related hospitalization increasing the risk for abnormal findings in an MR-MSK exam (OR 3.1, 95% CI 1.9-5.3).\nCOVID-19 impacted the use of ambulatory imaging exams, with greater avoidance among patients at higher risk for COVID-19 complications: older patients, patients with comorbidities, and nonvaccinated patients. Causal analysis results imply that PCS may have contributed to the observed gains in abnormal findings in MR-MSK and CT-brain exams.", "journal": "JMIR formative research", "date": "2023-03-30", "authors": ["MichalOzery-Flato", "LiatEin-Dor", "OraPinchasov", "MielDabush Kasa", "EfratHexter", "GabrielChodick", "MichalRosen-Zvi", "MichalGuindy"], "doi": "10.2196/42930\n10.1136/bmjopen-2020-045343\n10.1093/eurpub/ckab047\n10.1371/journal.pmed.1003854\n10.1371/journal.pmed.1003854\n10.1177/23743735211065274?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.1177/23743735211065274\n10.1056/nejmoa2101765\n10.1016/j.puhe.2021.02.025\n10.1200/CCI.20.00134?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.1200/CCI.20.00134\n10.1177/17562848221117636?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.1177/17562848221117636\n10.1200/cci.21.00200\n10.1038/s41591-020-0968-3\n10.1038/s41591-021-01283-z\n10.1073/pnas.2016632118\n10.1161/hypertensionaha.114.03718\n10.1111/j.1464-5491.2010.03174.x\n10.1023/b:ejep.0000006635.36802.c8\n10.1016/j.ijcard.2010.08.002\n10.1007/s11657-015-0210-y\n10.1016/0021-9681(87)90171-8\n10.1145/2939672.2939785\n10.1038/s42256-019-0138-9\n10.1080/00273171.2011.568786\n10.1016/j.jclinepi.2013.01.013\n10.1111/j.2517-6161.1995.tb02031.x\n10.1038/s41592-019-0686-2\n10.25080/majora-92bf1922-011\n10.1186/s13584-021-00481-x\n10.1186/s13584-021-00481-x\n10.1038/s43018-020-0074-y\n10.3389/fonc.2021.675038\n10.1038/s41746-021-00412-9\n10.1038/s41746-021-00412-9\n10.3390/ijerph18084363\n10.3390/vaccines9030300\n10.1038/s41467-022-28200-3\n10.1038/s41467-022-28200-3\n10.1038/s41598-021-97065-1\n10.1038/s41598-021-97065-1\n10.1136/bmjsem-2020-000960\n10.1038/s41598-021-02702-4\n10.1038/s41598-021-02702-4\n10.3390/ijerph18168433\n10.1007/s00330-020-06934-2\n10.1038/s41467-020-18786-x\n10.1038/s41467-020-18786-x\n10.1016/j.ijid.2022.12.002\n10.1186/s13195-020-00640-3\n10.1186/s13195-020-00640-3\n10.1016/j.jns.2020.117085\n10.1016/j.bbih.2021.100290\n10.1016/j.neulet.2020.135529\n10.1016/j.neulet.2020.135564\n10.1101/2021.06.11.21258690\n10.1101/2021.06.11.21258690\n10.3389/fneur.2023.1136348\n10.1016/j.jcot.2021.03.002\n10.1007/s00256-021-03734-7\n10.1016/j.clinimag.2021.08.003\n10.2106/JBJS.RVW.22.00013\n10.1148/rg.220036\n10.1038/s41597-021-00878-y\n10.1038/s41597-021-00878-y"}
{"title": "Multi-head deep learning framework for pulmonary disease detection and severity scoring with modified progressive learning.", "abstract": "Chest X-rays (CXR) are the most commonly used imaging methodology in radiology to diagnose pulmonary diseases with close to 2 billion CXRs taken every year. The recent upsurge of COVID-19 and its variants accompanied by pneumonia and tuberculosis can be fatal in some cases and lives could be saved through early detection and appropriate intervention for the advanced cases. Thus CXRs can be used for an automated severity grading of pulmonary diseases that can aid radiologists in making better and informed diagnoses. In this article, we propose a single framework for disease classification and severity scoring produced by segmenting the lungs into six regions. We present a modified progressive learning technique in which the amount of augmentations at each step is capped. Our base network in the framework is first trained using modified progressive learning and can then be tweaked for new data sets. Furthermore, the segmentation task makes use of an attention map generated within and by the network itself. This attention mechanism allows to achieve segmentation results that are on par with networks having an order of magnitude or more parameters. We also propose severity score grading for 4 thoracic diseases that can provide a single-digit score corresponding to the spread of opacity in different lung segments with the help of radiologists. The proposed framework is evaluated using the BRAX data set for segmentation and classification into six classes with severity grading for a subset of the classes. On the BRAX validation data set, we achieve F1 scores of 0.924 and 0.939 without and with fine-tuning, respectively. A mean matching score of 80.8% is obtained for severity score grading while an average area under receiver operating characteristic curve of 0.88 is achieved for classification.", "journal": "Biomedical signal processing and control", "date": "2023-03-30", "authors": ["Asad MansoorKhan", "Muhammad UsmanAkram", "SajidNazir", "TaimurHassan", "Sajid GulKhawaja", "TatheerFatima"], "doi": "10.1016/j.bspc.2023.104855"}
{"title": "Computer-Aided Diagnosis of COVID-19 from Chest X-ray Images Using Hybrid-Features and Random Forest Classifier.", "abstract": "In recent years, a lot of attention has been paid to using radiology imaging to automatically find COVID-19. (1) Background: There are now a number of computer-aided diagnostic schemes that help radiologists and doctors perform diagnostic COVID-19 tests quickly, accurately, and consistently. (2) Methods: Using chest X-ray images, this study proposed a cutting-edge scheme for the automatic recognition of COVID-19 and pneumonia. First, a pre-processing method based on a Gaussian filter and logarithmic operator is applied to input chest X-ray (CXR) images to improve the poor-quality images by enhancing the contrast, reducing the noise, and smoothing the image. Second, robust features are extracted from each enhanced chest X-ray image using a Convolutional Neural Network (CNNs) transformer and an optimal collection of grey-level co-occurrence matrices (GLCM) that contain features such as contrast, correlation, entropy, and energy. Finally, based on extracted features from input images, a random forest machine learning classifier is used to classify images into three classes, such as COVID-19, pneumonia, or normal. The predicted output from the model is combined with Gradient-weighted Class Activation Mapping (Grad-CAM) visualisation for diagnosis. (3) Results: Our work is evaluated using public datasets with three different train-test splits (70-30%, 80-20%, and 90-10%) and achieved an average accuracy, F1 score, recall, and precision of 97%, 96%, 96%, and 96%, respectively. A comparative study shows that our proposed method outperforms existing and similar work. The proposed approach can be utilised to screen COVID-19-infected patients effectively. (4) Conclusions: A comparative study with the existing methods is also performed. For performance evaluation, metrics such as accuracy, sensitivity, and F1-measure are calculated. The performance of the proposed method is better than that of the existing methodologies, and it can thus be used for the effective diagnosis of the disease.", "journal": "Healthcare (Basel, Switzerland)", "date": "2023-03-30", "authors": ["KashifShaheed", "PiotrSzczuko", "QaisarAbbas", "AyyazHussain", "MubarakAlbathan"], "doi": "10.3390/healthcare11060837\n10.1002/jmv.25678\n10.1016/S0140-6736(21)02046-8\n10.1016/S0140-6736(21)02249-2\n10.1148/radiol.2020200230\n10.1109/TMI.2020.3040950\n10.1109/TMI.2020.2993291\n10.1152/physiolgenomics.00029.2020\n10.3390/ijerph19042013\n10.1148/radiol.2020200527\n10.1016/j.jcv.2020.104384\n10.1016/S1473-3099(20)30086-4\n10.1038/s41598-020-76550-z\n10.1016/j.asoc.2022.109319\n10.1016/j.compbiomed.2022.105233\n10.1016/j.compbiomed.2020.103792\n10.1109/TMI.2020.2996645\n10.1007/s10489-020-01826-w\n10.1007/s10044-021-00984-y\n10.1016/j.compbiomed.2020.103795\n10.3390/ijerph20032035\n10.1101/2020.03.30.20047787\n10.1016/j.compbiomed.2021.104781\n10.1016/j.ipm.2022.103025\n10.1016/j.neucom.2022.01.055\n10.1016/j.radi.2022.03.011\n10.1007/s00521-020-05017-z\n10.3390/diagnostics12123109\n10.1109/ICCV48922.2021.00009\n10.1007/978-3-030-62008-0_35\n10.1016/j.ijid.2020.06.058\n10.1371/journal.pone.0096385\n10.7717/peerj.5518\n10.1007/978-3-540-74825-0_11\n10.1136/bmjopen-2018-025925\n10.1016/j.chaos.2020.109944\n10.1177/2472630320962002\n10.1007/s11042-020-09431-2\n10.1109/ACCESS.2021.3058854\n10.3991/ijoe.v18i07.30807"}
{"title": "Comparative Analysis of Clinical and CT Findings in Patients with SARS-CoV-2 Original Strain, Delta and Omicron Variants.", "abstract": "To compare the clinical characteristics and chest CT findings of patients infected with Omicron and Delta variants and the original strain of COVID-19.\nA total of 503 patients infected with the original strain (245 cases), Delta variant (90 cases), and Omicron variant (168 cases) were retrospectively analyzed. The differences in clinical severity and chest CT findings were analyzed. We also compared the infection severity of patients with different vaccination statuses and quantified pneumonia by a deep-learning approach.\nThe rate of severe disease decreased significantly from the original strain to the Delta variant and Omicron variant (27% vs. 10% vs. 4.8%, \nCompared with the original strain and Delta variant, the Omicron variant had less clinical severity and less lung injury on CT scans.", "journal": "Biomedicines", "date": "2023-03-30", "authors": ["XiaoyuHan", "JingzeChen", "LuChen", "XiJia", "YanqingFan", "YutingZheng", "OsamahAlwalid", "JieLiu", "YuminLi", "NaLi", "JinGu", "JiangtaoWang", "HeshuiShi"], "doi": "10.3390/biomedicines11030901\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30185-9\n10.1016/S0140-6736(21)00370-6\n10.1038/s41579-021-00573-0\n10.1016/S0140-6736(20)30211-7\n10.1016/S1473-3099(20)30086-4\n10.1016/S0140-6736(20)30566-3\n10.1148/radiol.220533\n10.1148/radiol.229022\n10.1038/s41467-020-18685-1\n10.1109/TMI.2020.2992546\n10.1148/radiol.2020200823\n10.15585/mmwr.mm7101a4\n10.1038/s41467-022-28089-y\n10.1080/22221751.2021.2022440\n10.1148/ryct.2020200152\n10.1148/radiol.2462070712\n10.1148/radiol.2363040958\n10.1016/S2589-7500(20)30199-0\n10.1148/ryct.2020200075\n10.1016/S0140-6736(22)00017-4\n10.1001/jama.2022.2274\n10.1016/S0140-6736(22)00462-7\n10.1016/S1473-3099(21)00475-8\n10.1503/cmaj.211248\n10.1016/j.cell.2020.04.004\n10.1038/s41598-020-74497-9\n10.1001/jama.2021.24315\n10.1136/bmj.n3144\n10.2214/AJR.21.26560\n10.1016/j.jacr.2020.06.006\n10.1016/S0140-6736(21)02249-2\n10.1038/s41422-022-00674-2\n10.3390/jpm12060955"}
{"title": "The Association between COVID-19 Related Anxiety, Stress, Depression, Temporomandibular Disorders, and Headaches from Childhood to Adulthood: A Systematic Review.", "abstract": "", "journal": "Brain sciences", "date": "2023-03-30", "authors": ["GiuseppeMinervini", "RoccoFranco", "Maria MaddalenaMarrapodi", "ViniMehta", "LucaFiorillo", "AlmirBadnjevi\u0107", "GabrieleCervino", "MarcoCicci\u00f9"], "doi": "10.3390/brainsci13030481\n10.23736/S1973-9087.20.06339-X\n10.1111/eje.12555\n10.1186/s12938-020-00820-0\n10.14712/23362936.2022.10\n10.7759/cureus.28167\n10.1109/eurocon.2013.6625037\n10.13140/rg.2.1.2760.3045\n10.1007/978-3-319-51234-1_18\n10.1016/j.bbe.2021.09.002\n10.3390/dj11020048\n10.1590/1678-7757-2020-0445\n10.1111/odi.13488\n10.1155/2020/8893423\n10.3390/jcm10163532\n10.3390/ijerph17155598\n10.1080/08869634.2019.1681621\n10.1097/SCS.0000000000006769\n10.23736/S2724-6329.20.04443-X\n10.1159/000369810\n10.3390/biomedicines7020033\n10.1097/SCS.0000000000003210\n10.1177/03331024211040753\n10.17219/dmp/150075\n10.1186/s40510-021-00355-7\n10.11138/orl/2016.9.4.190\n10.3390/jcm9103250\n10.1080/08869634.2021.1883364\n10.3390/medicina59020410\n10.3390/molecules28010106\n10.3390/prosthesis4020025\n10.3390/app122312409\n10.1097/SCS.0000000000009103\n10.1080/08869634.2022.2137129\n10.1097/SCS.0000000000008771\n10.3390/ijerph17103399\n10.3390/ijerph18105131\n10.3390/ijerph17062094\n10.3390/jpm12111920\n10.3233/THC-220910\n10.3390/ijerph19095415\n10.3390/prosthesis2020005\n10.1155/2020/8896812\n10.1080/08869634.2021.1989768\n10.5603/PJNNS.a2022.0049\n10.1590/1678-7757-2020-1089\n10.1016/j.medin.2017.10.003\n10.3390/jcm11030589\n10.3390/ijerph17238907\n10.1590/2177-6709.27.3.e2220422.oar\n10.1371/journal.pone.0245999\n10.1590/1678-7757-2020-0263\n10.3390/ijerph18168858\n10.1016/j.ijcha.2021.100715\n10.1055/s-0042-1755192\n10.3390/brainsci12010028\n10.1038/s41598-022-18745-0\n10.1111/ipd.12843"}
{"title": "Design and Analysis of a Deep Learning Ensemble Framework Model for the Detection of COVID-19 and Pneumonia Using Large-Scale CT Scan and X-ray Image Datasets.", "abstract": "Recently, various methods have been developed to identify COVID-19 cases, such as PCR testing and non-contact procedures such as chest X-rays and computed tomography (CT) scans. Deep learning (DL) and artificial intelligence (AI) are critical tools for early and accurate detection of COVID-19. This research explores the different DL techniques for identifying COVID-19 and pneumonia on medical CT and radiography images using ResNet152, VGG16, ResNet50, and DenseNet121. The ResNet framework uses CT scan images with accuracy and precision. This research automates optimum model architecture and training parameters. Transfer learning approaches are also employed to solve content gaps and shorten training duration. An upgraded VGG16 deep transfer learning architecture is applied to perform multi-class classification for X-ray imaging tasks. Enhanced VGG16 has been proven to recognize three types of radiographic images with 99% accuracy, typical for COVID-19 and pneumonia. The validity and performance metrics of the proposed model were validated using publicly available X-ray and CT scan data sets. The suggested model outperforms competing approaches in diagnosing COVID-19 and pneumonia. The primary outcomes of this research result in an average F-score (95%, 97%). In the event of healthy viral infections, this research is more efficient than existing methodologies for coronavirus detection. The created model is appropriate for recognition and classification pre-training. The suggested model outperforms traditional strategies for multi-class categorization of various illnesses.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2023-03-30", "authors": ["XingsiXue", "SeelammalChinnaperumal", "Ghaida MuttasharAbdulsahib", "Rajasekhar ReddyManyam", "RajaMarappan", "Sekar KidambiRaju", "Osamah IbrahimKhalaf"], "doi": "10.3390/bioengineering10030363\n10.1007/s42979-021-00823-1\n10.1016/j.compbiomed.2022.105344\n10.1038/s42003-020-01535-7\n10.1007/s11517-020-02299-2\n10.1007/s10044-021-00970-4\n10.31224/osf.io/wx89s\n10.1038/s41598-021-99015-3\n10.1007/s40846-021-00653-9\n10.3390/cmim.2021.10008\n10.1007/s12530-021-09385-2\n10.1109/JIOT.2021.3056185\n10.1007/s10489-020-01829-7\n10.1016/j.imu.2020.100412\n10.1007/s10489-020-01867-1\n10.1155/2021/5513679\n10.1109/ACCESS.2020.3016780\n10.3390/s20236985\n10.1007/s10489-021-02393-4\n10.48084/etasr.4613\n10.11591/ijece.v11i1.pp844-850\n10.1007/s10522-021-09946-7\n10.21817/indjcse/2021/v12i1/211201064\n10.3390/s21175813\n10.3390/diagnostics11020340.2021\n10.1166/jctn.2020.9439\n10.14704/WEB/V19I1/WEB19071\n10.1016/j.chaos.2020.110120\n10.1007/s10489-020-02055-x\n10.1016/j.irbm.2020.05.003\n10.1007/s40747-021-00509-4\n10.1109/CIMSim.2013.17\n10.1109/ICCIC.2013.6724190\n10.1109/ICICES.2016.7518914\n10.3390/telecom4010008c\n10.1007/s13369-017-2686-9\n10.3390/math8030303\n10.3390/math8071106\n10.3390/math9020197\n10.3390/bioengineering10020138\n10.1155/2022/9227343\n10.32604/iasc.2022.025609\n10.1155/2021/5574376\n10.4018/IJRQEH.289176\n10.1007/s13369-021-06323-x\n10.1109/ICICES.2016.7518911\n10.1109/ICCIC.2018.8782425\n10.1007/s41870-023-01165-2"}
{"title": "Perceptive SARS-CoV-2 End-To-End Ultrasound Video Classification through X3D and Key-Frames Selection.", "abstract": "The SARS-CoV-2 pandemic challenged health systems worldwide, thus advocating for practical, quick and highly trustworthy diagnostic instruments to help medical personnel. It features a long incubation period and a high contagion rate, causing bilateral multi-focal interstitial pneumonia, generally growing into acute respiratory distress syndrome (ARDS), causing hundreds of thousands of casualties worldwide. Guidelines for first-line diagnosis of pneumonia suggest Chest X-rays (CXR) for patients exhibiting symptoms. Potential alternatives include Computed Tomography (CT) scans and Lung UltraSound (LUS). Deep learning (DL) has been helpful in diagnosis using CT scans, LUS, and CXR, whereby the former commonly yields more precise results. CXR and CT scans present several drawbacks, including high costs. Radiation-free LUS imaging requires high expertise, and physicians thus underutilise it. LUS demonstrated a strong correlation with CT scans and reliability in pneumonia detection, even in the early stages. Here, we present an LUS video-classification approach based on contemporary DL strategies in close collaboration with Fondazione IRCCS Policlinico San Matteo's Emergency Department (ED) of Pavia. This research addressed SARS-CoV-2 patterns detection, ranked according to three severity scales by operating a trustworthy dataset comprising ultrasounds from linear and convex probes in 5400 clips from 450 hospitalised subjects. The main contributions of this study are related to the adoption of a standardised severity ranking scale to evaluate pneumonia. This evaluation relies on video summarisation through key-frame selection algorithms. Then, we designed and developed a video-classification architecture which emerged as the most promising. In contrast, the literature primarily concentrates on frame-pattern recognition. By using advanced techniques such as transfer learning and data augmentation, we were able to achieve an F1-Score of over 89% across all classes.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2023-03-30", "authors": ["MarcoGazzoni", "MarcoLa Salvia", "EmanueleTorti", "GianmarcoSecco", "StefanoPerlini", "FrancescoLeporati"], "doi": "10.3390/bioengineering10030282\n10.1056/NEJMoa2001316\n10.1186/s13000-020-01017-8\n10.1016/S1473-3099(20)30086-4\n10.1002/jum.15285\n10.1002/jmv.25727\n10.1164/ajrccm.163.7.at1010\n10.4103/IJMR.IJMR_3669_20\n10.1016/j.compbiomed.2021.104742\n10.1186/1465-9921-15-50\n10.1007/s11739-015-1297-2\n10.1016/j.ajem.2013.10.003\n10.3389/FRAI.2022.912022/BIBTEX\n10.1016/j.imu.2021.100687\n10.3390/s21165486\n10.3390/jpm12101707\n10.1055/s-0042-120260\n10.4081/ecj.2020.9017\n10.1007/978-3-319-15371-1_8\n10.1136/bmjopen-2020-045120\n10.1109/TMI.2020.2994459\n10.1109/ACCESS.2021.3058537\n10.1016/j.compbiomed.2021.104375\n10.1186/s40537-019-0197-0\n10.1016/J.PROCS.2015.10.021\n10.3390/e18030073\n10.1016/S1007-0214(05)70050-X\n10.1109/ICEIC49074.2020.9051332\n10.1007/978-3-030-87199-4_16/COVER\n10.1109/THMS.2022.3144000\n10.1109/ICCV48922.2021.00675\n10.1016/j.cviu.2022.103484\n10.1109/CVPR42600.2020.00028\n10.1007/978-0-387-84858-7\n10.1109/TUFFC.2020.3002249\n10.1109/ACCESS.2020.3016780"}
{"title": "GW- CNNDC: Gradient weighted CNN model for diagnosing COVID-19 using radiography X-ray images.", "abstract": "COVID-19 is one of the dangerous viruses that cause death if the patient doesn't identify it in the early stages. Firstly, this virus is identified in China, Wuhan city. This virus spreads very fast compared with other viruses. Many tests are there for detecting this virus, and also side effects may find while testing this disease. Corona-virus tests are now rare; there are restricted COVID-19 testing units and they can't be made quickly enough, causing alarm. Thus, we want to depend on other determination measures. There are three distinct sorts of COVID-19 testing systems: RTPCR, CT, and CXR. There are certain limitations to RTPCR, which is the most time-consuming technique, and CT-scan results in exposure to radiation which may cause further diseases. So, to overcome these limitations, the CXR technique emits comparatively less radiation, and the patient need not be close to the medical staff. COVID-19 detection from CXR images has been tested using a diversity of pre-trained deep-learning algorithms, with the best methods being fine-tuned to maximize detection accuracy. In this work, the model called GW-CNNDC is presented. The Lung Radiography pictures are portioned utilizing the Enhanced CNN model, deployed with RESNET-50 Architecture with an image size of 255*255 pixels. Afterward, the Gradient Weighted model is applied, which shows the specific separation regardless of whether the individual is impacted by Covid-19 affected area. This framework can perform twofold class assignments with exactness and accuracy, precision, recall, F1-score, and Loss value, and the model turns out proficiently for huge datasets with less measure of time.", "journal": "Measurement. Sensors", "date": "2023-03-28", "authors": ["PamulaUdayaraju", "T VenkataNarayana", "Sri HarshaVemparala", "ChopparapuSrinivasarao", "BhV S R KRaju"], "doi": "10.1016/j.measen.2023.100735\n10.1007/s12098-020\n10.1109/TII.2021.3057683\n10.1109/TLA.2021.9451239\n10.1109/JAS.2020.1003393\n10.1109/TMI.2020.2994459"}
{"title": "Lightweight deep CNN-based models for early detection of COVID-19 patients from chest X-ray images.", "abstract": "Hundreds of millions of people worldwide have recently been infected by the novel Coronavirus disease (COVID-19), causing significant damage to the health, economy, and welfare of the world's population. Moreover, the unprecedented number of patients with COVID-19 has placed a massive burden on healthcare centers, making timely and rapid diagnosis challenging. A crucial step in minimizing the impact of such problems is to automatically detect infected patients and place them under special care as quickly as possible. Deep learning algorithms, such as Convolutional Neural Networks (CNN), can be used to meet this need. Despite the desired results, most of the existing deep learning-based models were built on millions of parameters (weights), which are not applicable to devices with limited resources. Inspired by such fact, in this research, we developed two new lightweight CNN-based diagnostic models for the automatic and early detection of COVID-19 subjects from chest X-ray images. The first model was built for binary classification (COVID-19 and Normal), whereas the second one was built for multiclass classification (COVID-19, viral pneumonia, or normal). The proposed models were tested on a relatively large dataset of chest X-ray images, and the results showed that the accuracy rates of the 2- and 3-class-based classification models are 98.55% and 96.83%, respectively. The results also revealed that our models achieved competitive performance compared with the existing heavyweight models while significantly reducing cost and memory requirements for computing resources. With these findings, we can indicate that our models are helpful to clinicians in making insightful diagnoses of COVID-19 and are potentially easily deployable on devices with limited computational power and resources.", "journal": "Expert systems with applications", "date": "2023-03-28", "authors": ["Haval IHussein", "Abdulhakeem OMohammed", "Masoud MHassan", "Ramadhan JMstafa"], "doi": "10.1016/j.eswa.2023.119900\n10.1109/ISIEA49364.2020.9188133\n10.1016/j.compbiomed.2022.105350\n10.3390/computation9010003\n10.3390/biology10111174\n10.1109/ACCESS.2021.3054484\n10.1016/j.compbiomed.2021.104672\n10.3390/s21020455\n10.1007/s40846-020-00529-4\n10.1007/s13246-020-00865-4\n10.1016/j.bspc.2021.103182\n10.1016/j.compbiomed.2021.104454\n10.1148/radiol.2020200230\n10.1007/s13748-019-00203-0\n10.1016/j.bbe.2022.07.009\n10.1016/j.bbe.2022.11.003\n10.1016/j.compbiomed.2021.104920\n10.1007/s12648-022-02425-w\n10.2139/ssrn.3833706\n10.1007/978-981-19-4453-6_7\n10.1007/s11042-022-12156-z\n10.1016/j.imu.2022.100945\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.bea.2022.100041\n10.1016/j.chaos.2020.110495\n10.1016/j.neucom.2022.01.055\n10.1016/j.eswa.2022.116942\n10.1016/j.compbiomed.2022.106331\n10.1109/ACCAI53970.2022.9752511\n10.1016/j.cmpb.2020.105581\n10.1016/j.eswa.2021.115695\n10.1109/TNNLS.2021.3084827\n10.1007/s42600-021-00151-6\n10.1016/j.media.2020.101794\n10.1007/s10044-021-00984-y\n10.1016/j.bspc.2020.102365\n10.3390/diagnostics13010131\n10.1016/j.bspc.2022.103977\n10.1109/RTEICT52294.2021.9573980\n10.1016/j.health.2022.100096\n10.1038/s41598-020-76550-z\n10.1001/jama.2020.3786\n10.1007/s13244-018-0639-9\n10.1007/s10489-020-01867-1"}
{"title": "MTMC-AUR2CNet: Multi-textural multi-class attention recurrent residual convolutional neural network for COVID-19 classification using chest X-ray images.", "abstract": "Coronavirus disease (COVID-19) has infected over 603 million confirmed cases as of September 2022, and its rapid spread has raised concerns worldwide. More than 6.4 million fatalities in confirmed patients have been reported. According to reports, the COVID-19 virus causes lung damage and rapidly mutates before the patient receives any diagnosis-specific medicine. Daily increasing COVID-19 cases and the limited number of diagnosis tool kits encourage the use of deep learning (DL) models to assist health care practitioners using chest X-ray (CXR) images. The CXR is a low radiation radiography tool available in hospitals to diagnose COVID-19 and combat this spread. We propose a Multi-Textural Multi-Class (MTMC) UNet-based Recurrent Residual Convolutional Neural Network (MTMC-UR2CNet) and MTMC-UR2CNet with attention mechanism (MTMC-AUR2CNet) for multi-class lung lobe segmentation of CXR images. The lung lobe segmentation output of MTMC-UR2CNet and MTMC-AUR2CNet are mapped individually with their input CXRs to generate the region of interest (ROI). The multi-textural features are extracted from the ROI of each proposed MTMC network. The extracted multi-textural features from ROI are fused and are trained to the Whale optimization algorithm (WOA) based DeepCNN classifier on classifying the CXR images into normal (healthy), COVID-19, viral pneumonia, and lung opacity. The experimental result shows that the MTMC-AUR2CNet has superior performance in multi-class lung lobe segmentation of CXR images with an accuracy of 99.47%, followed by MTMC-UR2CNet with an accuracy of 98.39%. Also, MTMC-AUR2CNet improves the multi-textural multi-class classification accuracy of the WOA-based DeepCNN classifier to 97.60% compared to MTMC-UR2CNet.", "journal": "Biomedical signal processing and control", "date": "2023-03-28", "authors": ["AnandbabuGopatoti", "PVijayalakshmi"], "doi": "10.1016/j.bspc.2023.104857\n10.1109/TMI.2018.2806086\n10.1016/j.bspc.2022.103860"}
{"title": "PDAtt-Unet: Pyramid Dual-Decoder Attention Unet for Covid-19 infection segmentation from CT-scans.", "abstract": "Since the emergence of the Covid-19 pandemic in late 2019, medical imaging has been widely used to analyze this disease. Indeed, CT-scans of the lungs can help diagnose, detect, and quantify Covid-19 infection. In this paper, we address the segmentation of Covid-19 infection from CT-scans. To improve the performance of the Att-Unet architecture and maximize the use of the Attention Gate, we propose the PAtt-Unet and DAtt-Unet architectures. PAtt-Unet aims to exploit the input pyramids to preserve the spatial awareness in all of the encoder layers. On the other hand, DAtt-Unet is designed to guide the segmentation of Covid-19 infection inside the lung lobes. We also propose to combine these two architectures into a single one, which we refer to as PDAtt-Unet. To overcome the blurry boundary pixels segmentation of Covid-19 infection, we propose a hybrid loss function. The proposed architectures were tested on four datasets with two evaluation scenarios (intra and cross datasets). Experimental results showed that both PAtt-Unet and DAtt-Unet improve the performance of Att-Unet in segmenting Covid-19 infections. Moreover, the combination architecture PDAtt-Unet led to further improvement. To Compare with other methods, three baseline segmentation architectures (Unet, Unet++, and Att-Unet) and three state-of-the-art architectures (InfNet, SCOATNet, and nCoVSegNet) were tested. The comparison showed the superiority of the proposed PDAtt-Unet trained with the proposed hybrid loss (PDEAtt-Unet) over all other methods. Moreover, PDEAtt-Unet is able to overcome various challenges in segmenting Covid-19 infections in four datasets and two evaluation scenarios.", "journal": "Medical image analysis", "date": "2023-03-27", "authors": ["FaresBougourzi", "CosimoDistante", "FadiDornaika", "AbdelmalikTaleb-Ahmed"], "doi": "10.1016/j.media.2023.102797\n10.1016/j.knosys.2020.106647\n10.3390/s21175878\n10.3390/jimaging7090189\n10.1016/j.eswa.2020.113459\n10.1007/s42979-021-00874-4\n10.1109/TMI.2020.2996645\n10.1186/s12967-021-02992-2\n10.1186/s40779-020-0233-6\n10.7326/M20-1495\n10.3390/diagnostics11020158\n10.1016/j.media.2021.102205\n10.1002/mp.14676\n10.1101/2020.05.20.20100362\n10.1016/j.patcog.2021.108168\n10.1148/radiol.2020200370\n10.1148/ryai.2019180031\n10.21037/qims-20-564\n10.3390/s21051742\n10.1109/TNNLS.2021.3126305\n10.1109/TMI.2020.3000314\n10.1109/TIP.2021.3058783\n10.1016/j.media.2021.101992\n10.1016/j.cell.2020.04.045\n10.1016/j.compbiomed.2021.104526"}
{"title": "Evidence of a cognitive bias in the quantification of COVID-19 with CT: an artificial intelligence randomised clinical trial.", "abstract": "Chest computed tomography (CT) has played a valuable, distinct role in the screening, diagnosis, and follow-up of COVID-19 patients. The quantification of COVID-19 pneumonia on CT has proven to be an important predictor of the treatment course and outcome of the patient although it remains heavily reliant on the radiologist's subjective perceptions. Here, we show that with the adoption of CT for COVID-19 management, a new type of psychophysical bias has emerged in radiology. A preliminary survey of 40 radiologists and a retrospective analysis of CT data from 109 patients from two hospitals revealed that radiologists overestimated the percentage of lung involvement by 10.23\u2009\u00b1\u20094.65% and 15.8\u2009\u00b1\u20096.6%, respectively. In the subsequent randomised controlled trial, artificial intelligence (AI) decision support reduced the absolute overestimation error (P\u2009<\u20090.001) from 9.5%\u2009\u00b1\u20096.6 (No-AI analysis arm, n\u2009=\u200938) to 1.0%\u2009\u00b1\u20095.2 (AI analysis arm, n\u2009=\u200938). These results indicate a human perception bias in radiology that has clinically meaningful effects on the quantitative analysis of COVID-19 on CT. The objectivity of AI was shown to be a valuable complement in mitigating the radiologist's subjectivity, reducing the overestimation tenfold.Trial registration: https://Clinicaltrial.gov . Identifier: NCT05282056, Date of registration: 01/02/2022.", "journal": "Scientific reports", "date": "2023-03-26", "authors": ["Bogdan ABercean", "AndreeaBirhala", "Paula GArdelean", "IoanaBarbulescu", "Marius MBenta", "Cristina DRasadean", "DanCostachescu", "CristianAvramescu", "AndreiTenescu", "StefanIarca", "Alexandru SBuburuzan", "MariusMarcu", "FlorinBirsasteanu"], "doi": "10.1038/s41598-023-31910-3\n10.1016/j.chest.2020.04.003\n10.1186/s43055-021-00525-x\n10.1186/s43168-021-00060-3\n10.1148/ryct.2020200047\n10.1007/s00330-020-06817-6\n10.1097/RLI.0000000000000672\n10.1038/s41598-021-87430-5\n10.3758/bf03198840\n10.1080/00221309.1936.9713146\n10.2307/1420573\n10.1287/mksc.20.4.405.9756\n10.1148/rg.2018170107\n10.1148/radiol.2017161659\n10.1016/j.chest.2020.08.512\n10.1186/s12879-021-05839-9\n10.1001/jamainternmed.2021.0269\n10.1016/j.acra.2020.08.038\n10.1177/0846537120938328\n10.3389/fmed.2021.643917\n10.1038/s41591-021-01595-0\n10.1016/j.obhdp.2018.12.005\n10.1038/s41598-022-18638-2\n10.18383/j.tom.2018.00055\n10.1038/s41592-019-0686-2"}
{"title": "Deep-Learning-Based Whole-Lung and Lung-Lesion Quantification Despite Inconsistent Ground Truth: Application to Computerized Tomography in SARS-CoV-2 Nonhuman Primate Models.", "abstract": "Animal modeling of infectious diseases such as coronavirus disease 2019 (COVID-19) is important for exploration of natural history, understanding of pathogenesis, and evaluation of countermeasures. Preclinical studies enable rigorous control of experimental conditions as well as pre-exposure baseline and longitudinal measurements, including medical imaging, that are often unavailable in the clinical research setting. Computerized tomography (CT) imaging provides important diagnostic, prognostic, and disease characterization to clinicians and clinical researchers. In that context, automated deep-learning systems for the analysis of CT imaging have been broadly proposed, but their practical utility has been limited. Manual outlining of the ground truth (i.e., lung-lesions) requires accurate distinctions between abnormal and normal tissues that often have vague boundaries and is subject to reader heterogeneity in interpretation. Indeed, this subjectivity is demonstrated as wide inconsistency in manual outlines among experts and from the same expert. The application of deep-learning data-science tools has been less well-evaluated in the preclinical setting, including in nonhuman primate (NHP) models of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection/COVID-19, in which the translation of human-derived deep-learning tools is challenging. The automated segmentation of the whole lung and lung lesions provides a potentially standardized and automated method to detect and quantify disease.\nWe used deep-learning-based quantification of the whole lung and lung lesions on CT scans of NHPs exposed to SARS-CoV-2. We proposed a novel multi-model ensemble technique to address the inconsistency in the ground truths for deep-learning-based automated segmentation of the whole lung and lung lesions. Multiple models were obtained by training the convolutional neural network (CNN) on different subsets of the training data instead of having a single model using the entire training dataset. Moreover, we employed a feature pyramid network (FPN), a CNN that provides predictions at different resolution levels, enabling the network to predict objects with wide size variations.\nWe achieved an average of 99.4 and 60.2% Dice coefficients for whole-lung and lung-lesion segmentation, respectively. The proposed multi-model FPN outperformed well-accepted methods U-Net (50.5%), V-Net (54.5%), and Inception (53.4%) for the challenging lesion-segmentation task. We show the application of segmentation outputs for longitudinal quantification of lung disease in SARS-CoV-2-exposed and mock-exposed NHPs.\nDeep-learning methods should be optimally characterized for and targeted specifically to preclinical research needs in terms of impact, automation, and dynamic quantification independently from purely clinical applications.", "journal": "Academic radiology", "date": "2023-03-26", "authors": ["Syed M SReza", "Winston TChu", "FatemehHomayounieh", "MaximBlain", "Fatemeh DFirouzabadi", "Pouria YAnari", "Ji HyunLee", "GabriellaWorwa", "Courtney LFinch", "Jens HKuhn", "AshkanMalayeri", "IanCrozier", "Bradford JWood", "Irwin MFeuerstein", "JeffreySolomon"], "doi": "10.1016/j.acra.2023.02.027\n10.1038/s41586-020-2787-6\n10.1038/s41572-020-0147-3\n10.3390/pathogens9030197\n10.1002/path.4444\n10.48550/arXiv.2004.1285220\n10.1371/journal.pone.0084599\n10.1097/HP.0000000000001280\n10.1117/12.2514000\n10.1117/12.2607154\n10.1007/s00134-020-06033-2\n10.48550/arXiv.2003.04655\n10.1109/CBMS.2014.59\n10.1007/978-3-030-59861-7_58"}
{"title": "MESTrans: Multi-scale embedding spatial transformer for medical image segmentation.", "abstract": "Transformers profiting from global information modeling derived from the self-attention mechanism have recently achieved remarkable performance in computer vision. In this study, a novel transformer-based medical image segmentation network called the multi-scale embedding spatial transformer (MESTrans) was proposed for medical image segmentation.\nFirst, a dataset called COVID-DS36 was created from 4369 computed tomography (CT) images of 36 patients from a partner hospital, of which 18 had COVID-19 and 18 did not. Subsequently, a novel medical image segmentation network was proposed, which introduced a self-attention mechanism to improve the inherent limitation of convolutional neural networks (CNNs) and was capable of adaptively extracting discriminative information in both global and local content. Specifically, based on U-Net, a multi-scale embedding block (MEB) and multi-layer spatial attention transformer (SATrans) structure were designed, which can dynamically adjust the receptive field in accordance with the input content. The spatial relationship between multi-level and multi-scale image patches was modeled, and the global context information was captured effectively. To make the network concentrate on the salient feature region, a feature fusion module (FFM) was established, which performed global learning and soft selection between shallow and deep features, adaptively combining the encoder and decoder features. Four datasets comprising CT images, magnetic resonance (MR) images, and H&E-stained slide images were used to assess the performance of the proposed network.\nExperiments were performed using four different types of medical image datasets. For the COVID-DS36 dataset, our method achieved a Dice similarity coefficient (DSC) of 81.23%. For the GlaS dataset, 89.95% DSC and 82.39% intersection over union (IoU) were obtained. On the Synapse dataset, the average DSC was 77.48% and the average Hausdorff distance (HD) was 31.69\u00a0mm. For the I2CVB dataset, 92.3% DSC and 85.8% IoU were obtained.\nThe experimental results demonstrate that the proposed model has an excellent generalization ability and outperforms other state-of-the-art methods. It is expected to be a potent tool to assist clinicians in auxiliary diagnosis and to promote the development of medical intelligence technology.", "journal": "Computer methods and programs in biomedicine", "date": "2023-03-26", "authors": ["YatongLiu", "YuZhu", "YingXin", "YananZhang", "DaweiYang", "TaoXu"], "doi": "10.1016/j.cmpb.2023.107493"}
{"title": "Machine learning COVID-19 detection from wearables.", "abstract": null, "journal": "The Lancet. Digital health", "date": "2023-03-25", "authors": ["BretNestor", "JarydHunter", "RaghuKainkaryam", "ErikDrysdale", "Jeffrey BInglis", "AllisonShapiro", "SujayNagaraj", "MarzyehGhassemi", "LucaFoschini", "AnnaGoldenberg"], "doi": "10.1016/S2589-7500(23)00045-6\n10.48550/arXiv.2205.13607"}
{"title": "COVID-19 diagnosis: A comprehensive review of pre-trained deep learning models based on feature extraction algorithm.", "abstract": "Due to the augmented rise of COVID-19, clinical specialists are looking for fast faultless diagnosis strategies to restrict Covid spread while attempting to lessen the computational complexity. In this way, swift diagnosis techniques for COVID-19 with high precision can offer valuable aid to clinical specialists. RT- PCR test is an expensive and tedious COVID diagnosis technique in practice. Medical imaging is feasible to diagnose COVID-19 by X-ray chest radiography to get around the shortcomings of RT-PCR. Through a variety of Deep Transfer-learning models, this research investigates the potential of Artificial Intelligence -based early diagnosis of COVID-19 via X-ray chest radiographs. With 10,192 normal and 3616 Covid X-ray chest radiographs, the deep transfer-learning models are optimized to further the accurate diagnosis. The x-ray chest radiographs undergo a data augmentation phase before developing a modified dataset to train the Deep Transfer-learning models. The Deep Transfer-learning architectures are trained using the extracted features from the Feature Extraction stage. During training, the classification of X-ray Chest radiographs based on feature extraction algorithm values is converted into a feature label set containing the classified image data with a feature string value representing the number of edges detected after edge detection. The feature label set is further tested with the SVM, KNN, NN, Naive Bayes and Logistic Regression classifiers to audit the quality metrics of the proposed model. The quality metrics include accuracy, precision, F1 score, recall and AUC. The Inception-V3 dominates the six Deep Transfer-learning models, according to the assessment results, with a training accuracy of 84.79% and a loss function of 2.4%. The performance of Cubic SVM was superior to that of the other SVM classifiers, with an AUC score of 0.99, precision of 0.983, recall of 0.8977, accuracy of 95.8%, and F1 score of 0.9384. Cosine KNN fared better than the other KNN classifiers with an AUC score of 0.95, precision of 0.974, recall of 0.777, accuracy of 90.8%, and F1 score of 0.864. Wide NN fared better than the other NN classifiers with an AUC score of 0.98, precision of 0.975, recall of 0.907, accuracy of 95.5%, and F1 score of 0.939. According to the findings, SVM classifiers topped other classifiers in terms of performance indicators like accuracy, precision, recall, F1-score, and AUC. The SVM classifiers reported better mean optimal scores compared to other classifiers. The performance assessment metrics uncover that the proposed methodology can aid in preliminary COVID diagnosis.", "journal": "Results in engineering", "date": "2023-03-23", "authors": ["Rahul GowthamPoola", "LahariPl", "Siva SankarY"], "doi": "10.1016/j.rineng.2023.101020\n10.1016/j.catena.2019.104426\n10.1021/acs.molpharmaceut.7b00578\n10.1109/TMI.2020.3040950\n10.1016/j.irbm.2020.05.003\n10.1007/s13089-009-0003-x\n10.1007/s10096-020-03901-z\n10.1007/s10489-020-01829-7\n10.1109/ACCESS.2020.3010287\n10.1080/07391102.2020.1767212"}
{"title": "Impact of inactivated COVID-19 vaccines on lung injury in B.1.617.2 (Delta) variant-infected patients.", "abstract": "Chest computerized tomography (CT) scan is an important strategy that quantifies the severity of COVID-19 pneumonia. To what extent inactivated COVID-19 vaccines could impact the COVID-19 pneumonia on chest CT is not clear.\nThis study recruited 357 SARS-COV-2 B.1.617.2 (Delta) variant-infected patients admitted to the Second Hospital of Nanjing from July to August 2021. An artificial intelligence-assisted CT imaging system was used to quantify the severity of COVID-19 pneumonia. We compared the volume of infection (VOI), percentage of infection (POI) and chest CT scores among patients with different vaccination statuses.\nOf the 357 Delta variant-infected patients included for analysis, 105 were unvaccinated, 72 were partially vaccinated and 180 were fully vaccinated. Fully vaccination had the least lung injuries when quantified by VOI (median VOI of 222.4\u00a0cm\nFully vaccination but not partially vaccination could significantly protect lung injury manifested on chest CT. Our study provides additional evidence to encourage a full course of vaccination.", "journal": "Annals of clinical microbiology and antimicrobials", "date": "2023-03-23", "authors": ["MiaoLai", "KaiWang", "ChengyuanDing", "YiYin", "XiaolingLin", "ChuanjunXu", "ZhiliangHu", "ZhihangPeng"], "doi": "10.1186/s12941-023-00569-z\n10.3389/fpubh.2020.00355\n10.1002/mp.14609\n10.1016/j.biopha.2020.110629\n10.1148/radiol.2020200370\n10.1007/s00134-020-05991-x\n10.1148/radiol.2020200642\n10.1097/RLI.0000000000000689\n10.2214/AJR.20.22976\n10.1016/j.ijid.2022.01.030\n10.1080/22221751.2021.1969291\n10.1007/s00330-020-06817-6\n10.1007/s00330-021-08435-2\n10.1007/s00330-021-08432-5\n10.1002/jmv.27428\n10.1136/bmj.m1443\n10.1186/s12916-022-02397-y\n10.1001/jamainternmed.2021.7949\n10.1016/j.ijid.2021.11.009\n10.1093/cid/ciz1024\n10.1002/jmv.21771\n10.1093/infdis/jir494\n10.1128/JCM.00742-14\n10.1038/s41467-020-19057-5\n10.1016/j.ajpath.2020.07.001\n10.1007/s11739-021-02786-w\n10.1093/cid/ciaa851\n10.1016/j.jinf.2020.03.037\n10.1007/s10753-020-01337-3\n10.3390/vaccines9080825\n10.1101/2021.03.16.21253686"}
{"title": "Computerization of the Work of General Practitioners: Mixed Methods Survey of Final-Year Medical Students in Ireland.", "abstract": "The potential for digital health technologies, including machine learning (ML)-enabled tools, to disrupt the medical profession is the subject of ongoing debate within biomedical informatics.\nWe aimed to describe the opinions of final-year medical students in Ireland regarding the potential of future technology to replace or work alongside general practitioners (GPs) in performing key tasks.\nBetween March 2019 and April 2020, using a convenience sample, we conducted a mixed methods paper-based survey of final-year medical students. The survey was administered at 4 out of 7 medical schools in Ireland across each of the 4 provinces in the country. Quantitative data were analyzed using descriptive statistics and nonparametric tests. We used thematic content analysis to investigate free-text responses.\nIn total, 43.1% (252/585) of the final-year students at 3 medical schools responded, and data collection at 1 medical school was terminated due to disruptions associated with the COVID-19 pandemic. With regard to forecasting the potential impact of artificial intelligence (AI)/ML on primary care 25 years from now, around half (127/246, 51.6%) of all surveyed students believed the work of GPs will change minimally or not at all. Notably, students who did not intend to enter primary care predicted that AI/ML will have a great impact on the work of GPs.\nWe caution that without a firm curricular foundation on advances in AI/ML, students may rely on extreme perspectives involving self-preserving optimism biases that demote the impact of advances in technology on primary care on the one hand and technohype on the other. Ultimately, these biases may lead to negative consequences in health care. Improvements in medical education could help prepare tomorrow's doctors to optimize and lead the ethical and evidence-based implementation of AI/ML-enabled tools in medicine for enhancing the care of tomorrow's patients.", "journal": "JMIR medical education", "date": "2023-03-21", "authors": ["CharlotteBlease", "AnnaKharko", "MichaelBernstein", "ColinBradley", "MuirisHouston", "IanWalsh", "KennethD Mandl"], "doi": "10.2196/42639\n10.1016/j.techfore.2016.08.019\n10.1001/jama.2017.15028\n10.1001/jama.2015.18421\n10.1056/nejmp1606181\n10.1371/journal.pone.0207418\n10.1371/journal.pone.0207418\n10.1016/j.artmed.2019.101753\n10.1177/2055207620968355?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.1177/2055207620968355\n10.3389/fpubh.2021.623088\n10.1002/jdd.12385\n10.1177/23821205211024078?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.1177/23821205211024078\n10.1111/jdv.16812\n10.1186/s13244-019-0830-7\n10.1007/s00330-018-5601-1\n10.2196/19827\n10.2466/pms.1987.64.2.359\n10.1136/bmjhci-2021-100480\n10.1371/journal.pone.0239947\n10.1371/journal.pone.0239947\n10.1136/bmjhci-2019-100015\n10.7326/0003-4819-157-1-201207030-00450?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.7326/0003-4819-157-1-201207030-00450\n10.3399/bjgp17x693509\n10.1136/bmjopen-2017-019966\n10.3399/bjgp18x694853\n10.37765/ajmc.2022.88771\n10.1176/appi.ps.201900555\n10.1001/jamahealthforum.2021.3282\n10.1371/journal.pone.0173160\n10.1371/journal.pone.0173160\n10.1371/journal.pone.0166718\n10.1371/journal.pone.0166718\n10.1056/nejmoa1712231\n10.1007/s11886-020-01431-w\n10.1258/135763303771005207\n10.1001/jamanetworkopen.2020.5873\n10.1016/j.jval.2017.01.014\n10.1089/tmj.2019.0179"}
{"title": "COVID-19 and pneumonia diagnosis from chest X-ray images using convolutional neural networks.", "abstract": "X-ray is a useful imaging modality widely utilized for diagnosing COVID-19 virus that infected a high number of people all around the world. The manual examination of these X-ray images may cause problems especially when there is lack of medical staff. Usage of deep learning models is known to be helpful for automated diagnosis of COVID-19 from the X-ray images. However, the widely used convolutional neural network architectures typically have many layers causing them to be computationally expensive. To address these problems, this study aims to design a lightweight differential diagnosis model based on convolutional neural networks. The proposed model is designed to classify the X-ray images belonging to one of the four classes that are Healthy, COVID-19, viral pneumonia, and bacterial pneumonia. To evaluate the model performance, accuracy, precision, recall, and F1-Score were calculated. The performance of the proposed model was compared with those obtained by applying transfer learning to the widely used convolutional neural network models. The results showed that the proposed model with low number of computational layers outperforms the pre-trained benchmark models, achieving an accuracy value of 89.89% while the best pre-trained model (Efficient-Net B2) achieved accuracy of 85.7%. In conclusion, the proposed lightweight model achieved the best overall result in classifying lung diseases allowing it to be used on devices with limited computational power. On the other hand, all the models showed a poor precision on viral pneumonia class and confusion in distinguishing it from bacterial pneumonia class, thus a decrease in the overall accuracy.", "journal": "Network modeling and analysis in health informatics and bioinformatics", "date": "2023-03-21", "authors": ["MuhabHariri", "ErcanAv\u015far"], "doi": "10.1007/s13721-023-00413-6\n10.31803//tg-20210422205610\n10.1016/j.bspc.2020.102257\n10.1109/ACCESS.2020.3010287\n10.1007/s12553-022-00700-8\n10.1016/j.rinp.2021.105045\n10.1109/ACCESS.2021.3078241\n10.1016/j.bspc.2022.103677\n10.1007/s11042-022-12484-0\n10.3390/app12136364\n10.1109/JAS.2020.1003393\n10.1016/j.compbiomed.2022.106083\n10.1007/s11548-020-02305-w\n10.1007/s11263-015-0816-y\n10.31803/tg-20190712095507\n10.1016/j.asoc.2021.107522\n10.3390/ijerph182111086\n10.1109/ACCESS.2019.2892795\n10.1016/j.micinf.2020.05.016"}
{"title": "Computed tomography-based COVID-19 triage through a deep neural network using mask-weighted global average pooling.", "abstract": "There is an urgent need to find an effective and accurate method for triaging coronavirus disease 2019 (COVID-19) patients from millions or billions of people. Therefore, this study aimed to develop a novel deep-learning approach for COVID-19 triage based on chest computed tomography (CT) images, including normal, pneumonia, and COVID-19 cases.\nA total of 2,809 chest CT scans (1,105 COVID-19, 854 normal, and 850 non-3COVID-19 pneumonia cases) were acquired for this study and classified into the training set (n = 2,329) and test set (n = 480). A U-net-based convolutional neural network was used for lung segmentation, and a mask-weighted global average pooling (GAP) method was proposed for the deep neural network to improve the performance of COVID-19 classification between COVID-19 and normal or common pneumonia cases.\nThe results for lung segmentation reached a dice value of 96.5% on 30 independent CT scans. The performance of the mask-weighted GAP method achieved the COVID-19 triage with a sensitivity of 96.5% and specificity of 87.8% using the testing dataset. The mask-weighted GAP method demonstrated 0.9% and 2% improvements in sensitivity and specificity, respectively, compared with the normal GAP. In addition, fusion images between the CT images and the highlighted area from the deep learning model using the Grad-CAM method, indicating the lesion region detected using the deep learning method, were drawn and could also be confirmed by radiologists.\nThis study proposed a mask-weighted GAP-based deep learning method and obtained promising results for COVID-19 triage based on chest CT images. Furthermore, it can be considered a convenient tool to assist doctors in diagnosing COVID-19.", "journal": "Frontiers in cellular and infection microbiology", "date": "2023-03-21", "authors": ["Hong-TaoZhang", "Ze-YuSun", "JuanZhou", "ShenGao", "Jing-HuiDong", "YuanLiu", "XuBai", "Jin-LinMa", "MingLi", "GuangLi", "Jian-MingCai", "Fu-GengSheng"], "doi": "10.3389/fcimb.2023.1116285\n10.1148/radiol.2020200642\n10.1118/1.3528204\n10.1016/j.jacr.2020.03.006\n10.1016/j.compbiomed.2022.106439\n10.1109/ICIP42928.2021.9506119\n10.1016/j.displa.2022.102150\n10.1148/radiol.2021211583\n10.1056/NEJMoa2002032\n10.1038/s41467-020-17971-2\n10.1016/j.imu.2020.100412\n10.1109/EMBC.2018.8512356\n10.1007/s11606-020-05762-w\n10.1615/CritRevBiomedEng.2022042286\n10.3390/s19092167\n10.1007/s00521-022-08127-y\n10.1177/00368504221135457\n10.1007/s00330-020-07042-x\n10.1167/tvst.12.1.22\n10.1007/s11042-022-13843-7\n10.1007/s00521-022-08099-z\n10.1016/j.cmpb.2022.107321\n10.1007/s00330-020-07022-1\n10.1007/s11356-022-24853-1\n10.1016/j.media.2022.102605\n10.1016/j.compbiomed.2021.105127\n10.1016/j.engappai.2023.105820\n10.1109/EMBC.2019.8856972\n10.1002/mp.16217\n10.1007/s00330-020-06801-0\n10.1088/1741-2552/acb089"}
{"title": "An AI-enabled research support tool for the classification system of COVID-19.", "abstract": "The outbreak of COVID-19, a little more than 2 years ago, drastically affected all segments of society throughout the world. While at one end, the microbiologists, virologists, and medical practitioners were trying to find the cure for the infection; the Governments were laying emphasis on precautionary measures like lockdowns to lower the spread of the virus. This pandemic is perhaps also the first one of its kind in history that has research articles in all possible areas as like: medicine, sociology, psychology, supply chain management, mathematical modeling, etc. A lot of work is still continuing in this area, which is very important also for better preparedness if such a situation arises in future. The objective of the present study is to build a research support tool that will help the researchers swiftly identify the relevant literature on a specific field or topic regarding COVID-19 through a hierarchical classification system. The three main tasks done during this study are data preparation, data annotation and text data classification through bi-directional long short-term memory (bi-LSTM).", "journal": "Frontiers in public health", "date": "2023-03-21", "authors": ["ArtiTiwari", "KamanasishBhattacharjee", "MilliePant", "ShilpaSrivastava", "VaclavSnasel"], "doi": "10.3389/fpubh.2023.1124998\n10.1186/s12859-019-2607-x\n10.1016/j.simpa.2022.100444\n10.1016/j.commatsci.2021.111121\n10.48550/arXiv.2004.10706\n10.1101/2020.02.27.20028027\n10.1371/journal.pone.0232391\n10.1109/TCBB.2021.3065361\n10.1007/s00330-021-07715-1\n10.1101/2020.04.03.20052936\n10.1016/j.patter.2020.100022\n10.1108/AJIM-05-2020-0134\n10.48550/arXiv.2004.00253\n10.1136/bmj.m993\n10.3389/fphy.2020.00296\n10.1016/S0140-6736(20)30459-1\n10.1057/s41599-022-01039-1\n10.2139/ssrn.3564898\n10.1101/2020.01.30.20019836\n10.24171/j.phrp.2020.11.1.01\n10.4103/jmms.jmms_12_20\n10.1093/jtm/taaa021\n10.1097/JCMA.0000000000000270\n10.1101/2020.02.27.20027169\n10.1101/2020.03.11.20034512\n10.1371/journal.pone.0230405\n10.1016/S1473-3099(20)30144-4\n10.3390/biology9030050\n10.3348/kjr.2020.0181\n10.3348/kjr.2020.0157\n10.12932/AP-200220-0772\n10.3348/kjr.2020.0163\n10.1016/j.psychres.2020.112954\n10.30773/pi.2020.0047\n10.1016/S2215-0366(20)30077-8\n10.1017/S0950268820001107\n10.1016/j.bbi.2020.03.007\n10.1016/j.vaccine.2020.04.034\n10.1101/2020.02.13.20022830\n10.3390/biom10020331\n10.1007/s11831-021-09647-x\n10.3390/info12070272\n10.3389/fams.2020.551138\n10.1162/neco.1997.9.8.1735"}
{"title": "Artificial intelligence-based point-of-care lung ultrasound for screening COVID-19 pneumoniae: Comparison with CT scans.", "abstract": "Although lung ultrasound has been reported to be a portable, cost-effective, and accurate method to detect pneumonia, it has not been widely used because of the difficulty in its interpretation. Here, we aimed to investigate the effectiveness of a novel artificial intelligence-based automated pneumonia detection method using point-of-care lung ultrasound (AI-POCUS) for the coronavirus disease 2019 (COVID-19).\nWe enrolled consecutive patients admitted with COVID-19 who underwent computed tomography (CT) in August and September 2021. A 12-zone AI-POCUS was performed by a novice observer using a pocket-size device within 24 h of the CT scan. Fifteen control subjects were also scanned. Additionally, the accuracy of the simplified 8-zone scan excluding the dorsal chest, was assessed. More than three B-lines detected in one lung zone were considered zone-level positive, and the presence of positive AI-POCUS in any lung zone was considered patient-level positive. The sample size calculation was not performed given the retrospective all-comer nature of the study.\nA total of 577 lung zones from 56 subjects (59.4 \u00b1 14.8 years, 23% female) were evaluated using AI-POCUS. The mean number of days from disease onset was 9, and 14% of patients were under mechanical ventilation. The CT-validated pneumonia was seen in 71.4% of patients at total 577 lung zones (53.3%). The 12-zone AI-POCUS for detecting CT-validated pneumonia in the patient-level showed the accuracy of 94.5% (85.1%- 98.1%), sensitivity of 92.3% (79.7%- 97.3%), specificity of 100% (80.6%- 100%), positive predictive value of 95.0% (89.6% - 97.7%), and Kappa of 0.33 (0.27-0.40). When simplified with 8-zone scan, the accuracy, sensitivity, and sensitivity were 83.9% (72.2%- 91.3%), 77.5% (62.5%- 87.7%), and 100% (80.6%- 100%), respectively. The zone-level accuracy, sensitivity, and specificity of AI-POCUS were 65.3% (61.4%- 69.1%), 37.2% (32.0%- 42.7%), and 97.8% (95.2%- 99.0%), respectively.\nAI-POCUS using the novel pocket-size ultrasound system showed excellent agreement with CT-validated COVID-19 pneumonia, even when used by a novice observer.", "journal": "PloS one", "date": "2023-03-18", "authors": ["YumiKuroda", "TomohiroKaneko", "HitomiYoshikawa", "SaoriUchiyama", "YuichiNagata", "YasushiMatsushita", "MakotoHiki", "TohruMinamino", "KazuhisaTakahashi", "HiroyukiDaida", "NobuyukiKagiyama"], "doi": "10.1371/journal.pone.0281127\n10.1038/s41577-021-00522-1\n10.1038/s41579-020-00459-7\n10.1001/jama.2020.12839\n10.1016/S2214-109X(20)30068-1\n10.1016/S0140-6736(20)31093-X\n10.1164/rccm.201802-0236CI\n10.1007/s00134-012-2513-4\n10.21037/jtd.2016.09.38\n10.1038/s41591-020-0842-3\n10.1161/JAHA.119.012788\n10.1109/TMI.2020.2994459\n10.1016/j.annemergmed.2021.07.020\n10.1002/jum.15849\n10.15557/JoU.2021.0046\n10.1080/01621459.1927.10502953\n10.1016/j.mayocp.2016.08.023\n10.1164/ajrccm.156.5.96-07096\n10.1007/s00134-020-06186-0\n10.1183/13993003.04283-2020\n10.1007/s00134-021-06373-7\n10.3390/diagnostics11081351\n10.1002/jmv.26218\n10.1016/j.jcmg.2020.07.015\n10.1530/ERP-18-0081\n10.1109/TUFFC.2020.3002249\n10.1002/jum.15935"}
{"title": "OzNet: A New Deep Learning Approach for Automated Classification of COVID-19 Computed Tomography Scans.", "abstract": "Coronavirus disease 2019 (COVID-19) is spreading rapidly around the world. Therefore, the classification of computed tomography (CT) scans alleviates the workload of experts, whose workload increased considerably during the pandemic. Convolutional neural network (CNN) architectures are successful for the classification of medical images. In this study, we have developed a new deep CNN architecture called OzNet. Moreover, we have compared it with pretrained architectures namely AlexNet, DenseNet201, GoogleNet, NASNetMobile, ResNet-50, SqueezeNet, and VGG-16. In addition, we have compared the classification success of three preprocessing methods with raw CT scans. We have not only classified the raw CT scans, but also have performed the classification with three different preprocessing methods, which are discrete wavelet transform (DWT), intensity adjustment, and gray to color red, green, blue image conversion on the data sets. Furthermore, it is known that the architecture's performance increases with the use of DWT preprocessing method rather than using the raw data set. The results are extremely promising with the CNN algorithms using the COVID-19 CT scans processed with the DWT. The proposed DWT-OzNet has achieved a high classification performance of more than 98.8% for each calculated metric.", "journal": "Big data", "date": "2023-03-18", "authors": ["OznurOzaltin", "OzgurYeniay", "AbdulhamitSubasi"], "doi": "10.1089/big.2022.0042"}
{"title": "Deep SVDD and Transfer Learning for COVID-19 Diagnosis Using CT Images.", "abstract": "The novel coronavirus disease (COVID-19), which appeared in Wuhan, China, is spreading rapidly worldwide. Health systems in many countries have collapsed as a result of this pandemic, and hundreds of thousands of people have died due to acute respiratory distress syndrome caused by this virus. As a result, diagnosing COVID-19 in the early stages of infection is critical in the fight against the disease because it saves the patient's life and prevents the disease from spreading. In this study, we proposed a novel approach based on transfer learning and deep support vector data description (DSVDD) to distinguish among COVID-19, non-COVID-19 pneumonia, and intact CT images. Our approach consists of three models, each of which can classify one specific category as normal and the other as anomalous. To our knowledge, this is the first study to use the one-class DSVDD and transfer learning to diagnose lung disease. For the proposed approach, we used two scenarios: one with pretrained VGG16 and one with ResNet50. The proposed models were trained using data gathered with the assistance of an expert radiologist from three internet-accessible sources in end-to-end fusion using three split data ratios. Based on training with 70%, 50%, and 30% of the data, the proposed VGG16 models achieved (0.8281, 0.9170, and 0.9294) for the F1 score, while the proposed ResNet50 models achieved (0.9109, 0.9188, and 0.9333).", "journal": "Computational intelligence and neuroscience", "date": "2023-03-18", "authors": ["Akram AAlhadad", "Reham RMostafa", "Hazem MEl-Bakry"], "doi": "10.1155/2023/6070970\n10.1056/nejmoa2001017\n10.1016/s0140-6736(20)30183-5\n10.32604/cmc.2022.024193\n10.1016/j.ijid.2020.01.009\n10.1155/2022/5681574\n10.1155/2021/2158184\n10.1016/j.compbiomed.2022.106156\n10.1016/j.ejrad.2020.108961\n10.1016/j.bspc.2021.102588\n10.1016/j.media.2017.07.005\n10.3390/electronics9091439\n10.1016/j.cmpb.2020.105581\n10.1016/j.compbiomed.2022.105233\n10.1146/annurev-bioeng-071516-044442\n10.1016/j.ejrad.2020.109041\n10.1016/j.compbiomed.2020.103795\n10.1016/j.compbiomed.2020.104037\n10.1109/tip.2021.3058783\n10.1371/journal.pone.0257119\n10.1109/tmi.2020.2994908\n10.1109/access.2020.3005510\n10.1007/s00330-021-07715-1\n10.1016/j.asoc.2020.106885\n10.1007/s13246-020-00865-4\n10.1109/tmi.2020.2996256\n10.1038/s41598-021-03287-8\n10.1007/s10916-022-01868-2\n10.3390/s21020455"}
{"title": "CCTCOVID: COVID-19 detection from chest X-ray images using Compact Convolutional Transformers.", "abstract": "COVID-19 is a novel virus that attacks the upper respiratory tract and the lungs. Its person-to-person transmissibility is considerably rapid and this has caused serious problems in approximately every facet of individuals' lives. While some infected individuals may remain completely asymptomatic, others have been frequently witnessed to have mild to severe symptoms. In addition to this, thousands of death cases around the globe indicated that detecting COVID-19 is an urgent demand in the communities. Practically, this is prominently done with the help of screening medical images such as Computed Tomography (CT) and X-ray images. However, the cumbersome clinical procedures and a large number of daily cases have imposed great challenges on medical practitioners. Deep Learning-based approaches have demonstrated a profound potential in a wide range of medical tasks. As a result, we introduce a transformer-based method for automatically detecting COVID-19 from X-ray images using Compact Convolutional Transformers (CCT). Our extensive experiments prove the efficacy of the proposed method with an accuracy of 99.22% which outperforms the previous works.", "journal": "Frontiers in public health", "date": "2023-03-17", "authors": ["AbdolrezaMarefat", "MahdiehMarefat", "JavadHassannataj Joloudari", "Mohammad AliNematollahi", "RezaLashgari"], "doi": "10.3389/fpubh.2023.1025746\n10.1007/s00405-020-06319-7\n10.1007/s00415-020-10067-3\n10.3390/v12040372\n10.1016/j.eswa.2020.114054\n10.1016/j.mlwa.2021.100134\n10.1145/3065386\n10.1007/s13244-018-0639-9\n10.1016/j.neucom.2016.12.038\n10.3390/s20020342\n10.1186/s12880-022-00793-7\n10.1016/j.asoc.2018.05.018\n10.18653/v1/2020.emnlp-demos.6\n10.18653/v1/D18-1045\n10.1002/widm.1412\n10.1007/s11432-018-9941-6\n10.1145/3505244\n10.1016/j.media.2017.07.005\n10.1038/s41598-020-76550-z\n10.1016/j.asoc.2020.106691\n10.1007/s11517-020-02299-2\n10.1016/j.imu.2020.100412\n10.1007/s10044-021-00984-y\n10.1007/s10489-020-01904-z\n10.3390/jpm12020310\n10.1109/JTEHM.2021.3134096\n10.1109/ACCESS.2021.3058854\n10.1007/s00264-020-04609-7\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.109944\n10.1007/s10489-020-01826-w\n10.1016/j.bspc.2021.102622\n10.1038/s41598-021-93543-8\n10.1016/j.patcog.2021.108255\n10.1016/j.compbiomed.2022.106483\n10.5555/3295222.3295349\n10.3389/fcvm.2021.760178\n10.1109/TMI.2020.2995965\n10.1148/radiol.2020201473\n10.1016/j.chaos.2020.110120\n10.1016/j.ipm.2022.103025\n10.1016/j.bspc.2022.103848\n10.36548/jismac.2021.2.006\n10.1016/j.radi.2022.03.011\n10.1016/j.bbe.2020.08.008\n10.32604/cmc.2021.012955\n10.1016/j.compbiomed.2020.103795\n10.3390/diagnostics11101887\n10.1007/s11042-022-12156-z\n10.1002/cpe.6747"}
{"title": "IRCM-Caps: An X-ray image detection method for COVID-19.", "abstract": "COVID-19 is ravaging the world, but traditional reverse transcription-polymerase reaction (RT-PCR) tests are time-consuming and have a high false-negative rate and lack of medical equipment. Therefore, lung imaging screening methods are proposed to diagnose COVID-19 due to its fast test speed. Currently, the commonly used convolutional neural network (CNN) model requires a large number of datasets, and the accuracy of the basic capsule network for multiple classification is limital. For this reason, this paper proposes a novel model based on CNN and CapsNet.\nThe proposed model integrates CNN and CapsNet. And attention mechanism module and multi-branch lightweight module are applied to enhance performance. Use the contrast adaptive histogram equalization (CLAHE) algorithm to preprocess the image to enhance image contrast. The preprocessed images are input into the network for training, and ReLU was used as the activation function to adjust the parameters to achieve the optimal.\nThe test dataset includes 1200 X-ray images (400 COVID-19, 400 viral pneumonia, and 400 normal), and we replace CNN of VGG16, InceptionV3, Xception, Inception-Resnet-v2, ResNet50, DenseNet121, and MoblieNetV2 and integrate with CapsNet. Compared with CapsNet, this network improves 6.96%, 7.83%, 9.37%, 10.47%, and 10.38% in accuracy, area under the curve (AUC), recall, and F1 scores, respectively. In the binary classification experiment, compared with CapsNet, the accuracy, AUC, accuracy, recall rate, and F1 score were increased by 5.33%, 5.34%, 2.88%, 8.00%, and 5.56%, respectively.\nThe proposed embedded the advantages of traditional convolutional neural network and capsule network and has a good classification effect on small COVID-19 X-ray image dataset.", "journal": "The clinical respiratory journal", "date": "2023-03-17", "authors": ["ShuoQiu", "JinlinMa", "ZipingMa"], "doi": "10.1111/crj.13599\n10.1016/j.eswa.2019.01.048\n10.1016/j.imu.2020.100360\n10.1016/j.patrec.2020.09.010\n10.1016/j.chaos.2021.110713\n10.1016/j.chaos.2020.110122\n10.1002/ima.22566\n10.1016/j.compbiomed.2021.104399\n10.1016/j.knosys.2020.105542\n10.5815/ijigsp.2020.02.04\n10.3389/frai.2021.598932\n10.1016/j.measurement.2021.110289\n10.1101/2020.03.12.20027185\n10.1007/s10044-021-00984-y\n10.1038/s41598-020-76550-z\n10.1183/13993003.00775-2020\n10.1016/j.bspc.2021.103272\n10.1016/j.bspc.2022.104268\n10.1016/j.bbe.2022.11.003\n10.1016/j.eswa.2022.118576\n10.1016/j.compeleceng.2022.108479"}
{"title": "Automated prediction of COVID-19 severity upon admission by chest X-ray images and clinical metadata aiming at accuracy and explainability.", "abstract": "In the past few years COVID-19 posed a huge threat to healthcare systems around the world. One of the first waves of the pandemic hit Northern Italy severely resulting in high casualties and in the near breakdown of primary care. Due to these facts, the Covid CXR Hackathon-Artificial Intelligence for Covid-19 prognosis: aiming at accuracy and explainability challenge had been launched at the beginning of February 2022, releasing a new imaging dataset with additional clinical metadata for each accompanying chest X-ray (CXR). In this article we summarize our techniques at correctly diagnosing chest X-ray images collected upon admission for severity of COVID-19 outcome. In addition to X-ray imagery, clinical metadata was provided and the challenge also aimed at creating an explainable model. We created a best-performing, as well as, an explainable model that makes an effort to map clinical metadata to image features whilst predicting the prognosis. We also did many ablation studies in order to identify crucial parts of the models and the predictive power of each feature in the datasets. We conclude that CXRs at admission do not help the predicting power of the metadata significantly by itself and contain mostly information that is also mutually present in the blood samples and other clinical factors collected at admission.", "journal": "Scientific reports", "date": "2023-03-16", "authors": ["AlexOlar", "Andr\u00e1sBiricz", "ZsoltBed\u0151h\u00e1zi", "Bendeg\u00fazSulyok", "P\u00e9terPollner", "Istv\u00e1nCsabai"], "doi": "10.1038/s41598-023-30505-2\n10.1155/2021/6658058\n10.1016/j.media.2021.102046\n10.1016/j.media.2021.102216\n10.3390/bdcc5040073\n10.1136/bmjopen-2020-042946\n10.1016/j.clinimag.2020.04.001\n10.1007/s11547-022-01456-x\n10.1109/JBHI.2021.3069169\n10.1016/j.clinimag.2021.06.039\n10.1007/s00330-020-07504-2\n10.3390/app11167174\n10.1109/TMI.2020.2993291\n10.3390/diagnostics12010025\n10.3390/ijerph17186933\n10.1080/23311916.2022.2124635\n10.1038/s41598-021-95561-y\n10.1038/s41598-021-03287-8\n10.1038/s42256-021-00307-0\n10.1371/journal.pone.0271931\n10.1007/s00330-020-07270-1\n10.1016/j.patcog.2021.108243\n10.1162/neco.1997.9.8.1735\n10.11613/BM.2012.031\n10.1148/radiol.2020201754\n10.1016/j.inffus.2019.12.001"}
{"title": "Implementation of deep learning artificial intelligence in vision-threatening disease screenings for an underserved community during COVID-19.", "abstract": "Age-related macular degeneration, diabetic retinopathy, and glaucoma are vision-threatening diseases that are leading causes of vision loss. Many studies have validated deep learning artificial intelligence for image-based diagnosis of vision-threatening diseases. Our study prospectively investigated deep learning artificial intelligence applications in student-run non-mydriatic screenings for an underserved, primarily Hispanic community during COVID-19.\nFive supervised student-run community screenings were held in West New York, New Jersey. Participants underwent non-mydriatic 45-degree retinal imaging by medical students. Images were uploaded to a cloud-based deep learning artificial intelligence for vision-threatening disease referral. An on-site tele-ophthalmology grader and remote clinical ophthalmologist graded images, with adjudication by a senior ophthalmologist to establish the gold standard diagnosis, which was used to assess the performance of deep learning artificial intelligence.\nA total of 385 eyes from 195 screening participants were included (mean age 52.43\u2009\u2009\u00b1\u2009\u200914.5 years, 40.0% female). A total of 48 participants were referred for at least one vision-threatening disease. Deep learning artificial intelligence marked 150/385 (38.9%) eyes as ungradable, compared to 10/385 (2.6%) ungradable as per the human gold standard (\nDeep learning artificial intelligence can increase the efficiency and accessibility of vision-threatening disease screenings, particularly in underserved communities. Deep learning artificial intelligence should be adaptable to different environments. Consideration should be given to how deep learning artificial intelligence can best be utilized in a real-world application, whether in computer-aided or autonomous diagnosis.", "journal": "Journal of telemedicine and telecare", "date": "2023-03-14", "authors": ["ArethaZhu", "PriyaTailor", "RashikaVerma", "IsisZhang", "BrianSchott", "CatherineYe", "BernardSzirth", "MiriamHabiel", "Albert SKhouri"], "doi": "10.1177/1357633X231158832"}
{"title": "COVID-Net USPro: An Explainable Few-Shot Deep Prototypical Network for COVID-19 Screening Using Point-of-Care Ultrasound.", "abstract": "As the Coronavirus Disease 2019 (COVID-19) continues to impact many aspects of life and the global healthcare systems, the adoption of rapid and effective screening methods to prevent the further spread of the virus and lessen the burden on healthcare providers is a necessity. As a cheap and widely accessible medical image modality, point-of-care ultrasound (POCUS) imaging allows radiologists to identify symptoms and assess severity through visual inspection of the chest ultrasound images. Combined with the recent advancements in computer science, applications of deep learning techniques in medical image analysis have shown promising results, demonstrating that artificial intelligence-based solutions can accelerate the diagnosis of COVID-19 and lower the burden on healthcare professionals. However, the lack of large, well annotated datasets poses a challenge in developing effective deep neural networks, especially in the case of rare diseases and new pandemics. To address this issue, we present COVID-Net USPro, an explainable few-shot deep prototypical network that is designed to detect COVID-19 cases from very few ultrasound images. Through intensive quantitative and qualitative assessments, the network not only demonstrates high performance in identifying COVID-19 positive cases, using an explainability component, but it is also shown that the network makes decisions based on the actual representative patterns of the disease. Specifically, COVID-Net USPro achieves 99.55% overall accuracy, 99.93% recall, and 99.83% precision for COVID-19-positive cases when trained with only five shots. In addition to the quantitative performance assessment, our contributing clinician with extensive experience in POCUS interpretation verified the analytic pipeline and results, ensuring that the network's decisions are based on clinically relevant image patterns integral to COVID-19 diagnosis. We believe that network explainability and clinical validation are integral components for the successful adoption of deep learning in the medical field. As part of the COVID-Net initiative, and to promote reproducibility and foster further innovation, the network is open-sourced and available to the public.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-03-12", "authors": ["JessySong", "AshkanEbadi", "AdrianFlorea", "PengchengXi", "St\u00e9phaneTremblay", "AlexanderWong"], "doi": "10.3390/s23052621\n10.31083/j.fbl2707198\n10.1002/14651858.CD013705.pub2\n10.1038/s41598-021-99015-3\n10.1038/s41598-020-76550-z\n10.3389/fmed.2021.729287\n10.3389/fmed.2020.608525\n10.18653/v1/D19-1045\n10.1109/ICCV.2017.74\n10.3389/fmed.2021.821120\n10.1016/j.compbiomed.2020.103792\n10.1016/j.patrec.2020.09.010\n10.1016/j.bspc.2021.102920\n10.1371/journal.pone.0255886\n10.1016/j.patcog.2020.107700\n10.48550/ARXIV.2109.03793\n10.1007/s13534-017-0021-8\n10.1186/s12911-020-01332-6\n10.1109/TPAMI.2019.2918284\n10.1378/chest.09-0001"}
{"title": "Deep Learning Algorithms with LIME and Similarity Distance Analysis on COVID-19 Chest X-ray Dataset.", "abstract": "In the last few years, many types of research have been conducted on the most harmful pandemic, COVID-19. Machine learning approaches have been applied to investigate chest X-rays of COVID-19 patients in many respects. This study focuses on the deep learning algorithm from the standpoint of feature space and similarity analysis. Firstly, we utilized Local Interpretable Model-agnostic Explanations (LIME) to justify the necessity of the region of interest (ROI) process and further prepared ROI via U-Net segmentation that masked out non-lung areas of images to prevent the classifier from being distracted by irrelevant features. The experimental results were promising, with detection performance reaching an overall accuracy of 95.5%, a sensitivity of 98.4%, a precision of 94.7%, and an F1 score of 96.5% on the COVID-19 category. Secondly, we applied similarity analysis to identify outliers and further provided an objective confidence reference specific to the similarity distance to centers or boundaries of clusters while inferring. Finally, the experimental results suggested putting more effort into enhancing the low-accuracy subspace locally, which is identified by the similarity distance to the centers. The experimental results were promising, and based on those perspectives, our approach could be more flexible to deploy dedicated classifiers specific to different subspaces instead of one rigid end-to-end black box model for all feature space.", "journal": "International journal of environmental research and public health", "date": "2023-03-12", "authors": ["Kuan-YungChen", "Hsi-ChiehLee", "Tsung-ChiehLin", "Chih-YingLee", "Zih-PingHo"], "doi": "10.3390/ijerph20054330\n10.1021/acsnano.0c02624\n10.1148/radiol.2020200432\n10.1148/rg.2017160130\n10.1016/j.media.2017.07.005\n10.1109/TMI.2016.2535302\n10.3390/app10020559\n10.1109/ACCESS.2020.3010287\n10.1038/s41598-020-76550-z\n10.1109/TII.2021.3057683\n10.1016/j.engappai.2022.105151\n10.1016/S0893-6080(05)80023-1"}
{"title": "On the Implementation of a Post-Pandemic Deep Learning Algorithm Based on a Hybrid CT-Scan/X-ray Images Classification Applied to Pneumonia Categories.", "abstract": "The identification and characterization of lung diseases is one of the most interesting research topics in recent years. They require accurate and rapid diagnosis. Although lung imaging techniques have many advantages for disease diagnosis, the interpretation of medial lung images has always been a major problem for physicians and radiologists due to diagnostic errors. This has encouraged the use of modern artificial intelligence techniques such as deep learning. In this paper, a deep learning architecture based on EfficientNetB7, known as the most advanced architecture among convolutional networks, has been constructed for classification of medical X-ray and CT images of lungs into three classes namely: common pneumonia, coronavirus pneumonia and normal cases. In terms of accuracy, the proposed model is compared with recent pneumonia detection techniques. The results provided robust and consistent features to this system for pneumonia detection with predictive accuracy according to the three classes mentioned above for both imaging modalities: radiography at 99.81% and CT at 99.88%. This work implements an accurate computer-aided system for the analysis of radiographic and CT medical images. The results of the classification are promising and will certainly improve the diagnosis and decision making of lung diseases that keep appearing over time.", "journal": "Healthcare (Basel, Switzerland)", "date": "2023-03-12", "authors": ["AbdelghaniMoussaid", "NabilaZrira", "IbtissamBenmiloud", "ZinebFarahat", "YoussefKarmoun", "YasmineBenzidia", "SoumayaMouline", "BahiaEl Abdi", "Jamal EddineBourkadi", "NabilNgote"], "doi": "10.3390/healthcare11050662\n10.1128/CMR.00028-20\n10.1016/j.ajem.2022.03.036\n10.1016/j.rmr.2021.11.004\n10.1016/j.gie.2020.06.040\n10.1371/journal.pone.0072457\n10.1016/j.ophtha.2017.02.008\n10.1371/journal.pone.0174944\n10.1016/j.bbe.2021.06.011\n10.1038/s41598-020-74539-2\n10.1016/j.mehy.2020.109761\n10.1109/TMI.2020.3040950\n10.1371/journal.pone.0262052\n10.1016/j.chemolab.2021.104256\n10.1007/s42979-021-00695-5\n10.1016/j.bspc.2021.103441\n10.1007/s11548-021-02317-0\n10.1088/1361-6560/abe838\n10.3389/frai.2021.694875\n10.1016/j.compbiomed.2021.104835\n10.17632/fvk7h5dg2p.3"}
{"title": "A hybrid deep learning approach for COVID-19 detection based on genomic image processing techniques.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has been spreading quickly, threatening the public health system. Consequently, positive COVID-19 cases must be rapidly detected and treated. Automatic detection systems are essential for controlling the COVID-19 pandemic. Molecular techniques and medical imaging scans are among the most effective approaches for detecting COVID-19. Although these approaches are crucial for controlling the COVID-19 pandemic, they have certain limitations. This study proposes an effective hybrid approach based on genomic image processing (GIP) techniques to rapidly detect COVID-19 while avoiding the limitations of traditional detection techniques, using whole and partial genome sequences of human coronavirus (HCoV) diseases. In this work, the GIP techniques convert the genome sequences of HCoVs into genomic grayscale images using a genomic image mapping technique known as the frequency chaos game representation. Then, the pre-trained convolution neural network, AlexNet, is used to extract deep features from these images using the last convolution (conv5) and second fully-connected (fc7) layers. The most significant features were obtained by removing the redundant ones using the ReliefF and least absolute shrinkage and selection operator (LASSO) algorithms. These features are then passed to two classifiers: decision trees and k-nearest neighbors (KNN). Results showed that extracting deep features from the fc7 layer, selecting the most significant features using the LASSO algorithm, and executing the classification process using the KNN classifier is the best hybrid approach. The proposed hybrid deep learning approach detected COVID-19, among other HCoV diseases, with 99.71% accuracy, 99.78% specificity, and 99.62% sensitivity.", "journal": "Scientific reports", "date": "2023-03-11", "authors": ["Muhammed SHammad", "Vidan FGhoneim", "Mai SMabrouk", "Walid IAl-Atabany"], "doi": "10.1038/s41598-023-30941-0\n10.1038/s41586-020-2008-3\n10.14309/ajg.0000000000000620\n10.1213/ANE.0000000000004845\n10.3390/pathogens9030186\n10.1038/s41591-020-0820-9\n10.1016/S0140-6736(20)30251-8\n10.1016/j.jinf.2020.03.041\n10.1109/ACCESS.2021.3076158\n10.1038/s41598-021-88807-2\n10.1016/j.eswa.2020.113909\n10.1002/ima.22469\n10.1016/j.compbiomed.2020.103805\n10.1007/s10489-020-01888-w\n10.1109/JIOT.2021.3055804\n10.1016/j.asoc.2020.106642\n10.1016/j.asoc.2022.108780\n10.1038/s41598-020-76550-z\n10.1148/radiol.2020200642\n10.1021/acsnano.0c02624\n10.1016/j.cie.2021.107666\n10.1038/s41598-020-80363-5\n10.3389/fgene.2021.569120\n10.1007/s11517-022-02591-3\n10.1038/s41598-021-90766-7\n10.1093/bib/bbaa170\n10.1371/journal.pone.0232391\n10.1016/j.bspc.2022.104192\n10.1016/j.compbiomed.2021.104650\n10.13053/rcs-148-3-9\n10.1016/j.jmgm.2020.107603\n10.1016/j.aej.2022.08.023\n10.1093/bioinformatics/17.5.429\n10.1016/j.gene.2004.10.021\n10.1016/j.neucom.2020.10.068\n10.1016/j.compbiomed.2017.08.001\n10.1145/3065386\n10.1109/TMI.2016.2535302\n10.1007/s42979-021-00815-1\n10.1109/ACCESS.2019.2919122\n10.1016/j.neucom.2017.11.077\n10.1109/ACCESS.2021.3053759\n10.1016/j.jbi.2018.07.014\n10.1111/j.1467-9868.2011.00771.x\n10.1111/j.1467-9868.2007.00577.x\n10.1016/j.ipm.2009.03.002"}
{"title": "A hybrid CNN and ensemble model for COVID-19 lung infection detection on chest CT scans.", "abstract": "COVID-19 is highly infectious and causes acute respiratory disease. Machine learning (ML) and deep learning (DL) models are vital in detecting disease from computerized chest tomography (CT) scans. The DL models outperformed the ML models. For COVID-19 detection from CT scan images, DL models are used as end-to-end models. Thus, the performance of the model is evaluated for the quality of the extracted feature and classification accuracy. There are four contributions included in this work. First, this research is motivated by studying the quality of the extracted feature from the DL by feeding these extracted to an ML model. In other words, we proposed comparing the end-to-end DL model performance against the approach of using DL for feature extraction and ML for the classification of COVID-19 CT scan images. Second, we proposed studying the effect of fusing extracted features from image descriptors, e.g., Scale-Invariant Feature Transform (SIFT), with extracted features from DL models. Third, we proposed a new Convolutional Neural Network (CNN) to be trained from scratch and then compared to the deep transfer learning on the same classification problem. Finally, we studied the performance gap between classic ML models against ensemble learning models. The proposed framework is evaluated using a CT dataset, where the obtained results are evaluated using five different metrics The obtained results revealed that using the proposed CNN model is better than using the well-known DL model for the purpose of feature extraction. Moreover, using a DL model for feature extraction and an ML model for the classification task achieved better results in comparison to using an end-to-end DL model for detecting COVID-19 CT scan images. Of note, the accuracy rate of the former method improved by using ensemble learning models instead of the classic ML models. The proposed method achieved the best accuracy rate of 99.39%.", "journal": "PloS one", "date": "2023-03-10", "authors": ["Ahmed AAkl", "Khalid MHosny", "Mostafa MFouda", "AhmadSalah"], "doi": "10.1371/journal.pone.0282608\n10.1016/j.compbiomed.2020.103795\n10.1097/RLI.0000000000000700\n10.1371/journal.pone.0236621\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105532\n10.1038/nature14539\n10.1109/JPROC.2020.3004555\n10.1371/journal.pone.0235187\n10.1371/journal.pone.0250688\n10.2196/19569\n10.1007/s10489-020-02055-x\n10.1007/s00500-020-05275-y\n10.1109/ACCESS.2020.3016780\n10.1016/j.eswa.2021.116377\n10.1007/s10723-022-09615-0\n10.1002/ima.22706\n10.1109/TMI.2017.2712367\n10.1016/j.aquaeng.2020.102117\n10.1023/B:VISI.0000029664.99615.94\n10.7717/peerj.10086\n10.1016/j.patrec.2020.10.001\n10.1007/s10489-020-01826-w\n10.1007/s00521-020-05437-x"}
{"title": "COVID-19 imaging, where do we go from here? Bibliometric analysis of medical imaging in COVID-19.", "abstract": "We conducted a systematic and comprehensive bibliometric analysis of COVID-19-related medical imaging to determine the current status and indicate possible future directions.\nThis research provides an analysis of Web of Science Core Collection (WoSCC) indexed articles on COVID-19 and medical imaging published between 1 January 2020 and 30 June 2022, using the search terms \"COVID-19\" and medical imaging terms (such as \"X-ray\" or \"CT\"). Publications based solely on COVID-19 themes or medical image themes were excluded. CiteSpace was used to identify the predominant topics and generate a visual map of countries, institutions, authors, and keyword networks.\nThe search included 4444 publications. The journal with the most publications was European Radiology, and the most co-cited journal was Radiology. China was the most frequently cited country in terms of co-authorship, with the Huazhong University of Science and Technology being the institution contributing with the highest number of relevant co-authorships. Research trends and leading topics included: assessment of initial COVID-19-related clinical imaging features, differential diagnosis using artificial intelligence (AI) technology and model interpretability, diagnosis systems construction, COVID-19 vaccination, complications, and predicting prognosis.\nThis bibliometric analysis of COVID-19-related medical imaging helps clarify the current research situation and developmental trends. Subsequent trends in COVID-19 imaging are likely to shift from lung structure to function, from lung tissue to other related organs, and from COVID-19 to the impact of COVID-19 on the diagnosis and treatment of other diseases. Key Points \u2022\u00a0We conducted a systematic and comprehensive bibliometric analysis of COVID-19-related medical imaging from 1 January 2020 to 30 June 2022. \u2022 Research trends and leading topics included assessment of initial COVID-19-related clinical imaging features, differential diagnosis using AI technology and model interpretability, diagnosis systems construction, COVID-19 vaccination, complications, and predicting prognosis. \u2022 Future trends in COVID-19-related imaging are likely to involve a shift from lung structure to function, from lung tissue to other related organs, and from COVID-19 to the impact of COVID-19 on the diagnosis and treatment of other diseases.", "journal": "European radiology", "date": "2023-03-10", "authors": ["RuWen", "MudanZhang", "RuiXu", "YingmingGao", "LinLiu", "HuiChen", "XingangWang", "WenyanZhu", "HuafangLin", "ChenLiu", "XianchunZeng"], "doi": "10.1007/s00330-023-09498-z\n10.1016/j.diii.2020.03.014\n10.1007/s00330-020-06934-2\n10.1007/s00330-020-06801-0\n10.1016/S0140-6736(20)30185-9\n10.1016/j.clinimag.2021.01.019\n10.1371/journal.pone.0223994\n10.1109/RBME.2020.2990959\n10.3389/frma.2020.607286\n10.1002/asi.20317\n10.1517/14712598.2014.920813\n10.1080/14789450.2019.1703678\n10.1016/j.scs.2021.102729\n10.1016/j.dsx.2021.102325\n10.21037/atm-20-4235\n10.1016/j.biopha.2020.110451\n10.1016/j.dsx.2020.06.063\n10.1016/j.tmaid.2020.101566\n10.1097/MD.0000000000022849\n10.2174/1573405618666211230105631\n10.2196/30692\n10.1007/s40336-021-00460-x\n10.1148/radiol.2021204417\n10.1093/bioinformatics/btm554\n10.1002/asi.21309\n10.1016/S0140-6736(20)30183-5\n10.1016/j.jamda.2021.01.073\n10.1016/j.compbiomed.2020.103869\n10.1016/j.patcog.2020.107613\n10.1016/j.chaos.2020.110170\n10.1007/s10044-021-00984-y:1-14\n10.1186/s12967-020-02324-w\n10.1109/TMI.2021.3104474\n10.1097/PHM.0000000000001729\n10.1016/S0140-6736(20)32656-8\n10.1038/s41746-021-00496-3\n10.1186/s12916-021-02056-8\n10.1111/j.1440-1843.2010.01720.x"}
{"title": "Artificial intelligence for assistance of radiology residents in chest CT evaluation for COVID-19 pneumonia: a comparative diagnostic accuracy study.", "abstract": "In hospitals, it is crucial to rule out coronavirus disease 2019 (COVID-19) timely and reliably. Artificial intelligence (AI) provides sufficient accuracy to identify chest computed tomography (CT) scans with signs of COVID-19.\nTo compare the diagnostic accuracy of radiologists with different levels of experience with and without assistance of AI in CT evaluation for COVID-19 pneumonia and to develop an optimized diagnostic pathway.\nThe retrospective, single-center, comparative case-control study included 160 consecutive participants who had undergone chest CT scan between March 2020 and May 2021 without or with confirmed diagnosis of COVID-19 pneumonia in a ratio of 1:3. Index tests were chest CT evaluation by five radiological senior residents, five junior residents, and an AI software. Based on the diagnostic accuracy in every group and on comparison of groups, a sequential CT assessment pathway was developed.\nAreas under receiver operating curves were 0.95 (95% confidence interval [CI]=0.88-0.99), 0.96 (95% CI=0.92-1.0), 0.77 (95% CI=0.68-0.86), and 0.95 (95% CI=0.9-1.0) for junior residents, senior residents, AI, and sequential CT assessment, respectively. Proportions of false negatives were 9%, 3%, 17%, and 2%, respectively. With the developed diagnostic pathway, junior residents evaluated all CT scans with the support of AI. Senior residents were only required as second readers in 26% (41/160) of the CT scans.\nAI can support junior residents with chest CT evaluation for COVID-19 and reduce the workload of senior residents. A review of selected CT scans by senior residents is mandatory.", "journal": "Acta radiologica (Stockholm, Sweden : 1987)", "date": "2023-03-10", "authors": ["LucjaMlynska", "AmerMalouhi", "MajaIngwersen", "FelixG\u00fcttler", "StephanieGr\u00e4ger", "UlfTeichgr\u00e4ber"], "doi": "10.1177/02841851231162085"}
{"title": "MCSC-Net: COVID-19 detection using deep-Q-neural network classification with RFNN-based hybrid whale optimization.", "abstract": "COVID-19 is the most dangerous virus, and its accurate diagnosis saves lives and slows its spread. However, COVID-19 diagnosis takes time and requires trained professionals. Therefore, developing a deep learning (DL) model on low-radiated imaging modalities like chest X-rays (CXRs) is needed.\nThe existing DL models failed to diagnose COVID-19 and other lung diseases accurately. This study implements a multi-class CXR segmentation and classification network (MCSC-Net) to detect COVID-19 using CXR images.\nInitially, a hybrid median bilateral filter (HMBF) is applied to CXR images to reduce image noise and enhance the COVID-19 infected regions. Then, a skip connection-based residual network-50 (SC-ResNet50) is used to segment (localize) COVID-19 regions. The features from CXRs are further extracted using a robust feature neural network (RFNN). Since the initial features contain joint COVID-19, normal, pneumonia bacterial, and viral properties, the conventional methods fail to separate the class of each disease-based feature. To extract the distinct features of each class, RFNN includes a disease-specific feature separate attention mechanism (DSFSAM). Furthermore, the hunting nature of the Hybrid whale optimization algorithm (HWOA) is used to select the best features in each class. Finally, the deep-Q-neural network (DQNN) classifies CXRs into multiple disease classes.\nThe proposed MCSC-Net shows the enhanced accuracy of 99.09% for 2-class, 99.16% for 3-class, and 99.25% for 4-class classification of CXR images compared to other state-of-art approaches.\nThe proposed MCSC-Net enables to conduct multi-class segmentation and classification tasks applying to CXR images with high accuracy. Thus, together with gold-standard clinical and laboratory tests, this new method is promising to be used in future clinical practice to evaluate patients.", "journal": "Journal of X-ray science and technology", "date": "2023-03-07", "authors": ["GerardDeepak", "MMadiajagan", "SanjeevKulkarni", "Ahmed NajatAhmed", "AnandbabuGopatoti", "VeeraswamyAmmisetty"], "doi": "10.3233/XST-221360"}
{"title": "Results of the COVID-19 mental health international for the health professionals (COMET-HP) study: depression, suicidal tendencies and conspiracism.", "abstract": "The current study aimed to investigate the rates of anxiety, clinical depression, and suicidality and their changes in health professionals during the COVID-19 outbreak.\nThe data came from the larger COMET-G study. The study sample includes 12,792 health professionals from 40 countries (62.40% women aged 39.76\u2009\u00b1\u200911.70; 36.81% men aged 35.91\u2009\u00b1\u200911.00 and 0.78% non-binary gender aged 35.15\u2009\u00b1\u200913.03). Distress and clinical depression were identified with the use of a previously developed cut-off and algorithm, respectively.\nDescriptive statistics were calculated. Chi-square tests, multiple forward stepwise linear regression analyses, and Factorial Analysis of Variance (ANOVA) tested relations among variables.\nClinical depression was detected in 13.16% with male doctors and 'non-binary genders' having the lowest rates (7.89 and 5.88% respectively) and 'non-binary gender' nurses and administrative staff had the highest (37.50%); distress was present in 15.19%. A significant percentage reported a deterioration in mental state, family dynamics, and everyday lifestyle. Persons with a history of mental disorders had higher rates of current depression (24.64% vs. 9.62%; p\u2009<\u20090.0001). Suicidal tendencies were at least doubled in terms of RASS scores. Approximately one-third of participants were accepting (at least to a moderate degree) a non-bizarre conspiracy. The highest Relative Risk (RR) to develop clinical depression was associated with a history of Bipolar disorder (RR\u2009=\u20094.23).\nThe current study reported findings in health care professionals similar in magnitude and quality to those reported earlier in the general population although rates of clinical depression, suicidal tendencies, and adherence to conspiracy theories were much lower. However, the general model of factors interplay seems to be the same and this could be of practical utility since many of these factors are modifiable.", "journal": "Social psychiatry and psychiatric epidemiology", "date": "2023-03-04", "authors": ["KonstantinosN Fountoulakis", "GrigoriosN Karakatsoulis", "SeriAbraham", "KristinaAdorjan", "Helal UddinAhmed", "Renato DAlarc\u00f3n", "KiyomiArai", "Sani SalihuAuwal", "JulioBobes", "TeresaBobes-Bascaran", "JulieBourgin-Duchesnay", "Cristina AnaBredicean", "LaurynasBukelskis", "AkakiBurkadze", "Indira IndianaCabrera Abud", "RubyCastilla-Puentes", "MarceloCetkovich", "HectorColon-Rivera", "RicardoCorral", "CarlaCortez-Vergara", "PiirikaCrepin", "Domenicode Berardis", "SergioZamora Delgado", "Davidde Lucena", "Avinashde Sousa", "Ramonadi Stefano", "SeetalDodd", "Livia PriyankaElek", "AnnaElissa", "BertaErdelyi-Hamza", "GamzeErzin", "Martin JEtchevers", "PeterFalkai", "AdrianaFarcas", "IlyaFedotov", "ViktoriiaFilatova", "Nikolaos KFountoulakis", "IrynaFrankova", "FrancescoFranza", "PedroFrias", "TatianaGalako", "Cristian JGaray", "LeticiaGarcia-\u00c1lvarez", "PazGarc\u00eda-Portilla", "XeniaGonda", "Tomasz MGondek", "DanielaMorera Gonz\u00e1lez", "HilaryGould", "PaoloGrandinetti", "ArturoGrau", "VioletaGroudeva", "MichalHagin", "TakayukiHarada", "Tasdik MHasan", "NurulAzreen Hashim", "JanHilbig", "SahadatHossain", "RossitzaIakimova", "MonaIbrahim", "FeliciaIftene", "YuliaIgnatenko", "MatiasIrarrazaval", "ZalihaIsmail", "JamilaIsmayilova", "AsafJacobs", "MiroJakovljevi\u0107", "NenadJak\u0161i\u0107", "AfzalJaved", "HelinYilmaz Kafali", "SagarKaria", "OlgaKazakova", "DoaaKhalifa", "OlenaKhaustova", "SteveKoh", "SvetlanaKopishinskaia", "KorneliiaKosenko", "Sotirios AKoupidis", "IllesKovacs", "BarbaraKulig", "AlishaLalljee", "JustineLiewig", "AbdulMajid", "EvgeniiaMalashonkova", "KhameliaMalik", "NajmaIqbal Malik", "GulayMammadzada", "BilveshMandalia", "DonatellaMarazziti", "DarkoMar\u010dinko", "StephanieMartinez", "EimantasMatiekus", "GabrielaMejia", "Roha SaeedMemon", "Xarah ElenneMeza Mart\u00ednez", "DaliaMickevi\u010di\u016bt\u0117", "RoumenMilev", "MuftauMohammed", "AlejandroMolina-L\u00f3pez", "PetrMorozov", "Nuru SuleimanMuhammad", "FilipMusta\u010d", "Mika SNaor", "AmiraNassieb", "AlvydasNavickas", "TarekOkasha", "MilenaPandova", "Anca-LiviaPanfil", "LiliyaPanteleeva", "IonPapava", "Mikaella EPatsali", "AlexeyPavlichenko", "BojanaPejuskovic", "MarianaPinto da Costa", "MikhailPopkov", "DinaPopovic", "Nor Jannah NasutionRaduan", "FranciscaVargas Ram\u00edrez", "ElmarsRancans", "SalmiRazali", "FedericoRebok", "AnnaRewekant", "Elena NinoskaReyes Flores", "Mar\u00eda TeresaRivera-Encinas", "Pilar ASaiz", "ManuelS\u00e1nchez de Carmona", "DavidSaucedo Mart\u00ednez", "Jo AnneSaw", "G\u00f6rkemSaygili", "PatriciaSchneidereit", "BhumikaShah", "TomohiroShirasaka", "KetevanSilagadze", "SattiSitanggang", "OlegSkugarevsky", "AnnaSpikina", "Sridevi SiraMahalingappa", "MariaStoyanova", "AnnaSzczegielniak", "Simona ClaudiaTamasan", "GiuseppeTavormina", "Maurilio Giuseppe MariaTavormina", "Pavlos NTheodorakis", "MauricioTohen", "Eva-MariaTsapakis", "DinaTukhvatullina", "IrfanUllah", "RatnarajVaidya", "Johann MVega-Dienstmaier", "JelenaVrublevska", "OliveraVukovic", "OlgaVysotska", "NataliaWidiasih", "AnnaYashikhina", "Panagiotis EPrezerakos", "MichaelBerk", "SarahLevaj", "DariaSmirnova"], "doi": "10.1007/s00127-023-02438-8\n10.2196/19458\n10.1017/S003329172000224X\n10.1017/S0033291721001434\n10.1037/0022-3514.41.6.1129\n10.1037//0096-3445.108.4.441\n10.1186/s12888-020-02864-x\n10.1002/brb3.1881\n10.3389/fpsyg.2021.646572\n10.1016/j.janxdis.2021.102368\n10.3389/fpubh.2021.610623\n10.1037//0021-843x.96.3.179\n10.1002/brb3.1964\n10.3389/fpsyg.2020.565128\n10.1111/bjso.12397\n10.1097/QAI.0b013e3181c57dbc\n10.1016/j.jad.2021.01.013\n10.1159/000513733\n10.3389/fpsyg.2020.597624\n10.1016/j.psychres.2020.113599\n10.2196/20737\n10.3390/healthcare10060979\n10.3390/healthcare9050510\n10.1016/j.euroneuro.2021.06.005\n10.1016/j.pnpbp.2020.110062\n10.1016/j.jad.2021.02.054\n10.3389/fpsyg.2021.646394\n10.1111/nyas.14506\n10.1007/s00038-020-01412-4\n10.3390/healthcare8030190\n10.1192/j.eurpsy.2020.59\n10.1186/s12910-021-00641-3\n10.3390/ijerph17217818\n10.1002/da.23162\n10.1080/10615806.2021.1878158\n10.1016/S2215-0366(20)30482-X\n10.1192/j.eurpsy.2020.35\n10.1186/1471-244x-1-3\n10.1016/j.jad.2020.10.061\n10.1016/j.euroneuro.2021.10.004\n10.1016/j.jad.2011.12.045\n10.1017/S0033291720005188\n10.18071/isz.72.0337\n10.3389/fpsyt.2020.568664\n10.1016/j.comppsych.2020.152222\n10.1016/j.comppsych.2020.152214\n10.3389/fpsyg.2020.01684\n10.3390/ijerph18073843\n10.3390/ijerph17134779\n10.1016/j.jpsychires.2020.07.024\n10.1186/s12888-021-03291-2\n10.1176/appi.ajp.2020.20070979\n10.1186/s12913-021-06555-5\n10.1111/jan.15175\n10.1016/j.ajp.2020.102052\n10.1080/13548506.2020.1754438\n10.1016/j.comppsych.2020.152213\n10.1371/journal.pone.0089177\n10.1111/bjso.12394\n10.1016/j.paid.2020.110216\n10.1186/s40359-021-00565-y\n10.1016/j.paid.2021.110771\n10.1016/j.paid.2021.110704\n10.1002/wps.20758\n10.1016/j.jpsychires.2019.08.002\n10.3389/fpsyt.2020.598712\n10.1037//0021-843x.88.1.33\n10.1177/00131640021970871\n10.1002/brb3.2318\n10.1186/s12889-021-10873-y\n10.1111/bjhp.12449\n10.1136/bmjopen-2020-042555\n10.1016/j.bbi.2020.04.048\n10.1002/da.23129\n10.1002/brb3.1730\n10.1037//0021-843x.86.4.379\n10.1001/jamainternmed.2014.190\n10.1177/0020764020927051\n10.1177/0020764020927051\n10.3389/fpsyg.2020.02065\n10.1186/s40359-021-00526-5\n10.1002/brb3.1745\n10.1016/j.euroneuro.2021.05.011\n10.1016/S2215-0366(21)00074-2\n10.1007/s11126-020-09796-5\n10.1177/0706743720986786\n10.1016/j.socscimed.2020.113356\n10.3389/fpsyt.2021.635832\n10.3389/fpsyg.2020.577684\n10.1017/S0033291720004067\n10.1186/s12992-020-00589-w\n10.1371/journal.pone.0243264\n10.1111/pcn.13004\n10.1177/13591053211012759\n10.1017/S0033291721001665\n10.3390/ijerph19031154\n10.3390/ijerph17144924\n10.1186/s12889-021-10643-w\n10.1186/s12889-020-09322-z\n10.3389/fpsyg.2021.626547\n10.1002/brb3.1837\n10.1016/S2215-0366(21)00084-5\n10.1002/acp.3770\n10.1080/08870446.2019.1673894\n10.1136/bmjopen-2020-044945\n10.7189/jogh.11.05009\n10.1177/0020764020934508\n10.1080/13814788.2021.1954154\n10.1016/j.jpsychires.2021.05.024\n10.3390/ijerph17051729\n10.1016/j.bbi.2020.04.028\n10.3399/bjgp20X713021\n10.1097/PSY.0000000000000922\n10.4269/ajtmh.20-0800\n10.3205/000281"}
{"title": "A Computational Approach in the Diagnostic Process of COVID-19: The Missing Link between the Laboratory and Emergency Department.", "abstract": "The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is responsible for the COVID-19 pandemic and so it is crucial the right evaluation of viral infection. According to the Centers for Disease Control and Prevention (CDC), the Real-Time Reverse Transcription PCR (RT-PCR) in respiratory samples is the gold standard for confirming the disease. However, it has practical limitations as time-consuming procedures and a high rate of false-negative results. We aim to assess the accuracy of COVID-19 classifiers based on Arificial Intelligence (AI) and statistical classification methods adapted on blood tests and other information routinely collected at the Emergency Departments (EDs).\nPatients admitted to the ED of Careggi Hospital from April 7th-30th 2020 with pre-specified features of suspected COVID-19 were enrolled. Physicians prospectively dichotomized them as COVID-19 likely/unlikely case, based on clinical features and bedside imaging support. Considering the limits of each method to identify a case of COVID-19, further evaluation was performed after an independent clinical review of 30-day follow-up data. Using this as a gold standard, several classifiers were implemented: Logistic Regression (LR), Quadratic Discriminant Analysis (QDA), Random Forest (RF), Support Vector Machine (SVM), Neural Networks (NN), K-nearest neighbor (K-NN), Naive Bayes (NB).\nMost of the classifiers show a ROC >0.80 on both internal and external validation samples but the best results are obtained applying RF, LR and NN. The performance from the external validation sustains the proof of concept to use such mathematical models fast, robust and efficient for a first identification of COVID-19 positive patients. These tools may constitute both a bedside support while waiting for RT-PCR results, and a tool to point to a deeper investigation, by identifying which patients are more likely to develop into positive cases within 7 days.\nConsidering the obtained results and with a rapidly changing virus, we believe that data processing automated procedures may provide a valid support to the physicians facing the decision to classify a patient as a COVID-19 case or not.", "journal": "Frontiers in bioscience (Landmark edition)", "date": "2023-03-04", "authors": ["LuisaLanzilao", "AntonellaMariniello", "BiancaPolenzani", "AlessandraAldinucci", "PeimanNazerian", "AlessioProta", "StefanoGrifoni", "BarbaraTonietti", "ChiaraNeri", "LiviaTurco", "AlessandraFanelli", "AmedeoAmedei", "ElenaStanghellini"], "doi": "10.31083/j.fbl2802031"}
{"title": "Robust framework for COVID-19 identication from a multicenter dataset of chest CT scans.", "abstract": "The main objective of this study is to develop a robust deep learning-based framework to distinguish COVID-19, Community-Acquired Pneumonia (CAP), and Normal cases based on volumetric chest CT scans, which are acquired in different imaging centers using different scanners and technical settings. We demonstrated that while our proposed model is trained on a relatively small dataset acquired from only one imaging center using a specific scanning protocol, it performs well on heterogeneous test sets obtained by multiple scanners using different technical parameters. We also showed that the model can be updated via an unsupervised approach to cope with the data shift between the train and test sets and enhance the robustness of the model upon receiving a new external dataset from a different center. More specifically, we extracted the subset of the test images for which the model generated a confident prediction and used the extracted subset along with the training set to retrain and update the benchmark model (the model trained on the initial train set). Finally, we adopted an ensemble architecture to aggregate the predictions from multiple versions of the model. For initial training and development purposes, an in-house dataset of 171 COVID-19, 60 CAP, and 76 Normal cases was used, which contained volumetric CT scans acquired from one imaging center using a single scanning protocol and standard radiation dose. To evaluate the model, we collected four different test sets retrospectively to investigate the effects of the shifts in the data characteristics on the model's performance. Among the test cases, there were CT scans with similar characteristics as the train set as well as noisy low-dose and ultra-low-dose CT scans. In addition, some test CT scans were obtained from patients with a history of cardiovascular diseases or surgeries. This dataset is referred to as the \"SPGC-COVID\" dataset. The entire test dataset used in this study contains 51 COVID-19, 28 CAP, and 51 Normal cases. Experimental results indicate that our proposed framework performs well on all test sets achieving total accuracy of 96.15% (95%CI: [91.25-98.74]), COVID-19 sensitivity of 96.08% (95%CI: [86.54-99.5]), CAP sensitivity of 92.86% (95%CI: [76.50-99.19]), Normal sensitivity of 98.04% (95%CI: [89.55-99.95]) while the confidence intervals are obtained using the significance level of 0.05. The obtained AUC values (One class vs Others) are 0.993 (95%CI: [0.977-1]), 0.989 (95%CI: [0.962-1]), and 0.990 (95%CI: [0.971-1]) for COVID-19, CAP, and Normal classes, respectively. The experimental results also demonstrate the capability of the proposed unsupervised enhancement approach in improving the performance and robustness of the model when being evaluated on varied external test sets.", "journal": "PloS one", "date": "2023-03-03", "authors": ["SadafKhademi", "ShahinHeidarian", "ParnianAfshar", "NastaranEnshaei", "FarnooshNaderkhani", "Moezedin JavadRafiee", "AnastasiaOikonomou", "AkbarShafiee", "FaranakBabaki Fard", "Konstantinos NPlataniotis", "ArashMohammadi"], "doi": "10.1371/journal.pone.0282121\n10.1148/radiol.2020200432\n10.1109/MSP.2021.3090674\n10.1016/j.cell.2020.04.045\n10.1148/radiol.2019190928\n10.1038/srep34921\n10.1016/j.numecd.2020.04.013\n10.3389/frai.2021.598932\n10.1016/j.imu.2022.100945\n10.1038/s41597-021-00900-3\n10.1038/s41598-022-08796-8\n10.1007/s00330-010-1990-5\n10.1016/j.patcog.2021.107942\n10.1109/LSP.2020.3034858\n10.1007/s42058-020-00034-2\n10.1002/widm.1353\n10.1016/j.media.2021.102062\n10.1109/ACCESS.2021.3084358\n10.2307/2685469\n10.1007/BF02295996\n10.1109/TMI.2020.3009029\n10.1109/TMI.2020.2971258\n10.1186/s13244-021-01096-1\n10.1016/j.chest.2021.04.004\n10.25259/JCIS_138_2020\n10.7189/jogh.10.010347\n10.1016/j.jhin.2020.03.001\n10.3389/fonc.2020.556334\n10.1016/j.chest.2020.04.003\n10.1016/j.tmaid.2020.101627\n10.1007/s15010-020-01467-8\n10.1007/s00330-020-06809-6"}
{"title": "Deep Learning Solution for Quantification of Fluorescence Particles on a Membrane.", "abstract": "The detection and quantification of severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) virus particles in ambient waters using a membrane-based in-gel loop-mediated isothermal amplification (mgLAMP) method can play an important role in large-scale environmental surveillance for early warning of potential outbreaks. However, counting particles or cells in fluorescence microscopy is an expensive, time-consuming, and tedious task that only highly trained technicians and researchers can perform. Although such objects are generally easy to identify, manually annotating cells is occasionally prone to fatigue errors and arbitrariness due to the operator's interpretation of borderline cases. In this research, we proposed a method to detect and quantify multiscale and shape variant SARS-CoV-2 fluorescent cells generated using a portable (", "journal": "Sensors (Basel, Switzerland)", "date": "2023-03-01", "authors": ["Abdellah ZakariaSellam", "AzeddineBenlamoudi", "Cl\u00e9ment AntoineCid", "LeopoldDobelle", "AminaSlama", "YassinEl Hillali", "AbdelmalikTaleb-Ahmed"], "doi": "10.3390/s23041794\n10.1038/s41586-021-04188-6\n10.1021/acs.est.0c02388\n10.1038/s41587-020-0684-z\n10.1016/j.watres.2020.116404\n10.1016/j.envint.2020.105689\n10.1016/j.scitotenv.2021.149618\n10.1128/jcm.02446-21\n10.1021/acs.est.1c04623\n10.1109/MSP.2012.2204190\n10.1109/TSMCB.2012.2228639\n10.1016/0031-3203(95)00067-4\n10.1109/TBME.2009.2035102\n10.3390/s22103760\n10.1016/j.neunet.2014.09.003\n10.1155/2018/7068349\n10.1109/ICCV.2015.169\n10.1109/MSP.2009.934181\n10.1007/978-3-319-10578-9_23\n10.1109/TPAMI.2016.2577031\n10.1109/TPAMI.2019.2956516"}
{"title": "MonkeyNet: A robust deep convolutional neural network for monkeypox disease detection and classification.", "abstract": "The monkeypox virus poses a new pandemic threat while we are still recovering from COVID-19. Despite the fact that monkeypox is not as lethal and contagious as COVID-19, new patient cases are recorded every day. If preparations are not made, a global pandemic is likely. Deep learning (DL) techniques are now showing promise in medical imaging for figuring out what diseases a person has. The monkeypox virus-infected human skin and the region of the skin can be used to diagnose the monkeypox early because an image has been used to learn more about the disease. But there is still no reliable Monkeypox database that is available to the public that can be used to train and test DL models. As a result, it is essential to collect images of monkeypox patients. The \"MSID\" dataset, short form of \"Monkeypox Skin Images Dataset\", which was developed for this research, is free to use and can be downloaded from the Mendeley Data database by anyone who wants to use it. DL models can be built and used with more confidence using the images in this dataset. These images come from a variety of open-source and online sources and can be used for research purposes without any restrictions. Furthermore, we proposed and evaluated a modified DenseNet-201 deep learning-based CNN model named MonkeyNet. Using the original and augmented datasets, this study suggested a deep convolutional neural network that was able to correctly identify monkeypox disease with an accuracy of 93.19% and 98.91% respectively. This implementation also shows the Grad-CAM which indicates the level of the model's effectiveness and identifies the infected regions in each class image, which will help the clinicians. The proposed model will also help doctors make accurate early diagnoses of monkeypox disease and protect against the spread of the disease.", "journal": "Neural networks : the official journal of the International Neural Network Society", "date": "2023-02-28", "authors": ["DiponkorBala", "Md ShamimHossain", "Mohammad AlamgirHossain", "Md IbrahimAbdullah", "Md MizanurRahman", "BalachandranManavalan", "NaijieGu", "Mohammad SIslam", "ZhangjinHuang"], "doi": "10.1016/j.neunet.2023.02.022\n10.17632/r9bfpnvyxr.3"}
{"title": "Dense regression activation maps for lesion segmentation in CT scans of COVID-19 patients.", "abstract": "Automatic lesion segmentation on thoracic CT enables rapid quantitative analysis of lung involvement in COVID-19 infections. However, obtaining a large amount of voxel-level annotations for training segmentation networks is prohibitively expensive. Therefore, we propose a weakly-supervised segmentation method based on dense regression activation maps (dRAMs). Most weakly-supervised segmentation approaches exploit class activation maps (CAMs) to localize objects. However, because CAMs were trained for classification, they do not align precisely with the object segmentations. Instead, we produce high-resolution activation maps using dense features from a segmentation network that was trained to estimate a per-lobe lesion percentage. In this way, the network can exploit knowledge regarding the required lesion volume. In addition, we propose an attention neural network module to refine dRAMs, optimized together with the main regression task. We evaluated our algorithm on 90 subjects. Results show our method achieved 70.2% Dice coefficient, substantially outperforming the CAM-based baseline at 48.6%. We published our source code at https://github.com/DIAGNijmegen/bodyct-dram.", "journal": "Medical image analysis", "date": "2023-02-28", "authors": ["WeiyiXie", "ColinJacobs", "Jean-PaulCharbonnier", "Bramvan Ginneken"], "doi": "10.1016/j.media.2023.102771\n10.1109/CVPR.2018.00523\n10.1109/TMI.2020.2996645\n10.1109/TMI.2020.2994463\n10.1109/iccv.2015.123\n10.1109/CVPR.2018.00733\n10.1038/s41592-020-01008-z\n10.1007/978-3-030-32248-9_20\n10.1148/radiol.2020202439\n10.1097/RLI.0000000000000672\n10.1109/JBHI.2020.2999588\n10.1109/ICCV.2015.203\n10.1109/CVPR.2015.7298780\n10.1148/radiol.2020201473\n10.1016/S1473-3099(20)30086-4\n10.1109/TMI.2020.2995965\n10.1109/CVPR42600.2020.01229\n10.1109/CVPR.2017.687\n10.1109/TPAMI.2016.2636150\n10.1109/TMI.2020.2995108\n10.1109/TMI.2022.3161829"}
{"title": "Conv-CapsNet: capsule based network for COVID-19 detection through X-Ray scans.", "abstract": "Coronavirus, a virus that spread worldwide rapidly and was eventually declared a pandemic. The rapid spread made it essential to detect Coronavirus infected people to control the further spread. Recent studies show that radiological images such as X-Rays and CT scans provide essential information in detecting infection using deep learning models. This paper proposes a shallow architecture based on Capsule Networks with convolutional layers to detect COVID-19 infected persons. The proposed method combines the ability of the capsule network to understand spatial information with convolutional layers for efficient feature extraction. Due to the model's shallow architecture, it has 23", "journal": "Multimedia tools and applications", "date": "2023-02-28", "authors": ["PulkitSharma", "RhythmArya", "RichaVerma", "BinduVerma"], "doi": "10.1007/s11042-023-14353-w\n10.1016/j.patrec.2020.09.010\n10.1007/s13246-020-00865-4\n10.1007/s42979-020-00383-w\n10.1016/j.cmpb.2022.106833\n10.1109/TGRS.2021.3090410\n10.1016/j.chemosphere.2021.132569\n10.1007/s42979-021-00881-5\n10.1016/j.eswa.2020.113909\n10.1109/ACCESS.2020.3010287\n10.1007/s11547-020-01232-9\n10.1016/j.imu.2020.100297\n10.1016/j.chaos.2020.110495\n10.1016/j.imu.2020.100412\n10.1109/ACCESS.2021.3058537\n10.1007/s42979-020-00335-4\n10.1016/j.eswa.2020.114054\n10.1016/j.asoc.2020.106744\n10.1007/s10140-020-01808-y\n10.1016/j.slast.2021.10.011\n10.1007/s42979-020-00216-w\n10.1016/j.chaos.2020.110245\n10.1016/j.imu.2020.100360\n10.1007/s42979-021-00774-7\n10.1016/j.imu.2020.100505\n10.1016/j.aej.2021.01.011\n10.1016/j.chaos.2020.110122\n10.1016/j.mehy.2020.109761\n10.1109/JSTSP.2019.2902305\n10.1007/s13244-018-0639-9\n10.1038/s41746-020-00372-6"}
{"title": "Detection of COVID-19 Case from Chest CT Images Using Deformable Deep Convolutional Neural Network.", "abstract": "The infectious coronavirus disease (COVID-19) has become a great threat to global human health. Timely and rapid detection of COVID-19 cases is very crucial to control its spreading through isolation measures as well as for proper treatment. Though the real-time reverse transcription-polymerase chain reaction (RT-PCR) test is a widely used technique for COVID-19 infection, recent researches suggest chest computed tomography (CT)-based screening as an effective substitute in cases of time and availability limitations of RT-PCR. In consequence, deep learning-based COVID-19 detection from chest CT images is gaining momentum. Furthermore, visual analysis of data has enhanced the opportunities of maximizing the prediction performance in this big data and deep learning realm. In this article, we have proposed two separate deformable deep networks converting from the conventional convolutional neural network (CNN) and the state-of-the-art ResNet-50, to detect COVID-19 cases from chest CT images. The impact of the deformable concept has been observed through performance comparative analysis among the designed deformable and normal models, and it is found that the deformable models show better prediction results than their normal form. Furthermore, the proposed deformable ResNet-50 model shows better performance than the proposed deformable CNN model. The gradient class activation mapping (Grad-CAM) technique has been used to visualize and check the targeted regions' localization effort at the final convolutional layer and has been found excellent. Total 2481 chest CT images have been used to evaluate the performance of the proposed models with a train-valid-test data splitting ratio of 80\u2009:\u200910\u2009:\u200910 in random fashion. The proposed deformable ResNet-50 model achieved training accuracy of 99.5% and test accuracy of 97.6% with specificity of 98.5% and sensitivity of 96.5% which are satisfactory compared with related works. The comprehensive discussion demonstrates that the proposed deformable ResNet-50 model-based COVID-19 detection technique can be useful for clinical applications.", "journal": "Journal of healthcare engineering", "date": "2023-02-28", "authors": ["MdFoysal", "A B M AowladHossain", "AbdulsalamYassine", "M ShamimHossain"], "doi": "10.1155/2023/4301745\n10.1016/j.ijid.2020.02.050\n10.1007/s10489-020-01943-6\n10.1016/j.patcog.2020.107700\n10.1007/s11042-020-09894-3\n10.1007/s00521-020-05437-x\n10.1007/s00330-020-07044-9\n10.1016/j.compbiomed.2020.104037\n10.1007/s10096-020-03901-z\n10.3389/fmed.2020.608525\n10.1109/tmi.2020.2994908\n10.1109/access.2020.3018498\n10.1007/s11548-020-02286-w\n10.3390/app11157004\n10.1109/INCET51464.2021.9456387\n10.1109/jbhi.2020.3019505\n10.1007/s42979-021-00782-7\n10.1007/s00330-021-07715-1\n10.1109/CITS49457.2020.9232574\n10.1016/j.inffus.2021.02.013\n10.1159/000509223\n10.1109/mnet.011.2000458\n10.1016/j.scs.2020.102582\n10.1155/2022/2950699\n10.1109/tnse.2020.3026637\n10.1109/jiot.2020.3033129\n10.1109/jiot.2020.3047662\n10.1109/jiot.2020.3013710\n10.1109/jiot.2017.2772959\n10.1016/j.eswa.2019.112821\n10.1148/radiol.2020200642\n10.1007/s11547-020-01237-4\n10.1007/s11263-019-01228-7"}
{"title": "A convolutional neural network with pixel-wise sparse graph reasoning for COVID-19 lesion segmentation in CT images.", "abstract": "The COVID-19 pandemic has extremely threatened human health, and automated algorithms are needed to segment infected regions in the lung using computed tomography (CT). Although several deep convolutional neural networks (DCNNs) have proposed for this purpose, their performance on this task is suppressed due to the limited local receptive field and deficient global reasoning ability. To address these issues, we propose a segmentation network with a novel pixel-wise sparse graph reasoning (PSGR) module for the segmentation of COVID-19 infected regions in CT images. The PSGR module, which is inserted between the encoder and decoder of the network, can improve the modeling of global contextual information. In the PSGR module, a graph is first constructed by projecting each pixel on a node based on the features produced by the encoder. Then, we convert the graph into a sparsely-connected one by keeping K strongest connections to each uncertainly segmented pixel. Finally, the global reasoning is performed on the sparsely-connected graph. Our segmentation network was evaluated on three publicly available datasets and compared with a variety of widely-used segmentation models. Our results demonstrate that (1) the proposed PSGR module can capture the long-range dependencies effectively and (2) the segmentation model equipped with this PSGR module can accurately segment COVID-19 infected regions in CT images and outperform all other competing models.", "journal": "Computers in biology and medicine", "date": "2023-02-27", "authors": ["HaozheJia", "HaotengTang", "GuixiangMa", "WeidongCai", "HengHuang", "LiangZhan", "YongXia"], "doi": "10.1016/j.compbiomed.2023.106698\n10.1007/978-3-030-58520-4\u02d91\n10.5281/zenodo.3757476"}
{"title": "Artificial Intelligence-Assisted Chest X-ray for the Diagnosis of COVID-19: A Systematic Review and Meta-Analysis.", "abstract": "Because it is an accessible and routine image test, medical personnel commonly use a chest X-ray for COVID-19 infections. Artificial intelligence (AI) is now widely applied to improve the precision of routine image tests. Hence, we investigated the clinical merit of the chest X-ray to detect COVID-19 when assisted by AI. We used PubMed, Cochrane Library, MedRxiv, ArXiv, and Embase to search for relevant research published between 1 January 2020 and 30 May 2022. We collected essays that dissected AI-based measures used for patients diagnosed with COVID-19 and excluded research lacking measurements using relevant parameters (i.e., sensitivity, specificity, and area under curve). Two independent researchers summarized the information, and discords were eliminated by consensus. A random effects model was used to calculate the pooled sensitivities and specificities. The sensitivity of the included research studies was enhanced by eliminating research with possible heterogeneity. A summary receiver operating characteristic curve (SROC) was generated to investigate the diagnostic value for detecting COVID-19 patients. Nine studies were recruited in this analysis, including 39,603 subjects. The pooled sensitivity and specificity were estimated as 0.9472 (", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-02-26", "authors": ["I-ShiangTzeng", "Po-ChunHsieh", "Wen-LinSu", "Tsung-HanHsieh", "Sheng-ChangChang"], "doi": "10.3390/diagnostics13040584\n10.1038/s41579-020-00459-7\n10.1001/jamainternmed.2020.0994\n10.1136/bmj.n693\n10.1017/S0950268820001399\n10.1038/s41598-019-45256-2\n10.1136/thoraxjnl-2021-218337\n10.3390/app11062751\n10.1038/s41591-022-01689-3\n10.1007/s11239-021-02447-x\n10.1016/S2213-2600(20)30304-0\n10.1056/NEJMoa2002032\n10.1002/jmv.25976\n10.1093/cvr/cvaa096\n10.1148/radiol.2020200463\n10.1007/s00117-020-00749-4\n10.12968/hmed.2020.0231\n10.1148/radiol.2020200343\n10.1148/radiol.2020201160\n10.1007/s12559-021-09955-1\n10.1109/TMI.2014.2377694\n10.1038/s41598-017-15617-w\n10.1016/j.clinimag.2020.04.001\n10.1016/j.ejrad.2020.109272\n10.1155/2020/9756518\n10.1186/1471-2288-3-25\n10.12788/fp.0045\n10.1145/3466690\n10.1007/s00330-021-08050-1\n10.1007/s42979-021-00690-w\n10.7717/peerj.10309\n10.1016/j.imu.2020.100405\n10.1007/s42600-020-00091-7\n10.2196/19569\n10.1136/bmj.323.7305.157\n10.1111/febs.15375\n10.1016/S2589-7500(20)30186-2\n10.1053/j.semnuclmed.2020.09.001\n10.1038/s42256-021-00307-0\n10.1101/2020.03.12.20027185\n10.1007/s10479-021-04006-2\n10.1109/ACCESS.2021.3085418\n10.1109/TCBB.2021.3066331\n10.1007/s10916-021-01747-2\n10.1371/journal.pone.0067863\n10.1109/JBHI.2020.3037127\n10.1155/2021/7788491\n10.1016/j.bspc.2021.102622\n10.5152/dir.2020.20205\n10.1148/radiol.2020201491\n10.1016/j.cell.2020.04.045\n10.1038/s41467-020-17971-2\n10.1007/s11547-020-01195-x\n10.1148/radiol.2020200905\n10.1183/13993003.00775-2020\n10.1016/j.ejrad.2020.109041\n10.1038/s41591-020-0931-3\n10.1007/s11547-020-01197-9\n10.1109/TMI.2020.2995965\n10.1007/s10096-020-03901-z\n10.1016/j.ejrad.2020.109402\n10.1016/j.media.2021.102216\n10.3390/diagnostics12040869"}
{"title": "Multi-scale Triplet Hashing for Medical Image Retrieval.", "abstract": "For medical image retrieval task, deep hashing algorithms are widely applied in large-scale datasets for auxiliary diagnosis due to the retrieval efficiency advantage of hash codes. Most of which focus on features learning, whilst neglecting the discriminate area of medical images and hierarchical similarity for deep features and hash codes. In this paper, we tackle these dilemmas with a new Multi-scale Triplet Hashing (MTH) algorithm, which can leverage multi-scale information, convolutional self-attention and hierarchical similarity to learn effective hash codes simultaneously. The MTH algorithm first designs multi-scale DenseBlock module to learn multi-scale information of medical images. Meanwhile, a convolutional self-attention mechanism is developed to perform information interaction of the channel domain, which can capture the discriminate area of medical images effectively. On top of the two paths, a novel loss function is proposed to not only conserve the category-level information of deep features and the semantic information of hash codes in the learning process, but also capture the hierarchical similarity for deep features and hash codes. Extensive experiments on the Curated X-ray Dataset, Skin Cancer MNIST Dataset and COVID-19 Radiography Dataset illustrate that the MTH algorithm can further enhance the effect of medical retrieval compared to other state-of-the-art medical image retrieval algorithms.", "journal": "Computers in biology and medicine", "date": "2023-02-25", "authors": ["YaxiongChen", "YiboTang", "JinghaoHuang", "ShengwuXiong"], "doi": "10.1016/j.compbiomed.2023.106633"}
{"title": "Deep learning attention-guided radiomics for COVID-19 chest radiograph classification.", "abstract": "Accurate assessment of coronavirus disease 2019 (COVID-19) lung involvement through chest radiograph plays an important role in effective management of the infection. This study aims to develop a two-step feature merging method to integrate image features from deep learning and radiomics to differentiate COVID-19, non-COVID-19 pneumonia and normal chest radiographs (CXR).\nIn this study, a deformable convolutional neural network (deformable CNN) was developed and used as a feature extractor to obtain 1,024-dimensional deep learning latent representation (DLR) features. Then 1,069-dimensional radiomics features were extracted from the region of interest (ROI) guided by deformable CNN's attention. The two feature sets were concatenated to generate a merged feature set for classification. For comparative experiments, the same process has been applied to the DLR-only feature set for verifying the effectiveness of feature concatenation.\nUsing the merged feature set resulted in an overall average accuracy of 91.0% for three-class classification, representing a statistically significant improvement of 0.6% compared to the DLR-only classification. The recall and precision of classification into the COVID-19 class were 0.926 and 0.976, respectively. The feature merging method was shown to significantly improve the classification performance as compared to using only deep learning features, regardless of choice of classifier (P value <0.0001). Three classes' F1-score were 0.892, 0.890, and 0.950 correspondingly (i.e., normal, non-COVID-19 pneumonia, COVID-19).\nA two-step COVID-19 classification framework integrating information from both DLR and radiomics features (guided by deep learning attention mechanism) has been developed. The proposed feature merging method has been shown to improve the performance of chest radiograph classification as compared to the case of using only deep learning features.", "journal": "Quantitative imaging in medicine and surgery", "date": "2023-02-24", "authors": ["DongrongYang", "GeRen", "RuiyanNi", "Yu-HuaHuang", "Ngo Fung DanielLam", "HongfeiSun", "Shiu Bun NelsonWan", "Man Fung EstherWong", "King KwongChan", "Hoi Ching HaileyTsang", "LuXu", "Tak ChiuWu", "Feng-Ming SpringKong", "Y\u00ec Xi\u00e1ng JW\u00e1ng", "JingQin", "Lawrence Wing ChiChan", "MichaelYing", "JingCai"], "doi": "10.21037/qims-22-531\n10.1186/s13244-021-01096-1\n10.1148/radiol.2021204522\n10.1148/radiol.2020203173\n10.21037/qims-20-771\n10.1007/s13755-021-00146-8\n10.1111/exsy.12749\n10.1080/00325481.2021.2021741\n10.1016/j.radi.2022.03.011\n10.1016/j.eswa.2022.117410\n10.21037/qims-21-791\n10.1016/j.asoc.2021.108190\n10.1016/j.bspc.2021.103182\n10.1007/s10489-020-01829-7\n10.1016/j.compbiomed.2020.104181\n10.3390/diagnostics12020267\n10.1016/j.chaos.2020.110495\n10.1016/j.bspc.2021.103286\n10.3389/fonc.2019.01050\n10.1088/1361-6560/aae56a\n10.1177/20552076221092543\n10.1016/j.ejrad.2021.109673\n10.1109/TNNLS.2021.3119071\n10.1016/j.compbiomed.2021.104304\n10.1007/s10278-021-00421-w\n10.3233/XST-200831\n10.3390/diagnostics11101812\n10.1016/j.compbiomed.2021.104665\n10.1002/mp.15582\n10.1109/CVPR.2017.243\n10.1109/CVPR.2017.243\n10.1109/WACV.2018.00097\n10.1109/WACV.2018.00097\n10.1016/j.compbiomed.2021.104319\n10.1016/j.compbiomed.2021.104319\n10.1109/CiSt49399.2021.9357250\n10.1109/CiSt49399.2021.9357250\n10.1109/ICCV.2017.89\n10.1109/ICCV.2017.89\n10.1007/s11263-015-0816-y\n10.1158/0008-5472.CAN-17-0339\n10.1038/srep11044\n10.1038/srep46349\n10.1002/mp.13891\n10.1007/s13278-021-00731-5\n10.21037/qims-20-1230"}
{"title": "SecureFed: federated learning empowered medical imaging technique to analyze lung abnormalities in chest X-rays.", "abstract": "Machine learning is an effective and accurate technique to diagnose COVID-19 infections using image data, and chest X-Ray (CXR) is no exception. Considering privacy issues, machine learning scientists end up receiving less medical imaging data. Federated Learning (FL) is a privacy-preserving distributed machine learning paradigm that generates an unbiased global model that follows local model (from clients) without exposing their personal data. In the\u00a0case of heterogeneous data among clients, vanilla or default FL mechanism still introduces an insecure method for updating models. Therefore, we proposed SecureFed-a secure aggregation method-which ensures fairness and robustness. In our experiments, we employed COVID-19 CXR dataset (of size 2100 positive cases) and compared it\u00a0with the existing FL frameworks such as FedAvg, FedMGDA+, and FedRAD. In our comparison, we primarily considered robustness (accuracy) and fairness (consistency). As the SecureFed produced consistently better results, it is generic enough to be considered for multimodal data.", "journal": "International journal of machine learning and cybernetics", "date": "2023-02-24", "authors": ["AaishaMakkar", "K CSantosh"], "doi": "10.1007/s13042-023-01789-7\n10.1016/j.patrec.2020.09.010\n10.1021/acsnano.0c04421\n10.1016/j.patrec.2019.11.013\n10.1007/s10916-020-01668-6\n10.3390/app10020559\n10.1109/ACCESS.2020.3010287\n10.3390/sym12040651\n10.1016/j.ins.2022.01.062\n10.1109/MCE.2020.3048926\n10.1007/s10489-020-01943-6\n10.1109/MCI.2018.2866730\n10.1007/s10916-020-01562-1\n10.1007/s10916-020-01645-z\n10.1007/s10916-021-01747-2\n10.1080/21642583.2022.2045645\n10.1109/JIOT.2021.3056185\n10.1109/ACCESS.2019.2903587"}
{"title": "DPDH-CapNet: A Novel Lightweight Capsule Network with Non-routing for COVID-19 Diagnosis Using X-ray Images.", "abstract": "COVID-19 has claimed millions of lives since its outbreak in December 2019, and the damage continues, so it is urgent to develop new technologies to aid its diagnosis. However, the state-of-the-art deep learning methods often rely on large-scale labeled data, limiting their clinical application in COVID-19 identification. Recently, capsule networks have achieved highly competitive performance for COVID-19 detection, but they require expensive routing computation or traditional matrix multiplication to deal with the capsule dimensional entanglement. A more lightweight capsule network is developed to effectively address these problems, namely DPDH-CapNet, which aims to enhance the technology of automated diagnosis for COVID-19 chest X-ray images. It adopts depthwise convolution (D), point convolution (P), and dilated convolution (D) to construct a new feature extractor, thus successfully capturing the local and global dependencies of COVID-19 pathological features. Simultaneously, it constructs the classification layer by homogeneous (H) vector capsules with an adaptive, non-iterative, and non-routing mechanism. We conduct experiments on two publicly available combined datasets, including normal, pneumonia, and COVID-19 images. With a limited number of samples, the parameters of the proposed model are reduced by 9x compared to the state-of-the-art capsule network. Moreover, our model has faster convergence speed and better generalization, and its accuracy, precision, recall, and F-measure are improved to 97.99%, 98.05%, 98.02%, and 98.03%, respectively. In addition, experimental results demonstrate that, contrary to the transfer learning method, the proposed model does not require pre-training and a large number of training samples.", "journal": "Journal of digital imaging", "date": "2023-02-23", "authors": ["JianjunYuan", "FujunWu", "YuxiLi", "JinyiLi", "GuojunHuang", "QuanyongHuang"], "doi": "10.1007/s10278-023-00791-3\n10.48550/arXiv.2002.04764"}
{"title": "A multiple instance learning approach for detecting COVID-19 in peripheral blood smears.", "abstract": "A wide variety of diseases are commonly diagnosed via the visual examination of cell morphology within a peripheral blood smear. For certain diseases, such as COVID-19, morphological impact across the multitude of blood cell types is still poorly understood. In this paper, we present a multiple instance learning-based approach to aggregate high-resolution morphological information across many blood cells and cell types to automatically diagnose disease at a per-patient level. We integrated image and diagnostic information from across 236 patients to demonstrate not only that there is a significant link between blood and a patient's COVID-19 infection status, but also that novel machine learning approaches offer a powerful and scalable means to analyze peripheral blood smears. Our results both backup and enhance hematological findings relating blood cell morphology to COVID-19, and offer a high diagnostic efficacy; with a 79% accuracy and a ROC-AUC of 0.90.", "journal": "PLOS digital health", "date": "2023-02-23", "authors": ["Colin LCooke", "KanghyunKim", "ShiqiXu", "AmeyChaware", "XingYao", "XiYang", "JadeeNeff", "PatriciaPittman", "ChadMcCall", "CarolynGlass", "Xiaoyin SaraJiang", "RoarkeHorstmeyer"], "doi": "10.1371/journal.pdig.0000078\n10.3343/alm.2013.33.1.1\n10.1136/jcp.2005.035402\n10.1177/1533033818802789\n10.1038/s41598-019-49942-z\n10.1371/journal.pcbi.1005746\n10.1371/journal.pone.0241955\n10.1007/s12015-020-09987-4\n10.2450/2020.0242-20\n10.1159/000510914\n10.1136/bcr-2020-236117\n10.1093/ajcp/aqaa108\n10.4269/ajtmh.20-1536\n10.1016/j.cca.2020.03.022\n10.1016/j.mehy.2020.110371\n10.3390/cells9102206\n10.3389/fimmu.2020.02063\n10.1182/blood.2020007008\n10.1016/j.intimp.2020.107233\n10.1038/2141341a0\n10.1111/bjh.16690\n10.1364/OL.426152\n10.1001/jamainternmed.2018.3763"}
{"title": "Diagnosis of COVID-19 from Multimodal Imaging Data Using Optimized Deep Learning Techniques.", "abstract": "COVID-19 had a global impact, claiming many lives and disrupting healthcare systems even in many developed countries. Various mutations of the severe acute respiratory syndrome coronavirus-2, continue to be an impediment to early detection of this disease, which is vital for social well-being. Deep learning paradigm has been widely applied to investigate multimodal medical image data such as chest X-rays and CT scan images to aid in early detection and decision making about disease containment and treatment. Any method for reliable and accurate screening of COVID-19 infection would be beneficial for rapid detection as well as reducing direct virus exposure in healthcare professionals. Convolutional neural networks (CNN) have previously proven to be quite successful in the classification of medical images. A CNN is used in this study to suggest a deep learning classification method for detecting COVID-19 from chest X-ray images and CT scans. Samples from the Kaggle repository were collected to analyse model performance. Deep learning-based CNN models such as VGG-19, ResNet-50, Inception v3 and Xception models are optimized and compared by evaluating their accuracy after pre-processing the data. Because X-ray is a less expensive process than CT scan, chest X-ray images are considered to have a significant impact on COVID-19 screening. According to this work, chest X-rays outperform CT scans in terms of detection accuracy. The fine-tuned VGG-19 model detected COVID-19 with high accuracy-up to 94.17% for chest X-rays and 93% for CT scans. This work thereby concludes that VGG-19 was found to be the best suited model to detect COVID-19 and chest X-rays yield better accuracy than CT scans for the model.", "journal": "SN computer science", "date": "2023-02-23", "authors": ["S EzhilMukhi", "R ThanujaVarshini", "S Eliza FemiSherley"], "doi": "10.1007/s42979-022-01653-5\n10.1007/s42600-021-00151-6\n10.1016/j.bspc.2020.102365\n10.1007/s10489-020-01902-1\n10.3390/math8060890\n10.3390/jcm9051547\n10.1101/2020.05.05.20085902\n10.1101/2020.03.19.20038950\n10.32604/csse.2022.021438\n10.1016/j.imu.2022.101059\n10.1109/JBHI.2021.3132157"}
{"title": "A qualitative analysis of radiography students' reflective essays regarding their experience of clinical placement during the COVID-19 pandemic.", "abstract": "The COVID-19 pandemic significantly impacted healthcare services and clinical placement for healthcare students. There is a paucity of qualitative research into radiography students' experiences of clinical placement during the pandemic.\nStudents in stages three and four of a 4-year BSc Radiography degree in Ireland wrote reflective essays regarding their experience of clinical placement during the COVID-19 healthcare crisis. Permission was granted by 108 radiography students and recent graduates for their reflections to be analysed as part of this study. A thematic approach to data analysis was used, allowing themes to emerge from the reflective essays. Two researchers independently coded each reflective essay using the Braun and Clarke model.\nFour themes were highlighted; 1) Challenges associated with undertaking clinical placement during the pandemic, such as reduced patient throughput and PPE-related communication barriers; 2) Benefits of clinical placement during the pandemic, in terms of personal and professional development and completing degree requirements to graduate without delay; 3) Emotional impact and 4) Supporting students in clinical practice. Students recognised their resilience and felt proud of their contribution during this healthcare crisis but feared transmitting COVID-19 to family. Educational and emotional support provided by tutors, clinical staff and the university was deemed essential by students during this placement.\nDespite the pressure hospitals were under during the pandemic, students had positive clinical placement experiences and perceived these experiences to have contributed to their professional and personal growth.\nThis study supports the argument for clinical placements to continue throughout healthcare crisis periods, albeit with additional learning and emotional support in place. Clinical placement experiences during the pandemic prompted a deep sense of pride amongst radiography students in their profession and contributed to the development of professional identity.", "journal": "Radiography (London, England : 1995)", "date": "2023-02-23", "authors": ["MO'Connor", "ALunney", "DKearney", "SMurphy"], "doi": "10.1016/j.radi.2023.01.022"}
{"title": "Hybrid feature engineering of medical data via variational autoencoders with triplet loss: a COVID-19 prognosis study.", "abstract": "Medical machine learning frameworks have received much attention in recent years. The recent COVID-19 pandemic was also accompanied by a surge in proposed machine learning algorithms for tasks such as diagnosis and mortality prognosis. Machine learning frameworks can be helpful medical assistants by extracting data patterns that are otherwise hard to detect by humans. Efficient feature engineering and dimensionality reduction are major challenges in most medical machine learning frameworks. Autoencoders are novel unsupervised tools that can perform data-driven dimensionality reduction with minimum prior assumptions. This study, in a novel approach, investigated the predictive power of latent representations obtained from a hybrid autoencoder (HAE) framework combining variational autoencoder (VAE) characteristics with mean squared error (MSE) and triplet loss for forecasting COVID-19 patients with high mortality risk in a retrospective framework. Electronic laboratory and clinical data of 1474 patients were used in the study. Logistic regression with elastic net regularization (EN) and random forest (RF) models were used as final classifiers. Moreover, we also investigated the contribution of utilized features towards latent representations via mutual information analysis. HAE Latent representations model achieved decent performance with an area under ROC curve of 0.921 (\u00b10.027) and 0.910 (\u00b10.036) with EN and RF predictors, respectively, over the hold-out data in comparison with the raw (AUC EN: 0.913 (\u00b10.022); RF: 0.903 (\u00b10.020)) models. The study aims to provide an interpretable feature engineering framework for the medical environment with the potential to integrate imaging data for efficient feature engineering in rapid triage and other clinical predictive models.", "journal": "Scientific reports", "date": "2023-02-23", "authors": ["MahdiMahdavi", "HadiChoubdar", "ZahraRostami", "BehnazNiroomand", "Alexandra TLevine", "AlirezaFatemi", "EhsanBolhasani", "Abdol-HosseinVahabie", "Stephen GLomber", "YaserMerrikhi"], "doi": "10.1038/s41598-023-29334-0\n10.1016/j.ijantimicag.2020.105955\n10.1371/journal.pone.0252384\n10.1371/journal.pmed.1003992\n10.1080/1744666X.2021.1908886\n10.1080/17476348.2022.2049760\n10.1007/s11010-021-04217-y\n10.1142/S1469026820500029\n10.1002/mpr.329\n10.1213/ANE.0000000000005247\n10.1016/j.jbi.2021.103763\n10.1109/ACCESS.2018.2789428\n10.1111/j.1467-9868.2005.00503.x\n10.1186/1471-2105-15-8\n10.1016/j.inffus.2017.12.007\n10.1007/s10462-011-9225-y\n10.1016/j.procs.2020.01.079\n10.1038/s41598-021-93543-8\n10.3390/ijerph19116763\n10.1073/pnas.1906831117\n10.3390/s20041176\n10.3390/app112110377\n10.3389/frai.2020.507973\n10.1002/nop2.1069\n10.1007/s00330-020-06801-0\n10.1038/s41591-020-0931-3\n10.1007/s10654-020-00678-5\n10.3389/fmed.2020.00301\n10.1016/j.ebiom.2020.102925\n10.1038/s41577-020-0407-1\n10.3390/cells9061383\n10.12998/wjcc.v8.i22.5535\n10.1038/s41392-020-00243-2"}
{"title": "Artificial intelligence based approach for categorization of COVID-19 ECG images in presence of other cardiovascular disorders.", "abstract": "Coronavirus disease (COVID-19) is a class of SARS-CoV-2 virus which is initially identified in the later half of the year 2019 and then evolved as a pandemic. If it is not identified in the early stage then the infection and mortality rates increase with time. A timely and reliable approach for COVID-19 identification has become important in order to prevent the disease from spreading rapidly. In recent times, many methods have been suggested for the detection of COVID-19 disease have various flaws, to increase diagnosis performance, fresh investigations are required. In this article, automatically diagnosing COVID-19 using ECG images and deep learning approaches like as Visual Geometry Group (VGG) and AlexNet architectures have been proposed. The proposed method is able to classify between COVID-19, myocardial infarction, normal sinus rhythm, and other abnormal heart beats using Lead-II ECG image only. The efficacy of the technique proposed is validated by using a publicly available ECG image database. We have achieved an accuracy of 77.42% using Alexnet model and 75% accuracy with the help of VGG19 model.", "journal": "Biomedical physics & engineering express", "date": "2023-02-23", "authors": ["M KrishnaChaitanya", "Lakhan DevSharma", "JagdeepRahul", "DikshaSharma", "AmarjitRoy"], "doi": "10.1088/2057-1976/acbd53"}
{"title": "Infrared image method for possible COVID-19 detection through febrile and subfebrile people screening.", "abstract": "This study proposed an infrared image-based method for febrile and subfebrile people screening to comply with the society need for alternative, quick response, and effective methods for COVID-19 contagious people screening. The methodology consisted of: (i) Developing a method based on facial infrared imaging for possible COVID-19 early detection in people with and without fever (subfebrile state); (ii) Using 1206 emergency room (ER) patients to develop an algorithm for general application of the method, and (iii) Testing the method and algorithm effectiveness in 2558 cases (RT-qPCR tested for COVID-19) from 227,261 workers evaluations in five different countries. Artificial intelligence was used through a convolutional neural network (CNN) to develop the algorithm that took facial infrared images as input and classified the tested individuals in three groups: fever (high risk), subfebrile (medium risk), and no fever (low risk). The results showed that suspicious and confirmed COVID-19 (+) cases characterized by temperatures below the 37.5\u00a0\u00b0C fever threshold were identified. Also, average forehead and eye temperatures greater than 37.5\u00a0\u00b0C were not enough to detect fever similarly to the proposed CNN algorithm. Most RT-qPCR confirmed COVID-19 (+) cases found in the 2558 cases sample (17 cases/89.5%) belonged to the CNN selected subfebrile group. The COVID-19 (+) main risk factor was to be in the subfebrile group, in comparison to age, diabetes, high blood pressure, smoking and others. In sum, the proposed method was shown to be a potentially important new tool for COVID-19 (+) people screening for air travel and public places in general.", "journal": "Journal of thermal biology", "date": "2023-02-17", "authors": ["Marcos LealBrioschi", "CarlosDalmaso Neto", "Marcos deToledo", "Eduardo BorbaNeves", "Jos\u00e9 Viriato CoelhoVargas", "Manoel JacobsenTeixeira"], "doi": "10.1016/j.jtherbio.2022.103444\n10.1073/pnas.94.6.2681\n10.1056/NEJMoa2008457\n10.1056/NEJM197201062860109\n10.1001/jama.2020.2565\n10.1212/01.wnl.0000275537.71623.8e\n10.1016/S0099-1767(99)70063-2\n10.1056/NEJMc2000231\n10.1016/S0079-6123(06)62001-3\n10.1016/j.pharmthera.2005.10.013\n10.2310/7060.2004.19102\n10.1016/S0929-6646(09)60017-6\n10.1111/j.1365-2702.2010.03565.x\n10.1016/B978-0-444-64074-1.00029-X\n10.1177/101053950501700107\n10.1371/journal.pone.0201562\n10.2741/1341\n10.1002/rmv.2141\n10.1016/j.jtherbio.2020.102616\n10.1109/EMBC.2018.8513513\n10.1016/j.patrec.2005.10.010\n10.25126/jitecs.20172235\n10.1371/journal.pone.0203302\n10.1111/j.1532-5415.2005.00500.x\n10.1186/1743-422X-7-240\n10.1007/s10278-019-00227-x\n10.1086/659404\n10.1016/S0140-6736(20)30183-5\n10.1016/0163-7258(85)90085-3\n10.1056/NEJMoa2001316\n10.1086/502351\n10.2139/ssrn.3548761\n10.1111/eci.13474\n10.1016/j.autrev.2020.102537\n10.3390/ijerph16234638\n10.1179/146532805X72412\n10.1201/b12938-25\n10.1118/1.1819532\n10.1109/MEMB.2008.931018\n10.1080/03091900500225136\n10.1142/S0219519405001370\n10.1016/j.mvr.2004.05.003\n10.1109/MEMB.2006.1636353\n10.3201/eid1611.100703\n10.1186/1471-2334-11-111\n10.1038/s41598-020-73777-8\n10.2807/1560-7917.ES.2020.25.5.2000080\n10.1142/S0219519413500450\n10.1201/b12938\n10.26717/bjstr.2018.04.0001102\n10.1111/j.1749-6632.1998.tb08317.x\n10.1097/PEC.0b013e3182854465\n10.1016/S1473-3099(20)30086-4\n10.1007/978-981-10-3147-2_19\n10.1016/j.neuron.2018.02.022\n10.1016/j.ymeth.2010.01.005\n10.1111/j.1365-2214.2011.01264.x\n10.1016/j.matcom.2020.04.031\n10.1002/9780470377963\n10.1088/1361-6579/ab2af6\n10.1016/s2213-2600(20)30461-6\n10.21203/rs.3.rs-23651/v2\n10.1016/S2213-2600(20)30079-5\n10.1117/1.jbo.25.9.097002"}
{"title": "Deep Learning With Chest Radiographs for Making Prognoses in Patients With COVID-19: Retrospective Cohort Study.", "abstract": "An artificial intelligence (AI) model using chest radiography (CXR) may provide good performance in making prognoses for COVID-19.\nWe aimed to develop and validate a prediction model using CXR based on an AI model and clinical variables to predict clinical outcomes in patients with COVID-19.\nThis retrospective longitudinal study included patients hospitalized for COVID-19 at multiple COVID-19 medical centers between February 2020 and October 2020. Patients at Boramae Medical Center were randomly classified into training, validation, and internal testing sets (at a ratio of 8:1:1, respectively). An AI model using initial CXR images as input, a logistic regression model using clinical information, and a combined model using the output of the AI model (as CXR score) and clinical information were developed and trained to predict hospital length of stay (LOS) \u22642 weeks, need for oxygen supplementation, and acute respiratory distress syndrome (ARDS). The models were externally validated in the Korean Imaging Cohort of COVID-19 data set for discrimination and calibration.\nThe AI model using CXR and the logistic regression model using clinical variables were suboptimal to predict hospital LOS \u22642 weeks or the need for oxygen supplementation but performed acceptably in the prediction of ARDS (AI model area under the curve [AUC] 0.782, 95% CI 0.720-0.845; logistic regression model AUC 0.878, 95% CI 0.838-0.919). The combined model performed better in predicting the need for oxygen supplementation (AUC 0.704, 95% CI 0.646-0.762) and ARDS (AUC 0.890, 95% CI 0.853-0.928) compared to the CXR score alone. Both the AI and combined models showed good calibration for predicting ARDS (P=.079 and P=.859).\nThe combined prediction model, comprising the CXR score and clinical information, was externally validated as having acceptable performance in predicting severe illness and excellent performance in predicting ARDS in patients with COVID-19.", "journal": "Journal of medical Internet research", "date": "2023-02-17", "authors": ["Hyun WooLee", "Hyun JunYang", "HyungjinKim", "Ue-HwanKim", "Dong HyunKim", "Soon HoYoon", "Soo-YounHam", "Bo DaNam", "Kum JuChae", "DabeeLee", "Jin YoungYoo", "So HyeonBak", "Jin YoungKim", "Jin HwanKim", "Ki BeomKim", "Jung ImJung", "Jae-KwangLim", "Jong EunLee", "Myung JinChung", "Young KyungLee", "Young SeonKim", "Sang MinLee", "WoocheolKwon", "Chang MinPark", "Yun-HyeonKim", "Yeon JooJeong", "Kwang NamJin", "Jin MoGoo"], "doi": "10.2196/42717\n10.1136/thoraxjnl-2020-216425\n10.1136/bmj.m1328\n10.3389/fmed.2021.704256\n10.2196/30157\n10.21037/atm.2020.02.71\n10.1183/13993003.04188-2020\n10.1002/emp2.12205\n10.1038/s41746-021-00546-w\n10.1038/s41746-021-00546-w\n10.1186/s12879-022-07617-7\n10.1186/s12879-022-07617-7\n10.1016/S2589-7500(21)00039-X\n10.1136/bmj.g7594\n10.3346/jkms.2020.35.e413\n10.1097/JTO.0b013e3181ec173d\n10.6339/jds.2005.03(3).206\n10.1001/jamanetworkopen.2019.0204\n10.1016/j.jbi.2017.10.008\n10.1007/s11547-020-01232-9\n10.1007/s00330-020-07504-2\n10.1371/journal.pone.0245518\n10.2214/AJR.20.24801\n10.7759/cureus.9448\n10.1038/s41598-021-86853-4\n10.1038/s41598-021-86853-4\n10.3904/kjim.2020.329\n10.1038/s41467-020-18786-x\n10.1038/s41467-020-18786-x\n10.1056/NEJMcp2009575\n10.1186/s13613-020-00650-2\n10.7861/clinmed.2020-0214\n10.4046/trd.2021.0009\n10.1080/17476348.2020.1804365\n10.1016/S2213-2600(21)00105-3\n10.1056/NEJMoa2021436\n10.1056/NEJMoa2007764"}
{"title": "LDANet: Automatic lung parenchyma segmentation from CT images.", "abstract": "Automatic segmentation of the lung parenchyma from computed tomography (CT) images is helpful for the subsequent diagnosis and treatment of patients. In this paper, based on a deep learning algorithm, a lung dense attention network (LDANet) is proposed with two mechanisms: residual spatial attention (RSA) and gated channel attention (GCA). RSA is utilized to weight the spatial information of the lung parenchyma and suppress feature activation in irrelevant regions, while the weights of each channel are adaptively calibrated using GCA to implicitly predict potential key features. Then, a dual attention guidance module (DAGM) is designed to maximize the integration of the advantages of both mechanisms. In addition, LDANet introduces a lightweight dense block (LDB) that reuses feature information and a positioned transpose block (PTB) that realizes accurate positioning and gradually restores the image resolution until the predicted segmentation map is generated. Experiments are conducted on two public datasets, LIDC-IDRI and COVID-19 CT Segmentation, on which LDANet achieves Dice similarity coefficient values of 0.98430 and 0.98319, respectively, outperforming a state-of-the-art lung segmentation model. Additionally, the effectiveness of the main components of LDANet is demonstrated through ablation experiments.", "journal": "Computers in biology and medicine", "date": "2023-02-16", "authors": ["YingChen", "LongfengFeng", "ChengZheng", "TaohuiZhou", "LanLiu", "PengfeiLiu", "YiChen"], "doi": "10.1016/j.compbiomed.2023.106659"}
{"title": "Home alone: A population neuroscience investigation of brain morphology substrates.", "abstract": "As a social species, ready exchange with peers is a pivotal asset - our \"social capital\". Yet, single-person households have come to pervade metropolitan cities worldwide, with unknown consequences in the long run. Here, we systematically explore the morphological manifestations associated with singular living in \u223c40,000 UK Biobank participants. The uncovered population-level signature spotlights the highly associative default mode network, in addition to findings such as in the amygdala central, cortical and corticoamygdaloid nuclei groups, as well as the hippocampal fimbria and dentate gyrus. Both positive effects, equating to greater gray matter volume associated with living alone, and negative effects, which can be interpreted as greater gray matter associations with not living alone, were found across the cortex and subcortical structures Sex-stratified analyses revealed male-specific neural substrates, including somatomotor, saliency and visual systems, while female-specific neural substrates centered on the dorsomedial prefrontal cortex. In line with our demographic profiling results, the discovered neural pattern of living alone is potentially linked to alcohol and tobacco consumption, anxiety, sleep quality as well as daily TV watching. The persistent trend for solitary living will require new answers from public-health decision makers. SIGNIFICANCE STATEMENT: Living alone has profound consequences for mental and physical health. Despite this, there has been a rapid increase in single-person households worldwide, with the long-term consequences yet unknown. In the largest study of its kind, we investigate how the objective lack of everyday social interaction, through living alone, manifests in the brain. Our population neuroscience approach uncovered a gray matter signature that converged on the 'default network', alongside targeted subcortical, sex and demographic profiling analyses. The human urge for social relationships is highlighted by the evolving COVID-19 pandemic. Better understanding of how social isolation relates to the brain will influence health and social policy decision-making of pandemic planning, as well as social interventions in light of global shifts in houseful structures.", "journal": "NeuroImage", "date": "2023-02-14", "authors": ["MaryAnnNoonan", "ChrisZajner", "DaniloBzdok"], "doi": "10.1016/j.neuroimage.2023.119936"}
{"title": "Classifying COVID-19 Patients From Chest X-ray Images Using Hybrid Machine Learning Techniques: Development and Evaluation.", "abstract": "The COVID-19 pandemic has raised global concern, with moderate to severe cases displaying lung inflammation and respiratory failure. Chest x-ray (CXR) imaging is crucial for diagnosis and is usually interpreted by experienced medical specialists. Machine learning has been applied with acceptable accuracy, but computational efficiency has received less attention.\nWe introduced a novel hybrid machine learning model to accurately classify COVID-19, non-COVID-19, and healthy patients from CXR images with reduced computational time and promising results. Our proposed model was thoroughly evaluated and compared with existing models.\nA retrospective study was conducted to analyze 5 public data sets containing 4200 CXR images using machine learning techniques including decision trees, support vector machines, and neural networks. The images were preprocessed to undergo image segmentation, enhancement, and feature extraction. The best performing machine learning technique was selected and combined into a multilayer hybrid classification model for COVID-19 (MLHC-COVID-19). The model consisted of 2 layers. The first layer was designed to differentiate healthy individuals from infected patients, while the second layer aimed to classify COVID-19 and non-COVID-19 patients.\nThe MLHC-COVID-19 model was trained and evaluated on unseen COVID-19 CXR images, achieving reasonably high accuracy and F measures of 0.962 and 0.962, respectively. These results show the effectiveness of the MLHC-COVID-19 in classifying COVID-19 CXR images, with improved accuracy and a reduction in interpretation time. The model was also embedded into a web-based MLHC-COVID-19 computer-aided diagnosis system, which was made publicly available.\nThe study found that the MLHC-COVID-19 model effectively differentiated CXR images of COVID-19 patients from those of healthy and non-COVID-19 individuals. It outperformed other state-of-the-art deep learning techniques and showed promising results. These results suggest that the MLHC-COVID-19 model could have been instrumental in early detection and diagnosis of COVID-19 patients, thus playing a significant role in controlling and managing the pandemic. Although the pandemic has slowed down, this model can be adapted and utilized for future similar situations. The model was also integrated into a publicly accessible web-based computer-aided diagnosis system.", "journal": "JMIR formative research", "date": "2023-02-14", "authors": ["ThanakornPhumkuea", "ThakerngWongsirichot", "KasikritDamkliang", "AsmaNavasakulpong"], "doi": "10.2196/42324\n10.1016/j.clim.2020.108427\n10.1038/s41564-020-0695-z\n10.1007/s12098-020-03263-6\n10.1056/NEJMoa2001316\n10.1016/j.jmii.2020.05.001\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1016/j.jaci.2020.04.029\n10.1001/jama.2020.2783\n10.1016/j.acra.2020.04.016\n10.1002/jmv.26830\n10.22037/aaem.v9i1.993\n10.1001/jama.2020.3786\n10.1016/j.ijid.2020.03.017\n10.1056/nejmoa2030340\n10.1016/j.patrec.2020.09.010\n10.1038/s41598-020-76550-z\n10.1038/s41598-020-76550-z\n10.1109/access.2020.3010287\n10.1038/s41746-020-00372-6\n10.1038/s41746-020-00372-6\n10.1371/journal.pone.0250688\n10.1371/journal.pone.0250688\n10.3390/a14060183\n10.1016/j.cmpb.2020.105581\n10.1016/j.mehy.2020.109761\n10.1016/j.media.2020.101794\n10.1007/s13755-020-00135-3\n10.1371/journal.pone.0256630\n10.1371/journal.pone.0256630\n10.1371/journal.pone.0247839\n10.1371/journal.pone.0247839\n10.1016/j.eswa.2020.114054\n10.1371/journal.pone.0242535\n10.1371/journal.pone.0242535\n10.1155/2021/8890226\n10.1155/2021/8890226\n10.1371/journal.pone.0029740\n10.1371/journal.pone.0029740\n10.1117/1.3115362\n10.26671/ijirg.2019.6.8.101\n10.1007/s10916-019-1376-4\n10.7763/IJIMT.2013.V4.426\n10.4015/S1016237218500412\n10.1016/j.procs.2017.08.021\n10.38094/JASTT20165\n10.1016/j.compedu.2019.04.001\n10.1016/j.procs.2019.02.085\n10.14569/IJACSA.2016.070203\n10.1007/s11227-018-2469-4\n10.1155/2018/9385947\n10.1109/MOCAST.2019.8741677\n10.5121/ijdkp.2015.5201\n10.1016/j.knosys.2011.06.013\n10.3389/fninf.2014.00014\n10.1613/jair.953\n10.1016/j.ins.2019.10.048"}
{"title": "FaxMatch: Multi-Curriculum Pseudo-Labeling for semi-supervised medical image classification.", "abstract": "Semi-supervised learning (SSL) can effectively use information from unlabeled data to improve model performance, which has great significance in medical imaging tasks. Pseudo-labeling is a classical SSL method that uses a model to predict unlabeled samples and selects the prediction with the highest confidence level as the pseudo-labels and then uses the generated pseudo-labels to train the model. Most of the current pseudo-label-based SSL algorithms use predefined fixed thresholds for all classes to select unlabeled data.\nHowever, data imbalance is a common problem in medical image tasks, where the use of fixed threshold to generate pseudo-labels ignores different classes of learning status and learning difficulties. The aim of this study is to develop an algorithm to solve this problem.\nIn this work, we propose Multi-Curriculum Pseudo-Labeling (MCPL), which evaluates the learning status of the model for each class at each epoch and automatically adjusts the thresholds for each class. We apply MCPL to FixMatch and propose a new SSL framework for medical image classification, which we call the improved algorithm FaxMatch. To mitigate the impact of incorrect pseudo-labels on the model, we use label smoothing (LS) strategy to generate soft labels (SL) for pseudo-labels.\nWe have conducted extensive experiments to evaluate our method on two public benchmark medical image classification datasets: the ISIC 2018 skin lesion analysis and COVID-CT datasets. Experimental results show that our method outperforms fully supervised baseline, which uses only labeled data to train the model. Moreover, our method also outperforms other state-of-the-art methods.\nWe propose MCPL and construct a semi-supervised medical image classification framework to reduce the reliance of the model on a large number of labeled images and reduce the manual workload of labeling medical image data.", "journal": "Medical physics", "date": "2023-02-14", "authors": ["ZhenPeng", "DezhiZhang", "ShengweiTian", "WeidongWu", "LongYu", "ShaofengZhou", "ShanhangHuang"], "doi": "10.1002/mp.16312"}
{"title": "CNN-RNN Network Integration for the Diagnosis of COVID-19 Using Chest X-ray and CT Images.", "abstract": "The 2019 coronavirus disease (COVID-19) has rapidly spread across the globe. It is crucial to identify positive cases as rapidly as humanely possible to provide appropriate treatment for patients and prevent the pandemic from spreading further. Both chest X-ray and computed tomography (CT) images are capable of accurately diagnosing COVID-19. To distinguish lung illnesses (i.e., COVID-19 and pneumonia) from normal cases using chest X-ray and CT images, we combined convolutional neural network (CNN) and recurrent neural network (RNN) models by replacing the fully connected layers of CNN with a version of RNN. In this framework, the attributes of CNNs were utilized to extract features and those of RNNs to calculate dependencies and classification base on extracted features. CNN models VGG19, ResNet152V2, and DenseNet121 were combined with long short-term memory (LSTM) and gated recurrent unit (GRU) RNN models, which are convenient to develop because these networks are all available as features on many platforms. The proposed method is evaluated using a large dataset totaling 16,210 X-ray and CT images (5252 COVID-19 images, 6154 pneumonia images, and 4804 normal images) were taken from several databases, which had various image sizes, brightness levels, and viewing angles. Their image quality was enhanced via normalization, gamma correction, and contrast-limited adaptive histogram equalization. The ResNet152V2 with GRU model achieved the best architecture with an accuracy of 93.37%, an F1 score of 93.54%, a precision of 93.73%, and a recall of 93.47%. From the experimental results, the proposed method is highly effective in distinguishing lung diseases. Furthermore, both CT and X-ray images can be used as input for classification, allowing for the rapid and easy detection of COVID-19.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-02-12", "authors": ["IsoonKanjanasurat", "KasiTenghongsakul", "BoonchanaPurahong", "AttasitLasakul"], "doi": "10.3390/s23031356\n10.1515/labmed-2020-0135\n10.1016/j.bios.2020.112830\n10.1016/j.eswa.2022.117275\n10.1007/s42452-021-04427-5\n10.3390/app12157953\n10.1109/ICEAST55249.2022.9826319\n10.1109/TMI.2020.3040950\n10.1016/j.imu.2020.100412\n10.1016/j.compbiomed.2021.104319\n10.1016/j.chemolab.2022.104695\n10.1016/j.ejrad.2020.109041\n10.1016/j.eng.2020.04.010\n10.1007/s10489-020-01831-z\n10.1016/j.asoc.2021.107918\n10.48550/arXiv.2006.11988\n10.1109/ACCESS.2020.3010287\n10.1016/j.cell.2020.04.045\n10.1016/j.cell.2018.02.010\n10.1109/ICACCI.2014.6968381\n10.1007/BF03178082\n10.48550/arXiv.1409.1556\n10.1145/3065386\n10.1007/978-3-319-46493-0_38\n10.1109/CVPR.2017.243\n10.1162/neco.1997.9.8.1735\n10.48550/arXiv.1406.1078\n10.48550/arxiv.1207.0580\n10.1021/ci0342472\n10.1140/epjs/s11734-022-00647-x\n10.1016/j.compbiomed.2020.103792\n10.1080/07391102.2020.1767212\n10.1016/j.imu.2020.100360\n10.48550/arXiv.2201.09952\n10.1159/000521658\n10.1145/3551647\n10.1051/matecconf/201927702001"}
{"title": "COVID-19 Classification on Chest X-ray Images Using Deep Learning Methods.", "abstract": "Since December 2019, the coronavirus disease has significantly affected millions of people. Given the effect this disease has on the pulmonary systems of humans, there is a need for chest radiographic imaging (CXR) for monitoring the disease and preventing further deaths. Several studies have been shown that Deep Learning models can achieve promising results for COVID-19 diagnosis towards the CXR perspective. In this study, five deep learning models were analyzed and evaluated with the aim of identifying COVID-19 from chest X-ray images. The scope of this study is to highlight the significance and potential of individual deep learning models in COVID-19 CXR images. More specifically, we utilized the ResNet50, ResNet101, DenseNet121, DenseNet169 and InceptionV3 using Transfer Learning. All models were trained and validated on the largest publicly available repository for COVID-19 CXR images. Furthermore, they were evaluated on unknown data that was not used for training or validation, authenticating their performance and clarifying their usage in a medical scenario. All models achieved satisfactory performance where ResNet101 was the superior model achieving 96% in Precision, Recall and Accuracy, respectively. Our outcomes show the potential of deep learning models on COVID-19 medical offering a promising way for the deeper understanding of COVID-19.", "journal": "International journal of environmental research and public health", "date": "2023-02-12", "authors": ["MariosConstantinou", "ThemisExarchos", "Aristidis GVrahatis", "PanagiotisVlamos"], "doi": "10.3390/ijerph20032035\n10.15167/2421-4248/jpmh2020.61.3.1530\n10.1080/14737159.2020.1757437\n10.1101/2022.02.11.22270873\n10.1148/radiol.2020200642\n10.2214/AJR.20.23034\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMra072149\n10.1109/RBME.2020.2987975\n10.1109/TIM.2021.3128703\n10.1109/TMI.2022.3219286\n10.1016/j.apacoust.2020.107279\n10.1016/j.measurement.2018.05.033\n10.21608/mjeer.2019.76962\n10.1038/nature21056\n10.1038/s41598-019-48995-4\n10.1109/ACCESS.2020.3010287\n10.1007/s00330-021-08050-1\n10.1016/j.compbiomed.2020.103805\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1038/s41598-020-76550-z\n10.1109/ACCESS.2020.2994762\n10.1016/j.compbiomed.2021.105002\n10.1007/s11042-022-12156-z\n10.1016/j.media.2020.101797\n10.1109/TMI.2013.2284099\n10.1109/TMI.2013.2290491\n10.1007/s13755-021-00146-8\n10.1088/1361-6560/ac34b2"}
{"title": "Interpretable Differential Diagnosis of Non-COVID Viral Pneumonia, Lung Opacity and COVID-19 Using Tuned Transfer Learning and Explainable AI.", "abstract": "The coronavirus epidemic has spread to virtually every country on the globe, inflicting enormous health, financial, and emotional devastation, as well as the collapse of healthcare systems in some countries. Any automated COVID detection system that allows for fast detection of the COVID-19 infection might be highly beneficial to the healthcare service and people around the world. Molecular or antigen testing along with radiology X-ray imaging is now utilized in clinics to diagnose COVID-19. Nonetheless, due to a spike in coronavirus and hospital doctors' overwhelming workload, developing an AI-based auto-COVID detection system with high accuracy has become imperative. On X-ray images, the diagnosis of COVID-19, non-COVID-19 non-COVID viral pneumonia, and other lung opacity can be challenging. This research utilized artificial intelligence (AI) to deliver high-accuracy automated COVID-19 detection from normal chest X-ray images. Further, this study extended to differentiate COVID-19 from normal, lung opacity and non-COVID viral pneumonia images. We have employed three distinct pre-trained models that are Xception, VGG19, and ResNet50 on a benchmark dataset of 21,165 X-ray images. Initially, we formulated the COVID-19 detection problem as a binary classification problem to classify COVID-19 from normal X-ray images and gained 97.5%, 97.5%, and 93.3% accuracy for Xception, VGG19, and ResNet50 respectively. Later we focused on developing an efficient model for multi-class classification and gained an accuracy of 75% for ResNet50, 92% for VGG19, and finally 93% for Xception. Although Xception and VGG19's performances were identical, Xception proved to be more efficient with its higher precision, recall, and f-1 scores. Finally, we have employed Explainable AI on each of our utilized model which adds interpretability to our study. Furthermore, we have conducted a comprehensive comparison of the model's explanations and the study revealed that Xception is more precise in indicating the actual features that are responsible for a model's predictions.This addition of explainable AI will benefit the medical professionals greatly as they will get to visualize how a model makes its prediction and won't have to trust our developed machine-learning models blindly.", "journal": "Healthcare (Basel, Switzerland)", "date": "2023-02-12", "authors": ["Md NazmulIslam", "Md Golam RabiulAlam", "Tasnim SakibApon", "Md ZiaUddin", "NasserAllheeib", "AlaaMenshawi", "Mohammad MehediHassan"], "doi": "10.3390/healthcare11030410\n10.1038/s41579-018-0118-9\n10.1001/jama.2020.3786\n10.1183/13993003.04188-2020\n10.1148/radiol.2020200823\n10.1016/j.media.2021.102216\n10.1016/j.media.2021.102225\n10.1016/j.media.2021.102205\n10.1038/s41598-020-76550-z\n10.1016/j.cmpb.2020.105581\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.110071\n10.3390/app10134640\n10.1007/s10044-021-00984-y\n10.1109/ACCESS.2020.2994762\n10.3390/sym12040651\n10.1109/ACCESS.2020.3044858\n10.1109/TMI.2020.3040950\n10.1080/07391102.2020.1767212\n10.1109/ACCESS.2020.3010287\n10.1007/s11042-022-12156-z\n10.1007/s11042-022-12156-z\n10.1109/CVPR.2016.90\n10.1109/CVPR.2009.5206848\n10.1109/CVPR.2017.195\n10.1093/comjnl/14.4.407\n10.1007/s11263-019-01228-7"}
{"title": "[Performance in prognostic capacity and efficiency of the Thoracic Care Suite GE AI tool applied to chest radiography of patients with COVID-19 pneumonia].", "abstract": "Rapid progression of COVID-19 pneumonia may put patients at risk of requiring ventilatory support, such as non-invasive mechanical ventilation or endotracheal intubation. Implementing tools that detect COVID-19 pneumonia can improve the patient's healthcare. We aim to evaluate the efficacy and efficiency of the artificial intelligence (AI) tool GE Healthcare's Thoracic Care Suite (featuring Lunit INSIGHT CXR, TCS) to predict the ventilatory support need based on pneumonic progression of COVID-19 on consecutive chest X-rays.\nOutpatients with confirmed SARS-CoV-2 infection, with chest X-ray (CXR) findings probable or indeterminate for COVID-19 pneumonia, who required a second CXR due to unfavorable clinical course, were collected. The number of affected lung fields for the two CXRs was assessed using the AI tool.\nOne hundred fourteen patients (57.4 \u00b1 14.2 years, 65 -57%- men) were retrospectively collected. Fifteen (13.2%) required ventilatory support. Progression of pneumonic extension \u2265 0.5 lung fields per day compared to pneumonia onset, detected using the TCS tool, increased the risk of requiring ventilatory support by 4-fold. Analyzing the AI output required 26 seconds of radiological time.\nApplying the AI tool, Thoracic Care Suite, to CXR of patients with COVID-19 pneumonia allows us to anticipate ventilatory support requirements requiring less than half a minute.\nLa r\u00e1pida progresi\u00f3n de la neumon\u00eda COVID-19 puede implicar la necesidad de recurrir a sistemas de respiraci\u00f3n asistida, como la ventilaci\u00f3n mec\u00e1nica no invasiva o la intubaci\u00f3n endotraqueal. La introducci\u00f3n de herramientas que detecten la neumon\u00eda COVID-19 puede mejorar la atenci\u00f3n sanitaria de los pacientes. Nuestro objetivo es evaluar la eficacia y la eficiencia de la herramienta de inteligencia artificial (IA) Thoracic Care Suite de GE Healthcare (que incorpora Lunit Insight CXR) para predecir la necesidad de recurrir a la respiraci\u00f3n asistida en funci\u00f3n de la progresi\u00f3n de la neumon\u00eda en la COVID-19 en radiograf\u00edas tor\u00e1cicas consecutivas.\nSe incluy\u00f3 a pacientes ambulatorios con infecci\u00f3n por SARS-CoV-2 confirmada, con hallazgos probables o indeterminados de neumon\u00eda COVID-19 en la radiograf\u00eda tor\u00e1cica (RXT) y que necesitaron una segunda RXT debido a la evoluci\u00f3n cl\u00ednica desfavorable. En las 2\u00a0RXT se evaluaron el n\u00famero de campos pulmonares afectados mediante la herramienta de IA.\nSe incluy\u00f3 a 114 pacientes (57,4\u00a0\u00b114,2 a\u00f1os; 65 de ellos varones, el 57%) de forma retrospectiva; 15 pacientes (el 13,2%) precisaron respiraci\u00f3n asistida. La progresi\u00f3n de la diseminaci\u00f3n neum\u00f3nica \u22650,5 campos pulmonares al d\u00eda en comparaci\u00f3n con el inicio de la neumon\u00eda, detectada mediante la herramienta TCS, cuadruplic\u00f3 el riesgo de precisar respiraci\u00f3n asistida. El an\u00e1lisis de los resultados de IA precis\u00f3 26 segundos.\nAplicar la herramienta de IA, Thoracic Care Suite, a la RXT de pacientes con neumon\u00eda COVID-19 nos permite predecir la necesidad de recurrir a la respiraci\u00f3n asistida en menos de medio minuto.", "journal": "Radiologia", "date": "2023-02-07", "authors": ["Juana Mar\u00edaPlasencia-Mart\u00ednez", "RafaelP\u00e9rez-Costa", "M\u00f3nicaBallesta-Ruiz", "Jos\u00e9Mar\u00eda Garc\u00eda-Santos"], "doi": "10.1016/j.rx.2022.11.012\n10.3389/fpubh.2021.647315\n10.1186/s12871-020-01095-7\n10.1186/s13244-020-00954-8\n10.1016/j.jacr.2020.02.008\n10.1007/s00330-021-08418-3\n10.1007/s11547-020-01200-3\n10.1067/j.cpradiol.2021.03.017\n10.1016/j.eswa.2021.115695\n10.1007/s11547-020-01232-9\n10.1136/bmj.j4683\n10.1016/j.ejrad.2021.109583\n10.1136/bmj.m1328\n10.1136/bmj.m3339\n10.1016/j.rx.2021.09.011\n10.1007/s00330-021-08049-8\n10.1016/j.chest.2020.04.003"}
{"title": "A novel approach for detection of COVID-19 and Pneumonia using only binary classification from chest CT-scans.", "abstract": "The novel Coronavirus, Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) spread all over the world, causing a dramatic shift in circumstances that resulted in a massive pandemic, affecting the world's well-being and stability. It is an RNA virus that can infect both humans as well as animals. Diagnosis of the virus as soon as possible could contain and avoid a serious COVID-19 outbreak. Current pharmaceutical techniques and diagnostic methods tests such as Reverse Transcription-Polymerase Chain Reaction (RT-PCR) and Serology tests are time-consuming, expensive, and require a well-equipped laboratory for analysis, making them restrictive and inaccessible to everyone. Deep Learning has grown in popularity in recent years, and it now plays a crucial role in Image Classification, which also involves Medical Imaging. Using chest CT scans, this study explores the problem statement automation of differentiating COVID-19 contaminated individuals from healthy individuals. Convolutional Neural Networks (CNNs) can be trained to detect patterns in computed tomography scans (CT scans). Hence, different CNN models were used in the current study to identify variations in chest CT scans, with accuracies ranging from 91% to 98%. The Multiclass Classification method is used to build these architectures. This study also proposes a new approach for classifying CT images that use two binary classifications combined to work together, achieving 98.38% accuracy. All of these architectures' performances are compared using different classification metrics.", "journal": "Neuroscience informatics", "date": "2023-02-07", "authors": ["SanskarHasija", "PeddaputhaAkash", "MagantiBhargav Hemanth", "AnkitKumar", "SanjeevSharma"], "doi": "10.1016/j.neuri.2022.100069\n10.1016/j.compbiomed.2020.103795\n10.1016/j.chaos.2020.110120\n10.1109/ACCESS.2020.3010287\n10.3389/fmed.2020.608525\n10.3389/fmed.2020.608525\n10.1109/ACCESS.2020.3001973\n10.1148/radiol.2017162326\n10.1101/2020.04.12.20062661\n10.1007/s00330-020-06731-x\n10.1016/j.chaos.2020.109944\n10.1007/s11263-019-01228-7\n10.1101/2020.07.11.20151332\n10.1016/j.compbiomed.2020.103805\n10.1016/j.asoc.2020.106897\n10.1148/ryct.2020200067\n10.1016/j.eng.2020.04.010\n10.1007/s13244-018-0639-9"}
{"title": "Classification of COVID-19 from community-acquired pneumonia: Boosting the performance with capsule network and maximum intensity projection image of CT scans.", "abstract": "The coronavirus disease 2019 (COVID-19) and community-acquired pneumonia (CAP) present a high degree of similarity in chest computed tomography (CT) images. Therefore, a procedure for accurately and automatically distinguishing between them is crucial.\nA deep learning method for distinguishing COVID-19 from CAP is developed using maximum intensity projection (MIP) images from CT scans. LinkNet is employed for lung segmentation of chest CT images. MIP images are produced by superposing the maximum gray of intrapulmonary CT values. The MIP images are input into a capsule network for patient-level pred iction and diagnosis of COVID-19. The network is trained using 333 CT scans (168 COVID-19/165 CAP) and validated on three external datasets containing 3581 CT scans (2110 COVID-19/1471 CAP).\nLinkNet achieves the highest Dice coefficient of 0.983 for lung segmentation. For the classification of COVID-19 and CAP, the capsule network with the DenseNet-121 feature extractor outperforms ResNet-50 and Inception-V3, achieving an accuracy of 0.970 on the training dataset. Without MIP or the capsule network, the accuracy decreases to 0.857 and 0.818, respectively. Accuracy scores of 0.961, 0.997, and 0.949 are achieved on the external validation datasets. The proposed method has higher or comparable sensitivity compared with ten state-of-the-art methods.\nThe proposed method illustrates the feasibility of applying MIP images from CT scans to distinguish COVID-19 from CAP using capsule networks. MIP images provide conspicuous benefits when exploiting deep learning to detect COVID-19 lesions from CT scans and the capsule network improves COVID-19 diagnosis.", "journal": "Computers in biology and medicine", "date": "2023-02-05", "authors": ["YananWu", "QianqianQi", "ShouliangQi", "LimingYang", "HanlinWang", "HuiYu", "JianpengLi", "GangWang", "PingZhang", "ZhenyuLiang", "RongchangChen"], "doi": "10.1016/j.compbiomed.2023.106567"}
{"title": "PneuNet: deep learning for COVID-19 pneumonia diagnosis on chest X-ray image analysis using Vision Transformer.", "abstract": "A long-standing challenge in pneumonia diagnosis is recognizing the pathological lung texture, especially the ground-glass appearance pathological texture. One main difficulty lies in precisely extracting and recognizing the pathological features. The patients, especially those with mild symptoms, show very little difference in lung texture, neither conventional computer vision methods nor convolutional neural networks perform well on pneumonia diagnosis based on chest X-ray (CXR) images. In the meanwhile, the Coronavirus Disease 2019 (COVID-19) pandemic continues wreaking havoc around the world, where quick and accurate diagnosis backed by CXR images is in high demand. Rather than simply recognizing the patterns, extracting feature maps from the original CXR image is what we need in the classification process. Thus, we propose a Vision Transformer (VIT)-based model called PneuNet to make an accurate diagnosis backed by channel-based attention through X-ray images of the lung, where multi-head attention is applied on channel patches rather than feature patches. The techniques presented in this paper are oriented toward the medical application of deep neural networks and VIT. Extensive experiment results show that our method can reach 94.96% accuracy in the three-categories classification problem on the test set, which outperforms previous deep learning models.", "journal": "Medical & biological engineering & computing", "date": "2023-02-01", "authors": ["TianmuWang", "ZhenguoNie", "RuijingWang", "QingfengXu", "HongshiHuang", "HandingXu", "FuguiXie", "Xin-JunLiu"], "doi": "10.1007/s11517-022-02746-2\n10.2471/BLT.07.048769\n10.1016/S1473-3099(20)30120-1\n10.1148/radiol.2020200432\n10.1148/ryct.2020200034\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200343\n10.1109/TMI.2018.2881415\n10.1038/nature14539\n10.1109/TPAMI.2013.50\n10.1016/j.inffus.2020.11.005\n10.1016/j.aej.2021.01.011\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105581\n10.1162/neco.1997.9.8.1735\n10.1016/j.slast.2021.10.011\n10.1007/s10489-020-02055-x"}
{"title": "A Review of Deep Learning Applications in Lung Ultrasound Imaging of COVID-19 Patients.", "abstract": "The massive and continuous spread of COVID-19 has motivated researchers around the world to intensely explore, understand, and develop new techniques for diagnosis and treatment. Although lung ultrasound imaging is a less established approach when compared to other medical imaging modalities such as X-ray and CT, multiple studies have demonstrated its promise to diagnose COVID-19 patients. At the same time, many deep learning models have been built to improve the diagnostic efficiency of medical imaging. The integration of these initially parallel efforts has led multiple researchers to report deep learning applications in medical imaging of COVID-19 patients, most of which demonstrate the outstanding potential of deep learning to aid in the diagnosis of COVID-19. This invited review is focused on deep learning applications in lung ultrasound imaging of COVID-19 and provides a comprehensive overview of ultrasound systems utilized for data acquisition, associated datasets, deep learning models, and comparative performance.", "journal": "BME frontiers", "date": "2023-01-31", "authors": ["LingyiZhao", "Muyinatu ALediju Bell"], "doi": "10.34133/2022/9780173"}
{"title": "Lightweight ResGRU: a deep learning-based prediction of SARS-CoV-2 (COVID-19) and its severity classification using multimodal chest radiography images.", "abstract": "The new COVID-19 emerged in a town in China named Wuhan in December 2019, and since then, this deadly virus has infected 324 million people worldwide and caused 5.53 million deaths by January 2022. Because of the rapid spread of this pandemic, different countries are facing the problem of a shortage of resources, such as medical test kits and ventilators, as the number of cases increased uncontrollably. Therefore, developing a readily available, low-priced, and automated approach for COVID-19 identification is the need of the hour. The proposed study uses chest radiography images (CRIs) such as X-rays and computed tomography (CTs) to detect chest infections, as these modalities contain important information about chest infections. This research introduces a novel hybrid deep learning model named ", "journal": "Neural computing & applications", "date": "2023-01-31", "authors": ["MugheesAhmad", "Usama IjazBajwa", "YasarMehmood", "Muhammad WaqasAnwar"], "doi": "10.1007/s00521-023-08200-0\n10.1016/S0140-6736(20)30183-5\n10.1038/s41586-020-2008-3\n10.1007/978-3-030-60188-1_2\n10.1016/j.chaos.2020.110495\n10.1016/j.chaos.2021.110713\n10.1016/S1473-3099(20)30134-1\n10.1016/j.bea.2021.100003\n10.2217/fmb-2020-0098\n10.1016/S0140-6736(20)30154-9\n10.1148/ryct.2020200028\n10.1148/radiol.2020201473\n10.1148/ryct.2020200213\n10.3389/fnins.2021.601109\n10.1002/widm.1312\n10.1145/3065386\n10.1109/CVPR.2016.90\n10.1051/matecconf/201927702001\n10.1016/j.compbiomed.2020.103795\n10.1016/j.imu.2020.100405\n10.1016/j.ejrad.2020.109402\n10.1007/s00330-021-07715-1\n10.1016/j.eswa.2020.114054\n10.1007/s11042-021-11388-9\n10.1016/j.compbiomed.2020.103792\n10.1016/j.imu.2020.100412\n10.1016/j.asoc.2021.107160\n10.1007/s10489-020-01888-w\n10.1007/s42600-021-00151-6\n10.1016/j.ijleo.2021.166405\n10.1038/s41568-020-00327-9\n10.1016/j.compmedimag.2019.05.001\n10.1038/nature14539\n10.1080/07391102.2020.1767212\n10.1016/j.bspc.2021.102490\n10.1016/j.patcog.2021.108255"}
{"title": "COVID-19 lung infection segmentation from chest CT images based on CAPA-ResUNet.", "abstract": "Coronavirus disease 2019 (COVID-19) epidemic has devastating effects on personal health around the world. It is significant to achieve accurate segmentation of pulmonary infection regions, which is an early indicator of disease. To solve this problem, a deep learning model, namely, the content-aware pre-activated residual UNet (CAPA-ResUNet), was proposed for segmenting COVID-19 lesions from CT slices. In this network, the pre-activated residual block was used for down-sampling to solve the problems of complex foreground and large fluctuations of distribution in datasets during training and to avoid gradient disappearance. The area loss function based on the false segmentation regions was proposed to solve the problem of fuzzy boundary of the lesion area. This model was evaluated by the public dataset (COVID-19 Lung CT Lesion Segmentation Challenge-2020) and compared its performance with those of classical models. Our method gains an advantage over other models in multiple metrics. Such as the Dice coefficient, specificity (Spe), and intersection over union (IoU), our CAPA-ResUNet obtained 0.775 points, 0.972 points, and 0.646 points, respectively. The Dice coefficient of our model was 2.51% higher than Content-aware residual UNet (CARes-UNet). The code is available at https://github.com/malu108/LungInfectionSeg.", "journal": "International journal of imaging systems and technology", "date": "2023-01-31", "authors": ["LuMa", "ShuniSong", "LitingGuo", "WenjunTan", "LishengXu"], "doi": "10.1002/ima.22819"}
{"title": "Longitudinal changes in global structural brain connectivity and cognitive performance in former hospitalized COVID-19 survivors: an exploratory study.", "abstract": "Long-term sequelae of COVID-19 can result in reduced functionality of the central nervous system and substandard quality of life. Gaining insight into the recovery trajectory of admitted COVID-19 patients on their cognitive performance and global structural brain connectivity may allow a better understanding of the diseases' relevance.\nTo\u00a0assess whole-brain structural connectivity in former non-intensive-care unit (ICU)- and ICU-admitted COVID-19 survivors over 2 months following hospital discharge and correlate structural connectivity measures to cognitive performance.\nParticipants underwent Magnetic Resonance Imaging brain scans and a cognitive test battery after hospital discharge to evaluate structural connectivity and cognitive performance. Multilevel models were constructed for each graph measure and cognitive test, assessing the groups' influence, time since discharge, and interactions. Linear regression models estimated whether the graph measurements affected cognitive measures and whether they differed between ICU and non-ICU patients.\nSix former ICU and six non-ICU patients completed the study. Across the various graph measures, the characteristic path length decreased over time (\u03b2\u2009=\u20090.97, p\u2009=\u20090.006). We detected no group-level effects (\u03b2\u2009=\u20091.07, p\u2009=\u20090.442) nor interaction effects (\u03b2\u2009=\u20091.02, p\u2009=\u20090.220). Cognitive performance improved for both non-ICU and ICU COVID-19 survivors on four out of seven cognitive tests 2 months later (p\u2009<\u20090.05).\nAdverse effects of COVID-19 on brain functioning and structure abate over time. These results should be supported by future research including larger sample sizes, matched control groups of healthy non-infected individuals, and more extended follow-up periods.", "journal": "Experimental brain research", "date": "2023-01-29", "authors": ["BTassignon", "ARadwan", "JBlommaert", "LStas", "S DAllard", "FDe Ridder", "EDe Waele", "L CBulnes", "NHoornaert", "PLacor", "ELathouwers", "RMertens", "MNaeyaert", "HRaeymaekers", "LSeyler", "A MVan Binst", "LVan Imschoot", "LVan Liedekerke", "JVan Schependom", "PVan Schuerbeek", "MVandekerckhove", "RMeeusen", "SSunaert", "GNagels", "JDe Mey", "KDe Pauw"], "doi": "10.1007/s00221-023-06545-5\n10.1016/j.neuroimage.2010.09.025\n10.1177/19714009211029177\n10.1093/sleep/34.5.581\n10.3357/AMHP.4343.2015\n10.1016/j.neuroimage.2018.09.073\n10.18637/jss.v067.i01\n10.1016/0028-3932(95)00035-2\n10.2967/jnumed.121.262128\n10.1016/j.bbi.2020.06.008\n10.1002/alz.12255\n10.1111/ene.14775\n10.1016/j.neuroimage.2006.01.021\n10.1038/s41586-022-04569-5\n10.1503/cmaj.1095958\n10.1016/j.neuroimage.2012.01.021\n10.3233/JAD-200581\n10.1212/WNL.0000000000010979\n10.1016/j.jns.2021.117486\n10.1038/s41564-020-0695-z\n10.1378/chest.130.3.869\n10.1093/brain/awab009\n10.1093/brain/awab435\n10.1016/j.cell.2020.08.028\n10.1016/j.nicl.2015.05.003\n10.1016/j.neuroimage.2011.09.015\n10.1016/j.media.2013.02.006\n10.1016/j.ebiom.2021.103512\n10.1186/s12879-022-07062-6\n10.1212/WNL.0000000000010112\n10.1148/radiol.2020202222\n10.18637/jss.v082.i13\n10.1007/s00330-020-06761-5\n10.1037/1076-898X.8.2.75\n10.1007/s00415-020-10313-8\n10.1016/j.bbi.2020.05.037\n10.1016/j.eclinm.2020.100484\n10.1111/ene.14444\n10.9778/cmajo.20210023\n10.1001/jamaneurol.2020.1127\n10.1006/nimg.1995.1012\n10.1016/j.cmi.2020.11.005\n10.1111/nyas.12807\n10.1016/j.clineuro.2020.105921\n10.1016/j.yebeh.2013.11.019\n10.1007/s11571-017-9461-1\n10.1007/s00406-021-01346-9\n10.1016/j.neuroimage.2008.05.046\n10.1016/j.mri.2019.05.008\n10.1002/brb3.518\n10.1016/j.neuroimage.2012.06.005\n10.1016/j.neuroimage.2012.11.049\n10.1016/j.neuroimage.2015.06.092\n10.1084/jem.20202135\n10.1001/jama.2016.0701\n10.1016/j.neuroimage.2007.02.016\n10.1162/NECO_a_00914\n10.1007/s10072-020-04575-3\n10.1038/s41398-021-01426-3\n10.1007/s11571-019-09562-9"}
{"title": "Artificial Intelligence in Paediatric Tuberculosis.", "abstract": "Tuberculosis (TB) continues to be a leading cause of death in children despite global efforts focused on early diagnosis and interventions to limit the spread of the disease. This challenge has been made more complex in the context of the coronavirus pandemic, which has disrupted the \"End TB Strategy\" and framework set out by the World Health Organization (WHO). Since the inception of artificial intelligence (AI) more than 60\u00a0years ago, the interest in AI has risen and more recently we have seen the emergence of multiple real-world applications, many of which relate to medical imaging. Nonetheless, real-world AI applications and clinical studies are limited in the niche area of paediatric imaging. This review article will focus on how AI, or more specifically deep learning, can be applied to TB diagnosis and management in children. We describe how deep learning can be utilised in chest imaging to provide computer-assisted diagnosis to augment workflow and screening efforts. We also review examples of recent AI applications for TB screening in resource constrained environments and we explore some of the challenges and the future directions of AI in paediatric TB.", "journal": "Pediatric radiology", "date": "2023-01-28", "authors": ["JaishreeNaidoo", "Susan ChengShelmerdine", "Carlos F Ugas-Charcape", "Arhanjit SinghSodhi"], "doi": "10.1007/s00247-023-05606-9\n10.7754/Clin.Lab.2015.150509\n10.21037/jtd-21-1342\n10.1093/cid/ciac011\n10.1155/2014/291841\n10.1097/INF.0000000000000792\n10.1136/adc.2004.062315\n10.5588/ijtld.15.0201\n10.5588/ijtld.18.0122\n10.1007/s00247-017-3866-1\n10.1007/s00247-020-04625-0\n10.1164/rccm.202202-0259OC\n10.1093/cid/ciab708\n10.1038/s41598-021-03265-0\n10.1007/s00330-020-07024-z\n10.1155/2021/5359084\n10.1016/S2589-7500(20)30221-1\n10.1148/radiol.2017162326\n10.1007/s00330-020-07219-4\n10.1148/radiol.2021210063\n10.1038/s41598-021-93967-2\n10.3389/fmolb.2022.874475\n10.1016/S2589-7500(21)00116-3\n10.1038/s41598-019-51503-3\n10.1038/s41746-020-0273-z\n10.1093/cid/ciab639\n10.1007/s00259-021-05432-x\n10.3389/frai.2022.827299\n10.1007/s00330-021-08365-z\n10.1016/j.clinimag.2022.04.009\n10.21037/qims-21-676\n10.1148/radiol.2018181422\n10.1038/s41591-018-0307-0\n10.1371/journal.pone.0212094\n10.1007/s00247-021-05146-0\n10.1093/cid/ciy967\n10.1371/journal.pone.0221339\n10.5588/ijtld.17.0520\n10.1038/s41746-021-00393-9\n10.3389/fdata.2022.850383\n10.1007/s00247-019-04593-0\n10.1038/s41598-020-73831-5\n10.1371/journal.pone.0206410\n10.1097/INF.0000000000001872\n10.1002/ppul.24230\n10.1002/ppul.24500\n10.1007/s00247-017-3895-9\n10.1038/srep12215\n10.1038/s41598-020-62148-y"}
{"title": "CT imaging of HIV-associated pulmonary disorders in COVID-19 pandemic.", "abstract": "Opportunistic infections in people living with human immunodeficiency virus (HIV) are readily detected with thoracic computed tomography (CT), but differential diagnosis remains a challenge. The global COVID-19 pandemic further exacerbates the issue, with SARS-CoV-2 having overlapping CT findings with infections common in HIV patients and complicating prior epidemiological data. We present a pictorial review of CT findings associated with COVID-19-mimicking opportunistic infections that can be encountered in HIV patients. PubMed database was searched for the complete list of relevant conditions, and a Venn diagram was constructed to highlight overlapping entities. The diagram showed five major disease groups: viral pneumonia, fungal pneumonia, bacterial pneumonia, sarcoidosis, and lung cancer. As these pathologies possess a wide range of features, the findings were grouped as \u201ctypical\u201d and \u201cother\u201d for easier comprehension with provided relevant epidemiological data and discrepancies observed in available literature. The review highlights the importance of a specific approach to differential diagnosis in immunocompromised patients compared to immunocompetent hosts and the utility of follow-up scans.", "journal": "Clinical imaging", "date": "2023-01-28", "authors": ["Liya RAbuladze", "Ivan ABlokhin", "Anna PGonchar", "Maria MSuchilova", "Anton VVladzymyrskyy", "Victor AGombolevskiy", "Eleonora ABalanyuk", "Oksana GNi", "Dmitry VTroshchansky", "Roman VReshetnikov"], "doi": "10.1016/j.clinimag.2023.01.006\n10.1098/rstb.2010.0031\n10.1182/blood.V68.1.281.281\n10.1371/journal.pone.0112237\n10.1148/rg.2018180149\n10.21037/jtd.2019.06.22\n10.3201/eid1010.030985\n10.1177/014107680109400907\n10.1183/09031936.00200210\n10.3389/fmed.2022.829624\n10.1080/22221751.2021.1957402\n10.3390/microorganisms10010178\n10.15585/mmwr.mm6937a6\n10.15585/mmwr.mm7129a1\n10.1007/s00330-003-2044-z\n10.1007/s10140-021-01919-0\n10.1186/s13244-020-00933-z\n10.1007/s10461-020-02983-2\n10.1097/RTI.0000000000000554\n10.1002/rmv.655\n10.1097/00002030-199703110-00009\n10.1097/QAD.0000000000002484\n10.1055/s-0036-1578802\n10.1371/journal.pone.0256452\n10.1016/S0002-9343(97)00015-6\n10.1098/rstb.2015.0468\n10.1097/QAD.0000000000002498\n10.1038/s41598-021-92605-1\n10.2147/HIV.S53910\n10.4172/2161-0517.1000113\n10.7860/JCDR/2017/24219.9277\n10.1126/scitranslmed.3004404\n10.4236/jbm.2020.89002\n10.1016/j.pt.2021.07.010\n10.1148/rg.2017160110\n10.1016/j.ejrad.2006.04.017\n10.1186/s12879-020-05217-x\n10.1183/13993003.01369-2016\n10.1164/rccm.200804-617OC\n10.1097/MCP.0b013e3283375825\n10.1080/14787210.2018.1495560\n10.1097/QAI.0b013e3181eef4f7\n10.1016/j.rcl.2005.10.009\n10.1007/s11604-011-0574-x\n10.1089/108729101750301906\n10.2169/internalmedicine.51.7326\n10.1183/13993003.congress-2021.PA1985\n10.1093/ofid/ofaa441\n10.1378/chest.119.3.978\n10.1148/radiology.218.1.r01ja25242\n10.1097/MCP.0000000000000800\n10.1016/j.crad.2010.03.004\n10.1067/j.cpradiol.2020.10.013\n10.1148/rg.306105512\n10.1097/QAD.0b013e328352d1ad\n10.1097/COH.0000000000000326\n10.1097/QAD.0000000000000943\n10.1148/radiol.2017161659\n10.1016/j.rcl.2018.01.004\n10.1378/chest.12-2355\n10.1513/AnnalsATS.201607-568BC\n10.2214/AJR.10.7262\n10.1016/j.lungcan.2003.07.001\n10.1097/JTO.0b013e31821038ab\n10.3389/fonc.2021.616149\n10.21037/jtd.2019.02.91\n10.21569/2222-7415-2020-10-1-252-256\n10.1016/j.chest.2016.10.010\n10.3109/03009731003695624\n10.1183/16000617.0019-2016\n10.3390/v14050972\n10.3390/jcm11072017\n10.2147/IDR.S335711\n10.1002/rcr2.725\n10.1016/j.jmii.2020.06.007"}
{"title": "Artificial intelligence for differentiating COVID-19 from other viral pneumonias on CT: comparative analysis of different models based on quantitative and radiomic approaches.", "abstract": "To develop a pipeline for automatic extraction of quantitative metrics and radiomic features from lung computed tomography (CT) and develop artificial intelligence (AI) models supporting differential diagnosis between coronavirus disease 2019 (COVID-19) and other viral pneumonia (non-COVID-19).\nChest CT of 1,031 patients (811 for model building; 220 as independent validation set (IVS) with positive swab for severe acute respiratory syndrome coronavirus-2 (647 COVID-19) or other respiratory viruses (384 non-COVID-19) were segmented automatically. A Gaussian model, based on the HU histogram distribution describing well-aerated and ill portions, was optimised to calculate quantitative metrics (QM, n = 20) in both lungs (2L) and four geometrical subdivisions (GS) (upper front, lower front, upper dorsal, lower dorsal; n = 80). Radiomic features (RF) of first (RF1, n = 18) and second (RF2, n = 120) order were extracted from 2L using PyRadiomics tool. Extracted metrics were used to develop four multilayer-perceptron classifiers, built with different combinations of QM and RF: Model1 (RF1-2L); Model2 (QM-2L, QM-GS); Model3 (RF1-2L, RF2-2L); Model4 (RF1-2L, QM-2L, GS-2L, RF2-2L).\nThe classifiers showed accuracy from 0.71 to 0.80 and area under the receiving operating characteristic curve (AUC) from 0.77 to 0.87 in differentiating COVID-19 versus non-COVID-19 pneumonia. Best results were associated with Model3 (AUC 0.867 \u00b1 0.008) and Model4 (AUC 0.870 \u00b1 0.011. For the IVS, the AUC values were 0.834 \u00b1 0.008 for Model3 and 0.828 \u00b1 0.011 for Model4.\nFour AI-based models for classifying patients as COVID-19 or non-COVID-19 viral pneumonia showed good diagnostic performances that could support clinical decisions.", "journal": "European radiology experimental", "date": "2023-01-24", "authors": ["GiuliaZorzi", "LucaBerta", "FrancescoRizzetto", "CristinaDe Mattia", "Marco Maria JacopoFelisi", "StefanoCarrazza", "SilviaNerini Molteni", "ChiaraVismara", "FrancescoScaglione", "AngeloVanzulli", "AlbertoTorresin", "Paola EnricaColombo"], "doi": "10.1186/s41747-022-00317-6\n10.1016/S0140-6736(21)00947-8\n10.1056/NEJMoa2035389\n10.1148/radiol.2020201365\n10.1016/j.diii.2020.03.014\n10.2214/AJR.20.22976\n10.1148/rg.2020200097\n10.1016/j.ejmp.2021.01.004\n10.1038/nrclinonc.2017.141\n10.1016/j.cmpb.2020.105608\n10.7150/thno.46428\n10.1016/j.ejro.2020.100271\n10.1016/j.ejmp.2021.04.022\n10.1007/s00330-020-07269-8\n10.1148/radiol.2020201491\n10.1016/S2589-7500(20)30199-0\n10.21037/atm-20-5328\n10.1007/s00259-020-05075-4\n10.1148/radiol.2020200823\n10.1016/j.ejrad.2021.110028\n10.3390/diagnostics12040869\n10.1186/s41747-020-00173-2\n10.1016/j.acra.2015.01.008\n10.1148/radiol.2020201433\n10.1097/RCT.0b013e31818da65c\n10.1158/0008-5472.CAN-17-0339\n10.1111/j.2517-6161.1996.tb02080.x\n10.1016/j.ejmp.2021.06.001\n10.7717/peerj-cs.564\n10.1038/s41467-020-17971-2\n10.1016/j.ejrad.2021.109552\n10.1007/s11547-021-01370-8\n10.1109/TSMC.1973.4309314\n10.1093/annonc/mdx034\n10.1038/s41598-019-56989-5\n10.21037/tcr.2016.07.11\n10.1016/j.ejrad.2021.109650\n10.1007/s00330-022-08685-8"}
{"title": "The role of mobile teledermoscopy in skin cancer triage and management during the COVID-19 pandemic.", "abstract": "The unprecedented onset of the COVID-19 crisis poses a significant challenge to all fields of medicine, including dermatology. Since the start of the coronavirus outbreak, a stark decline in new skin cancer diagnoses has been reported by countries worldwide. One of the greatest challenges during the pandemic has been the reduced access to face-to-face dermatologic evaluation and non-urgent procedures, such as biopsies or surgical excisions. Teledermatology is a well-integrated alternative when face-to-face dermatological assistance is not available. Teledermoscopy, an extension of teledermatology, comprises consulting dermoscopic images to improve the remote assessment of pigmented and non-pigmented lesions when direct visualisation of lesions is difficult. One of teledermoscopy's greatest strengths may be its utility as a triage and monitoring tool, which is critical in the early detection of skin cancer, as it can reduce the number of unnecessary referrals, wait times, and the cost of providing and receiving dermatological care. Mobile teledermoscopy may act as a communication tool between medical practitioners and patients. By using their smartphone (mobile phone) patients can monitor a suspicious skin lesion identified by their medical practitioner, or alternatively self-detect concerning lesions and forward valuable dermoscopic images for remote medical evaluation. Several mobile applications that allow users to photograph suspicious lesions with their smartphones and have them evaluated using artificial intelligence technology have recently emerged. With the growing popularity of mobile apps and consumer-involved healthcare, this will likely be a key component of skin cancer screening in the years to come. However, most of these applications apply artificial intelligence technology to assess clinical images rather than dermoscopic images, which may lead to lower diagnostic accuracy. Incorporating the direct-to-consumer mobile dermoscopy model in combination with mole-scanning artificial intelligence as a mobile app may be the future of skin cancer detection.", "journal": "Indian journal of dermatology, venereology and leprology", "date": "2023-01-24", "authors": ["ClaudiaLee", "AlexanderWitkowski", "Magdalena\u017bychowska", "JoannaLudzik"], "doi": "10.25259/IJDVL_118_2022"}
{"title": "Towards precision medicine: Omics approach for COVID-19.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic had a devastating impact on human society. Beginning with genome surveillance of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the development of omics technologies brought a clearer understanding of the complex SARS-CoV-2 and COVID-19. Here, we reviewed how omics, including genomics, proteomics, single-cell multi-omics, and clinical phenomics, play roles in answering biological and clinical questions about COVID-19. Large-scale sequencing and advanced analysis methods facilitate COVID-19 discovery from virus evolution and severity risk prediction to potential treatment identification. Omics would indicate precise and globalized prevention and medicine for the COVID-19 pandemic under the utilization of big data capability and phenotypes refinement. Furthermore, decoding the evolution rule of SARS-CoV-2 by deep learning models is promising to forecast new variants and achieve more precise data to predict future pandemics and prevent them on time.", "journal": "Biosafety and health", "date": "2023-01-24", "authors": ["XiaopingCen", "FengaoWang", "XinheHuang", "DragomirkaJovic", "FredDubee", "HuanmingYang", "YixueLi"], "doi": "10.1016/j.bsheal.2023.01.002\n10.1073/pnas.0408290102\n10.1038/s41588-022-01033-y\n10.1126/scitranslmed.abk3445\n10.1126/science.abd7331\n10.1126/science.abm1208\n10.1016/j.gpb.2022.01.001\n10.3389/fimmu.2021.622176\n10.1038/s41588-021-00854-7\n10.1038/s41421-021-00318-6\n10.1056/nejmoa2020283\n10.1038/s41586-021-03767-x\n10.1038/s41588-021-00996-8\n10.1038/s41588-021-00955-3\n10.1038/s41588-021-00986-w\n10.1038/s41588-022-01042-x\n10.1038/s41588-021-01006-7\n10.1038/s41586-022-04826-7\n10.1038/s43587-021-00067-x\n10.1016/j.cell.2021.01.004\n10.1016/j.immuni.2020.10.008\n10.1038/s41586-021-03493-4\n10.1016/j.celrep.2021.110271\n10.1038/s41467-021-27716-4\n10.1038/s41467-021-24482-1\n10.1016/j.cell.2022.01.012\n10.1126/scitranslmed.abj7521\n10.1038/s41586-020-2588-y\n10.7554/eLife.62522\n10.1084/jem.20210582\n10.1016/j.immuni.2020.11.017\n10.1016/j.cell.2020.10.037\n10.1038/s41586-021-03570-8\n10.1038/s42255-021-00425-4\n10.1038/s41591-021-01329-2\n10.1038/s41746-021-00399-3\n10.1038/s41467-020-17971-2\n10.1038/s41467-020-17280-8\n10.1016/j.cell.2020.04.045\n10.1038/s41591-020-0931-3\n10.1038/s41551-020-00633-5\n10.1016/j.xcrm.2022.100580\n10.1080/10408363.2020.1851167\n10.1002/mco2.90\n10.1038/s41587-021-01131-y\n10.1016/j.xinn.2022.100289\n10.1055/s-0040-1712549\n10.1093/cid/ciab754\n10.1080/14760584.2021.1976153\n10.1002/jmv.27524\n10.1038/s41392-022-01105-9\n10.1038/s41586-021-04188-6\n10.1002/mco2.110\n10.2807/1560-7917.ES.2021.26.24.2100509\n10.46234/ccdcw2021.255\n10.1016/j.gpb.2020.09.001\n10.1093/nar/gkw1065\n10.1093/ve/veab064\n10.1126/science.abe3261\n10.1126/science.abc0523\n10.1126/science.abb9263\n10.1038/s41467-022-31511-0\n10.1038/s41467-020-18314-x\n10.1126/scitranslmed.abn7979\n10.1126/science.abp8337\n10.1038/s41467-020-19345-0\n10.1038/s41591-020-0997-y\n10.1038/s41591-020-1000-7\n10.1126/science.abq5358\n10.1038/s41576-022-00483-8\n10.1016/S1473-3099(21)00170-5\n10.1016/S2468-2667(21)00055-4\n10.1016/j.cell.2020.11.020\n10.7554/eLife.65365\n10.1038/s41576-021-00408-x\n10.1038/s41586-021-04352-y\n10.1038/s41586-021-03792-w\n10.1089/omi.2021.0182\n10.1021/acs.jproteome.1c00475\n10.1039/d0cb00163e\n10.1038/s41746-021-00431-6\n10.1038/s42256-021-00307-0\n10.1038/s42256-021-00377-0\n10.1038/s41591-022-01843-x\n10.1089/jwh.2021.0411\n10.1001/jamanetworkopen.2021.47053\n10.1016/S0140-6736(22)00941-2\n10.1038/s41591-022-01840-0\n10.1001/jamapsychiatry.2022.2640\n10.1016/j.cell.2022.01.014\n10.1016/j.immuni.2022.01.017\n10.1126/sciimmunol.abk1741\n10.1038/s41591-022-01837-9\n10.3389/fimmu.2022.838132\n10.1126/sciimmunol.abm7996\n10.1136/bmj.o407\n10.1038/s41586-020-2355-0\n10.1016/j.jval.2021.10.007\n10.1038/d41586-022-03181-x"}
{"title": "Development of a Fast Fourier Transform-based Analytical Method for COVID-19 Diagnosis from Chest X-Ray Images Using GNU Octave.", "abstract": "Many artificial intelligence-based computational procedures are developed to diagnose COVID-19 infection from chest X-ray (CXR) images, as diagnosis by CXR imaging is less time consuming and economically cheap compared to other detection procedures. Due to unavailability of skilled computer professionals and high computer architectural resource, majority of the employed methods are difficult to implement in rural and poor economic settings. Majority of such reports are devoid of codes and ignores related diseases (pneumonia). The absence of codes makes limitation in applying them widely. Hence, validation testing followed by evidence-based medical practice is difficult. The present work was aimed to develop a simple method that requires a less computational expertise and minimal level of computer resource, but with statistical inference.\nA Fast Fourier Transform-based (FFT) method was developed with GNU Octave, a free and open-source platform. This was employed to the images of CXR for further analysis. For statistical inference, two variables, i.e., the highest peak and number of peaks in the FFT distribution plot were considered.\nThe comparison of mean values among different groups (normal, COVID-19, viral, and bacterial pneumonia [BP]) showed statistical significance, especially when compared to normal, except between viral and BP groups.\nParametric statistical inference from our result showed high level of significance (", "journal": "Journal of medical physics", "date": "2023-01-24", "authors": ["DurjoyMajumder"], "doi": "10.4103/jmp.jmp_26_22"}
{"title": "DMFL_Net: A Federated Learning-Based Framework for the Classification of COVID-19 from Multiple Chest Diseases Using X-rays.", "abstract": "Coronavirus Disease 2019 (COVID-19) is still a threat to global health and safety, and it is anticipated that deep learning (DL) will be the most effective way of detecting COVID-19 and other chest diseases such as lung cancer (LC), tuberculosis (TB), pneumothorax (PneuTh), and pneumonia (Pneu). However, data sharing across hospitals is hampered by patients' right to privacy, leading to unexpected results from deep neural network (DNN) models. Federated learning (FL) is a game-changing concept since it allows clients to train models together without sharing their source data with anybody else. Few studies, however, focus on improving the model's accuracy and stability, whereas most existing FL-based COVID-19 detection techniques aim to maximize secondary objectives such as latency, energy usage, and privacy. In this work, we design a novel model named decision-making-based federated learning network (DMFL_Net) for medical diagnostic image analysis to distinguish COVID-19 from four distinct chest disorders including LC, TB, PneuTh, and Pneu. The DMFL_Net model that has been suggested gathers data from a variety of hospitals, constructs the model using the DenseNet-169, and produces accurate predictions from information that is kept secure and only released to authorized individuals. Extensive experiments were carried out with chest X-rays (CXR), and the performance of the proposed model was compared with two transfer learning (TL) models, i.e., VGG-19 and VGG-16 in terms of accuracy (ACC), precision (PRE), recall (REC), specificity (SPF), and F1-measure. Additionally, the DMFL_Net model is also compared with the default FL configurations. The proposed DMFL_Net + DenseNet-169 model achieves an accuracy of 98.45% and outperforms other approaches in classifying COVID-19 from four chest diseases and successfully protects the privacy of the data among diverse clients.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-01-22", "authors": ["HassaanMalik", "AhmadNaeem", "Rizwan AliNaqvi", "Woong-KeeLoh"], "doi": "10.3390/s23020743\n10.3390/electronics11172714\n10.1109/OJCS.2022.3206407\n10.3390/life12070958\n10.1002/int.22777\n10.1109/TCBB.2022.3184319\n10.1109/ACCESS.2020.3037474\n10.1038/s41746-020-00323-1\n10.1109/bigdata50022.2020.9377873\n10.3389/fpubh.2022.892499\n10.1109/JIOT.2021.3056185\n10.1109/JIOT.2021.3120998\n10.1145/3501296\n10.1016/j.asoc.2021.107330\n10.1038/s41591-021-01506-3\n10.2196/24207\n10.1007/978-3-030-11723-8_9\n10.1109/JIOT.2019.2956615\n10.1109/isbi.2019.8759317\n10.1145/3528580.3532845\n10.1109/JBHI.2022.3143576\n10.1109/icic53490.2021.9693006\n10.1007/s00530-021-00878-3\n10.1371/journal.pone.0266462\n10.18280/ijdne.170106\n10.1007/978-981-16-7618-5_13\n10.14419/ijet.v9i3.30655\n10.1007/s11042-022-13843-7\n10.1007/s10796-022-10307-z\n10.3233/shti220697\n10.1109/JIOT.2022.3144450\n10.1016/j.compbiomed.2022.105233\n10.1109/TMM.2018.2889934\n10.1109/JIOT.2019.2920987\n10.1109/ACCESS.2018.2885997\n10.1016/j.asoc.2020.106859\n10.1016/j.knosys.2021.106775\n10.32604/cmc.2022.020344\n10.1016/j.dib.2020.106520\n10.1111/exsy.13173\n10.1109/ACCESS.2021.3102399\n10.1145/3431804\n10.32604/cmc.2021.013191\n10.1007/s11042-022-13499-3\n10.3390/diagnostics11091735\n10.1001/jamanetworkopen.2019.1095\n10.1016/j.cell.2018.02.010\n10.2214/ajr.174.1.1740071\n10.1016/j.fcij.2017.12.001\n10.1109/ACCESS.2020.3031384\n10.1038/s41598-020-76282-0\n10.26599/TST.2021.9010026\n10.1109/ubmk52708.2021.9558913\n10.1109/JBHI.2020.3005160\n10.1016/j.bspc.2021.102588\n10.1016/j.patrec.2020.09.010\n10.1111/exsy.12759\n10.1146/annurev-bioeng-110220-012203\n10.2174/1573405617666210414101941\n10.1007/s42600-021-00135-6\n10.3390/s22155652\n10.1016/j.jpha.2021.12.006\n10.1007/s00530-021-00826-1\n10.3390/jpm12020275\n10.1002/wcms.1597\n10.1109/ACCESS.2020.3001507\n10.1007/s40747-022-00866-8"}
{"title": "Novel Comparative Study for the Detection of COVID-19 Using CT Scan and Chest X-ray Images.", "abstract": "The number of coronavirus disease (COVID-19) cases is constantly rising as the pandemic continues, with new variants constantly emerging. Therefore, to prevent the virus from spreading, coronavirus cases must be diagnosed as soon as possible. The COVID-19 pandemic has had a devastating impact on people's health and the economy worldwide. For COVID-19 detection, reverse transcription-polymerase chain reaction testing is the benchmark. However, this test takes a long time and necessitates a lot of laboratory resources. A new trend is emerging to address these limitations regarding the use of machine learning and deep learning techniques for automatic analysis, as these can attain high diagnosis results, especially by using medical imaging techniques. However, a key question arises whether a chest computed tomography scan or chest X-ray can be used for COVID-19 detection. A total of 17,599 images were examined in this work to develop the models used to classify the occurrence of COVID-19 infection, while four different classifiers were studied. These are the convolutional neural network (proposed architecture (named, SCovNet) and Resnet18), support vector machine, and logistic regression. Out of all four models, the proposed SCoVNet architecture reached the best performance with an accuracy of almost 99% and 98% on chest computed tomography scan images and chest X-ray images, respectively.", "journal": "International journal of environmental research and public health", "date": "2023-01-22", "authors": ["AhatshamHayat", "PreetyBaglat", "F\u00e1bioMendon\u00e7a", "Sheikh ShanawazMostafa", "FernandoMorgado-Dias"], "doi": "10.3390/ijerph20021268\n10.1016/j.jds.2020.02.002\n10.1016/j.ajem.2020.03.036\n10.1007/s11042-021-10714-5\n10.1093/aje/kwab093\n10.1038/s41597-021-00900-3\n10.1038/s41598-020-76282-0\n10.1007/s11547-019-00990-5\n10.1007/s11547-020-01135-9\n10.1007/s11547-020-01277-w\n10.1007/s12559-020-09773-x\n10.1109/TNNLS.2018.2790388\n10.1016/j.jksuci.2020.03.013\n10.1016/j.chaos.2020.110190\n10.1016/S2589-7500(21)00039-X\n10.1007/s00500-020-05424-3\n10.1016/j.bbe.2021.05.013\n10.1155/2021/6658058\n10.1016/j.chaos.2020.109944\n10.3389/frai.2021.694875\n10.1016/j.cmpb.2020.105581\n10.1101/2020.03.26.20044610\n10.3390/app11083414\n10.3390/healthcare10020343\n10.17632/8h65ywd2jr.3\n10.1155/2021/5587188\n10.1038/s41598-021-86735-9\n10.1109/72.788646\n10.1088/1742-6596/1748/4/042054\n10.1007/s11263-019-01228-7"}
{"title": "Research on the Application of Artificial Intelligence in Public Health Management: Leveraging Artificial Intelligence to Improve COVID-19 CT Image Diagnosis.", "abstract": "Since the start of 2020, the outbreak of the Coronavirus disease (COVID-19) has been a global public health emergency, and it has caused unprecedented economic and social disaster. In order to improve the diagnosis efficiency of COVID-19 patients, a number of researchers have conducted extensive studies on applying artificial intelligence techniques to the analysis of COVID-19-related medical images. The automatic segmentation of lesions from computed tomography (CT) images using deep learning provides an important basis for the quantification and diagnosis of COVID-19 cases. For a deep learning-based CT diagnostic method, a few of accurate pixel-level labels are essential for the training process of a model. However, the translucent ground-glass area of the lesion usually leads to mislabeling while performing the manual labeling operation, which weakens the accuracy of the model. In this work, we propose a method for correcting rough labels; that is, to hierarchize these rough labels into precise ones by performing an analysis on the pixel distribution of the infected and normal areas in the lung. The proposed method corrects the incorrectly labeled pixels and enables the deep learning model to learn the infected degree of each infected pixel, with which an aiding system (named DLShelper) for COVID-19 CT image diagnosis using the hierarchical labels is also proposed. The DLShelper targets lesion segmentation from CT images, as well as the severity grading. The DLShelper assists medical staff in efficient diagnosis by providing rich auxiliary diagnostic information (including the severity grade, the proportions of the lesion and the visualization of the lesion area). A comprehensive experiment based on a public COVID-19 CT image dataset is also conducted, and the experimental results show that the DLShelper significantly improves the accuracy of segmentation for the lesion areas and also achieves a promising accuracy for the severity grading task.", "journal": "International journal of environmental research and public health", "date": "2023-01-22", "authors": ["TianchengHe", "HongLiu", "ZhihaoZhang", "ChaoLi", "YoumeiZhou"], "doi": "10.3390/ijerph20021158\n10.3390/diagnostics10110901\n10.1007/s11548-021-02466-2\n10.1148/radiol.2020200642\n10.1016/j.chest.2020.06.025\n10.1109/RBME.2020.2987975\n10.1016/j.cell.2020.04.045\n10.1109/TIP.2021.3058783\n10.1609/aaai.v35i6.16617\n10.1109/TMI.2020.2996645\n10.1016/j.media.2018.11.010\n10.1109/TMI.2020.3000314\n10.1007/s00521-022-07709-0\n10.1109/TMI.2017.2775636\n10.1016/j.jvcir.2016.11.019\n10.1186/s12938-015-0014-8\n10.1109/TMI.2012.2196285\n10.3389/fpubh.2022.1015876\n10.1002/mp.12273\n10.1016/j.cmpb.2019.06.005\n10.1007/s10278-019-00254-8\n10.1007/978-3-319-24574-4_28\n10.1109/TPAMI.2016.2644615\n10.1613/jair.953"}
{"title": "COVID-19 Detection Mechanism in Vehicles Using a Deep Extreme Machine Learning Approach.", "abstract": "COVID-19 is a rapidly spreading pandemic, and early detection is important to halting the spread of infection. Recently, the outbreak of this virus has severely affected people around the world with increasing death rates. The increased death rates are because of its spreading nature among people, mainly through physical interactions. Therefore, it is very important to control the spreading of the virus and detect people's symptoms during the initial stages so proper preventive measures can be taken in good time. In response to COVID-19, revolutionary automation such as deep learning, machine learning, image processing, and medical images such as chest radiography (CXR) and computed tomography (CT) have been developed in this environment. Currently, the coronavirus is identified via an RT-PCR test. Alternative solutions are required due to the lengthy moratorium period and the large number of false-negative estimations. To prevent the spreading of the virus, we propose the Vehicle-based COVID-19 Detection System to reveal the related symptoms of a person in the vehicles. Moreover, deep extreme machine learning is applied. The proposed system uses headaches, flu, fever, cough, chest pain, shortness of breath, tiredness, nasal congestion, diarrhea, breathing difficulty, and pneumonia. The symptoms are considered parameters to reveal the presence of COVID-19 in a person. Our proposed approach in Vehicles will make it easier for governments to perform COVID-19 tests timely in cities. Due to the ambiguous nature of symptoms in humans, we utilize fuzzy modeling for simulation. The suggested COVID-19 detection model achieved an accuracy of more than 90%.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-01-22", "authors": ["AreejFatima", "TariqShahzad", "SagheerAbbas", "AbdurRehman", "YousafSaeed", "MeshalAlharbi", "Muhammad AdnanKhan", "KhmaiesOuahada"], "doi": "10.3390/diagnostics13020270\n10.1056/NEJMoa2001017\n10.46234/ccdcw2020.017\n10.1016/S0140-6736(20)30154-9\n10.1016/S0140-6736(20)30183-5\n10.1017/ice.2020.61\n10.9781/ijimai.2020.02.002\n10.3390/jcm9030674\n10.1136/jim-2020-001491\n10.3390/diagnostics12040846\n10.1007/s10140-021-01937-y\n10.3390/diagnostics12071617\n10.9781/ijimai.2018.04.003\n10.1198/004017005000000058\n10.3978/j.issn.2072-1439.2015.04.61\n10.1088/1742-6596/892/1/012016\n10.15837/ijccc.2010.3.2481\n10.1016/j.artmed.2016.12.003\n10.1080/00401706.1980.10486139\n10.3390/app12094493\n10.1007/s00330-021-07715-1\n10.1038/s41598-020-76550-z\n10.1109/ACCESS.2020.2976452\n10.3233/AIS-200554\n10.32604/cmc.2020.011155\n10.1007/s13042-011-0019-y"}
{"title": "Automated Pneumonia Based Lung Diseases Classification with Robust Technique Based on a Customized Deep Learning Approach.", "abstract": "Many people have been affected by infectious lung diseases (ILD). With the outbreak of the COVID-19 disease in the last few years, many people have waited for weeks to recover in the intensive care wards of hospitals. Therefore, early diagnosis of ILD is of great importance to reduce the occupancy rates of health institutions and the treatment time of patients. Many artificial intelligence-based studies have been carried out in detecting and classifying diseases from medical images using imaging applications. The most important goal of these studies was to increase classification performance and model reliability. In this approach, a powerful algorithm based on a new customized deep learning model (ACL model), which trained synchronously with the attention and LSTM model with CNN models, was proposed to classify healthy, COVID-19 and Pneumonia. The important stains and traces in the chest X-ray (CX-R) image were emphasized with the marker-controlled watershed (MCW) segmentation algorithm. The ACL model was trained for different training-test ratios (90-10%, 80-20%, and 70-30%). For 90-10%, 80-20%, and 70-30% training-test ratios, accuracy scores were 100%, 96%, and 96%, respectively. The best performance results were obtained compared to the existing methods. In addition, the contribution of the strategies utilized in the proposed model to classification performance was analyzed in detail. Deep learning-based applications can be used as a useful decision support tool for physicians in the early diagnosis of ILD diseases. However, for the reliability of these applications, it is necessary to undertake verification with many datasets.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-01-22", "authors": ["YamanAkbulut"], "doi": "10.3390/diagnostics13020260\n10.1038/s41586-020-2008-3\n10.1016/S0140-6736(20)30211-7\n10.1148/radiol.2020200463\n10.1148/radiol.2020200432\n10.1007/s10044-021-00984-y\n10.1038/s41598-020-76550-z\n10.1016/j.chemolab.2020.104054\n10.1016/j.aiia.2020.10.002\n10.1001/jama.2018.19323\n10.1002/emmm.201100182\n10.1016/j.aiia.2020.09.002\n10.1016/j.patrec.2020.09.010\n10.33889/IJMEMS.2020.5.4.052\n10.1007/s13246-020-00865-4\n10.1109/ACCESS.2020.3016780\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.110071\n10.3892/etm.2020.8797\n10.1016/j.asoc.2021.107160\n10.1016/j.eswa.2020.114054\n10.1016/j.asoc.2022.108610\n10.1007/s00354-021-00152-0\n10.14358/PERS.70.3.351\n10.1109/JSTARS.2018.2830410\n10.14569/IJACSA.2017.080853\n10.3390/jpm12010055\n10.1016/j.bspc.2022.103625\n10.1016/j.bspc.2020.102194\n10.1016/j.bbe.2021.07.004\n10.3390/jpm11121276\n10.1016/j.apacoust.2021.108260\n10.1016/j.mehy.2020.109761\n10.1016/j.asoc.2020.106580\n10.1007/s10489-020-01888-w\n10.1016/j.compbiomed.2020.103805"}
{"title": "Deep Learning for Detecting COVID-19 Using Medical Images.", "abstract": "The global spread of COVID-19 (also known as SARS-CoV-2) is a major international public health crisis [...].", "journal": "Bioengineering (Basel, Switzerland)", "date": "2023-01-22", "authors": ["JiaLiu", "JingQi", "WeiChen", "YiWu", "YongjianNian"], "doi": "10.3390/bioengineering10010019\n10.3390/bioengineering8070098\n10.3390/bioengineering8040049\n10.1016/j.media.2020.101794\n10.1007/s00500-020-05424-3\n10.1038/s41598-020-76550-z\n10.1016/j.ins.2020.09.041\n10.1016/j.neucom.2022.01.055\n10.1109/TNNLS.2021.3114747\n10.1016/j.media.2021.102299\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2022.105233\n10.1109/TMI.2020.3040950\n10.1016/j.compbiomed.2022.105732\n10.1109/TNNLS.2022.3201198\n10.1016/j.irbm.2020.05.003\n10.1109/JBHI.2020.3023246\n10.1109/JBHI.2020.3030853\n10.1109/TCYB.2020.3042837\n10.1109/TMI.2020.2995508\n10.1109/TMI.2020.2994908\n10.1016/j.media.2021.102105\n10.1016/j.media.2020.101913\n10.1109/TMI.2020.2996256\n10.1109/TMI.2020.2995965\n10.3390/bioengineering8020026"}
{"title": "Classification of Pulmonary Damage Stages Caused by COVID-19 Disease from CT Scans via Transfer Learning.", "abstract": "The COVID-19 pandemic has produced social and economic changes that are still affecting our lives. The coronavirus is proinflammatory, it is replicating, and it is quickly spreading. The most affected organ is the lung, and the evolution of the disease can degenerate very rapidly from the early phase, also known as mild to moderate and even severe stages, where the percentage of recovered patients is very low. Therefore, a fast and automatic method to detect the disease stages for patients who underwent a computer tomography investigation can improve the clinical protocol. Transfer learning is used do tackle this issue, mainly by decreasing the computational time. The dataset is composed of images from public databases from 118 patients and new data from 55 patients collected during the COVID-19 spread in Romania in the spring of 2020. Even if the disease detection by the computerized tomography scans was studied using deep learning algorithms, to our knowledge, there are no studies related to the multiclass classification of the images into pulmonary damage stages. This could be helpful for physicians to automatically establish the disease severity and decide on the proper treatment for patients and any special surveillance, if needed. An evaluation study was completed by considering six different pre-trained CNNs. The results are encouraging, assuring an accuracy of around 87%. The clinical impact is still huge, even if the disease spread and severity are currently diminished.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2023-01-22", "authors": ["Irina AndraTache", "DimitriosGlotsos", "Silviu MarcelStanciu"], "doi": "10.3390/bioengineering10010006\n10.1016/j.jacr.2020.02.008\n10.1371/journal.pone.0235844\n10.1148/ryct.2020200028\n10.1148/radiol.2020200230\n10.1007/s10916-020-01562-1\n10.1016/j.patcog.2022.108538\n10.1016/j.chaos.2020.109944\n10.1142/S0218348X20501145\n10.1007/s10489-020-01826-w\n10.1016/j.bspc.2021.102588\n10.1007/s10916-021-01707-w\n10.1016/j.bbe.2021.04.006\n10.1016/j.chaos.2020.110153\n10.3390/e22050517\n10.1016/j.bspc.2022.104250\n10.3389/fonc.2020.01560\n10.1056/NEJMoa2001316\n10.1186/s43055-020-00236-9\n10.1148/radiol.2020200463\n10.2214/AJR.20.22976\n10.1016/j.jormas.2019.06.002\n10.1146/annurev-bioeng-071516-044442\n10.1016/j.bspc.2021.103326\n10.1186/s40537-019-0235-y\n10.1186/s40537-021-00428-8"}
{"title": "Long-term respiratory follow-up of ICU hospitalized COVID-19 patients: Prospective cohort study.", "abstract": "Coronavirus disease (COVID-19) survivors exhibit multisystemic alterations after hospitalization. Little is known about long-term imaging and pulmonary function of hospitalized patients intensive care unit (ICU) who survive COVID-19. We aimed to investigate long-term consequences of COVID-19 on the respiratory system of patients discharged from hospital ICU and identify risk factors associated with chest computed tomography (CT) lesion severity.\nA prospective cohort study of COVID-19 patients admitted to a tertiary hospital ICU in Brazil (March-August/2020), and followed-up six-twelve months after hospital admission. Initial assessment included: modified Medical Research Council dyspnea scale, SpO2 evaluation, forced vital capacity, and chest X-Ray. Patients with alterations in at least one of these examinations were eligible for CT and pulmonary function tests (PFTs) approximately 16 months after hospital admission. Primary outcome: CT lesion severity (fibrotic-like or non-fibrotic-like). Baseline clinical variables were used to build a machine learning model (ML) to predict the severity of CT lesion.\nIn total, 326 patients (72%) were eligible for CT and PFTs. COVID-19 CT lesions were identified in 81.8% of patients, and half of them showed mild restrictive lung impairment and impaired lung diffusion capacity. Patients with COVID-19 CT findings were stratified into two categories of lesion severity: non-fibrotic-like (50.8%-ground-glass opacities/reticulations) and fibrotic-like (49.2%-traction bronchiectasis/architectural distortion). No association between CT feature severity and altered lung diffusion or functional restrictive/obstructive patterns was found. The ML detected that male sex, ICU and invasive mechanic ventilation (IMV) period, tracheostomy and vasoactive drug need during hospitalization were predictors of CT lesion severity(sensitivity,0.78\u00b10.02;specificity,0.79\u00b10.01;F1-score,0.78\u00b10.02;positive predictive rate,0.78\u00b10.02; accuracy,0.78\u00b10.02; and area under the curve,0.83\u00b10.01).\nICU hospitalization due to COVID-19 led to respiratory system alterations six-twelve months after hospital admission. Male sex and critical disease acute phase, characterized by a longer ICU and IMV period, and need for tracheostomy and vasoactive drugs, were risk factors for severe CT lesions six-twelve months after hospital admission.", "journal": "PloS one", "date": "2023-01-21", "authors": ["Carlos RobertoRibeiro Carvalho", "Celina AlmeidaLamas", "Rodrigo CarusoChate", "Jo\u00e3o MarcosSalge", "Marcio Valente YamadaSawamura", "Andr\u00e9 L Pde Albuquerque", "CarlosToufen Junior", "Daniel MarioLima", "Michelle LouvaesGarcia", "Paula GobiScudeller", "Cesar HigaNomura", "Marco AntonioGutierrez", "Bruno GuedesBaldi", "NoneNone"], "doi": "10.1371/journal.pone.0280567\n10.1186/s12931-020-01429-6\n10.1513/AnnalsATS.202004-324OC\n10.1016/j.cmi.2020.09.023\n10.1038/s41591-021-01283-z\n10.3390/ijerph18084350\n10.1080/17476348.2021.1916472\n10.1001/jamanetworkopen.2020.36142\n10.1038/s41591-022-01840-0\n10.1136/bmjopen-2021-059110\n10.1016/S0140-6736(20)32656-8\n10.1186/s13054-021-03465-0\n10.1038/s41413-020-0084-5\n10.1097/01.rti.0000213581.14225.f1\n10.1590/s1807-59322011000600002\n10.1056/NEJMoa022450\n10.1186/s13613-021-00882-w\n10.1136/bmjopen-2021-051706\n10.1148/radiol.2021203153\n10.1164/ajrccm/144.5.1202\n10.1590/s0100-879x1999000600006\n10.1590/s1806-37132007000400008\n10.1590/s0100-879x1999000600008\n10.1186/s13613-018-0469-4\n10.1038/s41598-021-01215-4\n10.1016/j.rmed.2021.106435\n10.1136/thx.2004.030205\n10.1136/thoraxjnl-2021-218275\n10.1007/s15010-022-01755-5\n10.1148/radiol.2021210972\n10.1148/radiol.211670\n10.1016/S2213-2600(21)00174-0\n10.1016/j.siny.2017.06.003\n10.1016/j.chest.2018.07.016\n10.4103/atm.atm_103_22\n10.1016/j.rmed.2021.106602\n10.1007/s11739-020-02614-7"}
{"title": "Social Media Devices' Influence on User Neck Pain during the COVID-19 Pandemic: Collaborating Vertebral-GLCM Extracted Features with a Decision Tree.", "abstract": "The prevalence of neck pain, a chronic musculoskeletal disease, has significantly increased due to the uncontrollable use of social media (SM) devices. The use of SM devices by younger generations increased enormously during the COVID-19 pandemic, being-in some cases-the only possibility for maintaining interpersonal, social, and friendship relationships. This study aimed to predict the occurrence of neck pain and its correlation with the intensive use of SM devices. It is based on nine quantitative parameters extracted from the retrospective X-ray images. The three parameters related to angle_1 (i.e., the angle between the global horizontal and the vector pointing from C7 vertebra to the occipito-cervical joint), angle_2 (i.e., the angle between the global horizontal and the vector pointing from C1 vertebra to the occipito-cervical joint), and the area between them were measured from the shape of the neck vertebrae, while the rest of the parameters were extracted from the images using the gray-level co-occurrence matrix (GLCM). In addition, the users' ages and the duration of the SM usage (H.mean) were also considered. The decision tree (DT) machine-learning algorithm was employed to predict the abnormal cases (painful subjects) against the normal ones (no pain). The results showed that angle_1, area, and the image contrast significantly increased statistically with the time of SM-device usage, precisely in the range of 2 to 9 h. The DT showed a promising result demonstrated by classification accuracy and F1-scores of 94% and 0.95, respectively. Our findings confirmed that the objectively detected parameters, which elucidate the negative impacts of SM-device usage on neck pain, can be predicted by DT machine learning.", "journal": "Journal of imaging", "date": "2023-01-21", "authors": ["BassamAl-Naami", "Bashar E ABadr", "Yahia ZRawash", "Hamza AbuOwida", "RobertoDe Fazio", "PaoloVisconti"], "doi": "10.3390/jimaging9010014\n10.1016/S0140-6736(15)60692-4\n10.1080/23808985.2021.1976070\n10.1590/bjpt-rbf.2014.0149\n10.3109/15360288.2012.678473\n10.1631/FITEE.2100085\n10.21307/ijssis-2021-003\n10.3390/children7090148\n10.1007/s41347-020-00134-x\n10.1097/ACM.0000000000002811\n10.1080/00140139.2013.820844\n10.1166/jmihi.2011.1034\n10.9734/jpri/2021/v33i50B33441\n10.17762/pae.v57i9.560\n10.3390/ijerph18168680\n10.1016/j.apmr.2020.10.019\n10.1177/21925682211041618\n10.3389/fneur.2020.573095\n10.47108/jidhealth.Vol3.IssSpecial1.65\n10.47626/1679-4435-2021-812\n10.13075/mp.5893.01189\n10.1038/s41598-021-01967-z\n10.3390/healthcare9111526\n10.1186/s12889-021-11144-6\n10.1145/1414471.1414519\n10.1109/MECBME.2011.5752156\n10.1109/TIM.2009.2022102\n10.1518/001872000779656598\n10.1037/1076-898X.5.1.35\n10.1518/001872001775992480\n10.1080/0014013031000090107\n10.1080/001401398186586\n10.1080/00140139.2011.576777\n10.1177/003335490712200511\n10.1109/ICPR.2010.764\n10.1109/CCDC.2019.8832978\n10.1016/j.jvcir.2017.03.003\n10.1140/epje/i2017-11587-3\n10.1097/SCS.0000000000002022\n10.1007/s00586-021-07019-4\n10.1097/MEG.0b013e3282202bb8\n10.1007/BF00116251\n10.1007/BF00993309\n10.1002/j.1538-7305.1948.tb01338.x\n10.48550/arXiv.cs/0102027\n10.1371/journal.pone.0217231\n10.3390/ijerph18073507\n10.24171/j.phrp.2018.9.6.04\n10.1109/CCNC.2013.6488520\n10.1016/j.chbr.2021.100108\n10.1155/2018/4518269\n10.1080/00140139.2015.1035762\n10.1016/j.ejpain.2006.02.006\n10.1046/j.1526-4610.2002.02178.x\n10.1109/COMPSAC48688.2020.0-176\n10.3390/ijerph18041565\n10.1080/00140139.2014.967311\n10.1080/00140130701711000\n10.1080/00140139.2011.568634\n10.1080/10447318.2015.1064639\n10.3390/electronics11060938"}
{"title": "A Survey on Deep Learning in COVID-19 Diagnosis.", "abstract": "According to the World Health Organization statistics, as of 25 October 2022, there have been 625,248,843 confirmed cases of COVID-19, including 65,622,281 deaths worldwide. The spread and severity of COVID-19 are alarming. The economy and life of countries worldwide have been greatly affected. The rapid and accurate diagnosis of COVID-19 directly affects the spread of the virus and the degree of harm. Currently, the classification of chest X-ray or CT images based on artificial intelligence is an important method for COVID-19 diagnosis. It can assist doctors in making judgments and reduce the misdiagnosis rate. The convolutional neural network (CNN) is very popular in computer vision applications, such as applied to biological image segmentation, traffic sign recognition, face recognition, and other fields. It is one of the most widely used machine learning methods. This paper mainly introduces the latest deep learning methods and techniques for diagnosing COVID-19 using chest X-ray or CT images based on the convolutional neural network. It reviews the technology of CNN at various stages, such as rectified linear units, batch normalization, data augmentation, dropout, and so on. Several well-performing network architectures are explained in detail, such as AlexNet, ResNet, DenseNet, VGG, GoogleNet, etc. We analyzed and discussed the existing CNN automatic COVID-19 diagnosis systems from sensitivity, accuracy, precision, specificity, and F1 score. The systems use chest X-ray or CT images as datasets. Overall, CNN has essential value in COVID-19 diagnosis. All of them have good performance in the existing experiments. If expanding the datasets, adding GPU acceleration and data preprocessing techniques, and expanding the types of medical images, the performance of CNN will be further improved. This paper wishes to make contributions to future research.", "journal": "Journal of imaging", "date": "2023-01-21", "authors": ["XueHan", "ZuojinHu", "ShuihuaWang", "YudongZhang"], "doi": "10.3390/jimaging9010001\n10.1016/j.procbio.2020.08.016\n10.1056/NEJMoa2002032\n10.1016/j.asoc.2020.106580\n10.1016/j.bios.2020.112349\n10.1016/j.bios.2020.112437\n10.1021/acsnano.0c02439\n10.1016/j.cartre.2020.100011\n10.1148/radiol.2020201237\n10.1148/radiol.2020200343\n10.1016/S0140-6736(20)30154-9\n10.1186/s12938-020-00831-x\n10.1016/j.neucom.2020.05.078\n10.1007/s10462-021-09985-z\n10.1016/j.aej.2021.07.007\n10.1016/j.eswa.2017.08.006\n10.1186/s41747-018-0061-6\n10.1142/S0129065718500582\n10.1049/cit2.12042\n10.3348/kjr.2017.18.4.570\n10.1038/s41746-022-00592-y\n10.20517/ais.2021.15\n10.1049/cit2.12060\n10.1038/nature14539\n10.1049/cit2.12059\n10.1002/ett.4080\n10.1016/j.job.2022.03.003\n10.1016/j.bspc.2021.103165\n10.1109/TIP.2005.852470\n10.1016/j.neunet.2012.02.023\n10.1007/s00521-021-06762-5\n10.1134/S1054661822020110\n10.1145/3507902\n10.3390/su14031447\n10.1155/2022/1830010\n10.1016/j.media.2021.102311\n10.1016/j.compbiomed.2022.105244\n10.1007/s42600-020-00120-5\n10.1016/j.displa.2022.102150\n10.1259/0007-1285-46-552-1016\n10.1007/s11604-020-01010-7\n10.1016/j.crad.2020.04.001\n10.1148/radiol.2020200463\n10.1148/radiol.2020201160\n10.1016/j.jacr.2018.09.012\n10.21037/atm.2017.07.20\n10.1177/0846537120924606\n10.1109/TMI.2020.2993291\n10.3390/jimaging8010002\n10.1109/TIP.2017.2713099\n10.1016/j.patcog.2017.10.013\n10.1007/s13369-020-04758-2\n10.1145/3065386\n10.1016/j.neunet.2015.07.007\n10.1016/j.cmpb.2019.05.004\n10.1016/j.knosys.2020.106396\n10.1016/j.swevo.2021.100863\n10.3390/app12178643\n10.1016/j.powtec.2022.117409\n10.1016/j.ymssp.2017.06.022\n10.32604/cmc.2022.020140\n10.3389/fpubh.2021.726144\n10.1016/j.patrec.2021.02.005\n10.1016/j.patrec.2020.04.018\n10.1016/j.jksuci.2021.05.001\n10.1016/j.neucom.2020.06.117\n10.1111/nph.16830\n10.1016/j.neucom.2018.03.080\n10.1007/s12145-019-00383-2\n10.1155/2021/6633755\n10.1109/JSEN.2020.3025855\n10.1016/j.jksuci.2021.07.005\n10.1186/s40537-019-0197-0\n10.1016/j.compbiomed.2021.104375\n10.1016/j.bspc.2021.103326\n10.1364/OL.390026\n10.1016/j.inffus.2021.07.001\n10.1364/BOE.10.006145\n10.1097/JU.0000000000000852.020\n10.3389/frobt.2019.00144\n10.1007/s11042-017-5243-3\n10.1109/TCSVT.2019.2935128\n10.1167/16.12.326\n10.1109/ACCESS.2017.2696121\n10.1109/5.726791\n10.1109/72.279181\n10.1186/s40537-016-0043-6\n10.1109/TKDE.2009.191\n10.1023/A:1007379606734\n10.1613/jair.1872\n10.1016/S0378-3758(00)00115-4\n10.21037/jtd-21-747\n10.3390/sym14071310\n10.1016/j.asoc.2020.106912\n10.1007/s11390-020-0679-8\n10.1016/j.eng.2020.04.010\n10.1016/j.ejrad.2020.109041\n10.1016/j.bspc.2021.102588\n10.1007/s00521-020-05437-x\n10.1371/journal.pone.0259179\n10.1007/s42979-021-00782-7\n10.1109/TCYB.2020.3042837\n10.1007/s10140-020-01886-y\n10.1007/s13755-021-00140-0\n10.1109/JSEN.2021.3062442\n10.3844/jcssp.2020.620.625\n10.1007/s00138-020-01128-8\n10.1038/s41598-020-74164-z\n10.1016/j.bspc.2021.102987\n10.1109/TLA.2021.9451239\n10.1155/2021/8829829\n10.1007/s10044-021-00984-y\n10.1109/ACCESS.2020.3010287\n10.1007/s10489-020-02055-x\n10.1088/1361-6501/ac8ca4\n10.1016/j.bspc.2021.102814\n10.1101/2022.03.13.22272311\n10.32628/IJSRST207614\n10.1016/j.ijmedinf.2020.104284"}
{"title": "Is the generalizability of a developed artificial intelligence algorithm for COVID-19 on chest CT sufficient for clinical use? Results from the International Consortium for COVID-19 Imaging AI (ICOVAI).", "abstract": "Only few published artificial intelligence (AI) studies for COVID-19 imaging have been externally validated. Assessing the generalizability of developed models is essential, especially when considering clinical implementation. We report the development of the International Consortium for COVID-19 Imaging AI (ICOVAI) model and perform independent external validation.\nThe ICOVAI model was developed using multicenter data (n = 1286 CT scans) to quantify disease extent and assess COVID-19 likelihood using the COVID-19 Reporting and Data System (CO-RADS). A ResUNet model was modified to automatically delineate lung contours and infectious lung opacities on CT scans, after which a random forest predicted the CO-RADS score. After internal testing, the model was externally validated on a multicenter dataset (n = 400) by independent researchers. CO-RADS classification performance was calculated using linearly weighted Cohen's kappa and segmentation performance using Dice Similarity Coefficient (DSC).\nRegarding internal versus external testing, segmentation performance of lung contours was equally excellent (DSC = 0.97 vs. DSC = 0.97, p = 0.97). Lung opacities segmentation performance was adequate internally (DSC = 0.76), but significantly worse on external validation (DSC = 0.59, p < 0.0001). For CO-RADS classification, agreement with radiologists on the internal set was substantial (kappa = 0.78), but significantly lower on the external set (kappa = 0.62, p < 0.0001).\nIn this multicenter study, a model developed for CO-RADS score prediction and quantification of COVID-19 disease extent was found to have a significant reduction in performance on independent external validation versus internal testing. The limited reproducibility of the model restricted its potential for clinical use. The study demonstrates the importance of independent external validation of AI models.\n\u2022 The ICOVAI model for prediction of CO-RADS and quantification of disease extent on chest CT of COVID-19 patients was developed using a large sample of multicenter data. \u2022 There was substantial performance on internal testing; however, performance was significantly reduced on external validation, performed by independent researchers. The limited generalizability of the model restricts its potential for clinical use. \u2022 Results of AI models for COVID-19 imaging on internal tests may not generalize well to external data, demonstrating the importance of independent external validation.", "journal": "European radiology", "date": "2023-01-19", "authors": ["LaurensTopff", "Kevin B WGroot Lipman", "FredericGuffens", "RianneWittenberg", "AnnemariekeBartels-Rutten", "Gerbenvan Veenendaal", "MircoHess", "KayLamerigts", "JorisWakkie", "ErikRanschaert", "StefanoTrebeschi", "Jacob JVisser", "Regina G HBeets-Tan", "NoneNone"], "doi": "10.1007/s00330-022-09303-3\n10.1109/RBME.2020.2987975\n10.1007/s00330-020-07033-y\n10.1148/ryct.2020200047\n10.2214/AJR.20.24044\n10.1007/s00330-020-07013-2\n10.1148/radiol.2020201473\n10.1016/j.chest.2020.11.026\n10.1186/s13244-021-00998-4\n10.1148/ryct.2020200492\n10.1038/s42256-021-00307-0\n10.1093/ckj/sfaa188\n10.1148/radiol.2020202439\n10.1148/radiol.2020201491\n10.1148/radiol.2020200905\n10.1038/s41467-020-18685-1\n10.1016/S2589-7500(20)30199-0\n10.1007/s00330-020-07042-x\n10.1007/s00330-020-07156-2\n10.1038/s41598-022-06854-9\n10.1016/j.asoc.2020.106897"}
{"title": "Pandemic disease detection through wireless communication using infrared image based on deep learning.", "abstract": "Rapid diagnosis to test diseases, such as COVID-19, is a significant issue. It is a routine virus test in a reverse transcriptase-polymerase chain reaction. However, a test like this takes longer to complete because it follows the serial testing method, and there is a high chance of a false-negative ratio (FNR). Moreover, there arises a deficiency of R.T.-PCR test kits. Therefore, alternative procedures for a quick and accurate diagnosis of patients are urgently needed to deal with these pandemics. The infrared image is self-sufficient for detecting these diseases by measuring the temperature at the initial stage. C.T. scans and other pathological tests are valuable aspects of evaluating a patient with a suspected pandemic infection. However, a patient's radiological findings may not be identified initially. Therefore, we have included an Artificial Intelligence (A.I.) algorithm-based Machine Intelligence (MI) system in this proposal to combine C.T. scan findings with all other tests, symptoms, and history to quickly diagnose a patient with a positive symptom of current and future pandemic diseases. Initially, the system will collect information by an infrared camera of the patient's facial regions to measure temperature, keep it as a record, and complete further actions. We divided the face into eight classes and twelve regions for temperature measurement. A database named patient-info-mask is maintained. While collecting sample data, we incorporate a wireless network using a cloudlets server to make processing more accessible with minimal infrastructure. The system will use deep learning approaches. We propose convolution neural networks (CNN) to cross-verify the collected data. For better results, we incorporated tenfold cross-verification into the synthesis method. As a result, our new way of estimating became more accurate and efficient. We achieved 3.29% greater accuracy by incorporating the \"decision tree level synthesis method\" and \"ten-folded-validation method\". It proves the robustness of our proposed method.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2023-01-19", "authors": ["MohammedAlhameed", "FatheJeribi", "Bushra Mohamed ElaminElnaim", "Mohammad AlamgirHossain", "Mohammed EltahirAbdelhag"], "doi": "10.3934/mbe.2023050"}
{"title": "Automated grading of chest x-ray images for viral pneumonia with convolutional neural networks ensemble and region of interest localization.", "abstract": "Following its initial identification on December 31, 2019, COVID-19 quickly spread around the world as a pandemic claiming more than six million lives. An early diagnosis with appropriate intervention can help prevent deaths and serious illness as the distinguishing symptoms that set COVID-19 apart from pneumonia and influenza frequently don't show up until after the patient has already suffered significant damage. A chest X-ray (CXR), one of many imaging modalities that are useful for detection and one of the most used, offers a non-invasive method of detection. The CXR image analysis can also reveal additional disorders, such as pneumonia, which show up as anomalies in the lungs. Thus these CXRs can be used for automated grading aiding the doctors in making a better diagnosis. In order to classify a CXR image into the Negative for Pneumonia, Typical, Indeterminate, and Atypical, we used the publicly available CXR image competition dataset SIIM-FISABIO-RSNA COVID-19 from Kaggle. The suggested architecture employed an ensemble of EfficientNetv2-L for classification, which was trained via transfer learning from the initialised weights of ImageNet21K on various subsets of data (Code for the proposed methodology is available at: https://github.com/asadkhan1221/siim-covid19.git). To identify and localise opacities, an ensemble of YOLO was combined using Weighted Boxes Fusion (WBF). Significant generalisability gains were made possible by the suggested technique's addition of classification auxiliary heads to the CNN backbone. The suggested method improved further by utilising test time augmentation for both classifiers and localizers. The results for Mean Average Precision score show that the proposed deep learning model achieves 0.617 and 0.609 on public and private sets respectively and these are comparable to other techniques for the Kaggle dataset.", "journal": "PloS one", "date": "2023-01-18", "authors": ["AsadKhan", "Muhammad UsmanAkram", "SajidNazir"], "doi": "10.1371/journal.pone.0280352\n10.1007/s13246-020-00865-4\n10.1186/s40537-020-00392-9\n10.1016/j.compbiomed.2021.104771\n10.1038/s41598-020-76550-z\n10.1007/s10489-020-01888-w\n10.1111/exsy.12759\n10.1007/s10462-020-09825-6\n10.1007/s13244-018-0639-9\n10.1016/j.bspc.2021.102764\n10.1016/j.asoc.2020.106691\n10.1016/j.compbiomed.2020.103792\n10.1007/s13755-021-00146-8\n10.1007/s00521-020-05636-6\n10.1109/ICCV.2017.74\n10.1016/j.compbiomed.2021.104356\n10.1016/j.compbiomed.2021.104306\n10.1016/j.compbiomed.2021.104304\n10.1007/978-3-030-88163-4_33\n10.1016/j.neucom.2020.07.144\n10.1007/s10044-021-00984-y\n10.1080/0952813X.2021.1908431\n10.1016/j.bspc.2022.103677\n10.1080/07391102.2020.1767212\n10.1016/j.compbiomed.2021.104348\n10.1016/j.compbiomed.2022.105604\n10.1016/j.compbiomed.2021.104375\n10.1002/ima.22627\n10.3390/app11062884\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.eswa.2020.114054\n10.1049/ipr2.12153\n10.1016/j.compbiomed.2021.105002\n10.1016/j.media.2021.102299\n10.1155/2021/8890226\n10.1016/j.media.2021.102046\n10.1136/bmjqs-2018-008370"}
{"title": "Carotid Vessel-Wall-Volume Ultrasound Measurement via a UNet++ Ensemble Algorithm Trained on Small Data Sets.", "abstract": "Vessel wall volume (VWV) is a 3-D ultrasound measurement for the assessment of therapy in patients with carotid atherosclerosis. Deep learning can be used to segment the media-adventitia boundary (MAB) and lumen-intima boundary (LIB) and to quantify VWV automatically; however, it typically requires large training data sets with expert manual segmentation, which are difficult to obtain. In this study, a UNet++ ensemble approach was developed for automated VWV measurement, trained on five small data sets (n\u00a0=\u00a030 participants) and tested on 100 participants with clinically diagnosed coronary artery disease enrolled in a multicenter CAIN trial. The Dice similarity coefficient (DSC), average symmetric surface distance (ASSD), Pearson correlation coefficient (r), Bland-Altman plots and coefficient of variation (CoV) were used to evaluate algorithm segmentation accuracy, agreement and reproducibility. The UNet++ ensemble yielded DSCs of 91.07%-91.56% and 87.53%-89.44% and ASSDs of 0.10-0.11 mm and 0.33-0.39 mm for the MAB and LIB, respectively; the algorithm VWV measurements were correlated (r\u00a0=\u00a00.763-0.795, p < 0.001) with manual segmentations, and the CoV for VWV was 8.89%. In addition, the UNet++ ensemble trained on 30 participants achieved a performance similar to that of U-Net and Voxel-FCN trained on 150 participants. These results suggest that our approach could provide accurate and reproducible carotid VWV measurements using relatively small training data sets, supporting deep learning applications for monitoring atherosclerosis progression in research and clinical trials.", "journal": "Ultrasound in medicine & biology", "date": "2023-01-16", "authors": ["RanZhou", "FuminGuo", "M RezaAzarpazhooh", "J DavidSpence", "HaitaoGan", "MingyueDing", "AaronFenster"], "doi": "10.1016/j.ultrasmedbio.2022.12.005"}
{"title": "ACSN: Attention capsule sampling network for diagnosing COVID-19 based on chest CT scans.", "abstract": "Automated diagnostic techniques based on computed tomography (CT) scans of the chest for the coronavirus disease (COVID-19) help physicians detect suspected cases rapidly and precisely, which is critical in providing timely medical treatment and preventing the spread of epidemic outbreaks. Existing capsule networks have played a significant role in automatic COVID-19 detection systems based on small datasets. However, extracting key slices is difficult because CT scans typically show many scattered lesion sections. In addition, existing max pooling sampling methods cannot effectively fuse the features from multiple regions. Therefore, in this study, we propose an attention capsule sampling network (ACSN) to detect COVID-19 based on chest CT scans. A key slices enhancement method is used to obtain critical information from a large number of slices by applying attention enhancement to key slices. Then, the lost active and background features are retained by integrating two types of sampling. The results of experiments on an open dataset of 35,000 slices show that the proposed ACSN achieve high performance compared with state-of-the-art models and exhibits 96.3% accuracy, 98.8% sensitivity, 93.8% specificity, and 98.3% area under the receiver operating characteristic curve.", "journal": "Computers in biology and medicine", "date": "2023-01-15", "authors": ["CuihongWen", "ShaowuLiu", "ShuaiLiu", "Ali AsgharHeidari", "MohammadHijji", "CarmenZarco", "KhanMuhammad"], "doi": "10.1016/j.compbiomed.2022.106338\n10.21203/rs.3.rs-32511/v1\n10.1109/ACCESS.2021.3067311\n10.3390/s22176709\n10.1109/TII.2021.3056386\n10.1109/CVPR.2016.90\n10.48550/arXiv.2010.16041"}
{"title": "Utilisation of deep learning for COVID-19 diagnosis.", "abstract": "The COVID-19 pandemic that began in 2019 has resulted in millions of deaths worldwide. Over this period, the economic and healthcare consequences of COVID-19 infection in survivors of acute COVID-19 infection have become apparent. During the course of the pandemic, computer analysis of medical images and data have been widely used by the medical research community. In particular, deep-learning methods, which are artificial intelligence (AI)-based approaches, have been frequently employed. This paper provides a review of deep-learning-based AI techniques for COVID-19 diagnosis using chest radiography and computed tomography. Thirty papers published from February 2020 to March 2022 that used two-dimensional (2D)/three-dimensional (3D) deep convolutional neural networks combined with transfer learning for COVID-19 detection were reviewed. The review describes how deep-learning methods detect COVID-19, and several limitations of the proposed methods are highlighted.", "journal": "Clinical radiology", "date": "2023-01-14", "authors": ["SAslani", "JJacob"], "doi": "10.1016/j.crad.2022.11.006\n10.1101/2020.02.11.20021493\n10.5555/3305890.3305954\n10.1109/ICCVW.2019.00052"}
{"title": "AMTLDC: a new adversarial multi-source transfer learning framework to diagnosis of COVID-19.", "abstract": "In recent years, deep learning techniques have been widely used to diagnose diseases. However, in some tasks, such as the diagnosis of COVID-19 disease, due to insufficient data, the model is not properly trained and as a result, the generalizability of the model decreases. For example, if the model is trained on a CT scan dataset and tested on another CT scan dataset, it predicts near-random results. To address this, data from several different sources can be combined using transfer learning, taking into account the intrinsic and natural differences in existing datasets obtained with different medical imaging tools and approaches. In this paper, to improve the transfer learning technique and better generalizability between multiple data sources, we propose a multi-source adversarial transfer learning model, namely AMTLDC. In AMTLDC, representations are learned that are similar among the sources. In other words, extracted representations are general and not dependent on the particular dataset domain. We apply the AMTLDC to predict Covid-19 from medical images using a convolutional neural network. We show that accuracy can be improved using the AMTLDC framework, and surpass the results of current successful transfer learning approaches. In particular, we show that the AMTLDC works well when using different dataset domains, or when there is insufficient data.", "journal": "Evolving systems", "date": "2023-01-12", "authors": ["HadiAlhares", "JafarTanha", "Mohammad AliBalafar"], "doi": "10.1007/s12530-023-09484-2\n10.1371/journal.pone.0244416\n10.1016/j.patrec.2020.09.010\n10.1148/radiol.2020200642\n10.1101/2020.04.13.20063479\n10.3390/s21020455\n10.1021/acscentsci.6b00367\n10.1016/j.compbiomed.2020.104037\n10.1101/2020.04.24.20078584\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103795\n10.1016/j.compbiomed.2020.103795\n10.1016/j.neucom.2022.09.017\n10.1155/2022/2564022\n10.1118/1.3528204\n10.1117/1.JMI.3.4.044506\n10.1515/cclm-2022-0508\n10.1515/cclm-2022-0623\n10.1016/j.cmpb.2020.105608\n10.1109/JBHI.2016.2636929\n10.1109/JBHI.2016.2636929\n10.1016/j.cmpb.2020.105874\n10.1016/j.compmedimag.2011.07.003\n10.1016/j.media.2017.01.009\n10.1109/access.2020.3007928\n10.1101/2020.04.16.20063990\n10.1038/nature21056\n10.1038/s42256-020-00257-z\n10.2196/27468\n10.1155/2021/9942873\n10.1002/int.22753\n10.1016/j.simpa.2022.100228\n10.1016/j.asoc.2021.108005\n10.1016/j.chaos.2021.111494\n10.1007/s10489-022-03895-5\n10.1001/jama.2016.17216\n10.3390/e22050517\n10.1016/j.asoc.2019.02.038\n10.1080/07391102.2020.1788642\n10.1101/2020.03.19.20039354\n10.1148/ryct.2020200026\n10.1016/j.ejrad.2020.108961\n10.1016/j.media.2020.101794\n10.1155/2020/8843664\n10.1148/ryct.2020200034\n10.1016/j.chaos.2020.110190\n10.1016/j.cmpb.2020.105532\n10.1016/j.imu.2020.100427\n10.1007/s10096-020-03901-z\n10.1007/s10096-020-03901-z\n10.1101/2020.04.24.20078584\n10.1148/radiol.2020200905\n10.1109/JBHI.2020.3037127\n10.3390/ijerph17186933\n10.1001/jamainternmed.2018.7117\n10.1109/JBHI.2020.3023246\n10.1007/s00330-021-07715-1\n10.1016/j.ejrad.2020.109041\n10.1016/j.eng.2020.04.010\n10.1016/j.asoc.2020.107052"}
{"title": "Sarcopenia - Definition, Radiological Diagnosis, Clinical Significance.", "abstract": "Sarcopenia is an age-related syndrome characterized by a loss of muscle mass and strength. As a result, the independence of the elderly is reduced and the hospitalization rate and mortality increase. The onset of sarcopenia often begins in middle age due to an unbalanced diet or malnutrition in association with a lack of physical activity. This effect is intensified by concomitant diseases such as obesity or metabolic diseases including diabetes mellitus.\nWith effective preventative diagnostic procedures and specific therapeutic treatment of sarcopenia, the negative effects on the individual can be reduced and the negative impact on health as well as socioeconomic effects can be prevented. Various diagnostic options are available for this purpose. In addition to basic clinical methods such as measuring muscle strength, sarcopenia can also be detected using imaging techniques like dual X-ray absorptiometry (DXA), computed tomography (CT), magnetic resonance imaging (MRI), and sonography. DXA, as a simple and cost-effective method, offers a low-dose option for assessing body composition. With cross-sectional imaging techniques such as CT and MRI, further diagnostic possibilities are available, including MR spectroscopy (MRS) for noninvasive molecular analysis of muscle tissue. CT can also be used in the context of examinations performed for other indications to acquire additional parameters of the skeletal muscles (opportunistic secondary use of CT data), such as abdominal muscle mass (total abdominal muscle area - TAMA) or the psoas as well as the pectoralis muscle index. The importance of sarcopenia is already well studied for patients with various tumor entities and also infections such as SARS-COV2.\nSarcopenia will become increasingly important, not least due to demographic changes in the population. In this review, the possibilities for the diagnosis of sarcopenia, the clinical significance, and therapeutic options are described. In particular, CT examinations, which are repeatedly performed on tumor patients, can be used for diagnostics. This opportunistic use can be supported by the use of artificial intelligence.\n\u00b7 Sarcopenia is an age-related syndrome with loss of muscle mass and strength.. \u00b7 Early detection and therapy can prevent negative effects of sarcopenia.. \u00b7 In addition to DEXA, cross-sectional imaging techniques (CT, MRI) are available for diagnostic purposes.. \u00b7 The use of artificial intelligence (AI) offers further possibilities in sarcopenia diagnostics..\n\u00b7 Vogele D, Otto S, Sollmann N et\u200aal. Sarcopenia - Definition, Radiological Diagnosis, Clinical Significance. Fortschr R\u00f6ntgenstr 2023; 195: 393\u200a-\u200a405.\n\u2002Bei der Sarkopenie handelt es sich um ein altersabh\u00e4ngiges Syndrom, welches durch einen Verlust an Muskelmasse und -kraft gekennzeichnet ist. In der Folge wird die Selbst\u00e4ndigkeit \u00e4lterer Menschen eingeschr\u00e4nkt und die Hospitalisierungsrate sowie die Mortalit\u00e4t steigen. Die Entwicklung einer Sarkopenie beginnt oftmals bereits im mittleren Lebensalter durch Fehl- und Mangelern\u00e4hrung bzw. in Kombination mit mangelnder k\u00f6rperlicher Aktivit\u00e4t. Verst\u00e4rkt wird dieser Effekt durch Begleiterkrankungen wie Adipositas oder Stoffwechselerkrankungen wie Diabetes mellitus.\n\u2002Durch effektive pr\u00e4ventiv-diagnostische Verfahren und die gezielte therapeutische Behandlung der Sarkopenie lassen sich die negativen Auswirkungen auf das Individuum reduzieren und negative gesundheitliche sowie sozio\u00f6konomische Effekte verhindern. Hierf\u00fcr stehen verschiedene diagnostische M\u00f6glichkeiten zur Verf\u00fcgung. Neben einfachen klinischen Methoden wie der Messung der Muskelkraft l\u00e4sst sich die Sarkopenie auch mit bildgebenden Verfahren erfassen, etwa mittels der Dual-R\u00f6ntgen-Absorptiometrie (DXA), der Computertomografie (CT), der Magnetresonanztomografie (MRT) oder der Sonografie. Die DXA bietet dabei als einfaches und kosteng\u00fcnstiges Verfahren eine dosisarme M\u00f6glichkeit der Erfassung der K\u00f6rperzusammensetzung. Mit den schnittbildgebenden Verfahren der CT und MRT ergeben sich weitere diagnostische M\u00f6glichkeiten bis hin zur MR-Spektroskopie (MRS) zur nicht invasiven molekularen Analyse von Muskelgewebe. Durch die CT k\u00f6nnen auch bei im Rahmen anderer Fragestellungen durchgef\u00fchrten Untersuchungen zus\u00e4tzlich Parameter der Skelettmuskulatur erfasst werden (opportunistische sekund\u00e4re Verwendung von CT-Daten), so beispielsweise die abdominelle Muskelmasse (total abdominal muscle area \u2013 TAMA) oder der Psoas- sowie der Pektoralis-Muskel-Index. Die Bedeutung der Sarkopenie ist bereits f\u00fcr Patienten mit verschiedenen Tumorentit\u00e4ten und auch Infektionen wie SARS-COV2 gut untersucht.\n\u2002Nicht zuletzt durch den demografischen Wandel der Bev\u00f6lkerung wird die Sarkopenie an Bedeutung zunehmen. In dieser \u00dcbersichtsarbeit werden die M\u00f6glichkeiten zur Diagnostik der Sarkopenie, die klinische Bedeutung und Therapiem\u00f6glichkeiten beschrieben. Dabei k\u00f6nnen insbesondere CT-Untersuchungen, die wiederholt bei Tumorpatienten durchgef\u00fchrt werden, zur Diagnostik herangezogen werden. Diese opportunistische Verwendung kann dabei durch den Einsatz k\u00fcnstlicher Intelligenz unterst\u00fctzt werden.\n\u00b7 Sarkopenie ist ein altersabh\u00e4ngiges Syndrom mit Verlust an Muskelmasse und kraft.. \u00b7 Durch Fr\u00fcherkennung und Therapie lassen sich negative Effekte einer Sarkopenie verhindern.. \u00b7 Zur Diagnostik stehen neben der DEXA auch Schnittbildverfahren (CT, MRT) zur Verf\u00fcgung.. \u00b7 Der Einsatz k\u00fcnstlicher Intelligenz (KI) bietet weitere M\u00f6glichkeiten bei der Sarkopenie-Diagnostik..\n\u00b7 Vogele D, Otto S, Sollmann N et\u200aal. Sarcopenia \u2013 Definition, Radiological Diagnosis, Clinical Significance. Fortschr R\u00f6ntgenstr 2023; 195: 393\u200a\u2013\u200a405.", "journal": "RoFo : Fortschritte auf dem Gebiete der Rontgenstrahlen und der Nuklearmedizin", "date": "2023-01-12", "authors": ["DanielVogele", "StephanieOtto", "NicoSollmann", "BenediktHaggenm\u00fcller", "DanielWolf", "MeinradBeer", "Stefan AndreasSchmidt"], "doi": "10.1055/a-1990-0201"}
{"title": "Coronavirus covid-19 detection by means of explainable deep learning.", "abstract": "The coronavirus is caused by the infection of the SARS-CoV-2 virus: it represents a complex and new condition, considering that until the end of December 2019 this virus was totally unknown to the international scientific community. The clinical management of patients with the coronavirus disease has undergone an evolution over the months, thanks to the increasing knowledge of the virus, symptoms and efficacy of the various therapies. Currently, however, there is no specific therapy for SARS-CoV-2 virus, know also as Coronavirus disease 19, and treatment is based on the symptoms of the patient taking into account the overall clinical picture. Furthermore, the test to identify whether a patient is affected by the virus is generally performed on sputum and the result is generally available within a few hours or days. Researches previously found that the biomedical imaging analysis is able to show signs of pneumonia. For this reason in this paper, with the aim of providing a fully automatic and faster diagnosis, we design and implement a method adopting deep learning for the novel coronavirus disease detection, starting from computed tomography medical images. The proposed approach is aimed to detect whether a computed tomography medical images is related to an healthy patient, to a patient with a pulmonary disease or to a patient affected with Coronavirus disease 19. In case the patient is marked by the proposed method as affected by the Coronavirus disease 19, the areas symptomatic of the Coronavirus disease 19 infection are automatically highlighted in the computed tomography medical images. We perform an experimental analysis to empirically demonstrate the effectiveness of the proposed approach, by considering medical images belonging from different institutions, with an average time for Coronavirus disease 19 detection of approximately 8.9 s and an accuracy equal to 0.95.", "journal": "Scientific reports", "date": "2023-01-11", "authors": ["FrancescoMercaldo", "Maria PaolaBelfiore", "AlfonsoReginelli", "LucaBrunese", "AntonellaSantone"], "doi": "10.1038/s41598-023-27697-y\n10.1016/j.procs.2020.09.258\n10.1038/s41577-020-00434-6\n10.1016/j.cmpb.2020.105608\n10.1038/d41573-020-00073-5\n10.1056/NEJMoa2001316\n10.1053/j.gastro.2020.02.054\n10.3390/biology9050097\n10.1002/jmv.25748\n10.1056/NEJMp030078\n10.1038/nm1024\n10.3201/eid2009.140378\n10.3390/jcm9020523\n10.1001/jama.2020.4344\n10.1016/S0140-6736(20)30185-9\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200905\n10.1016/j.ejrad.2020.108961\n10.1111/irv.12734\n10.1148/radiol.2020200432\n10.1016/S1470-2045(19)30739-9\n10.1097/RLI.0b013e318074fd81\n10.2967/jnumed.117.189704\n10.1007/s00330-007-0613-2\n10.1007/978-3-031-01821-3\n10.1109/5.726791\n10.1038/s41598-020-76282-0\n10.1007/s00330-021-07715-1\n10.1016/j.eng.2020.04.010\n10.1007/s13246-020-00865-4\n10.1109/TCBB.2021.3065361\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2020.103795\n10.1016/j.bbe.2015.12.005\n10.1016/j.measurement.2020.108116\n10.1016/j.ifacol.2021.10.282\n10.1016/j.cose.2021.102198\n10.1109/ACCESS.2019.2961754"}
{"title": "Artificial intelligence-assisted multistrategy image enhancement of chest X-rays for COVID-19 classification.", "abstract": "The coronavirus disease 2019 (COVID-19) led to a dramatic increase in the number of cases of patients with pneumonia worldwide. In this study, we aimed to develop an AI-assisted multistrategy image enhancement technique for chest X-ray (CXR) images to improve the accuracy of COVID-19 classification.\nOur new classification strategy consisted of 3 parts. First, the improved U-Net model with a variational encoder segmented the lung region in the CXR images processed by histogram equalization. Second, the residual net (ResNet) model with multidilated-rate convolution layers was used to suppress the bone signals in the 217 lung-only CXR images. A total of 80% of the available data were allocated for training and validation. The other 20% of the remaining data were used for testing. The enhanced CXR images containing only soft tissue information were obtained. Third, the neural network model with a residual cascade was used for the super-resolution reconstruction of low-resolution bone-suppressed CXR images. The training and testing data consisted of 1,200 and 100 CXR images, respectively. To evaluate the new strategy, improved visual geometry group (VGG)-16 and ResNet-18 models were used for the COVID-19 classification task of 2,767 CXR images. The accuracy of the multistrategy enhanced CXR images was verified through comparative experiments with various enhancement images. In terms of quantitative verification, 8-fold cross-validation was performed on the bone suppression model. In terms of evaluating the COVID-19 classification, the CXR images obtained by the improved method were used to train 2 classification models.\nCompared with other methods, the CXR images obtained based on the proposed model had better performance in the metrics of peak signal-to-noise ratio and root mean square error. The super-resolution CXR images of bone suppression obtained based on the neural network model were also anatomically close to the real CXR images. Compared with the initial CXR images, the classification accuracy rates of the internal and external testing data on the VGG-16 model increased by 5.09% and 12.81%, respectively, while the values increased by 3.51% and 18.20%, respectively, for the ResNet-18 model. The numerical results were better than those of the single-enhancement, double-enhancement, and no-enhancement CXR images.\nThe multistrategy enhanced CXR images can help to classify COVID-19 more accurately than the other existing methods.", "journal": "Quantitative imaging in medicine and surgery", "date": "2023-01-10", "authors": ["HongfeiSun", "GeRen", "XinzhiTeng", "LimingSong", "KangLi", "JianhuaYang", "XiaofeiHu", "YuefuZhan", "Shiu Bun NelsonWan", "Man Fung EstherWong", "King KwongChan", "Hoi Ching HaileyTsang", "LuXu", "Tak ChiuWu", "Feng-Ming SpringKong", "Yi Xiang JWang", "JingQin", "Wing Chi LawrenceChan", "MichaelYing", "JingCai"], "doi": "10.21037/qims-22-610\n10.1016/j.ijantimicag.2020.105924\n10.1016/j.arr.2020.101205\n10.1155/2022/5681574\n10.1155/2021/2158184\n10.3233/XST-200784\n10.1016/j.compbiomed.2020.103792\n10.1016/j.bspc.2020.102365\n10.1109/TMI.2020.2993291\n10.1007/s10489-020-01867-1\n10.1016/j.cmpb.2020.105581\n10.1002/mp.12831\n10.1002/mp.13891\n10.3348/kjr.2021.0146\n10.1259/bjr.20201384\n10.1109/ISBI.2019.8759510\n10.1109/ISBI.2019.8759510\n10.2147/IJGM.S325609\n10.3390/jcm10143100\n10.1109/TMI.2021.3134270\n10.1016/j.media.2022.102369\n10.21037/qims-21-791\n10.1109/TMI.2013.2284016\n10.1088/0031-9155/61/6/2283\n10.1109/TMI.2006.871549\n10.1016/j.media.2006.06.002\n10.1007/978-3-540-89208-3_116\n10.1007/978-3-540-89208-3_116\n10.1109/TMI.2013.2274212\n10.1016/j.compmedimag.2021.102008\n10.21037/qims-20-1230\n10.1109/TMI.2020.2986242\n10.1016/j.cmpb.2022.106627\n10.1109/TNNLS.2021.3114747\n10.1016/j.compbiomed.2022.105213\n10.3390/diagnostics11050840\n10.1371/journal.pone.0265691\n10.3390/jcm11113013\n10.3788/AOS201535.0110001\n10.3788/AOS201737.0318011\n10.4304/jcp.9.8.1959-1966\n10.1007/s10489-020-02123-2\n10.3390/diagnostics12030741\n10.3390/mi12111418\n10.1007/s11042-020-09773-x\n10.1007/s42979-021-00690-w\n10.1016/j.compbiomed.2020.104139\n10.1016/j.compbiomed.2020.103869\n10.3390/s21175813\n10.1016/j.chaos.2020.110495\n10.1142/S0218001421510046\n10.1016/j.compbiomed.2021.104425\n10.3390/app10124282\n10.1109/TPAMI.2018.2856256\n10.2214/ajr.174.1.1740071\n10.1007/978-3-642-13039-7_90\n10.1007/978-3-642-13039-7_90\n10.1016/j.compbiomed.2021.105002\n10.1148/radiol.2021203957\n10.3390/diagnostics12030717\n10.1007/s11042-021-10907-y\n10.1007/s42979-021-00695-5\n10.1002/mp.13468\n10.1007/s12652-020-01701-z\n10.1007/s10489-020-02055-x\n10.1038/s41598-021-03287-8"}
{"title": "LDDNet: A Deep Learning Framework for the Diagnosis of Infectious Lung Diseases.", "abstract": "This paper proposes a new deep learning (DL) framework for the analysis of lung diseases, including COVID-19 and pneumonia, from chest CT scans and X-ray (CXR) images. This framework is termed optimized DenseNet201 for lung diseases (LDDNet). The proposed LDDNet was developed using additional layers of 2D global average pooling, dense and dropout layers, and batch normalization to the base DenseNet201 model. There are 1024 Relu-activated dense layers and 256 dense layers using the sigmoid activation method. The hyper-parameters of the model, including the learning rate, batch size, epochs, and dropout rate, were tuned for the model. Next, three datasets of lung diseases were formed from separate open-access sources. One was a CT scan dataset containing 1043 images. Two X-ray datasets comprising images of COVID-19-affected lungs, pneumonia-affected lungs, and healthy lungs exist, with one being an imbalanced dataset with 5935 images and the other being a balanced dataset with 5002 images. The performance of each model was analyzed using the Adam, Nadam, and SGD optimizers. The best results have been obtained for both the CT scan and CXR datasets using the Nadam optimizer. For the CT scan images, LDDNet showed a COVID-19-positive classification accuracy of 99.36%, a 100% precision recall of 98%, and an F1 score of 99%. For the X-ray dataset of 5935 images, LDDNet provides a 99.55% accuracy, 73% recall, 100% precision, and 85% F1 score using the Nadam optimizer in detecting COVID-19-affected patients. For the balanced X-ray dataset, LDDNet provides a 97.07% classification accuracy. For a given set of parameters, the performance results of LDDNet are better than the existing algorithms of ResNet152V2 and XceptionNet.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-01-09", "authors": ["PrajoyPodder", "Sanchita RaniDas", "M Rubaiyat HossainMondal", "SubratoBharati", "AzraMaliha", "Md JunayedHasan", "FarzinPiltan"], "doi": "10.3390/s23010480\n10.1016/j.scitotenv.2020.138762\n10.1007/s10489-020-01826-w\n10.1101/2020.04.22.056283\n10.1007/s10489-021-02393-4\n10.1016/j.bea.2021.100003\n10.1148/radiol.2020200527\n10.1148/radiol.2020200823\n10.1016/j.ejrad.2020.108961\n10.1155/2021/5527923\n10.1016/j.compbiomed.2020.103795\n10.3390/s21020369\n10.1016/j.cmpb.2019.06.005\n10.1109/RBME.2020.2990959\n10.1007/s42979-021-00785-4\n10.1007/s10489-020-01943-6\n10.1016/j.compbiomed.2021.104575\n10.3233/HIS-210008\n10.1101/2020.04.24.20078584\n10.2174/1573405617666210713113439\n10.1016/j.bspc.2021.102588\n10.1109/ACCESS.2020.3010287\n10.20944/preprints202003.0300.v1\n10.1007/s10140-020-01886-y\n10.1101/2020.03.12.20027185\n10.1007/s00330-021-07715-1\n10.1371/journal.pone.0259179\n10.1016/j.compbiomed.2022.105213\n10.3390/s22020669\n10.1016/j.compbiomed.2022.105418\n10.3390/info11090419\n10.1038/s41597-021-00900-3\n10.5281/zenodo.3757476\n10.3390/app10217639\n10.1016/j.patrec.2021.08.035\n10.1109/JBHI.2022.3177854"}
{"title": "Habitat Imaging Biomarkers for Diagnosis and Prognosis in Cancer Patients Infected with COVID-19.", "abstract": "Cancer patients have worse outcomes from the COVID-19 infection and greater need for ventilator support and elevated mortality rates than the general population. However, previous artificial intelligence (AI) studies focused on patients without cancer to develop diagnosis and severity prediction models. Little is known about how the AI models perform in cancer patients. In this study, we aim to develop a computational framework for COVID-19 diagnosis and severity prediction particularly in a cancer population and further compare it head-to-head to a general population.\nWe have enrolled multi-center international cohorts with 531 CT scans from 502 general patients and 420 CT scans from 414 cancer patients. In particular, the habitat imaging pipeline was developed to quantify the complex infection patterns by partitioning the whole lung regions into phenotypically different subregions. Subsequently, various machine learning models nested with feature selection were built for COVID-19 detection and severity prediction.\nThese models showed almost perfect performance in COVID-19 infection diagnosis and predicting its severity during cross validation. Our analysis revealed that models built separately on the cancer population performed significantly better than those built on the general population and locked to test on the cancer population. This may be because of the significant difference among the habitat features across the two different cohorts.\nTaken together, our habitat imaging analysis as a proof-of-concept study has highlighted the unique radiologic features of cancer patients and demonstrated effectiveness of CT-based machine learning model in informing COVID-19 management in the cancer population.", "journal": "Cancers", "date": "2023-01-09", "authors": ["MuhammadAminu", "DivyaYadav", "LingzhiHong", "EllianaYoung", "PaulEdelkamp", "MaliazurinaSaad", "MortezaSalehjahromi", "PingjunChen", "Sheeba JSujit", "Melissa MChen", "BradleySabloff", "GregoryGladish", "Patricia Mde Groot", "Myrna C BGodoy", "TinaCascone", "Natalie IVokes", "JianjunZhang", "Kristy KBrock", "NavalDaver", "Scott EWoodman", "Hussein ATawbi", "AjaySheshadri", "J JackLee", "DavidJaffray", "NoneD Code Team", "Carol CWu", "CarolineChung", "JiaWu"], "doi": "10.3390/cancers15010275\n10.1111/eci.13706\n10.7326/M20-1495\n10.1016/j.mayocp.2020.04.004\n10.1148/radiol.2020203173\n10.1097/RLI.0000000000000672\n10.1016/j.tmaid.2020.101623\n10.1093/brain/awaa240\n10.1016/j.jinf.2020.03.004\n10.1001/jamaoncol.2021.4083\n10.1038/s41746-020-00372-6\n10.1016/j.imu.2020.100378\n10.1515/cclm-2020-1294\n10.1016/S2589-7500(21)00272-7\n10.1016/S2589-7500(20)30274-0\n10.1038/s41591-020-0931-3\n10.1515/cclm-2020-0593\n10.1038/s41746-021-00453-0\n10.21037/tlcr-20-892\n10.3390/tomography8010041\n10.1016/j.jtho.2020.05.001\n10.1111/cas.14882\n10.1186/s12879-021-06038-2\n10.1002/cncr.31630\n10.1126/science.1256930\n10.1148/radiol.2018172462\n10.7937/tcia.bbag-2923\n10.1007/s10278-013-9622-7\n10.7937/91ah-v663\n10.1148/radiol.2021203957\n10.1186/s41747-020-00173-2\n10.1016/j.ijrobp.2016.03.018\n10.2967/jnumed.119.230037\n10.1109/TPAMI.2012.120\n10.1148/ryct.2020200152\n10.1016/j.cell.2020.04.045\n10.1001/jama.2020.2648\n10.1016/j.annonc.2020.03.296\n10.1016/j.patcog.2021.108071\n10.1038/s42256-021-00421-z\n10.1007/s00330-020-07032-z\n10.1007/s00259-020-05075-4\n10.1148/ryct.2020200322\n10.1016/j.asoc.2021.107323\n10.1038/s41467-020-18685-1\n10.1016/j.aej.2021.03.052\n10.1109/TCBB.2021.3065361\n10.7150/thno.50565\n10.36227/techrxiv.14100890.v1"}
{"title": "A Holistic Approach to Identify and Classify COVID-19 from Chest Radiographs, ECG, and CT-Scan Images Using ShuffleNet Convolutional Neural Network.", "abstract": "Early and precise COVID-19 identification and analysis are pivotal in reducing the spread of COVID-19. Medical imaging techniques, such as chest X-ray or chest radiographs, computed tomography (CT) scan, and electrocardiogram (ECG) trace images are the most widely known for early discovery and analysis of the coronavirus disease (COVID-19). Deep learning (DL) frameworks for identifying COVID-19 positive patients in the literature are limited to one data format, either ECG or chest radiograph images. Moreover, using several data types to recover abnormal patterns caused by COVID-19 could potentially provide more information and restrict the spread of the virus. This study presents an effective COVID-19 detection and classification approach using the Shufflenet CNN by employing three types of images, i.e., chest radiograph, CT-scan, and ECG-trace images. For this purpose, we performed extensive classification experiments with the proposed approach using each type of image. With the chest radiograph dataset, we performed three classification experiments at different levels of granularity, i.e., binary, three-class, and four-class classifications. In addition, we performed a binary classification experiment with the proposed approach by classifying CT-scan images into COVID-positive and normal. Finally, utilizing the ECG-trace images, we conducted three experiments at different levels of granularity, i.e., binary, three-class, and five-class classifications. We evaluated the proposed approach with the baseline COVID-19 Radiography Database, SARS-CoV-2 CT-scan, and ECG images dataset of cardiac and COVID-19 patients. The average accuracy of 99.98% for COVID-19 detection in the three-class classification scheme using chest radiographs, optimal accuracy of 100% for COVID-19 detection using CT scans, and average accuracy of 99.37% for five-class classification scheme using ECG trace images have proved the efficacy of our proposed method over the contemporary methods. The optimal accuracy of 100% for COVID-19 detection using CT scans and the accuracy gain of 1.54% (in the case of five-class classification using ECG trace images) from the previous approach, which utilized ECG images for the first time, has a major contribution to improving the COVID-19 prediction rate in early stages. Experimental findings demonstrate that the proposed framework outperforms contemporary models. For example, the proposed approach outperforms state-of-the-art DL approaches, such as Squeezenet, Alexnet, and Darknet19, by achieving the accuracy of 99.98 (proposed method), 98.29, 98.50, and 99.67, respectively.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-01-09", "authors": ["NaeemUllah", "Javed AliKhan", "ShakerEl-Sappagh", "NoraEl-Rashidy", "Mohammad SohailKhan"], "doi": "10.3390/diagnostics13010162\n10.3390/app12126269\n10.1038/s41368-020-0075-9\n10.3389/fmed.2022.1005920\n10.1016/j.radi.2020.10.013\n10.1016/j.patcog.2021.108255\n10.1007/s00521-021-06737-6\n10.1016/j.compbiomed.2022.105350\n10.1016/j.cmpb.2022.106731\n10.1101/2020.03.30.20047787\n10.1016/j.cma.2022.114570\n10.1016/j.cma.2020.113609\n10.1016/j.cie.2021.107250\n10.1016/j.eswa.2021.116158\n10.1109/ACCESS.2022.3147821\n10.1007/s13246-020-00888-x\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105581\n10.1007/s40846-020-00529-4\n10.1016/j.mehy.2020.109761\n10.18576/amis/100122\n10.12785/amis/080617\n10.1016/j.jksuci.2021.12.017\n10.1155/2012/205391\n10.1038/s41591-020-0931-3\n10.1007/s00330-021-07715-1\n10.1016/j.patcog.2021.108135\n10.3390/s21175702\n10.1155/2021/3366057\n10.1007/s13755-021-00169-1\n10.1016/j.jrras.2022.02.002\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1101/2020.04.24.20078584\n10.1016/j.dib.2021.106762\n10.1145/3065386\n10.3390/technologies10020037\n10.1016/j.eswa.2021.116377\n10.1155/2022/4130674\n10.1155/2022/6486570\n10.3390/app12115645\n10.3390/s22197575\n10.3390/electronics11071146\n10.1109/ACCESS.2022.3189676\n10.1109/ACCESS.2019.2909969\n10.1109/ACCESS.2019.2904800\n10.3390/s22051747\n10.1007/s00521-022-08007-5\n10.1007/s00500-022-07420-1\n10.1007/s00521-021-06631-1"}
{"title": "An Efficient Deep Learning Method for Detection of COVID-19 Infection Using Chest X-ray Images.", "abstract": "The research community has recently shown significant interest in designing automated systems to detect coronavirus disease 2019 (COVID-19) using deep learning approaches and chest radiography images. However, state-of-the-art deep learning techniques, especially convolutional neural networks (CNNs), demand more learnable parameters and memory. Therefore, they may not be suitable for real-time diagnosis. Thus, the design of a lightweight CNN model for fast and accurate COVID-19 detection is an urgent need. In this paper, a lightweight CNN model called LW-CORONet is proposed that comprises a sequence of convolution, rectified linear unit (ReLU), and pooling layers followed by two fully connected layers. The proposed model facilitates extracting meaningful features from the chest X-ray (CXR) images with only five learnable layers. The proposed model is evaluated using two larger CXR datasets (Dataset-1: 2250 images and Dataset-2: 15,999 images) and the classification accuracy obtained are 98.67% and 99.00% on Dataset-1 and 95.67% and 96.25% on Dataset-2 for multi-class and binary classification cases, respectively. The results are compared with four contemporary pre-trained CNN models as well as state-of-the-art models. The effect of several hyperparameters: different optimization techniques, batch size, and learning rate have also been investigated. The proposed model demands fewer parameters and requires less memory space. Hence, it is effective for COVID-19 detection and can be utilized as a supplementary tool to assist radiologists in their diagnosis.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-01-09", "authors": ["Soumya RanjanNayak", "Deepak RanjanNayak", "UtkarshSinha", "VaibhavArora", "Ram BilasPachori"], "doi": "10.3390/diagnostics13010131\n10.1016/S0140-6736(20)30183-5\n10.32604/cmc.2020.010691\n10.1148/radiol.2020200432\n10.1109/RBME.2020.2990959\n10.1148/radiol.2020200527\n10.1148/radiol.2020200230\n10.1109/JBHI.2022.3196489\n10.1148/radiol.2020200343\n10.1109/RBME.2020.2987975\n10.3390/app10020559\n10.1148/radiol.2017162326\n10.1371/journal.pmed.1002686\n10.1016/j.compbiomed.2020.103792\n10.1007/s10044-021-00984-y\n10.1016/j.mehy.2020.109761\n10.1016/j.imu.2020.100360\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103805\n10.1016/j.chaos.2020.110122\n10.1109/TMI.2020.2996256\n10.1109/JSEN.2020.3025855\n10.1016/j.inffus.2020.11.005\n10.1016/j.compbiomed.2021.104454\n10.1145/3551647\n10.1016/j.bspc.2021.103182\n10.1016/j.compbiomed.2022.106331\n10.1016/j.bspc.2020.102365\n10.1007/s11042-022-12156-z\n10.1109/ACCESS.2019.2950228\n10.1007/BF03178082\n10.1016/j.neucom.2017.12.030\n10.1109/TMI.2016.2528162\n10.1186/s40537-019-0197-0\n10.1016/j.patrec.2020.04.018\n10.1007/s12652-020-02612-9\n10.1016/j.compmedimag.2019.05.001\n10.1016/j.media.2017.07.005"}
{"title": "The 2000HIV study: Design, multi-omics methods and participant characteristics.", "abstract": "Even during long-term combination antiretroviral therapy (cART), people living with HIV (PLHIV) have a dysregulated immune system, characterized by persistent immune activation, accelerated immune ageing and increased risk of non-AIDS comorbidities. A multi-omics approach is applied to a large cohort of PLHIV to understand pathways underlying these dysregulations in order to identify new biomarkers and novel genetically validated therapeutic drugs targets.\nThe 2000HIV study is a prospective longitudinal cohort study of PLHIV on cART. In addition, untreated HIV spontaneous controllers were recruited. In-depth multi-omics characterization will be performed, including genomics, epigenomics, transcriptomics, proteomics, metabolomics and metagenomics, functional immunological assays and extensive immunophenotyping. Furthermore, the latent viral reservoir will be assessed through cell associated HIV-1 RNA and DNA, and full-length individual proviral sequencing on a subset. Clinical measurements include an ECG, carotid intima-media thickness and plaque measurement, hepatic steatosis and fibrosis measurement as well as psychological symptoms and recreational drug questionnaires. Additionally, considering the developing pandemic, COVID-19 history and vaccination was recorded. Participants return for a two-year follow-up visit. The 2000HIV study consists of a discovery and validation cohort collected at separate sites to immediately validate any finding in an independent cohort.\nOverall, 1895 PLHIV from four sites were included for analysis, 1559 in the discovery and 336 in the validation cohort. The study population was representative of a Western European HIV population, including 288 (15.2%) \nThe 2000HIV study established a cohort of 1895 PLHIV that employs multi-omics to discover new biological pathways and biomarkers to unravel non-AIDS comorbidities, extreme phenotypes and the latent viral reservoir that impact the health of PLHIV. The ultimate goal is to contribute to a more personalized approach to the best standard of care and a potential cure for PLHIV.", "journal": "Frontiers in immunology", "date": "2023-01-07", "authors": ["Wilhelm A J WVos", "Albert LGroenendijk", "Marc J TBlaauw", "Louise Evan Eekeren", "AdrianaNavas", "Maartje C PCleophas", "NadiraVadaq", "VasilikiMatzaraki", "J\u00e9ssica CDos Santos", "Elise M GMeeder", "JaneriFr\u00f6berg", "GertWeijers", "YueZhang", "JingyuanFu", "RobTer Horst", "ChristophBock", "RainerKnoll", "Anna CAschenbrenner", "JoachimSchultze", "LinosVanderkerckhove", "TalentHwandih", "Elizabeth RWonderlich", "Sai VVemula", "Mikevan der Kolk", "Sterre C Pde Vet", "Willem LBlok", "KeesBrinkman", "CasperRokx", "Arnt F ASchellekens", "Quirijnde Mast", "Leo A BJoosten", "Marvin A HBerrevoets", "Janneke EStalenhoef", "AnneliesVerbon", "Janvan Lunzen", "Mihai GNetea", "Andre J A Mvan der Ven"], "doi": "10.3389/fimmu.2022.982746\n10.3390/v11030200\n10.1016/S2352-3018(18)30039-0\n10.1371/journal.pbio.1002050\n10.3389/fgene.2017.00084\n10.1016/j.cell.2016.10.017\n10.1016/j.cell.2016.10.018\n10.1016/j.cell.2016.10.020\n10.1016/j.celrep.2016.10.053\n10.1016/j.cell.2016.10.040\n10.1161/ATVBAHA.120.314508\n10.1210/clinem/dgaa356\n10.1002/eji.201948390\n10.1016/j.metabol.2021.154795\n10.1111/dom.14172\n10.1172/jci.insight.145928\n10.1038/s41598-021-85775-5\n10.3389/fimmu.2021.661990\n10.1038/s41590-021-00867-8\n10.1172/JCI133935\n10.1136/annrheumdis-2019-216233\n10.1038/s41591-021-01590-5\n10.1016/j.ultrasmedbio.2015.11.004\n10.1136/bmjopen-2018-022516\n10.1161/CIRCULATIONAHA.107.699579\n10.1016/j.jelectrocard.2014.07.022\n10.1016/j.amjcard.2013.09.005\n10.1016/s0022-3999(01)00296-3\n10.1002/1097-4679(199511)51:6<768\n10.1111/j.1360-0443.2009.02889.x\n10.1089/bio.2015.0104\n10.1515/cclm-2022-0094\n10.2450/2015.0007-15\n10.1371/journal.pntd.0007183\n10.1371/journal.pntd.0009187\n10.7554/eLife.63195\n10.1038/s41467-021-25949-x\n10.1097/QAI.0000000000000842\n10.1177/20499361221075454\n10.2174/1874613601408010058\n10.1093/cid/civ171\n10.1038/s41467-019-10884-9\n10.1371/journal.pmed.1003991\n10.2165/00003495-200666060-00004\n10.1161/CIRCULATIONAHA.117.033369"}
{"title": "Development and validation of a deep learning model to diagnose COVID-19 using time-series heart rate values before the onset of symptoms.", "abstract": "One of the effective ways to minimize the spread of COVID-19 infection is to diagnose it as early as possible before the onset of symptoms. In addition, if the infection can be simply diagnosed using a smartwatch, the effectiveness of preventing the spread will be greatly increased. In this study, we aimed to develop a deep learning model to diagnose COVID-19 before the onset of symptoms using heart rate (HR) data obtained from a smartwatch. In the deep learning model for the diagnosis, we proposed a transformer model that learns HR variability patterns in presymptom by tracking relationships in sequential HR data. In the cross-validation (CV) results from the COVID-19 unvaccinated patients, our proposed deep learning model exhibited high accuracy metrics: sensitivity of 84.38%, specificity of 85.25%, accuracy of 84.85%, balanced accuracy of 84.81%, and area under the receiver operating characteristics\u00a0(AUROC) of 0.8778. Furthermore, we validated our model using external multiple datasets including healthy subjects, COVID-19 patients, as well as vaccinated patients. In the external healthy subject group, our model also achieved high specificity of 77.80%. In the external COVID-19 unvaccinated patient group, our model also provided similar accuracy metrics to those from the CV: balanced accuracy of 87.23% and AUROC of 0.8897. In the COVID-19 vaccinated patients, the balanced accuracy and AUROC dropped by 66.67% and 0.8072, respectively. The first finding in this study is that our proposed deep learning model can simply and accurately diagnose COVID-19 patients using HRs obtained from a smartwatch before the onset of symptoms. The second finding is that the model trained from unvaccinated patients may provide less accurate diagnosis performance compared with the vaccinated patients. The last finding is that the model trained in a certain period of time may provide degraded diagnosis performances as the virus continues to mutate.", "journal": "Journal of medical virology", "date": "2023-01-06", "authors": ["HeewonChung", "HoonKo", "HooseokLee", "Dong KeonYon", "Won HeeLee", "Tae-SeongKim", "Kyung WonKim", "JinseokLee"], "doi": "10.1002/jmv.28462"}
{"title": "Diagnostic performance of artificial intelligence algorithms for detection of pulmonary involvement by COVID-19 based on portable radiography.", "abstract": "To evaluate the diagnostic performance of different artificial intelligence (AI) algorithms for the identification of pulmonary involvement by SARS-CoV-2 based on portable chest radiography (RX).\nProspective observational study that included patients admitted for suspected COVID-19 infection in a university hospital between July and November 2020. The reference standard of pulmonary involvement by SARS-CoV-2 comprised a positive PCR test and low-tract respiratory symptoms.\n493 patients were included, 140 (28%) with positive PCR and 32 (7%) with SARS-CoV-2 pneumonia. The AI-B algorithm had the best diagnostic performance (areas under the ROC curve AI-B 0.73, vs. AI-A 0.51, vs. AI-C 0.57). Using a detection threshold greater than 55%, AI-B had greater diagnostic performance than the specialist [(area under the curve of 0.68 (95% CI 0.64-0.72), vs. 0.54 (95% CI 0.49-0.59)].\nAI algorithms based on portable RX enabled a diagnostic performance comparable to human assessment for the detection of SARS-CoV-2 lung involvement.\nEvaluar el rendimiento diagn\u00f3stico de diferentes algoritmos de inteligencia artificial (IA) para la identificaci\u00f3n de compromiso pulmonar por SARS-CoV-2 basados en radiograf\u00eda (Rx) de t\u00f3rax port\u00e1til.\nEstudio observacional prospectivo que incluy\u00f3 pacientes ingresados por sospecha de infecci\u00f3n por COVID-19 en un hospital universitario entre julio y noviembre de 2020. El patr\u00f3n de referencia de compromiso pulmonar por SARS-CoV-2 comprendi\u00f3 una PCR positiva y s\u00edntomas respiratorios bajos.\nSe incluyeron 493 pacientes, 140 (28%) con PCR positiva y 32 (7%) con neumon\u00eda por SARS-CoV-2. El algoritmo AI-B tuvo el mejor rendimiento diagn\u00f3stico (\u00e1reas bajo la curva ROC AI-B 0,73 vs. AI-A 0,51 vs. AI-C 0,57). Utilizando un umbral de detecci\u00f3n superior al 55%. AI-B present\u00f3 mayor precisi\u00f3n que el especialista (\u00e1rea bajo la curva de 0,68 [IC 95%: 0,64\u20130,72] vs. 0,54 [IC 95%: 0,49\u20130,59]).\nLos algoritmos de IA basados en Rx port\u00e1tiles permiten una precisi\u00f3n diagn\u00f3stica comparable a la humana para la detecci\u00f3n de compromiso pulmonar por SARS-CoV-2.", "journal": "Medicina clinica (English ed.)", "date": "2023-01-05", "authors": ["Ricardo LuisCobe\u00f1as", "Mar\u00edade Vedia", "JuanFlorez", "DanielaJaramillo", "LucianaFerrari", "RicardoRe"], "doi": "10.1016/j.medcle.2022.04.020\n10.2196/19104\n10.1109/RBME.2020.2987975\n10.7717/peerj-cs.551\n10.1186/s41747-020-00195-w\n10.1148/radiol.2020201874\n10.1016/j.jiph.2020.06.028"}
{"title": "Identification of Asymptomatic COVID-19 Patients on Chest CT Images Using Transformer-Based or Convolutional Neural Network-Based Deep Learning Models.", "abstract": "Novel coronavirus disease 2019 (COVID-19) has rapidly spread throughout the world; however, it is difficult for clinicians to make early diagnoses. This study is to evaluate the feasibility of using deep learning (DL) models to identify asymptomatic COVID-19 patients based on chest CT images. In this retrospective study, six DL models (Xception, NASNet, ResNet, EfficientNet, ViT, and Swin), based on convolutional neural networks (CNNs) or transformer architectures, were trained to identify asymptomatic patients with COVID-19 on chest CT images. Data from Yangzhou were randomly split into a training set (n\u2009=\u20092140) and an internal-validation set (n\u2009=\u2009360). Data from Suzhou was the external-test set (n\u2009=\u2009200). Model performance was assessed by the metrics accuracy, recall, and specificity and was compared with the assessments of two radiologists. A total of 2700 chest CT images were collected in this study. In the validation dataset, the Swin model achieved the highest accuracy of 0.994, followed by the EfficientNet model (0.954). The recall and the precision of the Swin model were 0.989 and 1.000, respectively. In the test dataset, the Swin model was still the best and achieved the highest accuracy (0.980). All the DL models performed remarkably better than the two experts. Last, the time on the test set diagnosis spent by two experts-42\u00a0min, 17\u00a0s (junior); and 29\u00a0min, 43\u00a0s (senior)-was significantly higher than those of the DL models (all below 2\u00a0min). This study evaluated the feasibility of multiple DL models in distinguishing asymptomatic patients with COVID-19 from healthy subjects on chest CT images. It found that a transformer-based model, the Swin model, performed best.", "journal": "Journal of digital imaging", "date": "2023-01-04", "authors": ["MinyueYin", "XiaolongLiang", "ZilanWang", "YijiaZhou", "YuHe", "YuhanXue", "JingwenGao", "JiaxiLin", "ChenyanYu", "LuLiu", "XiaolinLiu", "ChaoXu", "JinzhouZhu"], "doi": "10.1007/s10278-022-00754-0\n10.1056/NEJMoa2002032\n10.1007/s00330-020-06886-7\n10.1186/s12911-021-01521-x\n10.1016/j.compbiomed.2020.103805\n10.1016/s0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1148/radiol.2020200642\n10.1148/radiol.2020200230\n10.1016/s0140-6736(20)30183-5\n10.1148/radiol.2020200463\n10.1016/j.chest.2020.04.003\n10.1001/jama.2020.12839\n10.1038/s41598-020-74164-z\n10.1148/radiol.2020200823\n10.1016/s1473-3099(14)70846-1\n10.1001/jama.2020.2565\n10.1016/j.ijid.2020.06.052\n10.1038/s41467-020-17971-2\n10.1007/s00330-020-07042-x\n10.1109/tmi.2020.3040950\n10.1016/j.cell.2020.04.045\n10.1016/j.compbiomed.2020.103792\n10.1038/s41598-020-76550-z\n10.3389/fimmu.2021.732756\n10.1002/smll.202002169\n10.1016/j.talanta.2020.121726\n10.1148/radiol.2020200490\n10.1148/radiol.2020201365\n10.1016/j.annonc.2020.04.003\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020200343\n10.1126/science.abb3221\n10.1007/s00330-021-07715-1\n10.1109/rbme.2020.2987975\n10.1155/2021/5185938\n10.1148/radiol.2020201491\n10.1109/tmi.2016.2528162"}
{"title": "COVID-19 Diagnosis on Chest Radiograph Using Artificial Intelligence.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has disrupted the world since 2019, causing significant morbidity and mortality in developed and developing countries alike. Although substantial resources have been diverted to developing diagnostic, preventative, and treatment measures, disparities in the availability and efficacy of these tools vary across countries. We seek to assess the ability of commercial artificial intelligence (AI) technology to diagnose COVID-19 by analyzing chest radiographs.\nChest radiographs taken from symptomatic patients within two days of polymerase chain reaction (PCR) tests were assessed for COVID-19 infection by board-certified radiologists and commercially available AI software. Sixty\u00a0patients with negative and 60 with positive COVID reverse transcription-polymerase chain reaction (RT-PCR) tests were chosen. Results were compared against results of the PCR test for accuracy and statistically analyzed by receiver operating characteristic (ROC) curves along with area under the curve (AUC) values.\nA total of 120 chest radiographs (60 positive and 60 negative RT-PCR tests) radiographs were analyzed. The AI software performed significantly better than chance (p = 0.001) and did not differ significantly from the radiologist ROC curve (p = 0.78).\nCommercially available AI software was not inferior compared with trained radiologists in accurately identifying COVID-19 cases by analyzing radiographs. While RT-PCR testing remains the standard, current advances in AI help correctly analyze chest radiographs to diagnose COVID-19 infection.", "journal": "Cureus", "date": "2022-12-30", "authors": ["DhirajBaruah", "LouisRunge", "Richard HJones", "Heather RCollins", "Ismail MKabakus", "Morgan PMcBee"], "doi": "10.7759/cureus.31897"}
{"title": "Covid-19 Diagnosis by WE-SAJ.", "abstract": "With a global COVID-19 pandemic, the number of confirmed patients increases rapidly, leaving the world with very few medical resources. Therefore, the fast diagnosis and monitoring of COVID-19 are one of the world's most critical challenges today. Artificial intelligence-based CT image classification models can quickly and accurately distinguish infected patients from healthy populations. Our research proposes a deep learning model (WE-SAJ) using wavelet entropy for feature extraction, two-layer FNNs for classification and the adaptive Jaya algorithm as a training algorithm. It achieves superior performance compared to the Jaya-based model. The model has a sensitivity of 85.47\u00b11.84, specificity of 87.23\u00b11.67 precision of 87.03\u00b11.34, an accuracy of 86.35\u00b10.70, and F1 score of 86.23\u00b10.77, Matthews correlation coefficient of 72.75\u00b11.38, and Fowlkes-Mallows Index of 86.24\u00b10.76. Our experiments demonstrate the potential of artificial intelligence techniques for COVID-19 diagnosis and the effectiveness of the Self-adaptive Jaya algorithm compared to the Jaya algorithm for medical image classification tasks.", "journal": "Systems science & control engineering", "date": "2022-12-27", "authors": ["WeiWang", "XinZhang", "Shui-HuaWang", "Yu-DongZhang"], "doi": "10.1080/21642583.2022.2045645"}
{"title": "A Review of COVID-19 Diagnostic Approaches in Computer Vision.", "abstract": "Computer vision has proven that it can solve many problems in the field of health in recent years. Processing the data obtained from the patients provided benefits in both disease detection and follow-up and control mechanisms. Studies on the use of computer vision for COVID-19, which is one of the biggest global health problems of the past years, are increasing daily. This study includes a preliminary review of COVID-19 computer vision research conducted in recent years. This review aims to help researchers who want to work in this field.", "journal": "Current medical imaging", "date": "2022-12-26", "authors": ["CemilZalluho\u011flu"], "doi": "10.2174/1573405619666221222161832"}
{"title": "Digital Health Applications to Establish a Remote Diagnosis of Orthopedic Knee Disorders: Scoping Review.", "abstract": "Knee pain is highly prevalent worldwide, and this number is expected to rise in the future. The COVID-19 outbreak, in combination with the aging population, rising health care costs, and the need to make health care more accessible worldwide, has led to an increasing demand for digital health care applications to deliver care for patients with musculoskeletal conditions. Digital health and other forms of telemedicine can add value in optimizing health care for patients and health care providers. This might reduce health care costs and make health care more accessible while maintaining a high level of quality. Although expectations are high, there is currently no overview comparing digital health applications with face-to-face contact in clinical trials to establish a primary knee diagnosis in orthopedic surgery.\nThis study aimed to investigate the currently available digital health and telemedicine applications to establish a primary knee diagnosis in orthopedic surgery in the general population in comparison with imaging or face-to-face contact between patients and physicians.\nA scoping review was conducted using the PubMed and Embase databases according to the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) statement. The inclusion criteria were studies reporting methods to determine a primary knee diagnosis in orthopedic surgery using digital health or telemedicine. On April 28 and 29, 2021, searches were conducted in PubMed (MEDLINE) and Embase. Data charting was conducted using a predefined form and included details on general study information, study population, type of application, comparator, analyses, and key findings. A risk-of-bias analysis was not deemed relevant considering the scoping review design of the study.\nAfter screening 5639 articles, 7 (0.12%) were included. In total, 2 categories to determine a primary diagnosis were identified: screening studies (4/7, 57%) and decision support studies (3/7, 43%). There was great heterogeneity in the included studies in algorithms used, disorders, input parameters, and outcome measurements. No more than 25 knee disorders were included in the studies. The included studies showed a relatively high sensitivity (67%-91%). The accuracy of the different studies was generally lower, with a specificity of 27% to 48% for decision support studies and 73% to 96% for screening studies.\nThis scoping review shows that there are a limited number of available applications to establish a remote diagnosis of knee disorders in orthopedic surgery. To date, there is limited evidence that digital health applications can assist patients or orthopedic surgeons in establishing the primary diagnosis of knee disorders. Future research should aim to integrate multiple sources of information and a standardized study design with close collaboration among clinicians, data scientists, data managers, lawyers, and service users to create reliable and secure databases.", "journal": "Journal of medical Internet research", "date": "2022-12-26", "authors": ["Sander Cvan Eijck", "Daan MJanssen", "Maria Cvan der Steen", "Eugenie J L GDelvaux", "Johannes G EHendriks", "Rob P AJanssen"], "doi": "10.2196/40504\n10.1109/JBHI.2016.2636665\n10.1177/1357633X15572201\n10.1016/j.gaitpost.2017.06.019\n10.1007/s00132-018-3604-x\n10.1016/j.arth.2019.05.055\n10.1080/17453674.2021.1884408\n10.1080/17453674.2021.1884408\n10.3928/01477447-20210819-09\n10.1097/CORR.0000000000001494\n10.1016/j.dsx.2020.06.007\n10.1136/annrheumdis-2013-204680\n10.1186/s12891-019-2510-7\n10.1186/s12891-019-2510-7\n10.1093/rheumatology/keu409\n10.1016/S0140-6736(20)30925-9\n10.7326/M18-0850?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.7326/M18-0850\n10.1186/s13643-016-0384-4\n10.1186/s13643-016-0384-4\n10.1177/0363546514541654\n10.1177/2325967116630286?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.1177/2325967116630286\n10.1055/s-0038-1656547\n10.3390/ijerph16071281\n10.1016/j.joca.2012.08.017\n10.1136/ard.2007.075952\n10.3399/bjgp15X686089\n10.1136/ard.62.10.957\n10.1093/ije/dyt228\n10.1186/s12891-017-1439-y\n10.1186/s12891-017-1439-y\n10.1007/s00264-009-0760-y\n10.1002/hec.3933\n10.1001/jamasurg.2019.4917\n10.1136/bmj.m127\n10.1007/s43681-020-00010-7\n10.1186/1748-7161-9-10\n10.1186/1748-7161-9-10\n10.2196/14172\n10.1016/j.medengphy.2017.02.004\n10.1002/pmrj.12393\n10.1016/j.diii.2019.02.007\n10.1186/s12984-021-00841-3\n10.1186/s12984-021-00841-3\n10.1038/s41746-018-0066-9\n10.1038/s41746-018-0066-9\n10.1001/jama.2016.17216\n10.1111/joim.12822\n10.1111/joim.12822\n10.1056/NEJMoa1901183\n10.3389/fbioe.2018.00075\n10.1136/ebmental-2020-300180\n10.1007/s00520-020-05539-1\n10.1016/j.hlpt.2018.04.003\n10.1016/j.hlpt.2018.04.003\n10.1371/journal.pone.0243043\n10.1371/journal.pone.0243043"}
{"title": "Detection of COVID-19 in X-ray Images Using Densely Connected Squeeze Convolutional Neural Network (DCSCNN): Focusing on Interpretability and Explainability of the Black Box Model.", "abstract": "The novel coronavirus (COVID-19), which emerged as a pandemic, has engulfed so many lives and affected millions of people across the world since December 2019. Although this disease is under control nowadays, yet it is still affecting people in many countries. The traditional way of diagnosis is time taking, less efficient, and has a low rate of detection of this disease. Therefore, there is a need for an automatic system that expedites the diagnosis process while retaining its performance and accuracy. Artificial intelligence (AI) technologies such as machine learning (ML) and deep learning (DL) potentially provide powerful solutions to address this problem. In this study, a state-of-the-art CNN model densely connected squeeze convolutional neural network (DCSCNN) has been developed for the classification of X-ray images of COVID-19, pneumonia, normal, and lung opacity patients. Data were collected from different sources. We applied different preprocessing techniques to enhance the quality of images so that our model could learn accurately and give optimal performance. Moreover, the attention regions and decisions of the AI model were visualized using the Grad-CAM and LIME methods. The DCSCNN combines the strength of the Dense and Squeeze networks. In our experiment, seven kinds of classification have been performed, in which six are binary classifications (COVID vs. normal, COVID vs. lung opacity, lung opacity vs. normal, COVID vs. pneumonia, pneumonia vs. lung opacity, pneumonia vs. normal) and one is multiclass classification (COVID vs. pneumonia vs. lung opacity vs. normal). The main contributions of this paper are as follows. First, the development of the DCSNN model which is capable of performing binary classification as well as multiclass classification with excellent classification accuracy. Second, to ensure trust, transparency, and explainability of the model, we applied two popular Explainable AI techniques (XAI). i.e., Grad-CAM and LIME. These techniques helped to address the black-box nature of the model while improving the trust, transparency, and explainability of the model. Our proposed DCSCNN model achieved an accuracy of 98.8% for the classification of COVID-19 vs normal, followed by COVID-19 vs. lung opacity: 98.2%, lung opacity vs. normal: 97.2%, COVID-19 vs. pneumonia: 96.4%, pneumonia vs. lung opacity: 95.8%, pneumonia vs. normal: 97.4%, and lastly for multiclass classification of all the four classes i.e., COVID vs. pneumonia vs. lung opacity vs. normal: 94.7%, respectively. The DCSCNN model provides excellent classification performance consequently, helping doctors to diagnose diseases quickly and efficiently.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-12-24", "authors": ["SikandarAli", "AliHussain", "SubrataBhattacharjee", "AliAthar", "NoneAbdullah", "Hee-CheolKim"], "doi": "10.3390/s22249983\n10.1016/S0140-6736(66)92364-6\n10.1016/j.celrep.2020.108175\n10.1159/000149390\n10.1001/jama.2020.1097\n10.1016/S0140-6736(20)30251-8\n10.1016/S0140-6736(21)02758-6\n10.15585/mmwr.mm7050e1\n10.2174/0929867328666210521164809\n10.1111/cbdd.13761\n10.1002/ped4.12178\n10.1093/cid/ciaa799\n10.1148/radiol.2020200642\n10.1001/jama.2020.3786\n10.3390/diagnostics10030165\n10.1001/jama.2020.1585\n10.1148/radiol.2020200432\n10.1016/j.measurement.2019.05.076\n10.54112/bcsrj.v2020i1.31\n10.1007/s12195-020-00629-w\n10.1007/s10044-021-00984-y\n10.1007/s00330-021-07715-1\n10.20944/preprints202003.0300.v1\n10.1016/j.chaos.2020.110120\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.104037\n10.1016/j.bbe.2020.08.008\n10.3390/life11101092\n10.3390/diagnostics11050829\n10.3390/jpm12060988\n10.1109/JBHI.2022.3168604\n10.32604/cmc.2020.013249\n10.1016/j.inffus.2021.07.016\n10.3892/etm.2020.8797\n10.1016/j.compbiomed.2022.105244\n10.1002/ima.22706\n10.1038/s41598-020-76550-z\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2021.104335\n10.1016/j.intimp.2020.106705\n10.1007/s10489-020-01829-7\n10.1007/s42600-020-00112-5\n10.1016/j.mri.2014.03.010\n10.1109/42.996338\n10.1016/j.chaos.2020.110190\n10.1016/j.compbiomed.2020.104041"}
{"title": "Image Translation by Ad CycleGAN for COVID-19 X-Ray Images: A New Approach for Controllable GAN.", "abstract": "We propose a new generative model named adaptive cycle-consistent generative adversarial network, or Ad CycleGAN to perform image translation between normal and COVID-19 positive chest X-ray images. An independent pre-trained criterion is added to the conventional Cycle GAN architecture to exert adaptive control on image translation. The performance of Ad CycleGAN is compared with the Cycle GAN without the external criterion. The quality of the synthetic images is evaluated by quantitative metrics including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQI), visual information fidelity (VIF), Frechet Inception Distance (FID), and translation accuracy. The experimental results indicate that the synthetic images generated either by the Cycle GAN or by the Ad CycleGAN have lower MSE and RMSE, and higher scores in PSNR, UIQI, and VIF in homogenous image translation (i.e., Y \u2192 Y) compared to the heterogenous image translation process (i.e., X \u2192 Y). The synthetic images by Ad CycleGAN through the heterogeneous image translation have significantly higher FID score compared to Cycle GAN (p < 0.01). The image translation accuracy of Ad CycleGAN is higher than that of Cycle GAN when normal images are converted to COVID-19 positive images (p < 0.01). Therefore, we conclude that the Ad CycleGAN with the independent criterion can improve the accuracy of GAN image translation. The new architecture has more control on image synthesis and can help address the common class imbalance issue in machine learning methods and artificial intelligence applications with medical images.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-12-24", "authors": ["ZhaohuiLiang", "Jimmy XiangjiHuang", "SameerAntani"], "doi": "10.3390/s22249628\n10.1016/S0140-6736(20)30211-7\n10.1002/14651858.CD013652\n10.1109/TMI.2020.3008025\n10.1016/j.sysarc.2020.101830\n10.1016/j.media.2020.101794\n10.1371/journal.pone.0243963\n10.48550/arXiv.1406.2661\n10.1109/CVPR.2017.632\n10.1109/ICCV.2017.244\n10.1016/j.media.2018.12.002\n10.1109/TIP.2019.2922854\n10.1109/iccv.2019.00028\n10.1088/2057-1976/ab6e1f\n10.3340/jkns.2019.0084\n10.21037/atm-21-4056\n10.1007/978-3-319-66179-7_48\n10.1002/mp.13047\n10.1088/1361-6560/aba5e9\n10.3390/s22124640\n10.48550/arXiv.1701.07875\n10.48550/arXiv.1505.04597\n10.48550/arXiv.1807.08536\n10.1109/TIP.2003.819861\n10.1109/97.995823\n10.48550/arXiv.1706.08500\n10.1016/j.imu.2021.100779\n10.1016/j.eswa.2021.115681\n10.1007/s12530-020-09346-1\n10.1109/TNNLS.2018.2875194"}
{"title": "The Capacity of Artificial Intelligence in COVID-19 Response: A Review in Context of COVID-19 Screening and Diagnosis.", "abstract": "Artificial intelligence (AI) has been shown to solve several issues affecting COVID-19 diagnosis. This systematic review research explores the impact of AI in early COVID-19 screening, detection, and diagnosis. A comprehensive survey of AI in the COVID-19 literature, mainly in the context of screening and diagnosis, was observed by applying the preferred reporting items for systematic reviews and meta-analyses (PRISMA) guidelines. Data sources for the years 2020, 2021, and 2022 were retrieved from google scholar, web of science, Scopus, and PubMed, with target keywords relating to AI in COVID-19 screening and diagnosis. After a comprehensive review of these studies, the results found that AI contributed immensely to improving COVID-19 screening and diagnosis. Some proposed AI models were shown to have comparable (sometimes even better) clinical decision outcomes, compared to experienced radiologists in the screening/diagnosing of COVID-19. Additionally, AI has the capacity to reduce physician work burdens and fatigue and reduce the problems of several false positives, associated with the RT-PCR test (with lower sensitivity of 60-70%) and medical imaging analysis. Even though AI was found to be timesaving and cost-effective, with less clinical errors, it works optimally under the supervision of a physician or other specialists.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-12-24", "authors": ["Dilber UzunOzsahin", "Nuhu AbdulhaqqIsa", "BernaUzun"], "doi": "10.3390/diagnostics12122943\n10.3390/diagnostics12081880\n10.1016/S0140-6736(20)30154-9\n10.1186/s12916-021-01954-1\n10.1007/s42399-020-00655-9\n10.21037/jmai-20-48\n10.1148/radiol.2020200432\n10.1016/j.compbiomed.2020.103792\n10.5858/arpa.2020-0901-SA\n10.3389/frai.2021.652669\n10.1007/s13369-022-06841-2\n10.1016/j.dsx.2020.04.012\n10.1155/2020/9756518\n10.1016/j.amsu.2021.102489\n10.1002/ima.22697\n10.1007/s12195-020-00629-w\n10.1016/j.chaos.2020.110059\n10.31083/j.rcm.2020.04.236\n10.2217/fvl-2020-0130\n10.3390/diagnostics11071155\n10.3390/diagnostics12081853\n10.1155/2021/5527271\n10.3390/diagnostics12040869\n10.1016/j.jacbts.2016.11.010\n10.1007/s10916-020-1536-6\n10.1155/2020/1560250\n10.1007/s00354-022-00172-4\n10.1146/annurev-bioeng-071516-044442\n10.1136/bmj.m1328\n10.2196/preprints.19526\n10.47102/annals-acadmedsg.202057\n10.1101/2020.03.30.20047308\n10.1148/radiol.2020200905\n10.1183/13993003.00775-2020\n10.1371/journal.pone.0213653\n10.1038/s41598-022-07890-1\n10.1093/jtm/taaa008\n10.1016/S1473-3099(20)30297-8\n10.1038/s41591-020-0931-3\n10.1016/S2213-2600(20)30076-X\n10.1186/s12968-019-0551-6\n10.1148/radiol.2020201491\n10.1109/RBME.2020.2987975\n10.1088/1361-6560/abbf9e\n10.1613/jair.1.12162\n10.1148/radiol.2020200230\n10.1148/radiol.2020200241\n10.1109/ACCESS.2020.3001973\n10.1145/3465398\n10.1007/s00500-020-05424-3\n10.1155/2021/5527923\n10.1016/j.bspc.2021.102490\n10.3390/app12094694\n10.32604/csse.2022.022158\n10.11591/ijece.v12i4.pp3655-3664\n10.1016/j.ibmed.2022.100049\n10.1016/j.eswa.2022.117410\n10.3390/s22020669\n10.1080/07391102.2021.1875049\n10.1016/j.neucom.2022.01.055\n10.1016/j.bspc.2020.102365\n10.1038/s41598-022-11990-3\n10.1016/j.bspc.2022.103778\n10.1186/s41747-018-0061-6\n10.1016/j.neunet.2014.09.003\n10.1016/j.chaos.2020.109944\n10.1016/j.matpr.2021.11.512\n10.1016/j.ssci.2020.105034\n10.1109/ACCESS.2020.3025010\n10.1007/s00264-020-04609-7\n10.1038/s41591-020-0824-5\n10.1109/TMI.2020.2996256\n10.1186/s12938-020-00809-9\n10.1007/s10489-020-01826-w\n10.1007/s10096-020-03901-z"}
{"title": "Diagnostic Performance in Differentiating COVID-19 from Other Viral Pneumonias on CT Imaging: Multi-Reader Analysis Compared with an Artificial Intelligence-Based Model.", "abstract": "Growing evidence suggests that artificial intelligence tools could help radiologists in differentiating COVID-19 pneumonia from other types of viral (non-COVID-19) pneumonia. To test this hypothesis, an R-AI classifier capable of discriminating between COVID-19 and non-COVID-19 pneumonia was developed using CT chest scans of 1031 patients with positive swab for SARS-CoV-2 (", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2022-12-23", "authors": ["FrancescoRizzetto", "LucaBerta", "GiuliaZorzi", "AntoninoCincotta", "FrancescaTravaglini", "DianaArtioli", "SilviaNerini Molteni", "ChiaraVismara", "FrancescoScaglione", "AlbertoTorresin", "Paola EnricaColombo", "Luca AlessandroCarbonaro", "AngeloVanzulli"], "doi": "10.3390/tomography8060235\n10.1016/j.ejrad.2021.109650\n10.1007/s00330-020-06827-4\n10.1148/radiol.2020202504\n10.2214/AJR.21.25640\n10.1148/rg.2018170048\n10.1148/radiol.2020200823\n10.1148/radiol.2020201473\n10.1007/s00330-022-08576-y\n10.1007/s00330-020-07273-y\n10.1148/ryai.2020200053\n10.1016/S2589-7500(20)30199-0\n10.21037/atm-20-5328\n10.1007/s00259-020-05075-4\n10.1148/radiol.2020201491\n10.3390/diagnostics12040869\n10.1016/j.ejrad.2021.110028\n10.1080/01621459.1927.10502953\n10.1080/03610918.2018.1490428\n10.1080/00031305.2016.1141708\n10.1007/s10826-019-01536-z\n10.1016/j.ejmp.2021.06.001\n10.1007/s10140-021-01967-6\n10.1186/s12890-020-1170-6\n10.1016/j.resmer.2021.100824\n10.1007/s11547-021-01370-8\n10.1016/j.ejrad.2021.109552\n10.1148/radiol.2020201365"}
{"title": "Interactive framework for Covid-19 detection and segmentation with feedback facility for dynamically improved accuracy and trust.", "abstract": "Due to the severity and speed of spread of the ongoing Covid-19 pandemic, fast but accurate diagnosis of Covid-19 patients has become a crucial task. Achievements in this respect might enlighten future efforts for the containment of other possible pandemics. Researchers from various fields have been trying to provide novel ideas for models or systems to identify Covid-19 patients from different medical and non-medical data. AI-based researchers have also been trying to contribute to this area by mostly providing novel approaches of automated systems using convolutional neural network (CNN) and deep neural network (DNN) for Covid-19 detection and diagnosis. Due to the efficiency of deep learning (DL) and transfer learning (TL) models in classification and segmentation tasks, most of the recent AI-based researches proposed various DL and TL models for Covid-19 detection and infected region segmentation from chest medical images like X-rays or CT images. This paper describes a web-based application framework for Covid-19 lung infection detection and segmentation. The proposed framework is characterized by a feedback mechanism for self learning and tuning. It uses variations of three popular DL models, namely Mask R-CNN, U-Net, and U-Net++. The models were trained, evaluated and tested using CT images of Covid patients which were collected from two different sources. The web application provide a simple user friendly interface to process the CT images from various resources using the chosen models, thresholds and other parameters to generate the decisions on detection and segmentation. The models achieve high performance scores for Dice similarity, Jaccard similarity, accuracy, loss, and precision values. The U-Net model outperformed the other models with more than 98% accuracy.", "journal": "PloS one", "date": "2022-12-23", "authors": ["KashfiaSailunaz", "DenizBestepe", "Tansel\u00d6zyer", "JonRokne", "RedaAlhajj"], "doi": "10.1371/journal.pone.0278487\n10.3390/v12040372\n10.1080/14737159.2020.1757437\n10.1148/radiol.2020203173\n10.3389/fpubh.2022.1046296\n10.1148/rg.2020200159\n10.1155/2021/2560388\n10.32604/cmc.2023.032064\n10.3390/s21062215\n10.3390/ijerph18031117\n10.1016/j.compbiomed.2022.105350\n10.3389/fcvm.2021.638011\n10.1155/2020/9756518\n10.1109/ACCESS.2021.3054484\n10.1016/j.dsx.2020.05.008\n10.1109/RBME.2020.2987975\n10.1016/j.jiph.2020.06.028\n10.3390/jcm10091961\n10.1007/s10489-020-02102-7\n10.1016/j.chaos.2020.110059\n10.3389/fpubh.2022.948205\n10.1016/j.neucom.2021.03.034\n10.1016/j.bea.2022.100041\n10.1007/s13369-021-05958-0\n10.1016/j.asoc.2021.107522\n10.1016/j.patcog.2020.107747\n10.1038/s41598-022-06854-9\n10.1007/s10489-021-02731-6\n10.3390/electronics11152296\n10.1007/s11063-022-10785-x\n10.1007/s13755-021-00146-8\n10.1038/s41598-020-76282-0\n10.1016/j.cmpbup.2021.100007\n10.1002/mp.15231\n10.3390/electronics11010130\n10.7717/peerj-cs.349\n10.1016/j.patcog.2021.107828\n10.3390/su13031224\n10.1016/j.cell.2020.04.045\n10.3389/fmed.2020.608525\n10.1016/j.compbiomed.2021.104319\n10.1186/s41747-020-00173-2\n10.5194/isprs-archives-XLIII-B3-2020-1507-2020\n10.1021/acs.jcim.8b00671\n10.1145/3352020.3352029\n10.3390/brainsci10070427\n10.3390/app10051897\n10.1016/j.neunet.2015.07.007\n10.1016/j.neucom.2022.01.014\n10.1016/j.media.2021.102035\n10.1186/s12880-015-0068-x"}
{"title": "Disease Recognition in X-ray Images with Doctor Consultation-Inspired Model.", "abstract": "The application of chest X-ray imaging for early disease screening is attracting interest from the computer vision and deep learning community. To date, various deep learning models have been applied in X-ray image analysis. However, models perform inconsistently depending on the dataset. In this paper, we consider each individual model as a medical doctor. We then propose a doctor consultation-inspired method that fuses multiple models. In particular, we consider both early and late fusion mechanisms for consultation. The early fusion mechanism combines the deep learned features from multiple models, whereas the late fusion method combines the confidence scores of all individual models. Experiments on two X-ray imaging datasets demonstrate the superiority of the proposed method relative to baseline. The experimental results also show that early consultation consistently outperforms the late consultation mechanism in both benchmark datasets. In particular, the early doctor consultation-inspired model outperforms all individual models by a large margin, i.e., 3.03 and 1.86 in terms of accuracy in the UIT COVID-19 and chest X-ray datasets, respectively.", "journal": "Journal of imaging", "date": "2022-12-23", "authors": ["Kim AnhPhung", "Thuan TrongNguyen", "NileshkumarWangad", "SamahBaraheem", "Nguyen DVo", "KhangNguyen"], "doi": "10.3390/jimaging8120323\n10.1148/radiol.2020200642\n10.1016/j.cca.2020.03.009\n10.1145/3065386\n10.7861/futurehosp.6-2-94\n10.3390/info13080360\n10.1038/s41551-018-0305-z\n10.1016/j.jacr.2019.05.047\n10.1016/j.imu.2020.100405\n10.20944/preprints202003.0300.v1\n10.1016/j.bbe.2020.08.005\n10.1038/s41591-020-0931-3\n10.1097/RTI.0000000000000512\n10.1016/j.chemolab.2020.104054\n10.1109/TPAMI.2007.1110\n10.1016/j.compbiomed.2020.103795\n10.1109/TPAMI.2020.2983686\n10.1109/ACCESS.2020.2994762\n10.1109/TMI.2020.2993291\n10.3390/s21217116\n10.3390/ijerph17186933\n10.1016/j.artmed.2021.102156\n10.1016/j.compbiomed.2022.105383\n10.1007/s10489-020-01831-z\n10.3390/jimaging7020012\n10.1109/TPAMI.2017.2723009"}
{"title": "Improving COVID-19 CT classification of CNNs by learning parameter-efficient representation.", "abstract": "The COVID-19 pandemic continues to spread rapidly over the world and causes a tremendous crisis in global human health and the economy. Its early detection and diagnosis are crucial for controlling the further spread. Many deep learning-based methods have been proposed to assist clinicians in automatic COVID-19 diagnosis based on computed tomography imaging. However, challenges still remain, including low data diversity in existing datasets, and unsatisfied detection resulting from insufficient accuracy and sensitivity of deep learning models. To enhance the data diversity, we design augmentation techniques of incremental levels and apply them to the largest open-access benchmark dataset, COVIDx CT-2A. Meanwhile, similarity regularization (SR) derived from contrastive learning is proposed in this study to enable CNNs to learn more parameter-efficient representations, thus improve the accuracy and sensitivity of CNNs. The results on seven commonly used CNNs demonstrate that CNN performance can be improved stably through applying the designed augmentation and SR techniques. In particular, DenseNet121 with SR achieves an average test accuracy of 99.44% in three trials for three-category classification, including normal, non-COVID-19 pneumonia, and COVID-19 pneumonia. The achieved precision, sensitivity, and specificity for the COVID-19 pneumonia category are 98.40%, 99.59%, and 99.50%, respectively. These statistics suggest that our method has surpassed the existing state-of-the-art methods on the COVIDx CT-2A dataset. Source code is available at https://github.com/YujiaKCL/COVID-CT-Similarity-Regularization.", "journal": "Computers in biology and medicine", "date": "2022-12-22", "authors": ["YujiaXu", "Hak-KeungLam", "GuangyuJia", "JianJiang", "JunkaiLiao", "XinqiBao"], "doi": "10.1016/j.compbiomed.2022.106417\n10.1136/bmj.n597\n10.3389/fmed.2020.608525\n10.5281/zenodo.4414861"}
{"title": "Virus Detection and Identification in Minutes Using Single-Particle Imaging and Deep Learning.", "abstract": "The increasing frequency and magnitude of viral outbreaks in recent decades, epitomized by the COVID-19 pandemic, has resulted in an urgent need for rapid and sensitive diagnostic methods. Here, we present a methodology for virus detection and identification that uses a convolutional neural network to distinguish between microscopy images of fluorescently labeled intact particles of different viruses. Our assay achieves labeling, imaging, and virus identification in less than 5 min and does not require any lysis, purification, or amplification steps. The trained neural network was able to differentiate SARS-CoV-2 from negative clinical samples, as well as from other common respiratory pathogens such as influenza and seasonal human coronaviruses. We were also able to differentiate closely related strains of influenza, as well as SARS-CoV-2 variants. Additional and novel pathogens can easily be incorporated into the test through software updates, offering the potential to rapidly utilize the technology in future infectious disease outbreaks or pandemics. Single-particle imaging combined with deep learning therefore offers a promising alternative to traditional viral diagnostic and genomic sequencing methods and has the potential for significant impact.", "journal": "ACS nano", "date": "2022-12-22", "authors": ["NicolasShiaelis", "AlexanderTometzki", "LeonPeto", "AndrewMcMahon", "ChristofHepp", "EricaBickerton", "CyrilFavard", "DelphineMuriaux", "MoniqueAndersson", "SarahOakley", "AliVaughan", "Philippa CMatthews", "NicoleStoesser", "Derrick WCrook", "Achillefs NKapanidis", "Nicole CRobb"], "doi": "10.1021/acsnano.2c10159\n10.1021/acsnano.0c02624\n10.1111/1751-7915.13586\n10.1007/s12250-020-00218-1\n10.1371/journal.pone.0234682\n10.1101/2020.02.26.20028373\n10.1039/D0AN01835J\n10.1021/acscentsci.0c01288\n10.1002/14651858.CD013705.pub2\n10.1038/s41598-019-52759-5\n10.1162/neco_a_00990\n10.1038/nature14539\n10.7554/eLife.40183\n10.1007/s12560-018-9335-7\n10.1093/cid/ciaa1382\n10.1016/S1473-3099(20)30113-4\n10.7150/ijbs.45018\n10.1111/j.1365-2672.2010.04663.x\n10.1002/elps.202000121\n10.1016/0166-0934(91)90012-O\n10.1038/s41598-021-91371-4\n10.1128/JVI.75.24.12359-12369.2001\n10.2807/1560-7917.ES.2021.26.3.2100008\n10.1136/bmj.308.6943.1552"}
{"title": "COVID-19 detection based on self-supervised transfer learning using chest X-ray images.", "abstract": "Considering several patients screened due to COVID-19 pandemic, computer-aided detection has strong potential in assisting clinical workflow efficiency and reducing the incidence of infections among radiologists and healthcare providers. Since many confirmed COVID-19 cases present radiological findings of pneumonia, radiologic examinations can be useful for fast detection. Therefore, chest radiography can be used to fast screen COVID-19 during the patient triage, thereby determining the priority of patient's care to help saturated medical facilities in a pandemic situation.\nIn this paper, we propose a new learning scheme called self-supervised transfer learning for detecting COVID-19 from chest X-ray (CXR) images. We compared six self-supervised learning (SSL) methods (Cross, BYOL, SimSiam, SimCLR, PIRL-jigsaw, and PIRL-rotation) with the proposed method. Additionally, we compared six pretrained DCNNs (ResNet18, ResNet50, ResNet101, CheXNet, DenseNet201, and InceptionV3) with the proposed method. We provide quantitative evaluation on the largest open COVID-19 CXR dataset and qualitative results for visual inspection.\nOur method achieved a harmonic mean (HM) score of 0.985, AUC of 0.999, and four-class accuracy of 0.953. We also used the visualization technique Grad-CAM++ to generate visual explanations of different classes of CXR images with the proposed method to increase the interpretability.\nOur method shows that the knowledge learned from natural images using transfer learning is beneficial for SSL of the CXR images and boosts the performance of representation learning for COVID-19 detection. Our method promises to reduce the incidence of infections among radiologists and healthcare providers.", "journal": "International journal of computer assisted radiology and surgery", "date": "2022-12-21", "authors": ["GuangLi", "RenTogo", "TakahiroOgawa", "MikiHaseyama"], "doi": "10.1007/s11548-022-02813-x\n10.1007/s11548-021-02466-2\n10.1016/j.cmpb.2020.105608\n10.1016/j.ejrad.2022.110164\n10.1038/s41746-021-00536-y\n10.1007/s11548-020-02286-w\n10.1016/j.cmpb.2020.105581\n10.1016/j.compbiomed.2021.104816\n10.1002/cam4.3384\n10.1016/j.cmpb.2022.107189\n10.1016/j.media.2020.101794\n10.1016/j.compbiomed.2020.103792\n10.1109/TKDE.2009.191\n10.1038/s42256-020-0181-6\n10.1007/s11548-020-02305-w\n10.1016/j.compbiomed.2021.104319\n10.1080/14737159.2020.1757437\n10.1016/j.ajem.2020.04.032\n10.1016/j.media.2020.101840"}
{"title": "Deep features to detect pulmonary abnormalities in chest X-rays due to infectious diseaseX: Covid-19, pneumonia, and tuberculosis.", "abstract": "Chest X-ray (CXR) imaging is a low-cost, easy-to-use imaging alternative that can be used to diagnose/screen pulmonary abnormalities due to infectious diseaseX: Covid-19, Pneumonia and Tuberculosis (TB). Not limited to binary decisions (with respect to healthy cases) that are reported in the state-of-the-art literature, we also consider non-healthy CXR screening using a lightweight deep neural network (DNN) with a reduced number of epochs and parameters. On three diverse publicly accessible and fully categorized datasets, for non-healthy versus healthy CXR screening, the proposed DNN produced the following accuracies: 99.87% on Covid-19 versus healthy, 99.55% on Pneumonia versus healthy, and 99.76% on TB versus healthy datasets. On the other hand, when considering non-healthy CXR screening, we received the following accuracies: 98.89% on Covid-19 versus Pneumonia, 98.99% on Covid-19 versus TB, and 100% on Pneumonia versus TB. To further precisely analyze how well the proposed DNN worked, we considered well-known DNNs such as ResNet50, ResNet152V2, MobileNetV2, and InceptionV3. Our results are comparable with the current state-of-the-art, and as the proposed CNN is light, it could potentially be used for mass screening in resource-constraint regions.", "journal": "Information sciences", "date": "2022-12-20", "authors": ["Md KawsherMahbub", "MilonBiswas", "LoveleenGaur", "FayadhAlenezi", "K CSantosh"], "doi": "10.1016/j.ins.2022.01.062"}
{"title": "LWSNet - a novel deep-learning architecture to segregate Covid-19 and pneumonia from x-ray imagery.", "abstract": "Automatic detection of lung diseases using AI-based tools became very much necessary to handle the huge number of cases occurring across the globe and support the doctors. This paper proposed a novel deep learning architecture named LWSNet (Light Weight Stacking Network) to separate Covid-19, cold pneumonia, and normal chest x-ray images. This framework is based on single, double, triple, and quadruple stack mechanisms to address the above-mentioned tri-class problem. In this framework, a truncated version of standard deep learning models and a lightweight CNN model was considered to conviniently deploy in resource-constraint devices. An evaluation was conducted on three publicly available datasets alongwith their combination. We received 97.28%, 96.50%, 97.41%, and 98.54% highest classification accuracies using quadruple stack. On further investigation, we found, using LWSNet, the average accuracy got improved from individual model to quadruple model by 2.31%, 2.55%, 2.88%, and 2.26% on four respective datasets.", "journal": "Multimedia tools and applications", "date": "2022-12-20", "authors": ["AsifuzzamanLasker", "MridulGhosh", "Sk MdObaidullah", "ChandanChakraborty", "KaushikRoy"], "doi": "10.1007/s11042-022-14247-3\n10.1111/exsy.12749\n10.1145/3431804\n10.1148/radiol.2020200642\n10.1007/s10489-021-02199-4\n10.1007/s13246-020-00865-4\n10.3390/ijerph19042013\n10.7717/peerj-cs.551\n10.1007/s11042-021-11103-8\n10.1007/s00371-021-02094-6\n10.1038/s41598-018-33214-3\n10.1016/S0140-6736(20)30183-5\n10.1016/j.imu.2020.100412\n10.1007/s10489-020-01902-1\n10.3390/su14116785\n10.1016/j.compbiomed.2020.104181\n10.1038/nature14539\n10.1016/j.neucom.2016.12.038\n10.1016/j.compbiomed.2022.105213\n10.31661/jbpe.v0i0.2008-1153\n10.1007/s10489-020-01943-6\n10.1016/j.bbe.2021.06.011\n10.1109/JBHI.2021.3051470\n10.1109/TNNLS.2021.3054746\n10.1016/j.compbiomed.2021.104319\n10.1016/j.imu.2020.100505\n10.1007/s11548-020-02286-w\n10.1016/j.bspc.2021.102622\n10.1142/S0218001421510046\n10.1109/TII.2021.3057683\n10.1007/s11042-021-11807-x\n10.1049/iet-ipr.2020.1127\n10.1016/S0893-6080(05)80023-1\n10.1002/jmv.26741\n10.1371/journal.pone.0236621"}
{"title": "Multi-objective automatic analysis of lung ultrasound data from COVID-19 patients by means of deep learning and decision trees.", "abstract": "COVID-19 raised the need for automatic medical diagnosis, to increase the physicians' efficiency in managing the pandemic. Among all the techniques for evaluating the status of the lungs of a patient with COVID-19, lung ultrasound (LUS) offers several advantages: portability, cost-effectiveness, safety. Several works approached the automatic detection of LUS imaging patterns related COVID-19 by using deep neural networks (DNNs). However, the decision processes based on DNNs are not fully explainable, which generally results in a lack of trust from physicians. This, in turn, slows down the adoption of such systems. In this work, we use two previously built DNNs as feature extractors at the frame level, and automatically synthesize, by means of an evolutionary algorithm, a decision tree (DT) that aggregates in an interpretable way the predictions made by the DNNs, returning the severity of the patients' conditions according to a LUS score of prognostic value. Our results show that our approach performs comparably or better than previously reported aggregation techniques based on an empiric combination of frame-level predictions made by DNNs. Furthermore, when we analyze the evolved DTs, we discover properties about the DNNs used as feature extractors. We make our data publicly available for further development and reproducibility.", "journal": "Applied soft computing", "date": "2022-12-20", "authors": ["Leonardo LucioCustode", "FedericoMento", "FrancescoTursi", "AndreaSmargiassi", "RiccardoInchingolo", "TizianoPerrone", "LibertarioDemi", "GiovanniIacca"], "doi": "10.1016/j.asoc.2022.109926\n10.1002/jum.15284\n10.1002/jum.15285\n10.1148/radiol.2020200847\n10.1016/j.ejro.2020.100231\n10.1016/j.jamda.2020.05.050\n10.4269/ajtmh.20-0280\n10.1186/s13054-020-02876-9\n10.1007/s00134-020-05996-6\n10.1007/s00134-020-06058-7\n10.1121/10.0002183\n10.1016/j.ultrasmedbio.2020.07.018\n10.1109/TUFFC.2020.3012289\n10.1121/10.0001797\n10.1121/10.0001797\n10.1007/s40477-017-0244-7\n10.1016/j.ultrasmedbio.2017.01.011\n10.1186/s12931-020-01338-8\n10.1109/TMI.2020.2994459\n10.1109/TUFFC.2020.3005512\n10.1016/j.media.2021.101975\n10.1109/TMI.2021.3117246\n10.1002/jum.15548\n10.1007/BFb0055930\n10.1007/BFb0055930"}
{"title": "An NLP tool for data extraction from electronic health records: COVID-19 mortalities and comorbidities.", "abstract": "The high infection rate, severe symptoms, and evolving aspects of the COVID-19 pandemic provide challenges for a variety of medical systems around the world. Automatic information retrieval from unstructured text is greatly aided by Natural Language Processing (NLP), the primary approach taken in this field. This study addresses COVID-19 mortality data from the intensive care unit (ICU) in Kuwait during the first 18 months of the pandemic. A key goal is to extract and classify the primary and intermediate causes of death from electronic health records (EHRs) in a timely way. In addition, comorbid conditions or concurrent diseases were retrieved and analyzed in relation to a variety of causes of mortality.\nAn NLP system using the Python programming language is constructed to automate the process of extracting primary and secondary causes of death, as well as comorbidities. The system is capable of handling inaccurate and messy data, this includes inadequate formats, spelling mistakes and mispositioned information. A machine learning decision trees method is used to classify the causes of death.\nFor 54.8% of the 1691 ICU patients we studied, septic shock or sepsis-related multiorgan failure was the leading cause of mortality. About three-quarters of patients die from acute respiratory distress syndrome (ARDS), a common intermediate cause of death. An arrhythmia (AF) disorder was determined to be the strongest predictor of intermediate cause of death, whether caused by ARDS or other causes.\nWe created an NLP system to automate the extraction of causes of death and comorbidities from EHRs. Our method processes messy and erroneous data and classifies the primary and intermediate causes of death of COVID-19 patients. We advocate arranging the EHR with well-defined sections and menu-driven options to reduce incorrect forms.", "journal": "Frontiers in public health", "date": "2022-12-20", "authors": ["Sana SBuHamra", "Abdullah NAlmutairi", "Abdullah KBuhamrah", "Sabah HAlmadani", "Yusuf AAlibrahim"], "doi": "10.3389/fpubh.2022.1070870\n10.4258/hir.2019.25.1.1\n10.1093/jamia/ocw082\n10.2196/12239\n10.4338/ACI-2013-10-RA-0080\n10.1038/nn.4493\n10.1101/2020.03.16.20036723\n10.1038/s41746-020-00372-6\n10.2196/21801\n10.1080/07853890.2020.1868564\n10.1080/13467581.2019.1696203\n10.1016/J.PROCS.2018.10.313\n10.1007/s00134-020-05991-x\n10.1002/jmv.25689\n10.1159/000513047\n10.3389/fmed.2020.00348\n10.3346/jkms.2020.35.e328\n10.1017/S0950268820001405\n10.1016/S0140-6736(20)30566-3\n10.1001/jama.2020.5394\n10.1097/CCE.0000000000000419\n10.1136/bmjdrc-2020-001343\n10.1007/s00392-020-01626-9\n10.1016/j.ijid.2020.03.017\n10.1001/jama.2020.4326\n10.1016/j.tmaid.2020.101623\n10.1001/jama.286.14.1754\n10.5694/mja2.50674\n10.1186/s13054-020-03240-7\n10.1371/journal.pone.0242768\n10.1038/s41598-021-82862-5"}
{"title": "Feature fusion based VGGFusionNet model to detect COVID-19 patients utilizing computed tomography scan images.", "abstract": "COVID-19 is one of the most life-threatening and dangerous diseases caused by the novel Coronavirus, which has already afflicted a larger human community worldwide. This pandemic disease recovery is possible if detected in the early stage. We proposed an automated deep learning approach from Computed Tomography (CT) scan images to detect COVID-19 positive patients by following a four-phase paradigm for COVID-19 detection: preprocess the CT scan images; remove noise from test image by using anisotropic diffusion techniques; make a different segment for the preprocessed images; and train and test COVID-19 detection using Convolutional Neural Network (CNN) models. This study employed well-known pre-trained models, including AlexNet, ResNet50, VGG16 and VGG19 to evaluate experiments. 80% of images are used to train the network in the detection process, while the remaining 20% are used to test it. The result of the experiment evaluation confirmed that the VGG19 pre-trained CNN model achieved better accuracy (98.06%). We used 4861 real-life COVID-19 CT images for experiment purposes, including 3068 positive and 1793 negative images. These images were acquired from a hospital in Sao Paulo, Brazil and two other different data sources. Our proposed method revealed very high accuracy and, therefore, can be used as an assistant to help professionals detect COVID-19 patients accurately.", "journal": "Scientific reports", "date": "2022-12-17", "authors": ["Khandaker Mohammad MohiUddin", "Samrat KumarDey", "Hafiz Md HasanBabu", "RafidMostafiz", "ShahadatUddin", "WatsharaShoombuatong", "Mohammad AliMoni"], "doi": "10.1038/s41598-022-25539-x\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMoa2001017\n10.1002/jmv.25743\n10.1016/j.ijsu.2020.02.034\n10.1016/S1473-3099(20)30120-1\n10.3389/fpubh.2020.00154\n10.1148/radiol.2020200432\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.1016/j.ejrad.2020.108961\n10.1016/S1473-3099(20)30134-1\n10.2214/AJR.20.22954\n10.1038/nature21056\n10.1016/j.patrec.2020.03.011\n10.1016/j.compmedimag.2019.101673\n10.1109/JSEN.2019.2959617\n10.1016/j.ejrad.2020.109041\n10.1371/journal.pone.0250952\n10.1038/s41467-020-18685-1\n10.1016/j.compbiomed.2020.103795\n10.1038/s41598-021-83424-5\n10.1109/TCBB.2021.3065361\n10.3390/e22050517\n10.1016/j.compbiomed.2020.104037\n10.1109/TMI.2020.2995965\n10.1148/radiol.2020200905\n10.3390/bdcc3020027\n10.1007/s12065-020-00550-1\n10.1109/ICMA.2013.6618111\n10.1148/radiol.2020200230\n10.1148/radiol.2020200241\n10.1016/j.eng.2020.04.010\n10.1183/13993003.00775-2020\n10.1016/j.imu.2020.100405\n10.1088/1361-6560/abe838"}
{"title": "Stacking Ensemble and ECA-EfficientNetV2 Convolutional Neural Networks on Classification of Multiple Chest Diseases Including COVID-19.", "abstract": "Early detection and treatment of COVID-19 patients is crucial. Convolutional neural networks have been proven to accurately extract features in medical images, which accelerates time required for testing and increases the effectiveness of COVID-19 diagnosis. This study proposes two classification models for multiple chest diseases including COVID-19.\nThe first is Stacking-ensemble model, which stacks six pretrained models including EfficientNetV2-B0, EfficientNetV2-B1, EfficientNetV2-B2, EfficientNetV2-B3, EfficientNetV2-S and EfficientNetV2-M. The second model is self-designed model ECA-EfficientNetV2 based on ECA-Net and EfficientNetV2. Ten-fold cross validation was performed for each model on chest X-ray and CT images. One more dataset, COVID-CT dataset, was tested to verify the performance of the proposed Stacking-ensemble and ECA-EfficientNetV2 models.\nThe best performance comes from the proposed ECA-EfficientNetV2 model with the highest Accuracy of 99.21%, Precision of 99.23%, Recall of 99.25%, F1-score of 99.20%, and (area under the curve) AUC of 99.51% on chest X-ray dataset; the best performance comes from the proposed ECA-EfficientNetV2 model with the highest Accuracy of 99.81%, Precision of 99.80%, Recall of 99.80%, F1-score of 99.81%, and AUC of 99.87% on chest CT dataset. The differences for five metrics between Stacking-ensemble and ECA-EfficientNetV2 models are not significant.\nEnsemble model achieves better performance than single pretrained models. Compared to the SOTA, Stacking-ensemble and ECA-EfficientNetV2 models proposed in this study demonstrate promising performance on classification of multiple chest diseases including COVID-19.", "journal": "Academic radiology", "date": "2022-12-17", "authors": ["Mei-LingHuang", "Yu-ChiehLiao"], "doi": "10.1016/j.acra.2022.11.027\n10.1016/j.compmedimag.2019.05.005\n10.1016/j.gltp.2021.01.004\n10.1016/j.neucom.2022.01.055\n10.1016/j.compbiomed.2021.104816\n10.1016/j.compbiomed.2022.105213\n10.1016/j.bspc.2021.102764\n10.1016/j.scs.2022.103713\n10.1016/j.matpr.2021.12.123\n10.1016/j.eij.2022.01.002\n10.1016/j.compmedimag.2021.102008\n10.1016/j.eswa.2022.116540\n10.1016/j.patrec.2021.08.035\n10.1016/j.bspc.2021.102588\n10.1016/j.compbiomed.2021.105182\n10.1016/j.patrec.2021.10.027\n10.1016/j.asoc.2021.108291\n10.1016/j.eswa.2021.115805\n10.1016/j.cmpb.2021.106406\n10.1016/j.compbiomed.2021.105014\n10.1016/j.gltp.2021.08.030\n10.3390/make2040027\n10.1016/j.compbiomed.2021.104425\n10.1016/j.bbe.2021.05.013\n10.1016/j.bbe.2021.12.001\n10.11591/ijece.v11i1.pp844-850\n10.1016/j.asoc.2021.107323\n10.1613/jair.614\n10.1109/CSIEC.2016.7482130\n10.7717/peerj.8693\n10.3390/ijerph191811193\n10.3390/jcm11185342\n10.1155/2020/8843664\n10.1101/2020.04.13.20063941\n10.1016/j.patrec.2020.10.001\n10.1016/j.ibmed.2021.100027\n10.1016/j.compbiomed.2021.105127"}
{"title": "AI support for accurate and fast radiological diagnosis of COVID-19: an international multicenter, multivendor CT study.", "abstract": "Differentiation between COVID-19 and community-acquired pneumonia (CAP) in computed tomography (CT) is a task that can be performed by human radiologists and artificial intelligence (AI). The present study aims to (1) develop an AI algorithm for differentiating COVID-19 from CAP and (2) evaluate its performance. (3) Evaluate the benefit of using the AI result as assistance for radiological diagnosis and the impact on relevant parameters such as accuracy of the diagnosis, diagnostic time, and confidence.\nWe included n = 1591 multicenter, multivendor chest CT scans and divided them into AI training and validation datasets to develop an AI algorithm (n = 991 CT scans; n = 462 COVID-19, and n = 529 CAP) from three centers in China. An independent Chinese and German test dataset of n = 600 CT scans from six centers (COVID-19 / CAP; n = 300 each) was used to test the performance of eight blinded radiologists and the AI algorithm. A subtest dataset (180 CT scans; n = 90 each) was used to evaluate the radiologists' performance without and with AI assistance to quantify changes in diagnostic accuracy, reporting time, and diagnostic confidence.\nThe diagnostic accuracy of the AI algorithm in the Chinese-German test dataset was 76.5%. Without AI assistance, the eight radiologists' diagnostic accuracy was 79.1% and increased with AI assistance to 81.5%, going along with significantly shorter decision times and higher confidence scores.\nThis large multicenter study demonstrates that AI assistance in CT-based differentiation of COVID-19 and CAP increases radiological performance with higher accuracy and specificity, faster diagnostic time, and improved diagnostic confidence.\n\u2022 AI can help radiologists to get higher diagnostic accuracy, make faster decisions, and improve diagnostic confidence. \u2022 The China-German multicenter study demonstrates the advantages of a human-machine interaction using AI in clinical radiology for diagnostic differentiation between COVID-19 and CAP in CT scans.", "journal": "European radiology", "date": "2022-12-17", "authors": ["FanyangMeng", "JonathanKottlors", "RahilShahzad", "HaifengLiu", "PhilippFervers", "YinhuaJin", "MiriamRinneburger", "DouLe", "MathildaWeisthoff", "WenyunLiu", "MengzheNi", "YeSun", "LiyingAn", "XiaochenHuai", "DorottyaM\u00f3r\u00e9", "AthanasiosGiannakis", "IsabelKaltenborn", "AndreasBucher", "DavidMaintz", "LeiZhang", "FrankThiele", "MingyangLi", "MichaelPerkuhn", "HuimaoZhang", "ThorstenPersigehl"], "doi": "10.1007/s00330-022-09335-9\n10.1148/radiol.2020203173\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.1038/s41568-018-0016-5\n10.1148/radiol.2020200905\n10.1148/radiol.2020201491\n10.7150/thno.46465\n10.1007/s00330-020-07044-9\n10.1038/s41591-020-0931-3\n10.1038/s41467-020-17971-2\n10.1183/13993003.00775-2020\n10.1183/13993003.01104-2020\n10.1038/s41467-020-18685-1\n10.1016/j.media.2020.101860\n10.1038/s41746-020-00369-1\n10.1148/radiol.2015141579\n10.1186/s12931-021-01670-7\n10.1186/s40779-020-0233-6\n10.1148/radiol.2020200370\n10.1109/TPAMI.2016.2644615\n10.1148/radiol.2020200823\n10.1080/22221751.2020.1750307\n10.1148/radiol.2020201473\n10.1097/RTI.0000000000000524\n10.1016/j.ejrad.2021.110002"}
{"title": "Diagnosis of COVID-19 Disease in Chest CT-Scan Images Based on Combination of Low-Level Texture Analysis and MobileNetV2 Features.", "abstract": "Since two years ago, the COVID-19 virus has spread strongly in the world and has killed more than 6 million people directly and has affected the lives of more than 500 million people. Early diagnosis of the virus can help to break the chain of transmission and reduce the death rate. In most cases, the virus spreads in the infected person's chest. Therefore, the analysis of a chest CT scan is one of the most efficient methods for diagnosing a patient. Until now, various methods have been presented to diagnose COVID-19 disease in chest CT-scan images. Most recent studies have proposed deep learning-based methods. But handcrafted features provide acceptable results in some studies too. In this paper, an innovative approach is proposed based on the combination of low-level and deep features. First of all, local neighborhood difference patterns are performed to extract handcrafted texture features. Next, deep features are extracted using MobileNetV2. Finally, a two-level decision-making algorithm is performed to improve the detection rate especially when the proposed decisions based on the two different feature set are not the same. The proposed approach is evaluated on a collected dataset of chest CT scan images from June 1, 2021, to December 20, 2021, of 238 cases in two groups of patient and healthy in different COVID-19 variants. The results show that the combination of texture and deep features can provide better performance than using each feature set separately. Results demonstrate that the proposed approach provides higher accuracy in comparison with some state-of-the-art methods in this scope.", "journal": "Computational intelligence and neuroscience", "date": "2022-12-13", "authors": ["AzitaYazdani", "ShervanFekri-Ershad", "SaeedJelvay"], "doi": "10.1155/2022/1658615\n10.1007/s10278-021-00445-2\n10.2196/27468\n10.1001/jama.2020.3786\n10.1016/j.ejrad.2020.108961\n10.1109/jbhi.2021.3074893\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1109/rbme.2020.2987975\n10.1007/s00330-021-07715-1\n10.1148/radiol.2021203957\n10.1007/s00330-021-08409-4\n10.1155/2022/2564022\n10.1101/2020.04.13.20063941\n10.3390/s21020455\n10.1016/j.cmpb.2020.105581\n10.1007/s10140-020-01886-y\n10.1007/s10044-021-00984-y\n10.3390/app12104825\n10.1016/j.cmpb.2020.105532\n10.1016/j.ins.2020.09.041\n10.3390/ijerph18063056\n10.3390/healthcare9050522\n10.3390/app11199023\n10.3390/math10142472\n10.3390/jpm12020309\n10.1109/CVPR40276.2018\n10.1007/3-540-45054-8_27\n10.1007/s13369-013-0725-8\n10.1109/tpami.2002.1017623\n10.1109/tip.2010.2042645\n10.1007/s11042-020-10321-w\n10.1007/s11042-017-4834-3\n10.11591/ijece.v11i1.pp844-850"}
{"title": "Automatic diagnosis of COVID-19 with MCA-inspired TQWT-based classification of chest X-ray images.", "abstract": "In this era of Coronavirus disease 2019 (COVID-19), an accurate method of diagnosis with less diagnosis time and cost can effectively help in controlling the disease spread with the new variants taking birth from time to time. In order to achieve this, a two-dimensional (2D) tunable Q-wavelet transform (TQWT) based on a memristive crossbar array (MCA) is introduced in this work for the decomposition of chest X-ray images of two different datasets. TQWT has resulted in promising values of peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) at the optimum values of its parameters namely quality factor (Q) of 4, and oversampling rate (r) of 3 and at a decomposition level (J) of 2. The MCA-based model is used to process decomposed images for further classification with efficient storage. These images have been further used for the classification of COVID-19 and non-COVID-19 images using ResNet50 and AlexNet convolutional neural network (CNN) models. The average accuracy values achieved for the processed chest X-ray images classification in the small and large datasets are 98.82% and 94.64%, respectively which are higher than the reported conventional methods based on different models of deep learning techniques. The average accuracy of detection of COVID-19 via the proposed method of image classification has also been achieved with less complexity, energy, power, and area consumption along with lower cost estimation as compared to CMOS-based technology.", "journal": "Computers in biology and medicine", "date": "2022-12-13", "authors": ["KumariJyoti", "SaiSushma", "SaurabhYadav", "PawanKumar", "Ram BilasPachori", "ShaibalMukherjee"], "doi": "10.1016/j.compbiomed.2022.106331\n10.1007/s13755-020-00135-3"}
{"title": "US-Net: A lightweight network for simultaneous speckle suppression and texture enhancement in ultrasound images.", "abstract": "Numerous traditional filtering approaches and deep learning-based methods have been proposed to improve the quality of ultrasound (US) image data. However, their results tend to suffer from over-smoothing and loss of texture and fine details. Moreover, they perform poorly on images with different degradation levels and mainly focus on speckle reduction, even though texture and fine detail enhancement are of crucial importance in clinical diagnosis.\nWe propose an end-to-end framework termed US-Net for simultaneous speckle suppression and texture enhancement in US images. The architecture of US-Net is inspired by U-Net, whereby a feature refinement attention block (FRAB) is introduced to enable an effective learning of multi-level and multi-contextual representative features. Specifically, FRAB aims to emphasize high-frequency image information, which helps boost the restoration and preservation of fine-grained and textural details. Furthermore, our proposed US-Net is trained essentially with real US image data, whereby real US images embedded with simulated multi-level speckle noise are used as an auxiliary training set.\nExtensive quantitative and qualitative experiments indicate that although trained with only one US image data type, our proposed US-Net is capable of restoring images acquired from different body parts and scanning settings with different degradation levels, while exhibiting favorable performance against state-of-the-art image enhancement approaches. Furthermore, utilizing our proposed US-Net as a pre-processing stage for COVID-19 diagnosis results in a gain of 3.6% in diagnostic accuracy.\nThe proposed framework can help improve the accuracy of ultrasound diagnosis.", "journal": "Computers in biology and medicine", "date": "2022-12-10", "authors": ["PatriceMonkam", "WenkaiLu", "SongbaiJin", "WenjunShan", "JingWu", "XiangZhou", "BoTang", "HuaZhao", "HongminZhang", "XinDing", "HuanChen", "LongxiangSu"], "doi": "10.1016/j.compbiomed.2022.106385"}
{"title": "[Chest imaging-based artificial intelligence in the diagnosis of coronavirus disease 2019 and prospects for future research].", "abstract": "Artificial intelligence (AI) has been applied increasingly in the medical field during the past 5 years. Within respiratory medicine, chest imaging AI is one of the relevant hotspots, commonly trained to identify pulmonary nodules/lung tumors, tuberculosis, pneumonia, interstitial lung disease, chronic obstructive pulmonary disease, pulmonary embolism and other pathologies. Due to the non-specific clinical manifestations and the low detection rate of pathogens, precise diagnosis and treatment of pneumonia remain challengeable. Since the outbreak of coronavirus disease 2019 (COVID-19), chest imaging AI has demonstrated its clinical value in accurate diagnosis and quantitative measurements of COVID-19. Moreover, an AI system can assist the clinicians to identify the high-risk COVID-19 patients who warrant close monitoring and timely intervention. However, there are still some limitations in the existing studies, such as small sample size, lack of multi-modal assessment of the AI model, and rough classification of pneumonia. Therefore, some suggestions for future research were put forward in this paper. Most of all, more attention should be paid to the collection of high-quality datasets, standardization of image annotation, technology innovation, algorithm optimization and model verification. Besides, the application of imaging AI on other types of pneumonia including viral pneumonia, bacterial pneumonia and pneumomycosis deserves further study. In conclusion, chest imaging AI is expected to play a vital role in decision-making for pneumonia in the future.\n\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u7814\u7a76\u8d8a\u6765\u8d8a\u591a\uff0c\u5f71\u50cfAI\u662f\u6700\u53d7\u5173\u6ce8\u7684\u70ed\u70b9\u4e4b\u4e00\u3002\u9274\u4e8e\u4e34\u5e8a\u8868\u73b0\u7f3a\u4e4f\u7279\u5f02\u6027\u3001\u75c5\u539f\u68c0\u6d4b\u7387\u4f4e\u7b49\u56e0\u7d20\uff0c\u80ba\u708e\u7684\u7cbe\u51c6\u8bca\u7597\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u80ba\u708e\uff08\u7b80\u79f0\u65b0\u51a0\u80ba\u708e\uff09\u75ab\u60c5\u66b4\u53d1\u4ee5\u6765\uff0c\u80f8\u90e8\u5f71\u50cfAI\u5c55\u793a\u4e86\u5176\u5728\u65b0\u51a0\u80ba\u708e\u5feb\u901f\u8bc6\u522b\u3001\u75c5\u7076\u5b9a\u91cf\u5206\u6790\u3001\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u53ca\u9884\u540e\u8bc4\u4f30\u7b49\u65b9\u9762\u7684\u4ef7\u503c\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u4e0d\u8db3\uff0c\u5982\u7814\u7a76\u6837\u672c\u91cf\u5c0f\uff0c\u6a21\u578b\u7f3a\u4e4f\u591a\u6a21\u5f0f\u8bc4\u4f30\uff0c\u80ba\u708e\u5206\u7c7b\u6b20\u7cbe\u7ec6\u7b49\u3002\u672c\u6587\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5bf9\u5f71\u50cfAI\u8f85\u52a9\u80ba\u708e\u8bca\u65ad\u7684\u4eca\u540e\u7814\u7a76\u63d0\u51fa\u4e00\u4e9b\u5efa\u8bae\uff0c\u5f3a\u8c03\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u91c7\u96c6\u3001\u5f71\u50cf\u6570\u636e\u6807\u6ce8\u7684\u6807\u51c6\u5316\u3001\u6280\u672f\u521b\u65b0\u3001\u7b97\u6cd5\u4f18\u5316\u548cAI\u6a21\u578b\u7684\u9a8c\u8bc1\uff0c\u4ee5\u53ca\u91cd\u89c6AI\u5728\u5176\u4ed6\u7c7b\u578b\u80ba\u708e\u4e2d\u7684\u7814\u7a76\uff0c\u671f\u5f85\u5f71\u50cfAI\u4e3a\u80ba\u708e\u7684\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u66f4\u591a\u53c2\u8003\u3002.", "journal": "Zhonghua jie he he hu xi za zhi = Zhonghua jiehe he huxi zazhi = Chinese journal of tuberculosis and respiratory diseases", "date": "2022-12-09", "authors": ["YLi", "S YLiu", "J PZheng"], "doi": "10.3760/cma.j.cn112147-20220519-00429"}
{"title": "Prediction of oxygen requirement in patients with COVID-19 using a pre-trained chest radiograph xAI model: efficient development of auditable risk prediction models via a fine-tuning approach.", "abstract": "Risk prediction requires comprehensive integration of clinical information and concurrent radiological findings. We present an upgraded chest radiograph (CXR) explainable artificial intelligence (xAI) model, which was trained on 241,723 well-annotated CXRs obtained prior to the onset of the COVID-19 pandemic. Mean area under the receiver operating characteristic curve (AUROC) for detection of 20 radiographic features was 0.955 (95% CI 0.938-0.955) on PA view and 0.909 (95% CI 0.890-0.925) on AP view. Coexistent and correlated radiographic findings are displayed in an interpretation table, and calibrated classifier confidence is displayed on an AI scoreboard. Retrieval of similar feature patches and comparable CXRs from a Model-Derived Atlas provides justification for model predictions. To demonstrate the feasibility of a fine-tuning approach for efficient and scalable development of xAI risk prediction models, we applied our CXR xAI model, in combination with clinical information, to predict oxygen requirement in COVID-19 patients. Prediction accuracy for high flow oxygen (HFO) and mechanical ventilation (MV) was 0.953 and 0.934 at 24\u00a0h and 0.932 and 0.836 at 72\u00a0h from the time of emergency department (ED) admission, respectively. Our CXR xAI model is auditable and captures key pathophysiological manifestations of cardiorespiratory diseases and cardiothoracic comorbidities. This model can be efficiently and broadly applied via a fine-tuning approach to provide fully automated risk and outcome predictions in various clinical scenarios in real-world practice.", "journal": "Scientific reports", "date": "2022-12-09", "authors": ["JoowonChung", "DoyunKim", "JongmunChoi", "SehyoYune", "Kyoung DooSong", "SeonkyoungKim", "MichelleChua", "Marc DSucci", "JohnConklin", "Maria G FigueiroLongo", "Jeanne BAckman", "MilenaPetranovic", "Michael HLev", "SynhoDo"], "doi": "10.1038/s41598-022-24721-5\n10.1109/access.2020.3034032\n10.1016/s2589-7500(21)00039-x\n10.1136/bmjresp-2021-001045\n10.1007/s00330-020-07269-8\n10.1038/s41746-021-00453-0\n10.1038/s41467-022-29437-8\n10.1016/j.clinimag.2020.11.004\n10.1186/s12890-020-01286-5\n10.1056/NEJMcp2009575\n10.1016/s2213-2600(20)30304-0\n10.1007/s42058-021-00078-y\n10.1001/jamanetworkopen.2019.1095\n10.1001/jamanetworkopen.2020.22779\n10.1148/radiol.2018180237\n10.1371/journal.pmed.1002686\n10.1007/s00330-019-06532-x\n10.1148/radiol.2020201874\n10.1148/radiol.2021204164\n10.1148/radiol.2281020126\n10.3390/diagnostics11112114\n10.1016/s2589-7500(21)00208-9\n10.1017/cem.2017.14\n10.1148/rg.266065168\n10.30953/bhty.v4.176"}
{"title": "Cov-caldas: A new COVID-19 chest X-Ray dataset from state of Caldas-Colombia.", "abstract": "The emergence of COVID-19 as a global pandemic forced researchers worldwide in various disciplines to investigate and propose efficient strategies and/or technologies to prevent COVID-19 from further spreading. One of the main challenges to be overcome is the fast and efficient detection of COVID-19 using deep learning approaches and medical images such as Chest Computed Tomography (CT) and Chest X-ray images. In order to contribute to this challenge, a new dataset was collected in collaboration with \"S.E.S Hospital Universitario de Caldas\" ( https://hospitaldecaldas.com/ ) from Colombia and organized following the Medical Imaging Data Structure (MIDS) format. The dataset contains 7,307 chest X-ray images divided into 3,077 and 4,230 COVID-19 positive and negative images. Images were subjected to a selection and anonymization process to allow the scientific community to use them freely. Finally, different convolutional neural networks were used to perform technical validation. This dataset contributes to the scientific community by tackling significant limitations regarding data quality and availability for the detection of COVID-19.", "journal": "Scientific data", "date": "2022-12-09", "authors": ["Jes\u00fas AlejandroAlzate-Grisales", "AlejandroMora-Rubio", "Harold BrayanArteaga-Arteaga", "Mario AlejandroBravo-Ortiz", "DanielArias-Garz\u00f3n", "Luis HumbertoL\u00f3pez-Murillo", "EstebanMercado-Ruiz", "Juan PabloVilla-Pulgarin", "OscarCardona-Morales", "SimonOrozco-Arias", "FelipeBuitrago-Carmona", "Maria JosePalancares-Sosa", "FernandaMart\u00ednez-Rodr\u00edguez", "Sonia HContreras-Ortiz", "Jose ManuelSaborit-Torres", "Joaquim \u00c1ngelMontell Serrano", "Mar\u00eda M\u00f3nicaRamirez-S\u00e1nchez", "Mario AlfonsoSierra-Gaber", "OscarJaramillo-Robledo", "Mariade la Iglesia-Vay\u00e1", "ReinelTabares-Soto"], "doi": "10.1038/s41597-022-01576-z\n10.1001/jama.2020.1585\n10.1109/ACCESS.2020.3028390\n10.1109/JAS.2020.1003393\n10.1001/archinternmed.2009.427.Radiation\n10.1038/s41597-020-00741-6\n10.1016/J.MEDIA.2021.102046\n10.1016/j.media.2020.101797\n10.1016/j.cell.2018.02.010\n10.1136/ADC.83.1.82\n10.6084/m9.figshare.c.5833484.v1\n10.1016/J.MLWA.2021.100138\n10.1007/s11263-015-0816-y"}
{"title": "MRI Assessment of Cerebral Blood Flow in Nonhospitalized Adults Who Self-Isolated Due to COVID-19.", "abstract": "Neurological symptoms associated with coronavirus disease 2019 (COVID-19), such as fatigue and smell/taste changes, persist beyond infection. However, little is known of brain physiology in the post-COVID-19 timeframe.\nTo determine whether adults who experienced flu-like symptoms due to COVID-19 would exhibit cerebral blood flow (CBF) alterations in the weeks/months beyond infection, relative to controls who experienced flu-like symptoms but tested negative for COVID-19.\nProspective observational.\nA total of 39 adults who previously self-isolated at home due to COVID-19 (41.9\u2009\u00b1\u200912.6\u2009years of age, 59% female, 116.5\u2009\u00b1\u200962.2\u2009days since positive diagnosis) and 11 controls who experienced flu-like symptoms but had a negative COVID-19 diagnosis (41.5\u2009\u00b1\u200913.4\u2009years of age, 55% female, 112.1\u2009\u00b1\u200959.5 since negative diagnosis).\nA 3.0\u2009T; T1-weighted magnetization-prepared rapid gradient and echo-planar turbo gradient-spin echo arterial spin labeling sequences.\nArterial spin labeling was used to estimate CBF. A self-reported questionnaire assessed symptoms, including ongoing fatigue. CBF was compared between COVID-19 and control groups and between those with (n\u00a0=\u00a011) and without self-reported ongoing fatigue (n\u00a0=\u00a028) within the COVID-19 group.\nBetween-group and within-group comparisons of CBF were performed in a voxel-wise manner, controlling for age and sex, at a family-wise error rate of 0.05.\nRelative to controls, the COVID-19 group exhibited significantly decreased CBF in subcortical regions including the thalamus, orbitofrontal cortex, and basal ganglia (maximum cluster size\u00a0=\u00a06012 voxels and maximum t-statistic\u00a0=\u00a05.21). Within the COVID-19 group, significant CBF differences in occipital and parietal regions were observed between those with and without self-reported on-going fatigue.\nThese cross-sectional data revealed regional CBF decreases in the COVID-19 group, suggesting the relevance of brain physiology in the post-COVID-19 timeframe. This research may help elucidate the heterogeneous symptoms of the post-COVID-19 condition.\n2.\nStage 3.", "journal": "Journal of magnetic resonance imaging : JMRI", "date": "2022-12-07", "authors": ["William S HKim", "XiangJi", "EugenieRoudaia", "J JeanChen", "AsafGilboa", "AllisonSekuler", "FuqiangGao", "ZhongminLin", "AravinthanJegatheesan", "MarioMasellis", "MagedGoubran", "Jennifer SRabin", "BenjaminLam", "IvyCheng", "RobertFowler", "ChrisHeyn", "Sandra EBlack", "Simon JGraham", "Bradley JMacIntosh"], "doi": "10.1002/jmri.28555"}
{"title": "Harris hawks optimization for COVID-19 diagnosis based on multi-threshold image segmentation.", "abstract": "Digital image processing techniques and algorithms have become a great tool to support medical experts in identifying, studying, diagnosing certain diseases. Image segmentation methods are of the most widely used techniques in this area simplifying image representation and analysis. During the last few decades, many approaches have been proposed for image segmentation, among which multilevel thresholding methods have shown better results than most other methods. Traditional statistical approaches such as the Otsu and the Kapur methods are the standard benchmark algorithms for automatic image thresholding. Such algorithms provide optimal results, yet they suffer from high computational costs when multilevel thresholding is required, which is considered as an optimization matter. In this work, the Harris hawks optimization technique is combined with Otsu's method to effectively reduce the required computational cost while maintaining optimal outcomes. The proposed approach is tested on a publicly available imaging datasets, including chest images with clinical and genomic correlates, and represents a rural COVID-19-positive (COVID-19-AR) population. According to various performance measures, the proposed approach can achieve a substantial decrease in the computational cost and the time to converge while maintaining a level of quality highly competitive with the Otsu method for the same threshold values.", "journal": "Neural computing & applications", "date": "2022-12-07", "authors": ["Mohammad HashemRyalat", "OsamaDorgham", "SaraTedmori", "ZainabAl-Rahamneh", "NijadAl-Najdawi", "SeyedaliMirjalili"], "doi": "10.1007/s00521-022-08078-4\n10.1109/ACCESS.2020.2990893\n10.1007/s11042-019-7515-6\n10.1166/jmihi.2016.1600\n10.1016/j.jksuci.2018.04.007\n10.1016/j.eswa.2019.113113\n10.1016/j.eswa.2019.112999\n10.1016/j.eswa.2019.113018\n10.1016/j.cmpb.2009.07.006\n10.1038/s41597-020-00741-6\n10.1016/j.imu.2020.100375\n10.1007/s11042-020-10147-6\n10.1109/TMI.2020.3002417\n10.1016/j.eswa.2019.01.075\n10.1016/j.eswa.2019.01.047\n10.1007/s11548-012-0783-5\n10.1117/1.JMI.6.2.020901\n10.1093/jcde/qwab082\n10.1093/jamia/ocy098\n10.1016/j.future.2019.02.028\n10.1109/JBHI.2017.2725903\n10.1016/j.compbiomed.2013.10.028\n10.1016/0734-189X(85)90125-2\n10.1016/j.compbiomed.2012.12.004\n10.1109/TPAMI.2021.3132068\n10.1109/ACCESS.2019.2891673\n10.1016/j.compbiomed.2012.09.003\n10.1016/j.media.2010.02.004\n10.1016/j.advengsoft.2017.07.002\n10.1016/j.advengsoft.2016.01.008\n10.1016/j.advengsoft.2013.12.007\n10.1016/j.asoc.2019.04.002\n10.1007/s10044-017-0653-4\n10.1007/s00500-017-2794-1\n10.1109/TSMC.1979.4310076\n10.1007/s40998-019-00251-1\n10.3390/diagnostics9010029\n10.1118/1.2948349\n10.1016/j.advengsoft.2017.01.004\n10.3348/kjr.2008.9.1.1\n10.1007/s11045-018-0603-3\n10.1118/1.3633941\n10.1016/j.eswa.2019.07.037\n10.1016/j.asoc.2019.105522\n10.1016/j.compbiomed.2018.10.033"}
{"title": "A novel intelligent radiomic analysis of perfusion SPECT/CT images to optimize pulmonary embolism diagnosis in COVID-19 patients.", "abstract": "COVID-19 infection, especially in cases with pneumonia, is associated with a high rate of pulmonary embolism (PE). In patients with contraindications for CT pulmonary angiography (CTPA) or non-diagnostic CTPA, perfusion single-photon emission computed tomography/computed tomography (Q-SPECT/CT) is a diagnostic alternative. The goal of this study is to develop a radiomic diagnostic system to detect PE based only on the analysis of Q-SPECT/CT scans.\nThis radiomic diagnostic system is based on a local analysis of Q-SPECT/CT volumes that includes both CT and Q-SPECT values for each volume point. We present a combined approach that uses radiomic features extracted from each scan as input into a fully connected classification neural network that optimizes a weighted cross-entropy loss trained to discriminate between three different types of image patterns (pixel sample level): healthy lungs (control group), PE and pneumonia. Four types of models using different configuration of parameters were tested.\nThe proposed radiomic diagnostic system was trained on 20 patients (4,927 sets of samples of three types of image patterns) and validated in a group of 39 patients (4,410 sets of samples of three types of image patterns). In the training group, COVID-19 infection corresponded to 45% of the cases and 51.28% in the test group. In the test group, the best model for determining different types of image patterns with PE presented a sensitivity, specificity, positive predictive value and negative predictive value of 75.1%, 98.2%, 88.9% and 95.4%, respectively. The best model for detecting pneumonia presented a sensitivity, specificity, positive predictive value and negative predictive value of 94.1%, 93.6%, 85.2% and 97.6%, respectively. The area under the curve (AUC) was 0.92 for PE and 0.91 for pneumonia. When the results obtained at the pixel sample level are aggregated into regions of interest, the sensitivity of the PE increases to 85%, and all metrics improve for pneumonia.\nThis radiomic diagnostic system was able to identify the different lung imaging patterns and is a first step toward a comprehensive intelligent radiomic system to optimize the diagnosis of PE by Q-SPECT/CT.\nArtificial intelligence applied to Q-SPECT/CT is a diagnostic option in patients with contraindications to CTPA or a non-diagnostic test in times of COVID-19.", "journal": "EJNMMI physics", "date": "2022-12-06", "authors": ["SoniaBaeza", "DeboraGil", "IgnasiGarcia-Oliv\u00e9", "MaiteSalcedo-Pujantell", "JordiDeport\u00f3s", "CarlesSanchez", "GuillermoTorres", "GloriaMoragas", "AntoniRosell"], "doi": "10.1186/s40658-022-00510-x\n10.1016/j.ejrad.2020.109336\n10.1055/s-0040-1710019\n10.1007/s00259-019-04450-0\n10.1111/1754-9485.12471\n10.2967/jnumed.120.245571\n10.1016/j.ajpath.2020.08.009\n10.1016/j.autrev.2020.102537\n10.1016/S2352-3026(20)30145-9\n10.1056/NEJMoa2015432\n10.1007/s11239-021-02394-7\n10.1111/jth.14888\n10.1016/j.thromres.2020.04.013\n10.1148/radiol.2020201955\n10.1148/radiol.2020203557\n10.1007/s11547-020-01328-2\n10.1148/radiol.2020201544\n10.1080/00325481.2021.1920723\n10.1007/s00259-020-05043-y\n10.1007/s00259-020-04837-4\n10.1007/s00259-020-04851-6\n10.1097/MNM.0000000000001246\n10.1378/chest.13-2090\n10.1159/000439543\n10.1148/radiol.2020200905\n10.1016/S2589-7500(20)30199-0\n10.1148/radiol.2020201491\n10.1148/radiol.2020204226\n10.1016/S1361-8415(01)80004-9\n10.1158/0008-5472.CAN-17-0339\n10.1148/radiol.2020202944\n10.1007/s12553-021-00520-2\n10.1007/s00146-020-00978-0\n10.1016/j.inffus.2021.04.008\n10.1155/2017/3762651"}
{"title": "Application of Machine Learning and Deep Learning Techniques for COVID-19 Screening Using Radiological Imaging: A Comprehensive Review.", "abstract": "Lung, being one of the most important organs in human body, is often affected by various SARS diseases, among which COVID-19 has been found to be the most fatal disease in recent times. In fact, SARS-COVID 19 led to pandemic that spreads fast among the community causing respiratory problems. Under such situation, radiological imaging-based screening [mostly chest X-ray and computer tomography (CT) modalities] has been performed for rapid screening of the disease as it is a non-invasive approach. Due to scarcity of physician/chest specialist/expert doctors, technology-enabled disease screening techniques have been developed by several researchers with the help of artificial intelligence and machine learning (AI/ML). It can be remarkably observed that the researchers have introduced several AI/ML/DL (deep learning) algorithms for computer-assisted detection of COVID-19 using chest X-ray and CT images. In this paper, a comprehensive review has been conducted to summarize the works related to applications of AI/ML/DL for diagnostic prediction of COVID-19, mainly using X-ray and CT images. Following the PRISMA guidelines, total 265 articles have been selected out of 1715 published articles till the third quarter of 2021. Furthermore, this review summarizes and compares varieties of ML/DL techniques, various datasets, and their results using X-ray and CT imaging. A detailed discussion has been made on the novelty of the published works, along with advantages and limitations.", "journal": "SN computer science", "date": "2022-12-06", "authors": ["AsifuzzamanLasker", "Sk MdObaidullah", "ChandanChakraborty", "KaushikRoy"], "doi": "10.1007/s42979-022-01464-8\n10.1109/TMI.2020.3001810\n10.1016/j.asoc.2020.106859\n10.1007/s13246-020-00934-8\n10.3348/kjr.2020.0536\n10.1056/nejmoa2001017\n10.3389/frai.2020.00065\n10.1016/j.cmpb.2020.105581\n10.1109/ICOASE51841.2020.9436542\n10.1016/j.compbiomed.2021.104868\n10.1016/j.sysarc.2020.101830\n10.3390/healthcare8010046\n10.1007/s42979-020-00383-w\n10.1007/s11042-021-10907-y\n10.1038/s41746-020-00372-6\n10.3389/fimmu.2020.01441\n10.3390/jpm11010028\n10.1007/s11042-020-10340-7\n10.26355/eurrev_202008_22510\n10.1016/j.acra.2020.09.004\n10.1186/s12911-020-01316-6\n10.1038/s41467-020-18684-2\n10.1016/j.ijsu.2010.02.007\n10.1016/j.compbiomed.2021.104605\n10.1016/j.patrec.2019.11.013\n10.1016/j.patcog.2012.10.005\n10.1016/j.eswa.2021.115152\n10.3390/e23020204\n10.1016/j.media.2020.101836\n10.1007/s10278-019-00227-x\n10.1016/j.compbiomed.2020.104037\n10.1002/mp.14676\n10.1016/j.cmpbup.2021.100007\n10.1109/TIP.2021.3058783\n10.1080/01431160600746456\n10.1109/ACCESS.2021.3054484\n10.1016/j.compbiomed.2021.104319\n10.1007/s13246-020-00865-4\n10.1016/j.chaos.2020.109944\n10.1007/s10489-020-01902-1\n10.3233/XST-200720\n10.1016/j.ijmedinf.2020.104284\n10.31661/jbpe.v0i0.2008-1153\n10.1371/journal.pone.0242535\n10.1097/RLI.0000000000000748\n10.14358/PERS.80.2.000\n10.1007/s00330-020-06801-0\n10.1148/radiol.2020200230\n10.2214/AJR.20.22954\n10.1016/j.media.2016.10.004\n10.1016/j.chaos.2020.110153\n10.1007/s00330-021-07715-1\n10.1016/j.compbiomed.2021.104588\n10.1038/s41598-021-83237-6\n10.1002/mp.14609\n10.1007/s00259-020-05075-4\n10.1007/s00259-020-04953-1\n10.34171/mjiri.34.174\n10.1007/s12539-021-00420-z\n10.1007/s42399-020-00643-z\n10.7759/cureus.10378\n10.1097/MD.0000000000023167\n10.1148/rg.2020200149\n10.1056/nejmp2000929\n10.1148/radiol.2462070712\n10.1148/radiol.2020200370\n10.1016/S2213-2600(20)30076-X\n10.1148/radiol.2020202791\n10.2214/AJR.20.23513\n10.1016/j.ultrasmedbio.2020.07.018\n10.1016/j.irbm.2020.07.001\n10.1007/s10489-020-01826-w\n10.16984/saufenbilder.459659\n10.1007/s12539-020-00393-5\n10.1109/ACCESS.2020.3016780\n10.1007/s42979-021-00605-9\n10.1007/s11042-021-10714-5\n10.1016/j.compbiomed.2021.104304\n10.1371/journal.pone.0235187\n10.3390/v12070769\n10.1186/s12938-020-00831-x\n10.1002/ima.22564\n10.1007/s00138-020-01101-5\n10.1016/j.cmpb.2020.105532\n10.1016/j.imu.2021.100621\n10.1038/s41598-021-88807-2\n10.1007/s11042-020-09894-3\n10.1016/j.bspc.2021.102622\n10.1016/j.chemolab.2020.104054\n10.1186/s12879-021-05839-9\n10.1186/s12938-020-00809-9\n10.1016/j.imu.2020.100505\n10.1007/s12539-020-00403-6\n10.1088/1742-6596/1933/1/012040\n10.1007/s00330-021-07957-z\n10.1007/s10489-020-01943-6\n10.1038/s41598-020-76282-0\n10.1038/s41598-020-80261-w\n10.32604/cmc.2021.016264\n10.1007/s11548-020-02305-w\n10.1109/ACCESS.2021.3061058\n10.1016/j.bbe.2020.08.008\n10.3233/SHTI210223\n10.1007/s11548-020-02286-w\n10.1049/iet-ipr.2020.1127\n10.1007/s11517-020-02299-2\n10.1007/s10489-021-02199-4\n10.1016/j.compbiomed.2020.103869\n10.1016/j.mehy.2020.109761\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.110495\n10.1038/s41598-020-76550-z\n10.1016/j.imu.2021.100620\n10.7717/peerj-cs.551\n10.1109/TII.2021.3057683\n10.2196/27468\n10.1007/s10140-020-01886-y\n10.47611/jsrhs.v9i2.1246\n10.1016/j.media.2020.101913\n10.1016/j.patrec.2020.09.010\n10.1016/j.neucom.2021.03.034\n10.1109/TNNLS.2021.3070467\n10.1109/EIConCIT50028.2021.9431887\n10.1016/j.iot.2021.100377\n10.2196/19569\n10.1109/ACCESS.2021.3083516\n10.9781/ijimai.2020.04.003\n10.1007/s10489-020-01867-1\n10.29194/njes.23040408\n10.7717/PEERJ-CS.345\n10.1145/3431804\n10.3389/fmed.2021.629134\n10.1148/radiol.2020201491\n10.32604/cmc.2021.014956\n10.1177/2472630320958376\n10.1016/j.irbm.2021.01.004\n10.1109/ICCC51575.2020.9344870\n10.1007/s13246-020-00888-x\n10.1007/s00330-020-07225-6\n10.1109/TMI.2020.2994908\n10.32604/cmc.2021.013228\n10.7717/PEERJ-CS.364\n10.1016/j.inffus.2020.10.004\n10.1016/j.ejrad.2020.109041\n10.2196/25535\n10.1016/j.bspc.2020.102365\n10.3233/XST-200715\n10.1109/TNNLS.2021.3054746\n10.1016/j.matpr.2021.01.820\n10.1038/s41598-020-80936-4\n10.1109/TMI.2020.3000314\n10.31763/sitech.v1i2.202\n10.1016/j.compbiomed.2020.104181\n10.1007/s12559-020-09775-9\n10.1109/TMI.2020.2996645\n10.1038/s41551-021-00704-1\n10.1007/s10489-020-02019-1\n10.1007/s00146-020-00978-0\n10.1136/bmj.m1808\n10.1016/j.bspc.2021.102490\n10.1016/j.inffus.2020.11.005\n10.7717/peerj.10309\n10.7150/ijbs.58855\n10.1109/ACCESS.2020.3003810\n10.1007/s11263-019-01228-7"}
{"title": "PulDi-COVID: Chronic obstructive pulmonary (lung) diseases with COVID-19 classification using ensemble deep convolutional neural network from chest X-ray images to minimize severity and mortality rates.", "abstract": "In the current COVID-19 outbreak, efficient testing of COVID-19 individuals has proven vital to limiting and arresting the disease's accelerated spread globally. It has been observed that the severity and mortality ratio of COVID-19 affected patients is at greater risk because of chronic pulmonary diseases. This study looks at radiographic examinations exploiting chest X-ray images (CXI), which have become one of the utmost feasible assessment approaches for pulmonary disorders, including COVID-19. Deep Learning(DL) remains an excellent image classification method and framework; research has been conducted to predict pulmonary diseases with COVID-19 instances by developing DL classifiers with nine class CXI. However, a few claim to have strong prediction results; because of noisy and small data, their recommended DL strategies may suffer from significant deviation and generality failures.\nTherefore, a unique CNN model(PulDi-COVID) for detecting nine diseases (atelectasis, bacterial-pneumonia, cardiomegaly, covid19, effusion, infiltration, no-finding, pneumothorax, viral-Pneumonia) using CXI has been proposed using the SSE algorithm. Several transfer-learning models: VGG16, ResNet50, VGG19, DenseNet201, MobileNetV2, NASNetMobile, ResNet152V2, DenseNet169 are trained on CXI of chronic lung diseases and COVID-19 instances. Given that the proposed thirteen SSE ensemble models solved DL's constraints by making predictions with different classifiers rather than a single, we present PulDi-COVID, an ensemble DL model that combines DL with ensemble learning. The PulDi-COVID framework is created by incorporating various snapshots of DL\u00a0models, which have spearheaded chronic lung diseases with COVID-19 cases identification process with a deep neural network produced CXI by applying a suggested SSE method.\u00a0That is familiar with the idea of various DL perceptions on different classes.\nPulDi-COVID findings were compared to thirteen existing studies for nine-class classification using COVID-19. Test results reveal that PulDi-COVID offers impressive outcomes for chronic diseases with COVID-19 identification with a 99.70% accuracy, 98.68% precision, 98.67% recall, 98.67% F1 score, lowest 12 CXIs zero-one loss, 99.24% AUC-ROC score, and lowest 1.33% error rate. Overall test results are superior to the existing Convolutional Neural Network(CNN). To the\u00a0best of our knowledge, the observed results for nine-class classification are significantly superior to the state-of-the-art approaches employed for COVID-19 detection. Furthermore, the CXI\u00a0that we used\u00a0to assess our algorithm is one of the larger datasets for COVID detection with pulmonary diseases.\nThe empirical findings of our suggested approach PulDi-COVID\u00a0show that it outperforms previously developed methods. The suggested SSE method with PulDi-COVID can effectively fulfill the COVID-19 speedy detection needs with different lung diseases for physicians to minimize patient severity and mortality.", "journal": "Biomedical signal processing and control", "date": "2022-12-06", "authors": ["Yogesh HBhosale", "K SridharPatnaik"], "doi": "10.1016/j.bspc.2022.104445\n10.3389/fmed.2021.588013\n10.1109/TII.2021.3057683"}
{"title": "Rapid diagnosis of Covid-19 infections by a progressively growing GAN and CNN optimisation.", "abstract": "Covid-19 infections are spreading around the globe since December 2019. Several diagnostic methods were developed based on biological investigations and the success of each method depends on the accuracy of identifying Covid infections. However, access to diagnostic tools can be limited, depending on geographic region and the diagnosis duration plays an important role in treating Covid-19. Since the virus causes pneumonia, its presence can also be detected using medical imaging by Radiologists. Hospitals with X-ray capabilities are widely distributed all over the world, so a method for diagnosing Covid-19 from chest X-rays would present itself. Studies have shown promising results in automatically detecting Covid-19 from medical images using supervised Artificial neural network (ANN) algorithms. The major drawback of supervised learning algorithms is that they require huge amounts of data to train. Also, the radiology equipment is not computationally efficient for deep neural networks. Therefore, we aim to develop a Generative Adversarial Network (GAN) based image augmentation to optimize the performance of custom, light, Convolutional networks used for the classification of Chest X-rays (CXR).\nA Progressively Growing Generative Adversarial Network (PGGAN) is used to generate synthetic and augmented data to supplement the dataset. We propose two novel CNN architectures to perform the Multi-class classification of Covid-19, healthy and pneumonia affected Chest X-rays. Comparisons have been drawn to the state of the art models and transfer learning methods to evaluate the superiority of the networks. All the models are trained using enhanced and augmented X-ray images and are compared based on classification metrics.\nThe proposed models had extremely high classification metrics with proposed Architectures having test accuracy of 98.78% and 99.2% respectively while having 40% lesser training parameters than their state of the art counterpart.\nIn the present study, a method based on artificial intelligence is proposed, leading to a rapid diagnostic tool for Covid infections based on Generative Adversarial Network (GAN) and Convolutional Neural Networks (CNN). The benefit will be a high accuracy of detection with up to 99% hit rate, a rapid diagnosis, and an accessible Covid identification method by chest X-ray images.", "journal": "Computer methods and programs in biomedicine", "date": "2022-12-05", "authors": ["RutwikGulakala", "BerndMarkert", "MarcusStoffel"], "doi": "10.1016/j.cmpb.2022.107262"}
{"title": "COVID-DSNet: A novel deep convolutional neural network for detection of coronavirus (SARS-CoV-2) cases from CT and Chest X-Ray images.", "abstract": "COVID-19 (SARS-CoV-2), which causes acute respiratory syndrome, is a contagious and deadly disease that has devastating effects on society and human life. COVID-19 can cause serious complications, especially in patients with pre-existing chronic health problems such as diabetes, hypertension, lung cancer, weakened immune systems, and the elderly. The most critical step in the fight against COVID-19 is the rapid diagnosis of infected patients. Computed Tomography (CT), chest X-ray (CXR), and RT-PCR diagnostic kits are frequently used to diagnose the disease. However, due to difficulties such as the inadequacy of RT-PCR test kits and false negative (FN) results in the early stages of the disease, the time-consuming examination of medical images obtained from CT and CXR imaging techniques by specialists/doctors, and the increasing workload on specialists, it is challenging to detect COVID-19. Therefore, researchers have suggested searching for new methods in COVID- 19 detection. In analysis studies with CT and CXR radiography images, it was determined that COVID-19-infected patients experienced abnormalities related to COVID-19. The anomalies observed here are the primary motivation for artificial intelligence researchers to develop COVID-19 detection applications with deep convolutional neural networks. Here, convolutional neural network-based deep learning algorithms from artificial intelligence technologies with high discrimination capabilities can be considered as an alternative approach in the disease detection process. This study proposes a deep convolutional neural network, COVID-DSNet, to diagnose typical pneumonia (bacterial, viral) and COVID-19 diseases from CT, CXR, hybrid CT\u00a0+\u00a0CXR images. In the multi-classification study with the CT dataset, 97.60\u00a0% accuracy and 97.60\u00a0% sensitivity values were obtained from the COVID-DSNet model, and 100\u00a0%, 96.30\u00a0%, and 96.58\u00a0% sensitivity values were obtained in the detection of typical, common pneumonia and COVID-19, respectively. The proposed model is an economical, practical deep learning network that data scientists can benefit from and develop. Although it is not a definitive solution in disease diagnosis, it may help experts as it produces successful results in detecting pneumonia and COVID-19.", "journal": "Artificial intelligence in medicine", "date": "2022-12-04", "authors": ["Hatice CatalReis", "VeyselTurk"], "doi": "10.1016/j.artmed.2022.102427\n10.1016/j.asoc.2020.106859\n10.1016/j.virs.2022.09.003\n10.1016/j.mtbio.2022.100265\n10.1111/1348-0421.12945\n10.1002/jmv.27132\n10.1038/s41579-021-00573-0\n10.1001/jama.2022.14711\n10.1038/s41565-022-01177-2\n10.1038/s41580-021-00418-x\n10.3390/ijms23031716\n10.1038/s41580-021-00432-z\n10.1016/j.media.2021.102096\n10.3390/cancers13020162\n10.3389/fimmu.2021.660632\n10.1007/s12559-020-09787-5\n10.3389/fphar.2021.664349\n10.3390/cells10030587\n10.1007/s11033-021-06358-1\n10.1001/jama.2021.13084\n10.1016/j.arbres.2021.06.003\n10.1038/s41598-021-96755-0\n10.1016/S0140-6736(22)00009-5\n10.1016/j.cmpb.2019.105162\n10.1148/ryct.2021200564\n10.1016/j.ejrad.2020.109147\n10.1186/s12890-021-01450-5\n10.1007/s12559-020-09779-5\n10.1093/bib/bbab412\n10.3390/v14020322\n10.1007/s00521-020-05410-8\n10.1016/j.nupar.2022.01.003\n10.1016/j.biochi.2022.01.015\n10.1109/ACCESS.2020.3010287\n10.1016/j.ijid.2020.10.069\n10.3390/biomedicines10020242\n10.1016/bs.acr.2020.10.001\n10.1084/jem.20202489\n10.17305/bjbms.2021.6340\n10.1007/s11154-021-09707-4\n10.1016/j.jacbts.2021.10.011\n10.1001/jamacardio.2020.3557\n10.1016/j.chom.2020.05.008\n10.1126/science.369.6500.125\n10.1093/cid/ciaa644\n10.1016/j.asoc.2022.109207\n10.1038/s41392-022-00884-5\n10.1038/s41579-020-00462-y\n10.1136/bmj.n597\n10.3390/diagnostics12020467\n10.1007/s10311-021-01369-7\n10.1056/NEJMc2119236\n10.1001/jama.2021.24315\n10.1056/NEJMoa2108891\n10.1126/science.abn7591\n10.1016/j.eclinm.2021.100861\n10.1016/j.compbiomed.2021.104742\n10.1016/j.bspc.2021.103415\n10.3390/diagnostics11111972\n10.1007/s00521-020-05636-6\n10.1007/s42979-021-00695-5\n10.1038/s41598-021-84630-x\n10.1038/s41598-021-03889-2\n10.3390/s22010372\n10.1038/s41598-022-06264-x\n10.1016/j.compbiomed.2022.105810\n10.1016/j.chaos.2020.110245\n10.1016/j.bspc.2022.103977\n10.1007/s10489-020-01943-6\n10.1007/s00500-021-06579-3\n10.1007/s11042-020-10165-4\n10.1007/s12559-021-09955-1\n10.3390/s22031211\n10.1007/s10522-021-09946-7\n10.1007/s11042-021-11319-8\n10.1038/s41598-020-76550-z\n10.48550/arXiv.2003.11597\n10.1016/j.patcog.2021.108255\n10.1007/s10489-020-01829-7\n10.1148/radiol.2020200905\n10.1016/j.patrec.2021.08.018\n10.1155/2022/6185013\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.243\n10.1162/neco.1997.9.8.1735\n10.3390/s21030832\n10.1016/j.chaos.2020.110153\n10.1016/j.compbiomed.2021.104319\n10.17632/rscbjbr9sj.2\n10.1609/aaai.v31i1.11231\n10.1109/CVPR.2016.308\n10.48550/arXiv.1704.04861\n10.1109/CVPR.2018.00907\n10.48550/arXiv.1412.6980\n10.1016/j.eswa.2020.113909\n10.1016/j.compbiomed.2021.105134\n10.1016/j.compbiomed.2022.105213\n10.1016/j.knosys.2021.106849\n10.1038/s41586-022-04569-5\n10.1007/s42452-019-1903-4"}
{"title": "Classification and visual explanation for COVID-19 pneumonia from CT images using triple learning.", "abstract": "This study presents a novel framework for classifying and visualizing pneumonia induced by COVID-19 from CT images. Although many image classification methods using deep learning have been proposed, in the case of medical image fields, standard classification methods are unable to be used in some cases because the medical images that belong to the same category vary depending on the progression of the symptoms and the size of the inflamed area. In addition, it is essential that the models used be transparent and explainable, allowing health care providers to trust the models and avoid mistakes. In this study, we propose a classification method using contrastive learning and an attention mechanism. Contrastive learning is able to close the distance for images of the same category and generate a better feature space for classification. An attention mechanism is able to emphasize an important area in the image and visualize the location related to classification. Through experiments conducted on two-types of classification using a three-fold cross validation, we confirmed that the classification accuracy was significantly improved; in addition, a detailed visual explanation was achieved comparison with conventional methods.", "journal": "Scientific reports", "date": "2022-12-03", "authors": ["SotaKato", "MasahiroOda", "KensakuMori", "AkinobuShimizu", "YoshitoOtake", "MasahiroHashimoto", "ToshiakiAkashi", "KazuhiroHotta"], "doi": "10.1038/s41598-022-24936-6\n10.1148/radiol.2020200905\n10.1016/j.ejrad.2020.109041\n10.1109/ACCESS.2020.3005510\n10.1016/j.asoc.2020.106885\n10.1109/TCBB.2021.3065361\n10.1016/j.compbiomed.2020.104037\n10.3390/diagnostics11050893\n10.1016/j.patcog.2021.107826\n10.1016/j.patcog.2021.107848\n10.1016/j.media.2021.102105\n10.1007/s11263-021-01559-4\n10.1148/radiol.2020200905\n10.1109/TMI.2020.2995965"}
{"title": "A lightweight network for COVID-19 detection in X-ray images.", "abstract": "The Novel Coronavirus 2019 (COVID-19) is a global pandemic which has a devastating impact. Due to its quick transmission, a prominent challenge in confronting this pandemic is the rapid diagnosis. Currently, the commonly-used diagnosis is the specific molecular tests aided with the medical imaging modalities such as chest X-ray (CXR). However, with the large demand, the diagnoses of CXR are time-consuming and laborious. Deep learning is promising for automatically diagnosing COVID-19 to ease the burden on medical systems. At present, the most applied neural networks are large, which hardly satisfy the rapid yet inexpensive requirements of COVID-19 detection. To reduce huge computation and memory demands, in this paper, we focus on implementing lightweight networks for COVID-19 detection in CXR. Concretely, we first augment data based on clinical visual features of CXR from expertise. Then, according to the fact that all the input data are CXR, we design a targeted four-layer network with either 11\u00a0\u00d7\u00a011 or 3\u00a0\u00d7\u00a03 kernels to recognize regional features and detail features. A pruning criterion based on the weights importance is also proposed to further prune the network. Experiments on a public COVID-19 dataset validate the effectiveness and efficiency of the proposed method.", "journal": "Methods (San Diego, Calif.)", "date": "2022-12-03", "authors": ["YongShi", "AndaTang", "YangXiao", "LingfengNiu"], "doi": "10.1016/j.ymeth.2022.11.004"}
{"title": "Structural Attention Graph Neural Network for Diagnosis and Prediction of COVID-19 Severity.", "abstract": "With rapid worldwide spread of Coronavirus Disease 2019 (COVID-19), jointly identifying severe COVID-19 cases from mild ones and predicting the conversion time (from mild to severe) is essential to optimize the workflow and reduce the clinician's workload. In this study, we propose a novel framework for COVID-19 diagnosis, termed as Structural Attention Graph Neural Network (SAGNN), which can combine the multi-source information including features extracted from chest CT, latent lung structural distribution, and non-imaging patient information to conduct diagnosis of COVID-19 severity and predict the conversion time from mild to severe. Specifically, we first construct a graph to incorporate structural information of the lung and adopt graph attention network to iteratively update representations of lung segments. To distinguish different infection degrees of left and right lungs, we further introduce a structural attention mechanism. Finally, we introduce demographic information and develop a multi-task learning framework to jointly perform both tasks of classification and regression. Experiments are conducted on a real dataset with 1687 chest CT scans, which includes 1328 mild cases and 359 severe cases. Experimental results show that our method achieves the best classification (e.g., 86.86% in terms of Area Under Curve) and regression (e.g., 0.58 in terms of Correlation Coefficient) performance, compared with other comparison methods.", "journal": "IEEE transactions on medical imaging", "date": "2022-12-03", "authors": ["YanbeiLiu", "HenanLi", "TaoLuo", "ChangqingZhang", "ZhitaoXiao", "YingWei", "YaozongGao", "FengShi", "FeiShan", "DinggangShen"], "doi": "10.1109/TMI.2022.3226575"}
{"title": "Radiomorphological signs and clinical severity of SARS-CoV-2 lineage B.1.1.7.", "abstract": "We aimed to assess the differences in the severity and chest-CT radiomorphological signs of SARS-CoV-2 B.1.1.7 and non-B.1.1.7 variants.\nWe collected clinical data of consecutive patients with laboratory-confirmed COVID-19 and chest-CT imaging who were admitted to the Emergency Department between September 1- November 13, 2020 (non-B.1.1.7 cases) and March 1-March 18, 2021 (B.1.1.7 cases). We also examined the differences in the severity and radiomorphological features associated with COVID-19 pneumonia. Total pneumonia burden (%), mean attenuation of ground-glass opacities and consolidation were quantified using deep-learning research software.\nThe final population comprised 500 B.1.1.7 and 500 non-B.1.1.7 cases. Patients with B.1.1.7 infection were younger (58.5 \u00b1 15.6 vs 64.8 \u00b1 17.3; \nDespite B.1.1.7 patients were younger and had fewer comorbidities, they experienced more severe disease than non-B.1.1.7 patients, however, the risk of death was the same between the two groups.\nOur study provides data on deep-learning based quantitative lung lesion burden and clinical outcomes of patients infected by B.1.1.7 VOC. Our findings might serve as a model for later investigations, as new variants are emerging across the globe.", "journal": "BJR open", "date": "2022-12-02", "authors": ["JuditSimon", "KajetanGrodecki", "SebastianCadet", "AdityaKillekar", "PiotrSlomka", "Samuel JamesZara", "EmeseZsarn\u00f3czay", "ChiaraNardocci", "NorbertNagy", "KatalinKrist\u00f3f", "BarnaV\u00e1s\u00e1rhelyi", "VeronikaM\u00fcller", "B\u00e9laMerkely", "DaminiDey", "P\u00e1lMaurovich-Horvat"], "doi": "10.1259/bjro.20220016\n10.1126/science.abg3055\n10.1016/S1473-3099(21)00170-5\n10.1136/bmj.n579\n10.2807/1560-7917.ES.2021.26.11.2100256\n10.1007/s00330-020-06748-2\n10.1148/radiol.2020200843\n10.2214/AJR.20.22954\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.1016/S1473-3099(20)30483-7\n10.1148/ryct.2020200389\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020200432\n10.1136/bmj.m1464\n10.1016/S1473-3099(20)30086-4\n10.1556/1647.2020.00002\n10.1038/s41379-020-0536-x\n10.1016/S0140-6736(20)30566-3\n10.1038/s41598-020-76550-z\n10.1007/s00330-021-07715-1\n10.1183/13993003.00775-2020\n10.1016/j.compbiomed.2020.103795"}
{"title": "3D CT-Inclusive Deep-Learning Model to Predict Mortality, ICU Admittance, and Intubation in COVID-19 Patients.", "abstract": "Chest CT is a useful initial exam in patients with coronavirus disease 2019 (COVID-19) for assessing lung damage. AI-powered predictive models could be useful to better allocate resources in the midst of the pandemic. Our aim was to build a deep-learning (DL) model for COVID-19 outcome prediction inclusive of 3D chest CT images acquired at hospital admission. This retrospective multicentric study included 1051 patients (mean age 69, SD\u2009=\u200915) who presented to the emergency department of three different institutions between 20th March 2020 and 20th January 2021 with COVID-19 confirmed by real-time reverse transcriptase polymerase chain reaction (RT-PCR). Chest CT at hospital admission were evaluated by a 3D residual neural network algorithm. Training, internal validation, and external validation groups included 608, 153, and 290 patients, respectively. Images, clinical, and laboratory data were fed into different customizations of a dense neural network to choose the best performing architecture for the prediction of mortality, intubation, and intensive care unit (ICU) admission. The AI model tested on CT and clinical features displayed accuracy, sensitivity, specificity, and ROC-AUC, respectively, of 91.7%, 90.5%, 92.4%, and 95% for the prediction of patient's mortality; 91.3%, 91.5%, 89.8%, and 95% for intubation; and 89.6%, 90.2%, 86.5%, and 94% for ICU admission (internal validation) in the testing cohort. The performance was lower in the validation cohort for mortality (71.7%, 55.6%, 74.8%, 72%), intubation (72.6%, 74.7%, 45.7%, 64%), and ICU admission (74.7%, 77%, 46%, 70%) prediction. The addition of the available laboratory data led to an increase in sensitivity for patient's mortality (66%) and specificity for intubation and ICU admission (50%, 52%, respectively), while the other metrics maintained similar performance results. We present a deep-learning model to predict mortality, ICU admittance, and intubation in COVID-19 patients. KEY POINTS: \u2022 3D CT-based deep learning model predicted the internal validation set with high accuracy, sensibility and specificity (>\u200990%) mortality, ICU admittance, and intubation in COVID-19 patients. \u2022 The model slightly increased prediction results when laboratory data were added to the analysis, despite data imbalance. However, the model accuracy dropped when CT images were not considered in the analysis, implying an important role of CT in predicting outcomes.", "journal": "Journal of digital imaging", "date": "2022-12-01", "authors": ["AlbertoDi Napoli", "EmanuelaTagliente", "LucaPasquini", "EnricaCipriano", "FilomenaPietrantonio", "PiermariaOrtis", "SimonaCurti", "AlessandroBoellis", "TeseoStefanini", "AntonioBernardini", "ChiaraAngeletti", "Sofia ChiatamoneRanieri", "PaolaFranchi", "Ioan PaulVoicu", "CarloCapotondi", "AntonioNapolitano"], "doi": "10.1007/s10278-022-00734-4"}
{"title": "Automatic detection of Covid-19 from chest X-ray and lung computed tomography images using deep neural networks and transfer learning.", "abstract": "The world has been undergoing the most ever unprecedented circumstances caused by the coronavirus pandemic, which is having a devastating global effect in different aspects of life. Since there are not effective antiviral treatments for Covid-19 yet, it is crucial to early detect and monitor the progression of the disease, thereby helping to reduce mortality. While different measures are being used to combat the virus, medical imaging techniques have been examined to support doctors in diagnosing the disease. In this paper, we present a practical solution for the detection of Covid-19 from chest X-ray (CXR) and lung computed tomography (LCT) images, exploiting cutting-edge Machine Learning techniques. As the main classification engine, we make use of EfficientNet and MixNet, two recently developed families of deep neural networks. Furthermore, to make the training more effective and efficient, we apply three transfer learning algorithms. The ultimate aim is to build a reliable expert system to detect Covid-19 from different sources of images, making it be a multi-purpose AI diagnosing system. We validated our proposed approach using four real-world datasets. The first two are CXR datasets consist of 15,000 and 17,905 images, respectively. The other two are LCT datasets with 2,482 and 411,528 images, respectively. The five-fold cross-validation methodology was used to evaluate the approach, where the dataset is split into five parts, and accordingly the evaluation is conducted in five rounds. By each evaluation, four parts are combined to form the training data, and the remaining one is used for testing. We obtained an encouraging prediction performance for all the considered datasets. In all the configurations, the obtained accuracy is always larger than 95.0%. Compared to various existing studies, our approach yields a substantial performance gain. Moreover, such an improvement is statistically significant.", "journal": "Applied soft computing", "date": "2022-12-01", "authors": ["Linh TDuong", "Phuong TNguyen", "LudovicoIovino", "MicheleFlammini"], "doi": "10.1016/j.asoc.2022.109851\n10.1109/TITS.2021.3053373\n10.1016/j.eswa.2021.115519\n10.1016/j.eswa.2017.12.020\n10.1038/s41591-020-0931-3\n10.1007/s11263-015-0816-y\n10.1016/j.compag.2018.02.016\n10.1186/s40537-016-0043-6\n10.1007/11564096_40\n10.3390/rs9090907\n10.1016/j.compag.2020.105326\n10.1101/2020.04.24.20078584\n10.1016/j.cell.2020.04.045\n10.1016/j.asoc.2020.106691\n10.1016/j.tube.2022.102234\n10.1016/j.asoc.2021.107323\n10.1016/j.compbiomed.2020.103792\n10.1101/2020.06.08.20125963\n10.1109/CVPR.2016.90\n10.1007/s11042-021-10783-6\n10.1038/s41598-021-99015-3\n10.1007/978-1-4612-4380-9_16\n10.1101/2020.06.08.20121541\n10.36227/techrxiv.12328061\n10.1016/j.compbiomed.2020.103795\n10.1148/radiol.2020201491\n10.1148/radiol.2020201491\n10.1101/2020.08.13.20173997"}
{"title": "Mentoring within the medical radiation sciences - Establishing a national program.", "abstract": "The aim of this study was to compare the accuracy and performance of 12 pre-trained deep learning models for classifying covid-19 and normal chest X-ray images from Kaggle.\na desktop computer with an Intel CPU i9-10900 2.80GHz and NVIDIA GPU GeForce RTX2070 SUPER, Anaconda3 software with 12 pre-trained models including VGG16, VGG19, DenseNet121, DenseNet169, DenseNet201, RestNet50V2, RestNet101V2, RestNet152V2, InceptionRestnetV2, InceptionV3, XceptionV1 and MobileNetV2, covid-19 and normal chest X-ray from Kaggle website.\nthe images were divided into three sets of train, test, and validation sets using a ratio of 70:20:10, respectively. The performance was recorded for each pre-train model with hyperparameters of epoch, batch size, and learning rate as 16, 16 and 0.0001 respectively. The prediction results of each model were recorded and compared.\nfrom the results of all 12 pre-trained deep learning model, five models that have highest validation accuracy were DenseNet169, DenseNet201, InceptionV3, DenseNet121 and InceptionRestNetV2, respectively.\nThe top-5 highest accuracy models for classifying the COVID-19 were DenseNet169, DenseNet201, InceptionV3, DenseNet121 and InceptionRestnetV2 with accuracies of 95.4%, 95.07%, 94.73%, 94.51% and 93.61% respectively.", "journal": "Journal of medical imaging and radiation sciences", "date": "2022-11-29", "authors": ["AllieTonks", "FranziskaJerjen"], "doi": "10.1016/j.jmir.2022.10.190"}
{"title": "The optimal use of colon capsule endoscopes in clinical practice.", "abstract": "Colon capsule endoscopy (CCE) has been available for nearly two decades but has grappled with being an equal diagnostic alternative to optical colonoscopy (OC). Due to the COVID-19 pandemic, CCE has gained more foothold in clinical practice. In this cutting-edge review, we aim to present the existing knowledge on the pros and cons of CCE and discuss whether the modality is ready for a larger roll-out in clinical settings. We have included clinical trials and reviews with the most significant impact on the current position of CCE in clinical practice and discuss the challenges that persist and how they could be addressed to make CCE a more sustainable imaging modality with an adenoma detection rate equal to OC and a low re-investigation rate by a proper preselection of suitable populations. CCE is embedded with a very low risk of severe complications and can be performed in the patient's home as a pain-free procedure. The diagnostic accuracy is found to be equal to OC. However, a significant drawback is low completion rates eliciting a high re-investigation rate. Furthermore, the bowel preparation before CCE is extensive due to the high demand for clean mucosa. CCE is currently not suitable for large-scale implementation in clinical practice mainly due to high re-investigation rates. By a better preselection before CCE and the implantation of artificial intelligence for picture and video analysis, CCE could be the alternative to OC needed to move away from in-hospital services and relieve long-waiting lists for OC.", "journal": "Therapeutic advances in chronic disease", "date": "2022-11-29", "authors": ["ThomasBj\u00f8rsum-Meyer", "AnastasiosKoulaouzidis", "GunnarBaatrup"], "doi": "10.1177/20406223221137501"}
{"title": "Optimal Ensemble learning model for COVID-19 detection using chest X-ray images.", "abstract": "COVID-19 pandemic is the main outbreak in the world, which has shown a bad impact on people's lives in more than 150 countries. The major steps in fighting COVID-19 are identifying the affected patients as early as possible and locating them with special care. Images from radiology and radiography are among the most effective tools for determining a patient's ailment. Recent studies have shown detailed abnormalities of affected patients with COVID-19 in the chest radiograms. The purpose of this work is to present a COVID-19 detection system with three\u00a0key steps: \"(i) preprocessing, (ii) Feature extraction, (iii) Classification.\" Originally, the input image is given to the preprocessing step as its input, extracting the deep features and texture features from the preprocessed image. Particularly, it extracts the deep features by inceptionv3. Then, the features like proposed Local Vector Patterns (LVP) and Local Binary Pattern (LBP) are extracted from the preprocessed image. Moreover, the extracted features are subjected to the proposed ensemble model based classification phase, including Support Vector Machine (SVM), Convolutional Neural Network (CNN), Optimized Neural Network (NN), and Random Forest (RF). A novel Self Adaptive Kill Herd Optimization (SAKHO) approach is used to properly tune the weight of NN to improve classification accuracy and precision. The performance of the proposed method is then compared to the performance of the conventional approaches using a variety of metrics, including recall, FNR, MCC, FDR, Thread score, FPR, precision, FOR, accuracy, specificity, NPV, FMS, and sensitivity, accordingly.", "journal": "Biomedical signal processing and control", "date": "2022-11-29", "authors": ["SBalasubramaniam", "KSatheesh Kumar"], "doi": "10.1016/j.bspc.2022.104392\n10.1109/TBDATA.2020.3035935\n10.1109/ACCESS.2020.3033762\n10.3233/HIS-120161\n10.1504/IJCSE.2013.053087\n10.1007/s40747-020-00216-6\n10.1109/ICIP.2014.7025047"}
{"title": "Diagnostic performance of attenuated total reflection Fourier-transform infrared spectroscopy for detecting COVID-19 from routine nasopharyngeal swab samples.", "abstract": "Attenuated total reflection Fourier-transform infrared (ATR-FTIR) spectroscopy coupled with machine learning-based partial least squares discriminant analysis (PLS-DA) was applied to study if severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) could be detected from nasopharyngeal swab samples originally collected for polymerase chain reaction (PCR) analysis. Our retrospective study included 558 positive and 558 negative samples collected from Northern Finland. Overall, we found moderate diagnostic performance for ATR-FTIR when PCR analysis was used as the gold standard: the average area under the receiver operating characteristics curve (AUROC) was 0.67-0.68 (min. 0.65, max. 0.69) with 20, 10 and 5\u00a0k-fold cross validations. Mean accuracy, sensitivity and specificity was 0.62-0.63 (min. 0.60, max. 0.65), 0.61 (min. 0.58, max. 0.65) and 0.64 (min. 0.59, max. 0.67) with 20, 10 and 5\u00a0k-fold cross validations. As a conclusion, our study with relatively large sample set clearly indicate that measured ATR-FTIR spectrum contains specific information for SARS-CoV-2 infection (P\u2009<\u20090.001 for AUROC in label permutation test). However, the diagnostic performance of ATR-FTIR remained only moderate, potentially due to low concentration of viral particles in the transport medium. Further studies are needed before ATR-FTIR can be recommended for fast screening of SARS-CoV-2 from nasopharyngeal swab samples.", "journal": "Scientific reports", "date": "2022-11-28", "authors": ["Helin\u00e4Heino", "LassiRieppo", "TuijaM\u00e4nnist\u00f6", "Mikko JSillanp\u00e4\u00e4", "VesaM\u00e4ntynen", "SimoSaarakkala"], "doi": "10.1038/s41598-022-24751-z\n10.1016/j.jiph.2020.03.019\n10.1038/s41579-020-00459-7\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.3233/BSI-200203\n10.1038/nprot.2014.110\n10.1155/2020/4343590\n10.1039/C7RA03361C\n10.1016/j.clispe.2020.100001\n10.1021/acs.analchem.0c04049\n10.1021/acs.analchem.1c00596\n10.1021/acs.analchem.0c04608\n10.1002/anie.202104453\n10.1038/s41598-021-93511-2\n10.1021/pr101067u\n10.1186/s12859-019-3310-7\n10.1016/j.aca.2015.02.012\n10.1039/C8AN00599K\n10.1016/j.patrec.2005.10.010\n10.1038/scientificamerican1000-82\n10.1002/jmv.27204"}
{"title": "Deep Learning-Based Computer-Aided Diagnosis (CAD): Applications for Medical Image Datasets.", "abstract": "Computer-aided diagnosis (CAD) has proved to be an effective and accurate method for diagnostic prediction over the years. This article focuses on the development of an automated CAD system with the intent to perform diagnosis as accurately as possible. Deep learning methods have been able to produce impressive results on medical image datasets. This study employs deep learning methods in conjunction with meta-heuristic algorithms and supervised machine-learning algorithms to perform an accurate diagnosis. Pre-trained convolutional neural networks (CNNs) or auto-encoder are used for feature extraction, whereas feature selection is performed using an ant colony optimization (ACO) algorithm. Ant colony optimization helps to search for the best optimal features while reducing the amount of data. Lastly, diagnosis prediction (classification) is achieved using learnable classifiers. The novel framework for the extraction and selection of features is based on deep learning, auto-encoder, and ACO. The performance of the proposed approach is evaluated using two medical image datasets: chest X-ray (CXR) and magnetic resonance imaging (MRI) for the prediction of the existence of COVID-19 and brain tumors. Accuracy is used as the main measure to compare the performance of the proposed approach with existing state-of-the-art methods. The proposed system achieves an average accuracy of 99.61% and 99.18%, outperforming all other methods in diagnosing the presence of COVID-19 and brain tumors, respectively. Based on the achieved results, it can be claimed that physicians or radiologists can confidently utilize the proposed approach for diagnosing COVID-19 patients and patients with specific brain tumors.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-11-27", "authors": ["Yezi AliKadhim", "Muhammad UmerKhan", "AlokMishra"], "doi": "10.3390/s22228999\n10.3389/fmed.2020.00027\n10.1097/00004424-196601000-00032\n10.1118/1.3013555\n10.3390/s22218326\n10.1148/83.6.1029\n10.1353/pbm.1992.0011\n10.1016/j.compmedimag.2007.02.002\n10.1002/mp.13764\n10.1007/s10489-020-02002-w\n10.1164/ajrccm.153.1.8542102\n10.12928/telkomnika.v15i4.3163\n10.1056/NEJM199105303242205\n10.1016/j.eswa.2020.113274\n10.1016/j.ijmedinf.2019.06.017\n10.1016/j.compbiomed.2019.103345\n10.1007/s10278-013-9600-0\n10.1080/21681163.2016.1138324\n10.22146/ijeis.34713\n10.1016/j.jocs.2018.12.003\n10.3390/e24070869\n10.3390/life12111709\n10.3390/s22155880\n10.1016/j.conbuildmat.2017.09.110\n10.1016/j.compeleceng.2018.07.042\n10.1186/s40537-019-0276-2\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2020.103795\n10.1186/s41256-020-00135-6\n10.1101/2020.08.31.20175828\n10.1371/journal.pone.0157112\n10.1109/ACCESS.2019.2912200\n10.1109/MGRS.2018.2853555\n10.1109/MCI.2006.329691\n10.1016/j.tcs.2005.05.020\n10.1016/j.engappai.2014.03.007\n10.1016/j.eswa.2006.04.010\n10.1016/j.eswa.2015.07.007\n10.1016/j.eswa.2014.04.019\n10.1371/journal.pone.0140381\n10.1016/j.bspc.2019.101678\n10.3390/ijerph17124204\n10.20944/preprints202003.0300.v1\n10.1016/j.chaos.2020.110210\n10.1016/j.patrec.2020.09.010\n10.1016/j.cor.2021.105359\n10.1016/j.ins.2017.12.047\n10.1023/B:ANOR.0000039523.95673.33\n10.1016/j.imu.2020.100330\n10.1155/2021/6621540\n10.3390/sym11020157\n10.1109/ACCESS.2021.3076756\n10.1109/ACCESS.2021.3051723"}
{"title": "Dual_Pachi: Attention-based dual path framework with intermediate second order-pooling for Covid-19 detection from chest X-ray images.", "abstract": "Numerous machine learning and image processing algorithms, most recently deep learning, allow the recognition and classification of COVID-19 disease in medical images. However, feature extraction, or the semantic gap between low-level visual information collected by imaging modalities and high-level semantics, is the fundamental shortcoming of these techniques. On the other hand, several techniques focused on the first-order feature extraction of the chest X-Ray thus making the employed models less accurate and robust. This study presents Dual_Pachi: Attention Based Dual Path Framework with Intermediate Second Order-Pooling for more accurate and robust Chest X-ray feature extraction for Covid-19 detection. Dual_Pachi consists of 4 main building Blocks; Block one converts the received chest X-Ray image to CIE LAB coordinates (L & AB channels which are separated at the first three layers of a modified Inception V3 Architecture.). Block two further exploit the global features extracted from block one via a global second-order pooling while block three focuses on the low-level visual information and the high-level semantics of Chest X-ray image features using a multi-head self-attention and an MLP Layer without sacrificing performance. Finally, the fourth block is the classification block where classification is done using fully connected layers and SoftMax activation. Dual_Pachi is designed and trained in an end-to-end manner. According to the results, Dual_Pachi outperforms traditional deep learning models and other state-of-the-art approaches described in the literature with an accuracy of 0.96656 (Data_A) and 0.97867 (Data_B) for the Dual_Pachi approach and an accuracy of 0.95987 (Data_A) and 0.968 (Data_B) for the Dual_Pachi without attention block model. A Grad-CAM-based visualization is also built to highlight where the applied attention mechanism is concentrated.", "journal": "Computers in biology and medicine", "date": "2022-11-25", "authors": ["Chiagoziem CUkwuoma", "ZhiguangQin", "Victor KAgbesi", "Bernard MCobbinah", "Sophyani BYussif", "Hassan SAbubakar", "Bona DLemessa"], "doi": "10.1016/j.compbiomed.2022.106324\n10.1142/S0218339020500096\n10.7150/ijbs.45053\n10.1016/j.jare.2020.03.005\n10.1016/j.cpcardiol.2020.100618\n10.1016/j.diii.2020.03.014\n10.1016/S1473-3099(20)30190-0\n10.1002/jmv.25721\n10.1002/jmv.25786\n10.1148/radiol.2020200642\n10.1016/j.jcct.2011.07.001\n10.1109/prai53619.2021.9551094\n10.1007/s10278-017-9983-4\n10.3390/s20113243\n10.1016/j.compbiomed.2020.103795\n10.1007/s10278-019-00227-x\n10.1007/s13246-020-00888-x\n10.1093/cid/ciaa1383\n10.1016/j.compbiomed.2020.103869\n10.1016/j.bspc.2021.102696\n10.1016/j.eswa.2021.114576\n10.3390/diagnostics12051152\n10.1016/j.sciaf.2022.e01151\n10.1016/j.patcog.2020.107613\n10.1016/j.compbiomed.2020.103792\n10.1016/j.pdpdt.2021.102473\n10.1038/s41598-020-76550-z\n10.1016/j.patrec.2020.09.010\n10.1007/s00330-021-07715-1\n10.1007/s10044-021-00984-y\n10.1016/j.media.2020.101794\n10.1007/s10489-020-01902-1\n10.1007/s13246-020-00865-4\n10.1109/SSCI47803.2020.9308571\n10.1016/j.chaos.2020.109944\n10.33889/IJMEMS.2020.5.4.052\n10.1109/ICCC51575.2020.9344870\n10.1007/s42600-021-00151-6\n10.1016/j.chaos.2020.110495\n10.1016/j.compbiomed.2020.104181\n10.1007/s10916-021-01745-4\n10.1016/j.compbiomed.2021.104816\n10.1016/j.knosys.2022.108207\n10.3233/idt-210002\n10.1007/s12065-021-00679-7\n10.1016/j.eswa.2020.114054\n10.1007/s00354-021-00152-0\n10.34133/2019/9237136\n10.1109/ACCESS.2020.3010287\n10.3390/covid1010034\n10.1016/j.cmpb.2020.105581\n10.1109/JBHI.2021.3058293\n10.1016/j.asoc.2022.108867\n10.1109/JBHI.2021.3074893\n10.3390/s22031211\n10.3390/ijerph182111086\n10.1007/s00521-019-04332-4"}
{"title": "Automated Lung-Related Pneumonia and COVID-19 Detection Based on Novel Feature Extraction Framework and Vision Transformer Approaches Using Chest X-ray Images.", "abstract": "According to research, classifiers and detectors are less accurate when images are blurry, have low contrast, or have other flaws which raise questions about the machine learning model's ability to recognize items effectively. The chest X-ray image has proven to be the preferred image modality for medical imaging as it contains more information about a patient. Its interpretation is quite difficult, nevertheless. The goal of this research is to construct a reliable deep-learning model capable of producing high classification accuracy on chest x-ray images for lung diseases. To enable a thorough study of the chest X-ray image, the suggested framework first derived richer features using an ensemble technique, then a global second-order pooling is applied to further derive higher global features of the images. Furthermore, the images are then separated into patches and position embedding before analyzing the patches individually via a vision transformer approach. The proposed model yielded 96.01% sensitivity, 96.20% precision, and 98.00% accuracy for the COVID-19 Radiography Dataset while achieving 97.84% accuracy, 96.76% sensitivity and 96.80% precision, for the Covid-ChestX-ray-15k dataset. The experimental findings reveal that the presented models outperform traditional deep learning models and other state-of-the-art approaches provided in the literature.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2022-11-25", "authors": ["Chiagoziem CUkwuoma", "ZhiguangQin", "Md Belal BinHeyat", "FaijanAkhtar", "AblaSmahi", "Jehoiada KJackson", "SyedFurqan Qadri", "Abdullah YMuaad", "Happy NMonday", "Grace UNneji"], "doi": "10.3390/bioengineering9110709\n10.3390/bioengineering9070305\n10.1007/s42399-020-00527-2\n10.1007/s00415-020-10067-3\n10.1007/s42399-020-00383-0\n10.1155/2022/9210947\n10.1109/ACCESS.2022.3194152\n10.1155/2022/5641727\n10.1016/j.jare.2022.08.021\n10.1186/s12890-020-01286-5\n10.1007/s11042-019-08394-3\n10.3390/bioengineering9040172\n10.1155/2022/5718501\n10.1109/ACCESS.2019.2928020\n10.3390/bios12060427\n10.1155/2022/3599246\n10.2174/1871527319666201110124954\n10.3390/app10217410\n10.2174/1389450121666201027125828\n10.1145/3465055\n10.32604/cmc.2021.014134\n10.3390/diagnostics10090649\n10.1155/2019/4180949\n10.1007/s10916-021-01745-4\n10.1016/j.compeleceng.2019.08.004\n10.1016/j.cmpb.2019.06.023\n10.3390/app10020559\n10.1016/j.measurement.2020.108046\n10.1016/j.bspc.2020.102365\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2021.110713\n10.1016/j.compbiomed.2021.104375\n10.1016/j.chaos.2021.110749\n10.1016/j.patcog.2021.108255\n10.1007/s10044-021-00984-y\n10.1038/s41598-020-76550-z\n10.1007/s11063-022-10834-5\n10.1109/ACCESS.2020.3010287\n10.3390/covid1010034\n10.1155/2022/9475162\n10.3389/fnins.2021.754058\n10.1109/ACCESS.2022.3212120\n10.1155/2022/3408501\n10.3390/app12031344\n10.31083/j.jin2101020\n10.1016/b978-0-323-99031-8.00012-0\n10.3390/diagnostics12112815\n10.1016/j.cmpb.2020.105581\n10.1109/JBHI.2021.3058293\n10.1016/j.asoc.2022.108867\n10.1109/JBHI.2021.3074893\n10.3390/s22031211\n10.18280/ts.380337\n10.18201/ijisae.2020466310\n10.3233/XST-211005\n10.1016/j.cell.2018.02.010\n10.1007/s12559-020-09787-5\n10.1016/j.chaos.2020.109944"}
{"title": "Portable, Automated and Deep-Learning-Enabled Microscopy for Smartphone-Tethered Optical Platform Towards Remote Homecare Diagnostics: A Review.", "abstract": "Globally new pandemic diseases induce urgent demands for portable diagnostic systems to prevent and control infectious diseases. Smartphone-based portable diagnostic devices are significantly efficient tools to user-friendly connect personalized health conditions and collect valuable optical information for rapid diagnosis and biomedical research through at-home screening. Deep learning algorithms for portable microscopes also help to enhance diagnostic accuracy by reducing the imaging resolution gap between benchtop and portable microscopes. This review highlighted recent progress and continued efforts in a smartphone-tethered optical platform through portable, automated, and deep-learning-enabled microscopy for personalized diagnostics and remote monitoring. In detail, the optical platforms through smartphone-based microscopes and lens-free holographic microscopy are introduced, and deep learning-based portable microscopic imaging is explained to improve the image resolution and accuracy of diagnostics. The challenges and prospects of portable optical systems with microfluidic channels and a compact microscope to screen COVID-19 in the current pandemic are also discussed. It has been believed that this review offers a novel guide for rapid diagnosis, biomedical imaging, and digital healthcare with low cost and portability.", "journal": "Small methods", "date": "2022-11-25", "authors": ["KisooKim", "Won GuLee"], "doi": "10.1002/smtd.202200979"}
{"title": "A systematic review: Chest radiography images (X-ray images) analysis and COVID-19 categorization diagnosis using artificial intelligence techniques.", "abstract": "COVID-19 pandemic created a turmoil across nations due to Severe Acute Respiratory Syndrome Corona virus-1(SARS - Co-V-2). The severity of COVID-19 symptoms is starting from cold, breathing problems, issues in respiratory system which may also lead to life threatening situations. This disease is widely contaminating and transmitted from man-to-man. The contamination is spreading when the human organs like eyes, nose, and mouth get in contact with contaminated fluids. This virus can be screened through performing a nasopharyngeal swab test which is time consuming. So the physicians are preferring the fast detection methods like chest radiography images and CT scans. At times some confusion in finding out the accurate disorder from chest radiography images can happen. To overcome this issue this study reviews several deep learning and machine learning procedures to be implemented in X-ray images of chest. This also helps the professionals to find out the other types of malfunctions happening in the chest other than COVID-19 also. This review can act as a guidance to the doctors and radiologists in identifying the COVID-19 and other types of viruses causing illness in the human anatomy and can provide aid soon.", "journal": "Network (Bristol, England)", "date": "2022-11-25", "authors": ["SaravananSuba", "MMuthulakshmi"], "doi": "10.1080/0954898X.2022.2147231"}
{"title": "Deep progressive learning achieves whole-body low-dose ", "abstract": "To validate a total-body PET-guided deep progressive learning reconstruction method (DPR) for low-dose \nList-mode data from the retrospective study (n\u2009=\u200926) were rebinned into short-duration scans and reconstructed with DPR. The standard uptake value (SUV) and tumor-to-liver ratio (TLR) in lesions and coefficient of variation (COV) in the liver in the DPR images were compared to the reference (OSEM images with full-duration data). In the prospective study, another 41 patients were injected with 1/3 of the activity based on the retrospective results. The DPR images (DPR_1/3(p)) were generated and compared with the reference (OSEM images with extended acquisition time). The SUV and COV were evaluated in three selected organs: liver, blood pool and muscle. Quantitative analyses were performed with lesion SUV and TLR, furthermore on small lesions (\u2264\u200910\u00a0mm in diameter). Additionally, a 5-point Likert scale visual analysis was performed on the following perspectives: contrast, noise and diagnostic confidence.\nIn the retrospective study, the DPR with one-third duration can maintain the image quality as the reference. In the prospective study, good agreement among the SUVs was observed in all selected organs. The quantitative results showed that there was no significant difference in COV between the DPR_1/3(p) group and the reference, while the visual analysis showed no significant differences in image contrast, noise and diagnostic confidence. The lesion SUVs and TLRs in the DPR_1/3(p) group were significantly enhanced compared with the reference, even for small lesions.\nThe proposed DPR method can reduce the administered activity of ", "journal": "EJNMMI physics", "date": "2022-11-23", "authors": ["TaisongWang", "WenliQiao", "YingWang", "JingyiWang", "YangLv", "YunDong", "ZhengQian", "YanXing", "JinhuaZhao"], "doi": "10.1186/s40658-022-00508-5\n10.1007/s00259-014-2961-x\n10.1016/S0377-1237(09)80099-3\n10.2967/jnumed.107.047787\n10.1007/s12350-016-0522-3\n10.1016/j.nuclcard.2007.04.006\n10.1136/jnnp.2003.028175\n10.2967/jnumed.117.200790\n10.1097/RLU.0000000000003075\n10.1007/s00259-020-05167-1\n10.1186/s13550-020-00695-1\n10.1007/s00259-021-05197-3.10.1007/s00259-021-05197-3\n10.1007/s00259-021-05478-x\n10.1109/TRPMS.2020.3014786\n10.1088/1361-6560/abfb17\n10.1007/s00247-006-0191-5\n10.1007/s00247-009-1404-5\n10.1259/bjr/01948454\n10.1007/s00259-020-05091-4\n10.1007/s00259-021-05304-4\n10.1007/s00259-021-05462-5\n10.1007/s00259-021-05537-3\n10.2967/jnumed.121.262038\n10.1007/s00259-021-05592-w\n10.1259/bjr.20201356\n10.1186/s13550-019-0536-3\n10.1007/s00259-017-3893-z\n10.1186/s13550-019-0565-y"}
{"title": "COVID-19 classification using chest X-ray images based on fusion-assisted deep Bayesian optimization and Grad-CAM visualization.", "abstract": "The COVID-19 virus's rapid global spread has caused millions of illnesses and deaths. As a result, it has disastrous consequences for people's lives, public health, and the global economy. Clinical studies have revealed a link between the severity of COVID-19 cases and the amount of virus present in infected people's lungs. Imaging techniques such as computed tomography (CT) and chest x-rays can detect COVID-19 (CXR). Manual inspection of these images is a difficult process, so computerized techniques are widely used. Deep convolutional neural networks (DCNNs) are a type of machine learning that is frequently used in computer vision applications, particularly in medical imaging, to detect and classify infected regions. These techniques can assist medical personnel in the detection of patients with COVID-19. In this article, a Bayesian optimized DCNN and explainable AI-based framework is proposed for the classification of COVID-19 from the chest X-ray images. The proposed method starts with a multi-filter contrast enhancement technique that increases the visibility of the infected part. Two pre-trained deep models, namely, EfficientNet-B0 and MobileNet-V2, are fine-tuned according to the target classes and then trained by employing Bayesian optimization (BO). Through BO, hyperparameters have been selected instead of static initialization. Features are extracted from the trained model and fused using a slicing-based serial fusion approach. The fused features are classified using machine learning classifiers for the final classification. Moreover, visualization is performed using a Grad-CAM that highlights the infected part in the image. Three publically available COVID-19 datasets are used for the experimental process to obtain improved accuracies of 98.8, 97.9, and 99.4%, respectively.", "journal": "Frontiers in public health", "date": "2022-11-22", "authors": ["AmeerHamza", "MuhammadAttique Khan", "Shui-HuaWang", "MajedAlhaisoni", "MeshalAlharbi", "Hany SHussein", "HammamAlshazly", "Ye JinKim", "JaehyukCha"], "doi": "10.3389/fpubh.2022.1046296\n10.3390/s21020455\n10.32604/cmc.2022.020140\n10.1111/exsy.12776\n10.1016/j.compbiomed.2022.105233\n10.1155/2022/7672196\n10.3389/fcomp.2020.00005\n10.1016/j.bbi.2020.04.081\n10.1148/radiol.2020200230\n10.7717/peerj-cs.655\n10.1109/TMI.2020.3040950\n10.1109/TMI.2020.2993291\n10.1109/TMI.2016.2528162\n10.1155/2022/7377502\n10.1016/j.media.2017.07.005\n10.1038/nature14539\n10.1148/radiol.2017162326\n10.1007/s11263-015-0816-y\n10.48550/arXiv.1602.07360\n10.3389/fpubh.2022.948205\n10.1155/2021/2560388\n10.1016/j.compbiomed.2022.105213\n10.3389/fmed.2020.00427\n10.1371/journal.pone.0242535\n10.1007/s13755-020-00119-3\n10.3390/info11090419\n10.1155/2020/8828855\n10.1155/2022/4254631\n10.1007/s00530-021-00826-1\n10.1007/s42600-020-00120-5\n10.1155/2022/1307944\n10.3390/jpm12020309\n10.3390/s21217286\n10.1016/j.ecoinf.2020.101182\n10.1007/s12530-020-09345-2\n10.1016/j.chaos.2020.110511\n10.1007/s00521-022-07052-4\n10.1016/j.asoc.2020.106580\n10.1016/j.compbiomed.2022.105244"}
{"title": "Efficient-ECGNet framework for COVID-19 classification and correlation prediction with the cardio disease through electrocardiogram medical imaging.", "abstract": "In the last 2 years, we have witnessed multiple waves of coronavirus that affected millions of people around the globe. The proper cure for COVID-19 has not been diagnosed as vaccinated people also got infected with this disease. Precise and timely detection of COVID-19 can save human lives and protect them from complicated treatment procedures. Researchers have employed several medical imaging modalities like CT-Scan and X-ray for COVID-19 detection, however, little concentration is invested in the ECG imaging analysis. ECGs are quickly available image modality in comparison to CT-Scan and X-ray, therefore, we use them for diagnosing COVID-19. Efficient and effective detection of COVID-19 from the ECG signal is a complex and time-taking task, as researchers usually convert them into numeric values before applying any method which ultimately increases the computational burden. In this work, we tried to overcome these challenges by directly employing the ECG images in a deep-learning (DL)-based approach. More specifically, we introduce an Efficient-ECGNet method that presents an improved version of the EfficientNetV2-B4 model with additional dense layers and is capable of accurately classifying the ECG images into healthy, COVID-19, myocardial infarction (MI), abnormal heartbeats (AHB), and patients with Previous History of Myocardial Infarction (PMI) classes. Moreover, we introduce a module to measure the similarity of COVID-19-affected ECG images with the rest of the diseases. To the best of our knowledge, this is the first effort to approximate the correlation of COVID-19 patients with those having any previous or current history of cardio or respiratory disease. Further, we generate the heatmaps to demonstrate the accurate key-points computation ability of our method. We have performed extensive experimentation on a publicly available dataset to show the robustness of the proposed approach and confirmed that the Efficient-ECGNet framework is reliable to classify the ECG-based COVID-19 samples.", "journal": "Frontiers in medicine", "date": "2022-11-22", "authors": ["MarriamNawaz", "TahiraNazir", "AliJaved", "Khalid MahmoodMalik", "Abdul Khader JilaniSaudagar", "Muhammad BadruddinKhan", "Mozaherul HoqueAbul Hasanat", "AbdullahAlTameem", "MohammedAlKhathami"], "doi": "10.3389/fmed.2022.1005920\n10.1016/j.ejim.2020.06.015\n10.1007/s15010-020-01401-y\n10.1016/j.jinf.2020.02.016\n10.1007/s13755-021-00169-1\n10.7717/peerj-cs.386\n10.1016/j.compeleceng.2020.106960\n10.1109/ICAI52203.2021.9445258\n10.1016/j.asoc.2020.106691\n10.1016/j.asoc.2021.107323\n10.1016/j.asoc.2021.107160\n10.1155/2021/9619079\n10.1155/2022/1575303\n10.1016/j.compbiomed.2021.104575\n10.1016/j.chaos.2020.110190\n10.1016/j.bspc.2021.102588\n10.1016/j.irbm.2021.01.004\n10.1002/jemt.23578\n10.1016/j.imu.2020.100360\n10.1007/s10489-020-01826-w\n10.1007/s00521-021-05910-1\n10.1007/s10489-020-01902-1\n10.1007/s10489-020-01943-6\n10.1007/s11356-020-10133-3\n10.21203/rs.3.rs-646890/v1\n10.1007/s00521-020-05410-8\n10.1186/s12911-021-01521-x\n10.1109/JIOT.2021.3051080\n10.1016/j.dib.2021.106762\n10.1109/CVPR.2015.7298594\n10.1016/j.inpa.2020.04.004\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.243\n10.1109/CVPR.2018.00474\n10.1109/CVPR.2018.00745\n10.1109/LSP.2016.2573042\n10.1109/TPAMI.2005.165\n10.1049/cit2.12101\n10.1007/s13369-021-06182-6\n10.1007/s42979-020-0114-9\n10.1109/ICCASIT50869.2020.9368658\n10.1145/3341095"}
{"title": "Developing medical imaging AI for emerging infectious diseases.", "abstract": "Advances in artificial intelligence (AI) and computer vision hold great promise for assisting medical staff, optimizing healthcare workflow, and improving patient outcomes. The COVID-19 pandemic, which caused unprecedented stress on healthcare systems around the world, presented what seems to be a perfect opportunity for AI to demonstrate its usefulness. However, of the several hundred medical imaging AI models developed for COVID-19, very few were fit for deployment in real-world settings, and some were potentially harmful. This review aims to examine the strengths and weaknesses of prior studies and provide recommendations for different stages of building useful AI models for medical imaging, among them: needfinding, dataset curation, model development and evaluation, and post-deployment considerations. In addition, this review summarizes the lessons learned to inform the scientific community about ways to create useful medical imaging AI\u00a0in a future pandemic.\nVery few of the COVID-19 ML models were fit for deployment in real-world settings. In this Comment, Huang et al. discuss the main steps required to develop clinically useful models in the context of an emerging infectious disease.", "journal": "Nature communications", "date": "2022-11-19", "authors": ["Shih-ChengHuang", "Akshay SChaudhari", "Curtis PLanglotz", "NigamShah", "SerenaYeung", "Matthew PLungren"], "doi": "10.1038/s41467-022-34234-4\n10.1038/s42256-021-00307-0\n10.2214/AJR.21.26717\n10.1136/bmj.m1328\n10.1016/j.patter.2021.100269\n10.2196/19786\n10.1038/s41591-020-0931-3\n10.1007/s00330-021-07937-3\n10.1007/s00330-020-06829-2\n10.1109/JBHI.2021.3069169\n10.1148/ryai.2020200079\n10.1148/radiol.2020202723\n10.3348/kjr.2020.0485\n10.7150/ijms.48432\n10.1148/radiol.2020201365\n10.1001/jama.2019.10306\n10.1016/j.media.2021.102225\n10.1038/s42256-021-00338-7\n10.1016/j.media.2021.102046\n10.1148/radiol.2020204214\n10.1001/amajethics.2019.167\n10.1126/science.aax2342\n10.1136/bmj.i6\n10.1016/j.ejim.2010.12.012\n10.1016/j.jbi.2021.103826\n10.1259/bjr.73.874.11271897\n10.1016/j.jacr.2007.02.003\n10.1371/journal.pone.0250602\n10.1038/s41598-020-78888-w\n10.1038/s41746-020-00341-z\n10.1038/s41746-022-00613-w\n10.1148/radiol.2020192536\n10.1093/jamia/ocy017\n10.1038/s42256-019-0048-x\n10.1016/j.jacr.2019.05.036\n10.1016/j.acra.2020.12.026\n10.1016/j.jacr.2021.08.022\n10.1056/NEJMc2104626"}
{"title": "Blockchain and homomorphic encryption based privacy-preserving model aggregation for medical images.", "abstract": "Medical healthcare centers are envisioned as a promising paradigm to handle the massive volume of data for COVID-19 patients using artificial intelligence (AI). Traditionally, AI techniques require centralized data collection and training models within a single organization. This practice can be considered a weakness as it leads to several privacy and security concerns related to raw data communication. To overcome this weakness and secure raw data communication, we propose a blockchain-based federated learning framework that provides a solution for collaborative data training. The proposed framework enables the coordination of multiple hospitals to train and share encrypted federated models while preserving data privacy. Blockchain ledger technology provides decentralization of federated learning models without relying on a central server. Moreover, the proposed homomorphic encryption scheme encrypts and decrypts the gradients of the model to preserve privacy. More precisely, the proposed framework: (i) train the local model by a novel capsule network for segmentation and classification of COVID-19 images, (ii) furthermore, we use the homomorphic encryption scheme to secure the local model that encrypts and decrypts the gradients, (iii) finally, the model is shared over a decentralized platform through the proposed blockchain-based federated learning algorithm. The integration of blockchain and federated learning leads to a new paradigm for medical image data sharing over the decentralized network. To validate our proposed model, we conducted comprehensive experiments and the results demonstrate the superior performance of the proposed scheme.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2022-11-18", "authors": ["RajeshKumar", "JayKumar", "Abdullah AmanKhan", "NoneZakria", "HubAli", "Cobbinah MBernard", "Riaz UllahKhan", "ShaoningZeng"], "doi": "10.1016/j.compmedimag.2022.102139\n10.1109/TII.2019.2942190"}
{"title": "COVID-19 Data Analytics Using Extended Convolutional Technique.", "abstract": "The healthcare system, lifestyle, industrial growth, economy, and livelihood of human beings worldwide were affected due to the triggered global pandemic by the COVID-19 virus that originated and was first reported in Wuhan city, Republic Country of China. COVID cases are difficult to predict and detect in their early stages, and their spread and mortality are uncontrollable. The reverse transcription polymerase chain reaction (RT-PCR) is still the first and foremost diagnostical methodology accepted worldwide; hence, it creates a scope of new diagnostic tools and techniques of detection approach which can produce effective and faster results compared with its predecessor. Innovational through current studies that complement the existence of the novel coronavirus (COVID-19) to findings in the thorax (chest) X-ray imaging, the projected research's method makes use of present deep learning (DL) models with the integration of various frameworks such as GoogleNet, U-Net, and ResNet50 to novel method those X-ray images and categorize patients as the corona positive (COVID\u2009+\u2009ve) or the corona negative (COVID -ve). The anticipated technique entails the pretreatment phase through dissection of the lung, getting rid of the environment which does now no longer provide applicable facts and can provide influenced consequences; then after this, the preliminary degree comes up with the category version educated below the switch mastering system; and in conclusion, consequences are evaluated and interpreted through warmth maps visualization. The proposed research method completed a detection accuracy of COVID-19 at around 99%.", "journal": "Interdisciplinary perspectives on infectious diseases", "date": "2022-11-18", "authors": ["Anand KumarGupta", "AsadiSrinivasulu", "Olutayo OyeyemiOyerinde", "GiovanniPau", "C VRavikumar"], "doi": "10.1155/2022/4578838\n10.1016/j.mlwa.2021.100138\n10.1155/2021/6621607\n10.1371/journal.pone.0262052\n10.1148/radiol.2020200642\n10.23750/abm.v91i1.9397\n10.1109/access.2020.2997311\n10.1056/NEJMoa2002032\n10.1155/2022/4838009\n10.1001/jama.2020.3786\n10.21227/4kcm-m312\n10.3390/ijerph18063056\n10.1007/s13755-021-00152-w\n10.1007/s13755-021-00158-4\n10.3390/healthcare9050522"}
{"title": "Detection of COVID-19: A Smartphone-Based Machine-Learning-Assisted ECL Immunoassay Approach with the Ability of RT-PCR CT Value Prediction.", "abstract": "The unstoppable spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has severely threatened public health over the past 2 years. The current ubiquitously accepted method for its diagnosis provides sensitive detection of the virus; however, it is relatively time-consuming and costly, not to mention the need for highly skilled personnel. There is a clear need to develop novel computer-based diagnostic tools to provide rapid, cost-efficient, and time-saving detection in places where massive traditional testing is not practical. Here, we develop an electrochemiluminescence (ECL)-based detection system whose results are quantified as reverse transcriptase polymerase chain reaction (RT-PCR) cyclic threshold (CT) values. A concentration-dependent signal is generated upon the introduction of the virus to the electrode and is recorded with a smartphone camera. The ECL images are used to train machine learning algorithms, and a model using artificial neural networks (ANNs) for 45 samples was developed. The model demonstrated more than 90% accuracy in the diagnosis of 50 unknown real samples, detecting up to a CT value of 32 and a limit of detection (LOD) of 10", "journal": "Analytical chemistry", "date": "2022-11-17", "authors": ["AliFiroozbakhtian", "MortezaHosseini", "Mahsa NaghaviSheikholeslami", "FoadSalehnia", "GuobaoXu", "HodjattallahRabbani", "EbtesamSobhanie"], "doi": "10.1021/acs.analchem.2c03502"}
{"title": "Calibrated bagging deep learning for image semantic segmentation: A case study on COVID-19 chest X-ray image.", "abstract": "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes coronavirus disease 2019 (COVID-19). Imaging tests such as chest X-ray (CXR) and computed tomography (CT) can provide useful information to clinical staff for facilitating a diagnosis of COVID-19 in a more efficient and comprehensive manner. As a breakthrough of artificial intelligence (AI), deep learning has been applied to perform COVID-19 infection region segmentation and disease classification by analyzing CXR and CT data. However, prediction uncertainty of deep learning models for these tasks, which is very important to safety-critical applications like medical image processing, has not been comprehensively investigated. In this work, we propose a novel ensemble deep learning model through integrating bagging deep learning and model calibration to not only enhance segmentation performance, but also reduce prediction uncertainty. The proposed method has been validated on a large dataset that is associated with CXR image segmentation. Experimental results demonstrate that the proposed method can improve the segmentation performance, as well as decrease prediction uncertainty.", "journal": "PloS one", "date": "2022-11-17", "authors": ["LucyNwosu", "XiangfangLi", "LijunQian", "SeungchanKim", "XishuangDong"], "doi": "10.1371/journal.pone.0276250\n10.1002/ima.22469\n10.1145/3411760\n10.33889/IJMEMS.2020.5.4.052\n10.1007/s13246-020-00865-4\n10.1007/s10044-021-00984-y\n10.3390/ijerph18063056\n10.1097/RTI.0000000000000532\n10.3390/s21217116\n10.1016/j.compbiomed.2021.104984\n10.1016/j.bspc.2021.103182\n10.1175/1520-0450(1967)006<0748:VOPPAB>2.0.CO;2\n10.1177/0962280213497434\n10.1136/amiajnl-2011-000291\n10.1007/BF00058655\n10.1186/s12880-020-00529-5\n10.1148/ryct.2020200082\n10.1038/nature21056"}
{"title": "Cardiovascular CT, MRI, and PET/CT in 2021: Review of Key Articles.", "abstract": "This review focuses on three key noninvasive cardiac imaging modalities-cardiac CT angiography (CTA), MRI, and PET/CT-and summarizes key publications in 2021 relevant to radiologists in clinical practice. Although this review focuses primarily on articles published in ", "journal": "Radiology", "date": "2022-11-16", "authors": ["GeorgiosTzimas", "David TRyan", "David JMurphy", "Jonathon ALeipsic", "Jonathan DDodd"], "doi": "10.1148/radiol.221181"}
{"title": "CNN Features and Optimized Generative Adversarial Network for COVID-19 Detection from Chest X-Ray Images.", "abstract": "Coronavirus is a RNA type virus, which makes various respiratory infections in both human as well as animals. In addition, it could cause pneumonia in humans. The Coronavirus affected patients has been increasing day to day, due to the wide spread of diseases. As the count of corona affected patients increases, most of the regions are facing the issue of test kit shortage. In order to resolve this issue, the deep learning approach provides a better solution for automatically detecting the COVID-19 disease. In this research, an optimized deep learning approach, named Henry gas water wave optimization-based deep generative adversarial network (HGWWO-Deep GAN) is developed. Here, the HGWWO algorithm is designed by the hybridization of Henry gas solubility optimization (HGSO) and water wave optimization (WWO) algorithm. The pre-processing method is carried out using region of interest (RoI) and median filtering in order to remove the noise from the images. Lung lobe segmentation is carried out using U-net architecture and lung region extraction is done using convolutional neural network (CNN) features. Moreover, the COVID-19 detection is done using Deep GAN trained by the HGWWO algorithm. The experimental result demonstrates that the developed model attained the optimal performance based on the testing accuracy of 0.9169, sensitivity of 0.9328, and specificity of 0.9032.", "journal": "Critical reviews in biomedical engineering", "date": "2022-11-15", "authors": ["GotlurKalpana", "A KanakaDurga", "GKaruna"], "doi": "10.1615/CritRevBiomedEng.2022042286"}
{"title": "A survey on deep learning applied to medical images: from simple artificial neural networks to generative models.", "abstract": "Deep learning techniques, in particular generative models, have taken on great importance in medical image analysis. This paper surveys fundamental deep learning concepts related to medical image generation. It provides concise overviews of studies which use some of the latest state-of-the-art models from last years applied to medical images of different injured body areas or organs that have a disease associated with (e.g., brain tumor and COVID-19 lungs pneumonia). The motivation for this study is to offer a comprehensive overview of artificial neural networks (NNs) and deep generative models in medical imaging, so more groups and authors that are not familiar with deep learning take into consideration its use in medicine works. We review the use of generative models, such as generative adversarial networks and variational autoencoders, as techniques to achieve semantic segmentation, data augmentation, and better classification algorithms, among other purposes. In addition, a collection of widely used public medical datasets containing magnetic resonance (MR) images, computed tomography (CT) scans, and common pictures is presented. Finally, we feature a summary of the current state of generative models in medical image including key features, current challenges, and future research paths.", "journal": "Neural computing & applications", "date": "2022-11-15", "authors": ["PCelard", "E LIglesias", "J MSorribes-Fdez", "RRomero", "A SearaVieira", "LBorrajo"], "doi": "10.1007/s00521-022-07953-4\n10.1016/j.artmed.2021.102164\n10.1016/j.artmed.2021.102165\n10.1145/3464423\n10.1145/3465398\n10.1007/s00521-022-07099-3\n10.1007/s00521-022-06960-9\n10.1016/j.artmed.2020.101938\n10.1016/j.neunet.2014.09.003\n10.1016/j.compmedimag.2019.04.005\n10.1245/ASO.2004.04.018\n10.3109/02841851.2010.498444\n10.7314/APJCP.2012.13.3.927\n10.1136/bjo.80.11.940\n10.1136/bjo.83.8.902\n10.1016/j.compbiomed.2005.01.006\n10.1109/10.959322\n10.1016/j.compbiomed.2021.104319\n10.1038/nature14539\n10.1016/0730-725X(93)90417-C\n10.1093/clinchem/48.10.1828\n10.1245/ASO.2004.03.007\n10.1179/016164104773026534\n10.1145/3065386\n10.1007/s00521-022-06953-8\n10.1007/BF00344251\n10.1109/42.476112\n10.1016/j.media.2017.07.005\n10.1007/978-3-319-46448-0_2\n10.1016/j.ejca.2019.04.001\n10.1371/journal.pmed.1002730\n10.1016/j.cmpb.2020.105532\n10.1016/j.media.2018.10.006\n10.1016/j.media.2019.01.012\n10.1016/j.media.2019.101557\n10.1016/j.bspc.2021.102901\n10.1007/s10278-019-00227-x\n10.1016/j.media.2020.101884\n10.1016/j.neucom.2019.02.003\n10.1109/TPAMI.2021.3059968\n10.1007/s10462-020-09854-1\n10.1016/j.compeleceng.2021.107036\n10.3390/diagnostics11020169\n10.1016/j.compbiomed.2021.104699\n10.3389/fgene.2021.639930\n10.1038/s41592-020-01008-z\n10.1155/2021/6625688\n10.1109/TMI.2020.2995508\n10.1016/j.neunet.2021.03.006\n10.1109/JBHI.2020.2986926\n10.1109/ACCESS.2019.2899108\n10.1016/j.bspc.2019.101678\n10.1016/j.bspc.2019.101641\n10.1002/mp.13927\n10.1002/mp.14006\n10.1016/j.remnie.2016.07.002\n10.1007/s00259-020-04816-9\n10.1186/s13195-021-00797-5\n10.1016/j.media.2020.101716\n10.1016/j.cmpb.2020.105568\n10.1007/s12539-020-00403-6\n10.2174/1573405616666200604163954\n10.1002/mp.15044\n10.1016/j.cmpb.2021.106018\n10.1561/2200000056\n10.1007/s11548-018-1898-0\n10.1016/j.ejmp.2021.02.013\n10.1016/j.artmed.2020.102006\n10.1007/s10278-020-00413-2\n10.1016/j.media.2020.101952\n10.1212/WNL.0b013e3181cb3e25\n10.1007/s10916-019-1475-2\n10.1016/j.compbiomed.2020.103764\n10.1371/journal.pone.0140381\n10.1016/j.compbiomed.2019.103345\n10.1109/TMI.2014.2377694\n10.1109/TMI.2018.2867350\n10.1016/j.compbiomed.2020.103774\n10.1016/j.acra.2011.09.014\n10.12913/22998624/137964\n10.1016/j.cell.2018.02.010\n10.1016/j.cmpb.2020.105581\n10.1016/j.media.2020.101794\n10.1118/1.3528204\n10.1038/s41597-021-00815-z\n10.3758/BRM.42.1.351\n10.1007/s13246-020-00865-4\n10.1016/j.artmed.2020.101880\n10.1016/j.media.2021.102327\n10.1109/TMI.2022.3147426\n10.1016/j.media.2022.102479\n10.1109/ACCESS.2022.3172975\n10.1016/j.dib.2022.108258"}
{"title": "SARS-CoV-2 Morphometry Analysis and Prediction of Real Virus Levels Based on Full Recurrent Neural Network Using TEM Images.", "abstract": "The SARS-CoV-2 virus is responsible for the rapid global spread of the COVID-19 disease. As a result, it is critical to understand and collect primary data on the virus, infection epidemiology, and treatment. Despite the speed with which the virus was detected, studies of its cell biology and architecture at the ultrastructural level are still in their infancy. Therefore, we investigated and analyzed the viral morphometry of SARS-CoV-2 to extract important key points of the virus's characteristics. Then, we proposed a prediction model to identify the real virus levels based on the optimization of a full recurrent neural network (RNN) using transmission electron microscopy (TEM) images. Consequently, identification of virus levels depends on the size of the morphometry of the area (width, height, circularity, roundness, aspect ratio, and solidity). The results of our model were an error score of training network performance 3.216 \u00d7 10", "journal": "Viruses", "date": "2022-11-12", "authors": ["Bakr AhmedTaha", "Yousif AlMashhadany", "Abdulmajeed H JAl-Jumaily", "Mohd Saiful Dzulkefly BinZan", "NorhanaArsad"], "doi": "10.3390/v14112386\n10.3390/v11010059\n10.3390/pathogens9030240\n10.1038/s41577-021-00578-z\n10.56770/jcp2021525\n10.7326/M20-1301\n10.1007/s11356-022-18849-0\n10.3390/diagnostics11061119\n10.3390/s21248362\n10.1126/scitranslmed.abc1931\n10.1007/s11801-013-3017-3\n10.1016/j.jcv.2020.104412\n10.1002/jmv.25721\n10.1021/acscentsci.0c00501\n10.1007/s11606-020-05762-w\n10.1007/s00253-022-11930-1\n10.3389/fgene.2021.569120\n10.1016/j.compbiomed.2020.103792\n10.3934/mbe.2021440\n10.3233/XST-200715\n10.1109/ACCESS.2019.2930111\n10.3390/bios11080253\n10.3390/s20143924\n10.3390/s20236764\n10.1016/j.cosrev.2009.03.005\n10.1007/s10462-017-9572-4\n10.1016/j.sbsr.2017.09.002\n10.1038/s41592-018-0261-2\n10.1016/j.cell.2018.03.040\n10.1016/j.jsb.2007.04.003\n10.1109/TMI.2020.3001810\n10.1109/ACCESS.2020.2993788\n10.1109/CVPR.2019.00864\n10.53293/jasn.2021.11016\n10.1007/s12596-022-00978-x\n10.1038/s41378-019-0069-y\n10.1007/s11082-022-03786-6\n10.1016/j.patcog.2021.107885\n10.1155/2020/4621403\n10.1038/s41377-021-00506-9\n10.3390/info12070272\n10.1016/j.trsl.2017.10.010\n10.1016/j.bbe.2014.07.003\n10.1128/JCM.01521-17\n10.1155/2022/9690940\n10.3390/w14050761\n10.1109/72.97934\n10.1162/neco.1991.3.2.246\n10.1007/s00703-012-0205-9\n10.3389/fmicb.2018.03255\n10.1038/s41598-021-82852-7\n10.1056/NEJMoa2001017\n10.1099/jgv.0.001453\n10.1038/s41467-020-15562-9\n10.21037/jtd-20-1368\n10.1126/science.abb3405\n10.1126/science.abb2762\n10.1016/j.bios.2021.112969\n10.1007/s11760-021-02045-7"}
{"title": "EVAE-Net: An Ensemble Variational Autoencoder Deep Learning Network for COVID-19 Classification Based on Chest X-ray Images.", "abstract": "The COVID-19 pandemic has had a significant impact on many lives and the economies of many countries since late December 2019. Early detection with high accuracy is essential to help break the chain of transmission. Several radiological methodologies, such as CT scan and chest X-ray, have been employed in diagnosing and monitoring COVID-19 disease. Still, these methodologies are time-consuming and require trial and error. Machine learning techniques are currently being applied by several studies to deal with COVID-19. This study exploits the latent embeddings of variational autoencoders combined with ensemble techniques to propose three effective EVAE-Net models to detect COVID-19 disease. Two encoders are trained on chest X-ray images to generate two feature maps. The feature maps are concatenated and passed to either a combined or individual reparameterization phase to generate latent embeddings by sampling from a distribution. The latent embeddings are concatenated and passed to a classification head for classification. The COVID-19 Radiography Dataset from Kaggle is the source of chest X-ray images. The performances of the three models are evaluated. The proposed model shows satisfactory performance, with the best model achieving 99.19% and 98.66% accuracy on four classes and three classes, respectively.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-11-12", "authors": ["DanielAddo", "ShijieZhou", "Jehoiada KofiJackson", "Grace UgochiNneji", "Happy NkantaMonday", "KwabenaSarpong", "Rutherford AgbeshiPatamia", "FavourEkong", "Christyn AkosuaOwusu-Agyei"], "doi": "10.3390/diagnostics12112569\n10.1148/radiol.2020200432\n10.1109/ACCESS.2020.3033762\n10.1128/JCM.01438-20\n10.1016/j.knosys.2020.106647\n10.3238/arztebl.2014.0181\n10.1007/s11548-019-01917-1\n10.1016/j.jemermed.2020.04.004\n10.1148/radiol.2020201160\n10.14245/ns.1938396.198\n10.1038/s41746-020-0273-z\n10.1109/TMI.2016.2535865\n10.1109/ISBI.2018.8363572\n10.1016/j.measurement.2019.05.076\n10.1016/j.bspc.2022.103848\n10.1016/j.bspc.2022.103595\n10.1016/j.neucom.2022.01.055\n10.1016/j.jksuci.2020.12.010\n10.1038/323533a0\n10.1561/2200000056\n10.21437/Interspeech.2016-1183\n10.1016/S0140-6736(20)30304-4\n10.1109/TCYB.2020.2990162\n10.1093/jtm/taaa080\n10.3390/healthcare8010046\n10.1038/s41467-020-17280-8\n10.1016/j.compbiomed.2020.103869\n10.3390/s21175813\n10.1148/radiol.2020200905\n10.3390/app12126269\n10.1016/j.compbiomed.2020.103792\n10.1109/CVPR.2017.690\n10.1016/j.cmpb.2020.105581\n10.1007/s12530-021-09385-2\n10.1109/TNNLS.2021.3070467\n10.3390/sym14071398\n10.1007/s11517-020-02299-2\n10.1007/s12539-020-00403-6\n10.3390/healthcare10071313\n10.1016/j.compbiomed.2022.105233\n10.1007/s13246-020-00865-4\n10.1109/CVPR.2017.243\n10.1007/s13755-021-00140-0\n10.1038/s41598-020-76550-z\n10.1109/TCBB.2021.3065361\n10.21037/atm.2020.03.132\n10.1007/s10489-020-01900-3\n10.1016/j.asoc.2020.106912\n10.1109/IJCNN.2015.7280578\n10.1109/ICCCI50826.2021.9402545\n10.1155/2021/5527923\n10.1007/978-3-030-74575-2_14\n10.1111/j.1365-2818.2010.03415.x\n10.1109/ICACCI.2014.6968381\n10.32604/cmc.2022.020698\n10.1109/TBDATA.2017.2717439\n10.14569/IJACSA.2021.0120717\n10.1155/2018/3078374\n10.3390/jimaging7050083\n10.1148/ryai.2021200218\n10.1111/srt.13145\n10.1007/s10489-020-01813-1\n10.1016/j.micpro.2020.103280\n10.1016/j.neucom.2015.08.104\n10.1016/j.cmpb.2022.106883\n10.1016/j.bbe.2021.09.004\n10.1016/j.compbiomed.2021.105134\n10.1155/2022/5329014\n10.32604/cmc.2021.018449\n10.1007/s10489-020-02002-w\n10.1145/3451357\n10.1016/j.patrec.2021.08.018\n10.1038/s41598-022-05532-0\n10.1016/j.bspc.2021.103326\n10.1016/j.compmedimag.2021.102008\n10.1016/j.bspc.2022.103677\n10.1016/j.compbiomed.2022.105340\n10.1016/j.jksuci.2021.07.005\n10.1016/j.jksuci.2022.04.006\n10.1016/j.compbiomed.2021.104319\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104834\n10.1016/j.compbiomed.2022.105244\n10.1016/j.bspc.2022.103860"}
{"title": "Generative adversarial network based data augmentation for CNN based detection of Covid-19.", "abstract": "Covid-19 has been a global concern since 2019, crippling the world economy and health. Biological diagnostic tools have since been developed to identify the virus from bodily fluids and since the virus causes pneumonia, which results in lung inflammation, the presence of the virus can also be detected using medical imaging by expert radiologists. The success of each diagnostic method is measured by the hit rate for identifying Covid infections. However, the access for people to each diagnosis tool can be limited, depending on the geographic region and, since Covid treatment denotes a race against time, the diagnosis duration plays an important role. Hospitals with X-ray opportunities are widely distributed all over the world, so a method investigating lung X-ray images for possible Covid-19 infections would offer itself. Promising results have been achieved in the literature in automatically detecting the virus using medical images like CT scans and X-rays using supervised artificial neural network algorithms. One of the major drawbacks of supervised learning models is that they require enormous amounts of data to train, and generalize on new data. In this study, we develop a Swish activated, Instance and Batch normalized Residual U-Net GAN with dense blocks and skip connections to create synthetic and augmented data for training. The proposed GAN architecture, due to the presence of instance normalization and swish activation, can deal with the randomness of luminosity, that arises due to different sources of X-ray images better than the classical architecture and generate realistic-looking synthetic data. Also, the radiology equipment is not generally computationally efficient. They cannot efficiently run state-of-the-art deep neural networks such as DenseNet and ResNet effectively. Hence, we propose a novel CNN architecture that is 40% lighter and more accurate than state-of-the-art CNN networks. Multi-class classification of the three classes of chest X-rays (CXR), ie Covid-19, healthy and Pneumonia, is performed using the proposed model which had an extremely high test accuracy of 99.2% which has not been achieved in any previous studies in the literature. Based on the mentioned criteria for developing Corona infection diagnosis, in the present study, an Artificial Intelligence based method is proposed, resulting in a rapid diagnostic tool for Covid infections based on generative adversarial and convolutional neural networks. The benefit will be a high accuracy of lung infection identification with 99% accuracy. This could lead to a support tool that helps in rapid diagnosis, and an accessible Covid identification method using CXR images.", "journal": "Scientific reports", "date": "2022-11-11", "authors": ["RutwikGulakala", "BerndMarkert", "MarcusStoffel"], "doi": "10.1038/s41598-022-23692-x\n10.1016/S1473-3099(20)30120-1\n10.1148/radiol.2020201160\n10.1038/s42003-020-01535-7\n10.1155/2021/3366057\n10.1515/cdbme-2020-3051\n10.1016/j.cmpb.2021.106279\n10.1016/j.medengphy.2016.10.010\n10.1016/j.cma.2020.112989\n10.1016/j.mechrescom.2021.103817\n10.1016/j.euromechsol.2006.12.002\n10.1016/j.mechmat.2005.06.001\n10.1038/s41598-021-93543-8\n10.1016/j.imu.2020.100412\n10.1016/j.imu.2020.100505\n10.1016/j.ijmedinf.2020.104284\n10.1148/radiol.2017162326\n10.1109/ACCESS.2020.2994762\n10.1007/s00521-022-06918-x\n10.1016/j.compbiomed.2020.103792\n10.1007/s10044-020-00950-0\n10.3390/diagnostics12020267\n10.1109/TMI.2020.2995518\n10.3390/app112210528\n10.1007/s00500-019-04602-2\n10.1016/j.imu.2021.100779\n10.1038/s41598-021-87994-2\n10.1109/TMI.2013.2290491\n10.1109/TMI.2013.2284099\n10.1186/s40537-021-00444-8\n10.1007/s13246-020-00865-4\n10.1371/journal.pone.0262052"}
{"title": "CXR-Net: A Multitask Deep Learning Network for Explainable and Accurate Diagnosis of COVID-19 Pneumonia From Chest X-Ray Images.", "abstract": "Accurate and rapid detection of COVID-19 pneumonia is crucial for optimal patient treatment. Chest X-Ray (CXR) is the first-line imaging technique for COVID-19 pneumonia diagnosis as it is fast, cheap and easily accessible. Currently, many deep learning (DL) models have been proposed to detect COVID-19 pneumonia from CXR images. Unfortunately, these deep classifiers lack the transparency in interpreting findings, which may limit their applications in clinical practice. The existing explanation methods produce either too noisy or imprecise results, and hence are unsuitable for diagnostic purposes. In this work, we propose a novel explainable CXR deep neural Network (CXR-Net) for accurate COVID-19 pneumonia detection with an enhanced pixel-level visual explanation using CXR images. An Encoder-Decoder-Encoder architecture is proposed, in which an extra encoder is added after the encoder-decoder structure to ensure the model can be trained on category samples. The method has been evaluated on real world CXR datasets from both public and private sources, including healthy, bacterial pneumonia, viral pneumonia and COVID-19 pneumonia cases. The results demonstrate that the proposed method can achieve a satisfactory accuracy and provide fine-resolution activation maps for visual explanation in the lung disease detection. Compared to current state-of-the-art visual explanation methods, the proposed method can provide more detailed, high-resolution, visual explanation for the classification results. It can be deployed in various computing environments, including cloud, CPU and GPU environments. It has a great potential to be used in clinical practice for COVID-19 pneumonia diagnosis.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-11-10", "authors": ["XinZhang", "LiangxiuHan", "TamSobeih", "LianghaoHan", "NinaDempsey", "SymeonLechareas", "AscanioTridente", "HaomingChen", "StephenWhite", "DaoqiangZhang"], "doi": "10.1109/JBHI.2022.3220813"}
{"title": "Mental health and chest CT scores mediate the relationship between COVID-19 vaccination status and seroconversion time: A cross-sectional observational study in B.1.617.2 (Delta) infection patients.", "abstract": "The coronavirus disease (COVID-19) pandemic, which has been ongoing for more than 2 years, has become one of the largest public health issues. Vaccination against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection is one of the most important interventions to mitigate the COVID-19 pandemic. Our objective is to investigate the relationship between vaccination status and time to seroconversion.\nWe conducted a cross-sectional observational study during the SARS-CoV-2 B.1.617.2 outbreak in Jiangsu, China. Participants who infected with the B.1.617.2 variant were enrolled. Cognitive performance, quality of life, emotional state, chest computed tomography (CT) score and seroconversion time were evaluated for each participant. Statistical analyses were performed using one-way ANOVA, univariate and multivariate regression analyses, Pearson correlation, and mediation analysis.\nA total of 91 patients were included in the analysis, of whom 37.3, 25.3, and 37.3% were unvaccinated, partially vaccinated, and fully vaccinated, respectively. Quality of life was impaired in 30.7% of patients, especially for mental component summary (MCS) score. Vaccination status, subjective cognitive decline, and depression were risk factors for quality-of-life impairment. The chest CT score mediated the relationship of vaccination status with the MCS score, and the MCS score mediated the relationship of the chest CT score with time to seroconversion.\nFull immunization course with an inactivated vaccine effectively lowered the chest CT score and improved quality of life in hospitalized patients. Vaccination status could influence time to seroconversion by affecting CT score and MCS score indirectly. Our study emphasizes the importance of continuous efforts in encouraging a full vaccination course.", "journal": "Frontiers in public health", "date": "2022-11-08", "authors": ["WenZhang", "QianChen", "JinghongDai", "JiamingLu", "JieLi", "YongxiangYi", "LinqingFu", "XinLi", "JianiLiu", "JinlongLiufu", "CongLong", "BingZhang"], "doi": "10.3389/fpubh.2022.974848\n10.1016/j.landusepol.2021.105772\n10.1016/j.ijid.2022.01.030\n10.5694/mja2.51182\n10.1016/S0140-6736(21)00677-2\n10.1001/jama.2021.8565\n10.1056/NEJMoa2107715\n10.1016/S0140-6736(21)01429-X\n10.7499/j.issn.1008-8830.2101133\n10.3390/vaccines10020277\n10.1002/jmv.27458\n10.1016/j.bbi.2020.04.027\n10.3390/vaccines9121444\n10.1016/j.psychres.2020.113172\n10.46234/ccdcw2020.032\n10.1080/22221751.2021.1969291\n10.3233/JAD-160119\n10.1016/j.dadm.2015.09.004\n10.1097/00005650-199206000-00002\n10.1002/da.20837\n10.1111/j.1600-0447.1983.tb09716.x\n10.1148/radiol.2020200463\n10.1148/radiol.2020200343\n10.1016/S0140-6736(20)32656-8\n10.1146/annurev-publhealth-032315-021402\n10.2807/1560-7917.ES.2021.26.24.2100509\n10.1016/S0262-4079(21)01121-0\n10.2147/IJGM.S323316\n10.3390/biomedicines9080900\n10.1186/s12877-021-02140-x\n10.1001/jamanetworkopen.2021.28568\n10.1016/j.clinimag.2021.01.023\n10.3389/fpsyt.2021.774504\n10.1183/13993003.01217-2020"}
{"title": "Strong semantic segmentation for Covid-19 detection: Evaluating the use of deep learning models as a performant tool in radiography.", "abstract": "With the increasing number of Covid-19 cases as well as care costs, chest diseases have gained increasing interest in several communities, particularly in medical and computer vision. Clinical and analytical exams are widely recognized techniques for diagnosing and handling Covid-19 cases. However, strong detection tools can help avoid damage to chest tissues. The proposed method provides an important way to enhance the semantic segmentation process using combined potential deep learning (DL) modules to increase consistency. Based on Covid-19 CT images, this work hypothesized that a novel model for semantic segmentation might be able to extract definite graphical features of Covid-19 and afford an accurate clinical diagnosis while optimizing the classical test and saving time.\nCT images were collected considering different cases (normal chest CT, pneumonia, typical viral causes, and Covid-19 cases). The study presents an advanced DL method to deal with chest semantic segmentation issues. The approach employs a modified version of the U-net to enable and support Covid-19 detection from the studied images.\nThe validation tests demonstrated competitive results with important performance rates: Precision (90.96%\u00a0\u00b1\u00a02.5) with an F-score of (91.08%\u00a0\u00b1\u00a03.2), an accuracy of (93.37%\u00a0\u00b1\u00a01.2), a sensitivity of (96.88%\u00a0\u00b1\u00a02.8) and a specificity of (96.91%\u00a0\u00b1\u00a02.3). In addition, the visual segmentation results are very close to the Ground truth.\nThe findings of this study reveal the proof-of-principle for using cooperative components to strengthen the semantic segmentation modules for effective and truthful Covid-19 diagnosis.\nThis paper has highlighted that DL based approach, with several modules, may be contributing to provide strong support for radiographers and physicians, and that further use of DL is required to design and implement performant automated vision systems to detect chest diseases.", "journal": "Radiography (London, England : 1995)", "date": "2022-11-07", "authors": ["HAllioui", "YMourdi", "MSadgal"], "doi": "10.1016/j.radi.2022.10.010\n10.1016/j.neucom.2017.08.043\n10.3389/fnins.2018.00777\n10.1007/s13369-021-05958-0\n10.3390/technologies10050105\n10.1109/ISBI.2016.7493515\n10.5114/pjr.2022.119027\n10.1109/CISP-BMEI.2018.8633056\n10.1016/j.asoc.2021.107160\n10.48550/arXiv.2110.09619\n10.7937/K9/TCIA.2017.3r3fvz08\n10.5281/zenodo.375747\n10.1111/cgf.14521"}
{"title": "Prediction of COVID-19 patients in danger of death using radiomic features of portable chest radiographs.", "abstract": "Computer-aided diagnostic systems have been developed for the detection and differential diagnosis of coronavirus disease 2019 (COVID-19) pneumonia using imaging studies to characterise a patient's current condition. In this radiomic study, we propose a system for predicting COVID-19 patients in danger of death using portable chest X-ray images.\nIn this retrospective study, we selected 100 patients, including ten that died and 90 that recovered from the COVID-19-AR database of the Cancer Imaging Archive. Since it can be difficult to analyse portable chest X-ray images of patients with COVID-19 because bone components overlap with the abnormal patterns of this disease, we employed a bone-suppression technique during pre-processing. A total of 620 radiomic features were measured in the left and right lung regions, and four radiomic features were selected using the least absolute shrinkage and selection operator technique. We distinguished death from recovery cases using a linear discriminant analysis (LDA) and a support vector machine (SVM). The leave-one-out method was used to train and test the classifiers, and the area under the receiver-operating characteristic curve (AUC) was used to evaluate discriminative performance.\nThe AUCs for LDA and SVM were 0.756 and 0.959, respectively. The discriminative performance was improved when the bone-suppression technique was employed. When the SVM was used, the sensitivity for predicting disease severity was 90.9% (9/10), and the specificity was 95.6% (86/90).\nWe believe that the radiomic features of portable chest X-ray images can predict COVID-19 patients in danger of death.", "journal": "Journal of medical radiation sciences", "date": "2022-11-06", "authors": ["MaokoNakashima", "YoshikazuUchiyama", "HirotakeMinami", "SatoshiKasai"], "doi": "10.1002/jmrs.631\n10.1007/s12652-020-02669-6"}
{"title": "Contrastive domain adaptation with consistency match for automated pneumonia diagnosis.", "abstract": "Pneumonia can be difficult to diagnose since its symptoms are too variable, and the radiographic signs are often very similar to those seen in other illnesses such as a cold or influenza. Deep neural networks have shown promising performance in automated pneumonia diagnosis using chest X-ray radiography, allowing mass screening and early intervention to reduce the severe cases and death toll. However, they usually require many well-labelled chest X-ray images for training to achieve high diagnostic accuracy. To reduce the need for training data and annotation resources, we propose a novel method called Contrastive Domain Adaptation with Consistency Match (CDACM). It transfers the knowledge from different but relevant datasets to the unlabelled small-size target dataset and improves the semantic quality of the learnt representations. Specifically, we design a conditional domain adversarial network to exploit discriminative information conveyed in the predictions to mitigate the domain gap between the source and target datasets. Furthermore, due to the small scale of the target dataset, we construct a feature cloud for each target sample and leverage contrastive learning to extract more discriminative features. Lastly, we propose adaptive feature cloud expansion to push the decision boundary to a low-density area. Unlike most existing transfer learning methods that aim only to mitigate the domain gap, our method instead simultaneously considers the domain gap and the data deficiency problem of the target dataset. The conditional domain adaptation and the feature cloud generation of our method are learning jointly to extract discriminative features in an end-to-end manner. Besides, the adaptive feature cloud expansion improves the model's generalisation ability in the target domain. Extensive experiments on pneumonia and COVID-19 diagnosis tasks demonstrate that our method outperforms several state-of-the-art unsupervised domain adaptation approaches, which verifies the effectiveness of CDACM for automated pneumonia diagnosis using chest X-ray imaging.", "journal": "Medical image analysis", "date": "2022-11-05", "authors": ["YangqinFeng", "ZizhouWang", "XinxingXu", "YanWang", "HuazhuFu", "ShaohuaLi", "LiangliZhen", "XiaofengLei", "YingnanCui", "JordanSim Zheng Ting", "YonghanTing", "Joey TianyiZhou", "YongLiu", "RickSiow Mong Goh", "CherHeng Tan"], "doi": "10.1016/j.media.2022.102664"}
{"title": "Assessment of COVID-19 lung involvement on computed tomography by deep-learning-, threshold-, and human reader-based approaches-an international, multi-center comparative study.", "abstract": "The extent of lung involvement in coronavirus disease 2019 (COVID-19) pneumonia, quantified on computed tomography (CT), is an established biomarker for prognosis and guides clinical decision-making. The clinical standard is semi-quantitative scoring of lung involvement by an experienced reader. We aim to compare the performance of automated deep-learning- and threshold-based methods to the manual semi-quantitative lung scoring. Further, we aim to investigate an optimal threshold for quantification of involved lung in COVID pneumonia chest CT, using a multi-center dataset.\nIn total 250 patients were included, 50 consecutive patients with RT-PCR confirmed COVID-19 from our local institutional database, and another 200 patients from four international datasets (n=50 each). Lung involvement was scored semi-quantitatively by three experienced radiologists according to the established chest CT score (CCS) ranging from 0-25. Inter-rater reliability was reported by the intraclass correlation coefficient (ICC). Deep-learning-based segmentation of ground-glass and consolidation was obtained by CT Pulmo Auto Results prototype plugin on IntelliSpace Discovery (Philips Healthcare, The Netherlands). Threshold-based segmentation of involved lung was implemented using an open-source tool for whole-lung segmentation under the presence of severe pathologies (R231CovidWeb, Hofmanninger \nMedian CCS among 250 evaluated patients was 10 [6-15]. Inter-rater reliability of the CCS was excellent [ICC 0.97 (0.97-0.98)]. Best attenuation threshold for identification of involved lung was -522 HU. While the relationship of deep-learning- and threshold-based quantification was linear and strong (r\nThe manual semi-quantitative CCS underestimates the extent of COVID pneumonia in higher score ranges, which limits its clinical usefulness in cases of severe disease. Clinical implementation of fully automated methods, such as deep-learning or threshold-based approaches (best threshold in our multi-center dataset: -522 HU), might save time of trained personnel, abolish inter-reader variability, and allow for truly quantitative, linear assessment of COVID lung involvement.", "journal": "Quantitative imaging in medicine and surgery", "date": "2022-11-05", "authors": ["PhilippFervers", "FlorianFervers", "AsthaJaiswal", "MiriamRinneburger", "MathildaWeisthoff", "PhilipPollmann-Schweckhorst", "JonathanKottlors", "HeikeCarolus", "SimonLennartz", "DavidMaintz", "RahilShahzad", "ThorstenPersigehl"], "doi": "10.21037/qims-22-175\n10.1148/rg.2020200159\n10.1038/s41467-020-18786-x\n10.3389/fpubh.2021.596938\n10.1148/ryct.2020200130\n10.1371/journal.pone.0237302\n10.1186/s12931-020-01411-2\n10.1007/s00330-020-07033-y\n10.1186/s43055-021-00525-x\n10.1148/ryct.2020200441\n10.1148/radiol.2020200370\n10.1148/ryai.2020200048\n10.1007/s11760-022-02183-6\n10.1148/radiol.2020201433\n10.1007/s00330-020-07013-2\n10.1148/radiol.2021203957\n10.1186/s13104-021-05592-x\n10.3390/bioengineering8020026\n10.1007/s10278-013-9622-7\n10.1007/s00330-021-08482-9\n10.1117/12.2293528\n10.1186/s41747-020-00173-2\n10.1038/s41586-020-2649-2\n10.1038/s41598-021-01489-8\n10.1177/1536867X0800800212\n10.3758/s13423-016-1039-0\n10.1353/bsp.2020.0011\n10.21037/jtd.2017.08.17\n10.1117/1.1631315\n10.1155/2021/6697677\n10.1007/s00330-021-08435-2"}
{"title": "A novel deep learning-based method for COVID-19 pneumonia detection from CT images.", "abstract": "The sensitivity of RT-PCR in diagnosing COVID-19 is only 60-70%, and chest CT plays an indispensable role in the auxiliary diagnosis of COVID-19 pneumonia, but the results of CT imaging are highly dependent on professional radiologists.\nThis study aimed to develop a deep learning model to assist radiologists in detecting COVID-19 pneumonia.\nThe total study population was 437. The training dataset contained 26,477, 2468, and 8104 CT images of normal, CAP, and COVID-19, respectively. The validation dataset contained 14,076, 1028, and 3376 CT images of normal, CAP, and COVID-19 patients, respectively. The test set included 51 normal cases, 28 CAP patients, and 51 COVID-19 patients. We designed and trained a deep learning model to recognize normal, CAP, and COVID-19 patients based on U-Net and ResNet-50. Moreover, the diagnoses of the deep learning model were compared with different levels of radiologists.\nIn the test set, the sensitivity of the deep learning model in diagnosing normal cases, CAP, and COVID-19 patients was 98.03%, 89.28%, and 92.15%, respectively. The diagnostic accuracy of the deep learning model was 93.84%. In the validation set, the accuracy was 92.86%, which was better than that of two novice doctors (86.73% and 87.75%) and almost equal to that of two experts (94.90% and 93.88%). The AI model performed\u00a0significantly\u00a0better\u00a0than all four radiologists in terms of time consumption (35\u00a0min vs. 75\u00a0min, 93\u00a0min, 79\u00a0min, and 82\u00a0min).\nThe AI model we obtained had strong decision-making ability, which could potentially assist doctors in detecting COVID-19 pneumonia.", "journal": "BMC medical informatics and decision making", "date": "2022-11-04", "authors": ["JuLuo", "YuhaoSun", "JingshuChi", "XinLiao", "CanxiaXu"], "doi": "10.1186/s12911-022-02022-1\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1056/NEJMoa2001316\n10.1056/NEJMoa2001191\n10.1101/2020.02.11.20021493v2\n10.1148/radiol.2020200432\n10.1109/TMI.2020.2996645\n10.1148/radiol.2020200343\n10.1016/S0140-6736(20)30154-9\n10.1148/ryct.2020204002\n10.1148/radiol.2020200642\n10.1038/s41467-020-17971-2\n10.1016/j.cell.2020.08.029\n10.1016/j.media.2021.102096\n10.1007/s00354-022-00172-4\n10.18280/ts.370313\n10.1186/s41747-020-00173-2\n10.1016/j.compbiomed.2020.103792\n10.18280/ts.380117\n10.1007/s10044-021-00984-y\n10.21203/rs.3.rs-104621/v1\n10.1148/radiol.2020200905\n10.1001/jama.2020.8259\n10.7326/M20-1495\n10.1001/jama.2020.12839\n10.1148/radiol.2020200702"}
{"title": "Improved Fine-Tuning of In-Domain Transformer Model for Inferring COVID-19 Presence in Multi-Institutional Radiology Reports.", "abstract": "Building a document-level classifier for COVID-19 on radiology reports could help assist providers in their daily clinical routine, as well as create large numbers of labels for computer vision models. We have developed such a classifier by fine-tuning a BERT-like model initialized from RadBERT, its continuous pre-training on radiology reports that can be used on all radiology-related tasks. RadBERT outperforms all biomedical pre-trainings on this COVID-19 task (P<0.01) and helps our fine-tuned model achieve an 88.9 macro-averaged F1-score, when evaluated on both X-ray and CT reports. To build this model, we rely on a multi-institutional dataset re-sampled and enriched with concurrent lung diseases, helping the model to resist to distribution shifts. In addition, we explore a variety of fine-tuning and hyperparameter optimization techniques that accelerate fine-tuning convergence, stabilize performance, and improve accuracy, especially when data or computational resources are limited. Finally, we provide a set of visualization tools and explainability methods to better understand the performance of the model, and support its practical use in the clinical setting. Our approach offers a ready-to-use COVID-19 classifier and can be applied similarly to other radiology report classification tasks.", "journal": "Journal of digital imaging", "date": "2022-11-04", "authors": ["PierreChambon", "Tessa SCook", "Curtis PLanglotz"], "doi": "10.1007/s10278-022-00714-8\n10.1093/bioinformatics/btz682\n10.1145/3458754\n10.1148/ryai.210258\n10.3345/kjp.2012.55.11.403\n10.3390/info11020108"}
{"title": "Multi Level Approach for Segmentation of Interstitial Lung Disease (ILD) Patterns Classification Based on Superpixel Processing and Fusion of ", "abstract": "During the COVID-19 pandemic, huge interstitial lung disease (ILD) lung images have been captured. It is high time to develop the efficient segmentation techniques utilized to separate the anatomical structures and ILD patterns for disease and infection level identification. The effectiveness of disease classification directly depends on the accuracy of initial stages like preprocessing and segmentation. This paper proposed a hybrid segmentation algorithm designed for ILD images by taking advantage of superpixel and ", "journal": "Computational intelligence and neuroscience", "date": "2022-11-02", "authors": ["Anni UGupta", "SaritaSingh Bhadauria"], "doi": "10.1155/2022/4431817\n10.1155/2019/2045432\n10.1186/s12880-020-00529-5\n10.1186/s12967-021-02992-2\n10.1007/s11042-021-10594-9\n10.3390/ijgi9050329\n10.1016/j.imavis.2009.10.009\n10.1016/j.procs.2017.11.282\n10.1016/j.heliyon.2020.e05267\n10.1007/s11517-015-1404-6\n10.3390/jimaging7020022\n10.1016/j.bspc.2021.103113\n10.1186/s13014-022-02035-0\n10.1016/j.jneumeth.2021.109296\n10.1038/s41598-021-82085-8\n10.1016/j.procs.2016.07.370\n10.3389/fmed.2022.794126\n10.3390/a13090207\n10.1007/s00521-021-06273-3\n10.1007/s11517-021-02379-x\n10.1016/j.net.2020.03.011\n10.1109/access.2020.3012160\n10.1109/tpami.2006.233\n10.1007/s41095-021-0239-3\n10.1109/tfuzz.2015.2505328\n10.1016/j.measurement.2019.107432\n10.1016/j.patcog.2017.03.012\n10.1186/s40537-019-0276-2\n10.1088/1742-6596/1613/1/012006\n10.1088/1757-899x/342/1/012060\n10.1186/s13640-018-0309-3\n10.1109/access.2020.2988796\n10.1007/s41095-020-0177-5\n10.1371/journal.pone.0240015\n10.1007/s11063-020-10330-8\n10.14569/ijacsa.2020.0111149\n10.2214/ajr.09.2843\n10.14419/ijet.v7i2.7.10275\n10.5120/ijca2016911409\n10.1109/tfuzz.2017.2743679\n10.1109/tip.2017.2675165\n10.1109/tpami.2014.2303095\n10.1109/tip.2017.2651389\n10.1155/2021/9654059\n10.1155/2021/8922656\n10.1155/2022/8961456\n10.1016/j.compmedimag.2011.07.003"}
{"title": "Severity detection of COVID-19 infection with machine learning of clinical records and CT images.", "abstract": "Coronavirus disease 2019 (COVID-19) is a deadly viral infection spreading rapidly around the world since its outbreak in 2019. In the worst case a patient's organ may fail leading to death. Therefore, early diagnosis is crucial to provide patients with adequate and effective treatment.\nThis paper aims to build machine learning prediction models to automatically diagnose COVID-19 severity with clinical and computed tomography (CT) radiomics features.\nP-V-Net was used to segment the lung parenchyma and then radiomics was used to extract CT radiomics features from the segmented lung parenchyma regions. Over-sampling, under-sampling, and a combination of over- and under-sampling methods were used to solve the data imbalance problem. RandomForest was used to screen out the optimal number of features. Eight different machine learning classification algorithms were used to analyze the data.\nThe experimental results showed that the COVID-19 mild-severe prediction model trained with clinical and CT radiomics features had the best prediction results. The accuracy of the GBDT classifier was 0.931, the ROUAUC 0.942, and the AUCPRC 0.694, which indicated it was better than other classifiers.\nThis study can help clinicians identify patients at risk of severe COVID-19 deterioration early on and provide some treatment for these patients as soon as possible. It can also assist physicians in prognostic efficacy assessment and decision making.", "journal": "Technology and health care : official journal of the European Society for Engineering and Medicine", "date": "2022-11-01", "authors": ["FubaoZhu", "ZelinZhu", "YijunZhang", "HanleiZhu", "ZhengyuanGao", "XiaomanLiu", "GuanbinZhou", "YanXu", "FeiShan"], "doi": "10.3233/THC-220321"}
{"title": "Application of artificial intelligence in diagnosing COVID-19 disease symptoms on chest X-rays: A systematic review.", "abstract": "This systematic review focuses on using artificial intelligence (AI) to detect COVID-19 infection with the help of X-ray images. ", "journal": "International journal of medical sciences", "date": "2022-11-01", "authors": ["JakubKufel", "KatarzynaBargie\u0142", "MaciejKo\u017alik", "\u0141ukaszCzogalik", "PiotrDudek", "AleksanderJaworski", "MaciejCebula", "KatarzynaGruszczy\u0144ska"], "doi": "10.7150/ijms.76515"}
{"title": "Towards smart diagnostic methods for COVID-19: Review of deep learning for medical imaging.", "abstract": "The infectious disease known as COVID-19 has spread dramatically all over the world since December 2019. The fast diagnosis and isolation of infected patients are key factors in slowing down the spread of this virus and better management of the pandemic. Although the CT and X-ray modalities are commonly used for the diagnosis of COVID-19, identifying COVID-19 patients from medical images is a time-consuming and error-prone task. Artificial intelligence has shown to have great potential to speed up and optimize the prognosis and diagnosis process of COVID-19. Herein, we review publications on the application of deep learning (DL) techniques for diagnostics of patients with COVID-19 using CT and X-ray chest images for a period from January 2020 to October 2021. Our review focuses solely on peer-reviewed, well-documented articles. It provides a comprehensive summary of the technical details of models developed in these articles and discusses the challenges in the smart diagnosis of COVID-19 using DL techniques. Based on these challenges, it seems that the effectiveness of the developed models in clinical use needs to be further investigated. This review provides some recommendations to help researchers develop more accurate prediction models.", "journal": "IPEM-translation", "date": "2022-11-01", "authors": ["MarjanJalali Moghaddam", "MinaGhavipour"], "doi": "10.1016/j.ipemt.2022.100008\n10.1038/s41591-020-0931-3\n10.1007/s00330-020-06801-0\n10.1016/j.tmaid.2020.101623\n10.1148/radiol.2020200823\n10.1007/s42600-021-00151-6\n10.3390/ai1030027\n10.3390/electronics8030292\n10.1155/2021/8829829\n10.22061/jecei.2022.8200.491\n10.1038/s41598-021-99015-3\n10.1109/UEMCON47517.2019.8993089\n10.1186/s40537-021-00444-8\n10.1186/s13634-021-00755-1\n10.1016/j.media.2021.102253\n10.1148/radiol.2020201491\n10.1109/ACCESS.2021.3086020\n10.3390/electronics11152296\n10.1109/ACCESS.2020.2994762\n10.1109/TMI.2020.2994459\n10.1007/s40477-020-00458-7\n10.1148/radiol.2020200490\n10.1007/s10489-020-01900-3\n10.1021/ci0342472\n10.1109/IACC.2016.25\n10.1016/S0031-3203(02)00121-8\n10.1109/TKDE.2009.191\n10.4018/978-1-60566-766-9.ch011\n10.1016/j.cmpb.2020.105608\n10.1108/IJPCC-06-2020-0060\n10.1007/978-3-540-75171-7_2\n10.1007/978-3-540-28650-9_5\n10.1093/nsr/nwx106\n10.1109/TMI.2020.2994908\n10.1109/TMI.2020.3000314\n10.1109/TMI.2020.2996645\n10.1038/s41746-021-00399-3\n10.1109/ACCESS.2020.3003810\n10.3390/app10165683\n10.1080/07391102.2020.1767212\n10.1148/radiol.2020200905\n10.1016/j.compbiomed.2020.103869\n10.1038/nbt1206-1565\n10.1007/s12010-021-03728-0\n10.7326/M20-1495\n10.1016/j.bea.2021.100003\n10.1148/radiol.2020200432\n10.1016/j.clinimag.2021.02.003\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.1148/rg.2020200159\n10.1016/j.clinimag.2020.04.001\n10.1016/j.chaos.2020.109947\n10.1186/s12890-020-01286-5\n10.1016/j.chaos.2020.109944\n10.1016/j.compbiomed.2020.103805\n10.1007/s10489-020-01829-7\n10.1016/j.mehy.2020.109761\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105532\n10.1016/j.cmpb.2020.105581\n10.1007/s40846-020-00529-4\n10.3390/sym12040651\n10.1007/s13246-020-00865-4\n10.1016/j.imu.2020.100360\n10.1007/s00264-020-04609-7\n10.1007/s13246-020-00888-x\n10.1109/TMI.2020.2993291\n10.1016/j.imu.2020.100405\n10.3389/fmed.2020.00427\n10.3892/etm.2020.8797\n10.1016/j.asoc.2020.106580\n10.1007/s10489-020-01867-1\n10.1016/j.radi.2020.10.018\n10.1007/s00521-020-05636-6\n10.1016/j.bbe.2020.08.008\n10.1016/j.chaos.2020.110245\n10.1007/s12652-020-02688-3\n10.1016/j.compbiomed.2021.104252\n10.1007/s11571-021-09712-y\n10.1016/j.compbiomed.2021.104927\n10.1007/s10489-020-01714-3\n10.1080/07391102.2020.1788642\n10.1016/j.ejrad.2020.109041\n10.1038/s41467-020-17971-2\n10.1016/j.compbiomed.2020.103795\n10.1109/TMI.2020.2995508\n10.21037/atm.2020.03.132\n10.1109/TMI.2020.2996256\n10.3390/e22050517\n10.1007/s00330-020-06956-w\n10.1016/j.irbm.2020.05.003\n10.1007/s10096-020-03901-z\n10.1007/s00259-020-04929-1\n10.1109/TMI.2020.2995108\n10.1109/ACCESS.2020.3005510\n10.1183/13993003.00775-2020\n10.2196/19569\n10.1007/s00330-021-07715-1\n10.1038/s41598-020-76282-0\n10.1109/TCBB.2021.3065361\n10.1016/j.compbiomed.2021.104306\n10.1016/j.compbiomed.2021.104837\n10.1016/j.chaos.2021.111310\n10.1016/j.patcog.2021.107826\n10.1007/s00521-020-05410-8\n10.1007/s10916-018-1088-1\n10.34133/2021/8786793\n10.4018/978-1-7998-8929-8.ch001\n10.1007/s00530-021-00794-6\n10.1109/ICIRCA48905.2020.9183278\n10.1007/s10462-020-09825-6\n10.1002/acm2.13121\n10.1007/s00521-021-06344-5\n10.1002/mp.15419\n10.1038/s41598-021-88807-2\n10.2196/19673\n10.1109/TIP.2021.3058783\n10.1097/MCP.0000000000000765\n10.3390/diagnostics11071155\n10.1016/j.ultrasmedbio.2020.07.003\n10.1002/int.22504\n10.1016/j.acra.2020.04.032\n10.1016/j.mri.2021.03.005\n10.1117/1.JBO.19.1.010901\n10.1021/acsnano.1c05226\n10.3389/fmicb.2020.02014\n10.1016/B978-0-08-100040-3.00002-X\n10.1016/j.asoc.2021.107150\n10.1016/j.compbiomed.2020.104037\n10.1101/2020.03.19.20039354\n10.1177/0846537120913033\n10.1186/s13244-020-00933-z\n10.1016/j.ijid.2020.06.026\n10.1016/j.diii.2020.03.014\n10.1093/cid/ciaa247\n10.1007/s10462-021-09975-1\n10.1007/978-1-4419-9326-7_1\n10.1109/ICC40277.2020.9148817\n10.1148/radiol.2020200330"}
{"title": "A deep transfer learning-based convolution neural network model for COVID-19 detection using computed tomography scan images for medical applications.", "abstract": "The Coronavirus (COVID-19) has become a critical and extreme epidemic because of its international dissemination. COVID-19 is the world's most serious health, economic, and survival danger. This disease affects not only a single country but the entire planet due to this infectious disease. Illnesses of Covid-19 spread at a much faster rate than usual influenza cases. Because of its high transmissibility and early diagnosis, it isn't easy to manage COVID-19. The popularly used RT-PCR method for COVID-19 disease diagnosis may provide false negatives. COVID-19 can be detected non-invasively using medical imaging procedures such as chest CT and chest x-ray. Deep learning is the most effective machine learning approach for examining a considerable quantity of chest computed tomography (CT) pictures that can significantly affect Covid-19 screening. Convolutional neural network (CNN) is one of the most popular deep learning techniques right now, and its gaining traction due to its potential to transform several spheres of human life. This research aims to develop conceptual transfer learning enhanced CNN framework models for detecting COVID-19 with CT scan images. Though with minimal datasets, these techniques were demonstrated to be effective in detecting the presence of COVID-19. This proposed research looks into several deep transfer learning-based CNN approaches for detecting the presence of COVID-19 in chest CT images.VGG16, VGG19, Densenet121, InceptionV3, Xception, and Resnet50 are the foundation models used in this work. Each model's performance was evaluated using a confusion matrix and various performance measures such as accuracy, recall, precision, f1-score, loss, and ROC. The VGG16 model performed much better than the other models in this study (98.00 % accuracy). Promising outcomes from experiments have revealed the merits of the proposed model for detecting and monitoring COVID-19 patients. This could help practitioners and academics create a tool to help minimal health professionals decide on the best course of therapy.", "journal": "Advances in engineering software (Barking, London, England : 1992)", "date": "2022-11-01", "authors": ["Nirmala DeviKathamuthu", "ShanthiSubramaniam", "Quynh HoangLe", "SureshMuthusamy", "HiteshPanchal", "Suma Christal MarySundararajan", "Ali JawadAlrubaie", "Musaddak Maher AbdulZahra"], "doi": "10.1016/j.advengsoft.2022.103317"}
{"title": "A robust semantic lung segmentation study for CNN-based COVID-19 diagnosis.", "abstract": "This paper aims to diagnose COVID-19 by using Chest X-Ray (CXR) scan images in a deep learning-based system. First of all, COVID-19 Chest X-Ray Dataset is used to segment the lung parts in CXR images semantically. DeepLabV3+ architecture is trained by using the masks of the lung parts in this dataset. The trained architecture is then fed with images in the COVID-19 Radiography Database. In order to improve the output images, some image preprocessing steps are applied. As a result, lung regions are successfully segmented from CXR images. The next step is feature extraction and classification. While features are extracted with modified AlexNet (mAlexNet), Support Vector Machine (SVM) is used for classification. As a result, 3-class data consisting of Normal, Viral Pneumonia and COVID-19 class are classified with 99.8% success. Classification results show that the proposed method is superior to previous state-of-the-art methods.", "journal": "Chemometrics and intelligent laboratory systems : an international journal sponsored by the Chemometrics Society", "date": "2022-11-01", "authors": ["Muhammet FatihAslan"], "doi": "10.1016/j.chemolab.2022.104695\n10.1109/RBME.2020.2987975\n10.1007/s11760-022-02302-3\n10.1016/j.eng.2020.04.010"}
{"title": "Deep-learning-based hepatic fat assessment (DeHFt) on non-contrast chest CT and its association with disease severity in COVID-19 infections: A multi-site retrospective study.", "abstract": "Hepatic steatosis (HS) identified on CT may provide an integrated cardiometabolic and COVID-19 risk assessment. This study presents a deep-learning-based hepatic fat assessment (DeHFt) pipeline for (a) more standardised measurements and (b) investigating the association between HS (liver-to-spleen attenuation ratio <1 in CT) and COVID-19 infections severity, wherein severity is defined as requiring invasive mechanical ventilation, extracorporeal membrane oxygenation, death.\nDeHFt comprises two steps. First, a deep-learning-based segmentation model (3D residual-UNet) is trained (N.\u00df=.\u00df80) to segment the liver and spleen. Second, CT attenuation is estimated using slice-based and volumetric-based methods. DeHFt-based mean liver and liver-to-spleen attenuation are compared with an expert's ROI-based measurements. We further obtained the liver-to-spleen attenuation ratio in a large multi-site cohort of patients with COVID-19 infections (D1, N.\u00df=.\u00df805; D2, N.\u00df=.\u00df1917; D3, N.\u00df=.\u00df169) using the DeHFt pipeline and investigated the association between HS and COVID-19 infections severity.\nThe DeHFt pipeline achieved a dice coefficient of 0.95, 95% CI [0.93...0.96] on the independent validation cohort (N.\u00df=.\u00df49). The automated slice-based and volumetric-based liver and liver-to-spleen attenuation estimations strongly correlated with expert's measurement. In the COVID-19 cohorts, severe infections had a higher proportion of patients with HS than non-severe infections (pooled OR.\u00df=.\u00df1.50, 95% CI [1.20...1.88], P.\u00df<.\u00df.001).\nThe DeHFt pipeline enabled accurate segmentation of liver and spleen on non-contrast CTs and automated estimation of liver and liver-to-spleen attenuation ratio. In three cohorts of patients with COVID-19 infections (N.\u00df=.\u00df2891), HS was associated with disease severity. Pending validation, DeHFt provides an automated CT-based metabolic risk assessment.\nFor a full list of funding bodies, please see the Acknowledgements.", "journal": "EBioMedicine", "date": "2022-10-30", "authors": ["GouravModanwal", "SadeerAl-Kindi", "JonathanWalker", "RohanDhamdhere", "LeiYuan", "MengyaoJi", "ChengLu", "PingfuFu", "SanjayRajagopalan", "AnantMadabhushi"], "doi": "10.1016/j.ebiom.2022.104315\n10.1016/j.acra.2012.02.022\n10.48550/arXiv.1901.04056"}
{"title": "Wasserstein-based texture analysis in radiomic studies.", "abstract": "The emerging field of radiomics that transforms standard-of-care images to quantifiable scalar statistics endeavors to reveal the information hidden in these macroscopic images. The concept of texture is widely used and essential in many radiomic-based studies. Practice usually reduces spatial multidimensional texture matrices, e.g., gray-level co-occurrence matrices (GLCMs), to summary scalar features. These statistical features have been demonstrated to be strongly correlated and tend to contribute redundant information; and does not account for the spatial information hidden in the multivariate texture matrices. This study proposes a novel pipeline to deal with spatial texture features in radiomic studies. A new set of textural features that preserve the spatial information inherent in GLCMs is proposed and used for classification purposes. The set of the new features uses the Wasserstein metric from optimal mass transport theory (OMT) to quantify the spatial similarity between samples within a given label class. In particular, based on a selected subset of texture GLCMs from the training cohort, we propose new representative spatial texture features, which we incorporate into a supervised image classification pipeline. The pipeline relies on the support vector machine (SVM) algorithm along with Bayesian optimization and the Wasserstein metric. The selection of the best GLCM references is considered for each classification label and is performed during the training phase of the SVM classifier using a Bayesian optimizer. We assume that sample fitness is defined based on closeness (in the sense of the Wasserstein metric) and high correlation (Spearman's rank sense) with other samples in the same class. Moreover, the newly defined spatial texture features consist of the Wasserstein distance between the optimally selected references and the remaining samples. We assessed the performance of the proposed classification pipeline in diagnosing the coronavirus disease 2019 (COVID-19) from computed tomographic (CT) images. To evaluate the proposed spatial features' added value, we compared the performance of the proposed classification pipeline with other SVM-based classifiers that account for different texture features, namely: statistical features only, optimized spatial features using Euclidean metric, non-optimized spatial features with Wasserstein metric. The proposed technique, which accounts for the optimized spatial texture feature with Wasserstein metric, shows great potential in classifying new COVID CT images that the algorithm has not seen in the training step. The MATLAB code of the proposed classification pipeline is made available. It can be used to find the best reference samples in other data cohorts, which can then be employed to build different prediction models.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2022-10-30", "authors": ["ZehorBelkhatir", "Ra\u00fal San Jos\u00e9Est\u00e9par", "Allen RTannenbaum"], "doi": "10.1016/j.compmedimag.2022.102129"}
{"title": "Conceptualising a channel-based overlapping CNN tower architecture for COVID-19 identification from CT-scan images.", "abstract": "Convolutional Neural Network (CNN) has been employed in classifying the COVID cases from the lungs' CT-Scan with promising quantifying metrics. However, SARS COVID-19 has been mutated, and we have many versions of the virus B.1.1.7, B.1.135, and P.1, hence there is a need for a more robust architecture that will classify the COVID positive patients from COVID negative patients with less training. We have developed a neural network based on the number of channels present in the images. The CNN architecture is developed in accordance with the number of the channels present in the dataset and are extracting the features separately from the channels present in the CT-Scan dataset. In the tower architecture, the first tower is dedicated for only the first channel present in the image; the second CNN tower is dedicated to the first and second channel feature maps, and finally the third channel takes account of all the feature maps from all three channels. We have used two datasets viz. one from Tongji Hospital, Wuhan, China and another SARS-CoV-2 dataset to train and evaluate our CNN architecture. The proposed model brought about an average accuracy of 99.4%, F1 score 0.988, and AUC 0.99.", "journal": "Scientific reports", "date": "2022-10-29", "authors": ["Ravi ShekharTiwari", "LakshmiD", "Tapan KumarDas", "KathiravanSrinivasan", "Chuan-YuChang"], "doi": "10.1038/s41598-022-21700-8\n10.1148/ryct.2020200034\n10.1038/s41598-021-95561-y\n10.1148/radiol.2020200642\n10.1155/2017/8314740\n10.1164/rccm.201705-0860OC\n10.1007/s00330-020-06801-0\n10.1038/s41598-021-93832-2\n10.1016/j.imu.2020.100427\n10.1088/2632-2153/abf22c\n10.1007/s00330-021-07715-1\n10.1007/s10140-020-01886-y\n10.1007/s10489-020-01943-6\n10.1038/s41598-020-74164-z\n10.1038/s41467-020-18685-1\n10.1007/s10916-021-01747-2\n10.1007/s10916-020-01562-1\n10.1101/2020.04.24.20078584\n10.5201/ipol.2021.344\n10.1109/ACCESS.2020.3037073"}
{"title": "Artificial Intelligence and Deep Learning Assisted Rapid Diagnosis of COVID-19 from Chest Radiographical Images: A Survey.", "abstract": "Artificial Intelligence (AI) has been applied successfully in many real-life domains for solving complex problems. With the invention of Machine Learning (ML) paradigms, it becomes convenient for researchers to predict the outcome based on past data. Nowadays, ML is acting as the biggest weapon against the COVID-19 pandemic by detecting symptomatic cases at an early stage and warning people about its futuristic effects. It is observed that COVID-19 has blown out globally so much in a short period because of the shortage of testing facilities and delays in test reports. To address this challenge, AI can be effectively applied to produce fast as well as cost-effective solutions. Plenty of researchers come up with AI-based solutions for preliminary diagnosis using chest CT Images, respiratory sound analysis, voice analysis of symptomatic persons with asymptomatic ones, and so forth. Some AI-based applications claim good accuracy in predicting the chances of being COVID-19-positive. Within a short period, plenty of research work is published regarding the identification of COVID-19. This paper has carefully examined and presented a comprehensive survey of more than 110 papers that came from various reputed sources, that is, Springer, IEEE, Elsevier, MDPI, arXiv, and medRxiv. Most of the papers selected for this survey presented candid work to detect and classify COVID-19, using deep-learning-based models from chest X-Rays and CT scan images. We hope that this survey covers most of the work and provides insights to the research community in proposing efficient as well as accurate solutions for fighting the pandemic.", "journal": "Contrast media & molecular imaging", "date": "2022-10-29", "authors": ["DeepakSinwar", "Vijaypal SinghDhaka", "Biniyam AlemuTesfaye", "GhanshyamRaghuwanshi", "AshishKumar", "Sunil KrMaakar", "SanjayAgrawal"], "doi": "10.1155/2022/1306664\n10.1016/j.jbi.2008.05.013\n10.1038/s41591-020-0931-3\n10.1016/j.asoc.2020.106282\n10.1016/j.chaos.2020.110055\n10.1007/s42979-021-00923-y\n10.3390/ai1020009\n10.1016/j.chaos.2020.110059\n10.1016/S2214-109X(20)30068-1\n10.1016/j.clinimag.2020.02.008\n10.1148/ryct.2020200082\n10.1007/978-3-030-00889-5_1\n10.1101/2020.03.12.20027185\n10.1109/TCBB.2021.3065361\n10.1016/j.cell.2020.04.045\n10.1155/2022/7377502\n10.1148/radiol.2020200905\n10.1016/j.compbiomed.2020.103795\n10.1038/s41598-020-76282-0\n10.1007/s00330-021-07715-1\n10.1016/j.irbm.2020.05.003\n10.1007/s00330-020-06713-z\n10.1016/j.ejrad.2020.109041\n10.1109/RBME.2020.2987975\n10.12669/pjms.36.covid19-s4.2778\n10.1155/2022/8549707\n10.3390/ijerph18063056\n10.3390/healthcare9050522\n10.1016/j.chaos.2020.109944\n10.1155/2019/4180949\n10.1038/s41598-020-76550-z\n10.37896/jxu14.8/061\n10.1007/s10489-020-01867-1\n10.1007/s40846-020-00529-4\n10.1109/CVPR.2018.00474\n10.1016/j.compbiomed.2020.103805\n10.1016/j.compbiomed.2020.103792\n10.1007/s10044-021-00984-y\n10.1109/CVPR.2015.7298594\n10.1089/pop.2014.0089\n10.33889/ijmems.2020.5.4.052\n10.1109/CVPR.2017.243\n10.1016/j.cmpb.2020.105581\n10.1016/j.imu.2020.100360\n10.1109/ACCESS.2020.3010287\n10.1371/journal.pmed.1002686\n10.1007/s13246-020-00865-4\n10.1016/j.chaos.2020.110071\n10.1016/j.cmpb.2020.105608\n10.1016/j.mehy.2020.109761\n10.1016/j.compbiomed.2020.103869\n10.1016/j.cmpb.2020.105532\n10.1007/s00330-020-07044-9\n10.1007/s10489-020-01826-w\n10.1007/s10489-020-01831-z\n10.1007/s00264-020-04609-7\n10.1007/s00500-020-05275-y\n10.1155/2022/9697285\n10.1007/s42979-021-00823-1\n10.1101/2020.04.02.20051136v1\n10.1101/2020.04.02.20051136\n10.1088/1361-6560/abe838\n10.1007/s41870-022-00949-2\n10.1101/2020.02.29.20029603\n10.1101/2020.03.25.20043331\n10.1101/2020.04.04.20052092\n10.1016/j.chaos.2020.110086\n10.1186/s12859-018-2340-x\n10.3390/a13100249\n10.1007/s41870-022-00967-0\n10.1101/2020.07.02.20145474\n10.1186/s12911-020-01266-z\n10.1101/2020.05.20.20107847\n10.1101/2020.06.25.20140004\n10.1101/2020.06.01.20119560\n10.1080/09720502.2020.1833443\n10.1016/j.patter.2020.100145\n10.1101/2020.05.23.20110189\n10.1016/j.chaos.2020.110058\n10.1016/j.iot.2020.100222\n10.1016/j.dsx.2020.04.012\n10.1007/s41870-022-00973-2\n10.1016/j.dsx.2020.04.032\n10.1016/S2589-7500(20)30059-5\n10.1007/s42247-020-00102-4\n10.1109/access.2020.2992341\n10.1088/1757-899x/1099/1/012005\n10.1007/s00259-020-04953-1\n10.1007/s10489-020-01770-9\n10.1007/s42979-020-00410-w\n10.1007/s41870-020-00571-0\n10.1101/2020.04.24.20078584"}
{"title": "A CNN-transformer fusion network for COVID-19 CXR image classification.", "abstract": "The global health crisis due to the fast spread of coronavirus disease (Covid-19) has caused great danger to all aspects of healthcare, economy, and other aspects. The highly infectious and insidious nature of the new coronavirus greatly increases the difficulty of outbreak prevention and control. The early and rapid detection of Covid-19 is an effective way to reduce the spread of Covid-19. However, detecting Covid-19 accurately and quickly in large populations remains to be a major challenge worldwide. In this study, A CNN-transformer fusion framework is proposed for the automatic classification of pneumonia on chest X-ray. This framework includes two parts: data processing and image classification. The data processing stage is to eliminate the differences between data from different medical institutions so that they have the same storage format; in the image classification stage, we use a multi-branch network with a custom convolution module and a transformer module, including feature extraction, feature focus, and feature classification sub-networks. Feature extraction subnetworks extract the shallow features of the image and interact with the information through the convolution and transformer modules. Both the local and global features are extracted by the convolution module and transformer module of feature-focus subnetworks, and are classified by the feature classification subnetworks. The proposed network could decide whether or not a patient has pneumonia, and differentiate between Covid-19 and bacterial pneumonia. This network was implemented on the collected benchmark datasets and the result shows that accuracy, precision, recall, and F1 score are 97.09%, 97.16%, 96.93%, and 97.04%, respectively. Our network was compared with other researchers' proposed methods and achieved better results in terms of accuracy, precision, and F1 score, proving that it is superior for Covid-19 detection. With further improvements to this network, we hope that it will provide doctors with an effective tool for diagnosing Covid-19.", "journal": "PloS one", "date": "2022-10-28", "authors": ["KaiCao", "TaoDeng", "ChuanlinZhang", "LimengLu", "LinLi"], "doi": "10.1371/journal.pone.0276758\n10.1155/2021/2560388\n10.1001/jama.2020.1585\n10.1148/radiol.2020200432\n10.1155/2022/1465173\n10.1002/mp.13264\n10.1148/radiol.2020200463\n10.1136/bmj.m1328\n10.1016/j.media.2017.07.005\n10.1109/TPAMI.2019.2913372\n10.1016/j.compbiomed.2020.103792\n10.3389/fmed.2020.00427/full\n10.1016/j.media.2020.101794\n10.1038/s41598-020-76550-z\n10.3389/fpubh.2022.948205/full\n10.3389/fpubh.2022.948205\n10.1155/2022/4254631\n10.1016/j.mlwa.2021.100138\n10.1007/978-981-33-4673-4_55\n10.1148/radiol.2021203957\n10.1007/s10278-013-9622-7"}
{"title": "Checklist for Artificial Intelligence in Medical Imaging Reporting Adherence in Peer-Reviewed and Preprint Manuscripts With the Highest Altmetric Attention Scores: A Meta-Research Study.", "abstract": "", "journal": "Canadian Association of Radiologists journal = Journal l'Association canadienne des radiologistes", "date": "2022-10-28", "authors": ["UmasehSivanesan", "KayWu", "Matthew D FMcInnes", "KiretDhindsa", "FatemeSalehi", "Christian Bvan der Pol"], "doi": "10.1177/08465371221134056"}
{"title": "Classification and Detection of COVID-19 and Other Chest-Related Diseases Using Transfer Learning.", "abstract": "COVID-19 has infected millions of people worldwide over the past few years. The main technique used for COVID-19 detection is reverse transcription, which is expensive, sensitive, and requires medical expertise. X-ray imaging is an alternative and more accessible technique. This study aimed to improve detection accuracy to create a computer-aided diagnostic tool. Combining other artificial intelligence applications techniques with radiological imaging can help detect different diseases. This study proposes a technique for the automatic detection of COVID-19 and other chest-related diseases using digital chest X-ray images of suspected patients by applying transfer learning (TL) algorithms. For this purpose, two balanced datasets, Dataset-1 and Dataset-2, were created by combining four public databases and collecting images from recently published articles. Dataset-1 consisted of 6000 chest X-ray images with 1500 for each class. Dataset-2 consisted of 7200 images with 1200 for each class. To train and test the model, TL with nine pretrained convolutional neural networks (CNNs) was used with augmentation as a preprocessing method. The network was trained to classify using five classifiers: two-class classifier (normal and COVID-19); three-class classifier (normal, COVID-19, and viral pneumonia), four-class classifier (normal, viral pneumonia, COVID-19, and tuberculosis (Tb)), five-class classifier (normal, bacterial pneumonia, COVID-19, Tb, and pneumothorax), and six-class classifier (normal, bacterial pneumonia, COVID-19, viral pneumonia, Tb, and pneumothorax). For two, three, four, five, and six classes, our model achieved a maximum accuracy of 99.83, 98.11, 97.00, 94.66, and 87.29%, respectively.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-10-28", "authors": ["Muhammad TahirNaseem", "TajmalHussain", "Chan-SuLee", "Muhammad AdnanKhan"], "doi": "10.3390/s22207977\n10.1016/j.jaut.2020.102433\n10.1109/ACCESS.2020.3010287\n10.1109/ACCESS.2017.2788044\n10.1155/2022/6112815\n10.1007/s11042-019-07820-w\n10.1109/JBHI.2015.2425041\n10.1016/j.irbm.2020.05.003\n10.1016/j.compbiomed.2020.103795\n10.1007/s10916-021-01745-4\n10.1007/s10044-021-00984-y\n10.3390/sym12040651\n10.1007/s42600-021-00151-6\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792\n10.1007/s11263-015-0816-y\n10.1016/j.chaos.2020.109944\n10.1016/j.cmpb.2020.105581\n10.1038/s41598-020-76550-z\n10.1016/j.eswa.2020.114054\n10.1007/s00264-020-04609-7\n10.1155/2018/2908517\n10.1016/j.cell.2018.02.010\n10.1109/TMI.2013.2284099\n10.1109/TMI.2013.2290491\n10.1016/j.compbiomed.2019.04.024\n10.1145/3065386\n10.1186/s40537-019-0192-5\n10.1016/j.compbiomed.2021.104608"}
{"title": "Machine Learning Sensors for Diagnosis of COVID-19 Disease Using Routine Blood Values for Internet of Things Application.", "abstract": "Healthcare digitalization requires effective applications of human sensors, when various parameters of the human body are instantly monitored in everyday life due to the Internet of Things (IoT). In particular, machine learning (ML) sensors for the prompt diagnosis of COVID-19 are an important option for IoT application in healthcare and ambient assisted living (AAL). Determining a COVID-19 infected status with various diagnostic tests and imaging results is costly and time-consuming. This study provides a fast, reliable and cost-effective alternative tool for the diagnosis of COVID-19 based on the routine blood values (RBVs) measured at admission. The dataset of the study consists of a total of 5296 patients with the same number of negative and positive COVID-19 test results and 51 routine blood values. In this study, 13 popular classifier machine learning models and the LogNNet neural network model were exanimated. The most successful classifier model in terms of time and accuracy in the detection of the disease was the histogram-based gradient boosting (HGB) (accuracy: 100%, time: 6.39 sec). The HGB classifier identified the 11 most important features (LDL, cholesterol, HDL-C, MCHC, triglyceride, amylase, UA, LDH, CK-MB, ALP and MCH) to detect the disease with 100% accuracy. In addition, the importance of single, double and triple combinations of these features in the diagnosis of the disease was discussed. We propose to use these 11 features and their binary combinations as important biomarkers for ML sensors in the diagnosis of the disease, supporting edge computing on Arduino and cloud IoT service.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-10-28", "authors": ["AndreiVelichko", "Mehmet TahirHuyut", "MaksimBelyaev", "YuriyIzotov", "DmitryKorzun"], "doi": "10.3390/s22207886\n10.4103/2045-9912.325992\n10.1080/00365513.2020.1855470\n10.12659/MSM.926178\n10.1016/j.intimp.2021.108127\n10.1016/j.intimp.2021.107838\n10.4103/2045-9912.326002\n10.1515/cclm-2020-1294\n10.1038/s41564-020-0761-6\n10.1038/s41379-020-00700-x\n10.1039/C7LC00955K\n10.1016/j.irbm.2022.05.006\n10.3390/s22134820\n10.1016/S0140-6736(20)30211-7\n10.1016/j.intimp.2022.108542\n10.2174/2666082218666220325105504\n10.1056/NEJMoa2002032\n10.1016/j.intimp.2020.106705\n10.5505/ejm.2022.35336\n10.1007/s12026-020-09145-5\n10.1016/S0140-6736(20)30628-0\n10.3389/fimmu.2019.00055\n10.1002/jmv.26543\n10.1186/1741-7015-11-185\n10.1016/j.cca.2020.03.022\n10.1002/jcla.23618\n10.1111/ijlh.13309\n10.1016/S0140-6736(20)30566-3\n10.19166/med.v8i2.3444\n10.1016/j.ajem.2020.12.076\n10.1016/j.compbiomed.2019.103375\n10.1111/bjh.16817\n10.1515/cclm-2020-0377\n10.1016/S0004-3702(03)00079-1\n10.1007/3-540-57868-4_57\n10.1007/978-3-540-88192-6_8\n10.1111/j.2517-6161.1996.tb02080.x\n10.1002/emp2.12205\n10.4018/978-1-6684-5295-0.CH047\n10.1145/3491208\n10.48550/arxiv.2206.03266\n10.3390/s22124362\n10.1186/s12916-020-01761-0\n10.1002/ajh.25847\n10.1515/cclm-2020-0398\n10.1101/2020.04.02.20051136\n10.1016/j.measurement.2019.05.027\n10.1109/EMBC.2012.6347211\n10.1016/j.alit.2019.04.010\n10.3390/app10010421\n10.1016/j.ekir.2017.11.002\n10.1016/j.cmpb.2018.12.032\n10.1093/bioinformatics/bty949\n10.1371/journal.pone.0194085\n10.1093/clinchem/hvaa200\n10.1007/s10916-020-01597-4\n10.1016/j.jointm.2021.04.001\n10.1371/journal.pone.0264785\n10.1186/s40560-021-00531-1\n10.1038/s41598-021-90265-9\n10.1038/s41591-020-0931-3\n10.1016/j.cca.2020.06.033\n10.1182/blood-2018-06-856500\n10.20944/preprints202002.0424.v1\n10.1016/j.prp.2021.153443\n10.1042/BSR20200817\n10.1080/10408363.2020.1776675\n10.1002/hep.31480\n10.1002/ajh.25829\n10.1007/s00277-021-04426-x\n10.1016/j.bbalip.2020.158849\n10.3389/fpubh.2021.705916\n10.1016/j.jacl.2020.04.008\n10.1093/bjs/znaa168\n10.1186/s12931-020-01427-8\n10.1186/s12902-021-00745-2\n10.1016/j.advms.2021.07.001\n10.22037/ghfbb.v13i4.2129\n10.1101/2020.02.27.20029009\n10.1016/j.adengl.2019.09.003\n10.1007/s13555-020-00372-0\n10.1093/jamia/ocaa258\n10.1109/ACCESS.2020.3045115\n10.1109/ACCESS.2022.3197164\n10.1016/j.matpr.2021.07.379\n10.4018/IJERTCS.2019040108"}
{"title": "Role of Drone Technology Helping in Alleviating the COVID-19 Pandemic.", "abstract": "The COVID-19 pandemic, caused by a new coronavirus, has affected economic and social standards as governments and healthcare regulatory agencies throughout the world expressed worry and explored harsh preventative measures to counteract the disease's spread and intensity. Several academics and experts are primarily concerned with halting the continuous spread of the unique virus. Social separation, the closing of borders, the avoidance of big gatherings, contactless transit, and quarantine are important methods. Multiple nations employ autonomous, digital, wireless, and other promising technologies to tackle this coronary pneumonia. This research examines a number of potential technologies, including unmanned aerial vehicles (UAVs), artificial intelligence (AI), blockchain, deep learning (DL), the Internet of Things (IoT), edge computing, and virtual reality (VR), in an effort to mitigate the danger of COVID-19. Due to their ability to transport food and medical supplies to a specific location, UAVs are currently being utilized as an innovative method to combat this illness. This research intends to examine the possibilities of UAVs in the context of the COVID-19 pandemic from several angles. UAVs offer intriguing options for delivering medical supplies, spraying disinfectants, broadcasting communications, conducting surveillance, inspecting, and screening patients for infection. This article examines the use of drones in healthcare as well as the advantages and disadvantages of strict adoption. Finally, challenges, opportunities, and future work are discussed to assist in adopting drone technology to tackle COVID-19-like diseases.", "journal": "Micromachines", "date": "2022-10-28", "authors": ["Syed Agha HassnainMohsan", "Qurat Ul AinZahra", "Muhammad AsgharKhan", "Mohammed HAlsharif", "Ismail AElhaty", "AbuJahid"], "doi": "10.3390/mi13101593\n10.1109/IOTM.0011.2100053\n10.1017/dmp.2021.9\n10.3390/drones6010015\n10.1007/s10796-021-10131-x\n10.3389/frcmn.2020.566853\n10.1080/14649365.2021.1921245\n10.1177/2043820620934267\n10.1016/j.future.2020.08.046\n10.1016/j.retram.2020.01.002\n10.2139/ssrn.3565463\n10.1109/ACCESS.2018.2875739\n10.1109/ACCESS.2019.2905347\n10.3390/drones4040068\n10.1109/TMM.2021.3075566\n10.1109/TITS.2021.3113787\n10.1109/TII.2022.3174160\n10.3389/fpubh.2022.855994\n10.1007/s11655-020-3192-6\n10.1038/s41598-020-73510-5\n10.1002/er.6007\n10.3390/drones6060147\n10.3390/rs11070820\n10.3390/rs12213539\n10.3390/mi13060977\n10.1016/j.iot.2020.100218\n10.1145/3001836\n10.1016/j.adhoc.2020.102324\n10.1080/17538947.2021.1952324\n10.1049/ntw2.12040\n10.3390/drones5030058\n10.1002/ett.4255\n10.3390/drones5010018\n10.3390/drones6050109\n10.3390/ijerph18052637\n10.53553/JCH.v09i01.009\n10.1016/j.trip.2021.100453\n10.1016/j.ijhm.2020.102758\n10.1016/j.ajic.2022.03.004\n10.1016/j.vaccine.2016.06.022\n10.3390/ijerph17239117\n10.28991/HIJ-2020-01-02-03\n10.1007/s13205-020-02581-y\n10.1155/2022/9718580\n10.1109/MNET.011.2000439\n10.1007/s10462-021-10106-z\n10.1007/s41666-020-00080-6\n10.1016/j.ceh.2020.03.001\n10.4108/eai.13-7-2018.163997\n10.1016/j.clineuro.2021.106655\n10.1109/MCE.2020.2992034\n10.1101/2020.04.06.20039909\n10.1002/ett.4245\n10.1108/IJPCC-05-2020-0046\n10.1016/j.scs.2020.102589\n10.1136/bmjsem-2020-000943\n10.1016/j.jnca.2022.103341\n10.1109/TII.2021.3101651\n10.1007/s10586-022-03722-z\n10.1016/j.ijhcs.2020.102573\n10.1007/s43926-021-00005-8\n10.1016/j.bspc.2022.103658\n10.3390/app12062828\n10.3389/fnbot.2022.840594\n10.1016/j.techfore.2020.120431\n10.32604/cmc.2022.021850\n10.1007/s00607-021-01022-9\n10.1186/s12909-020-02245-8\n10.3389/fneur.2021.646902\n10.1007/s12311-020-01139-1\n10.1002/adma.202103646\n10.1016/j.jnlssr.2020.06.011\n10.1109/JBHI.2021.3103404\n10.1109/IOTM.1100.2000068\n10.1109/MWC.001.2000429\n10.1109/JSEN.2022.3188929\n10.1109/ACCESS.2021.3133796\n10.1007/s00607-022-01064-7\n10.3991/ijim.v15i22.22623\n10.1007/s11036-018-1193-x\n10.3390/drones4040065\n10.1016/j.jnca.2016.12.012\n10.1109/JIOT.2020.3007518\n10.1111/1758-5899.13007"}
{"title": "Machine Learning and Deep Learning in Cardiothoracic Imaging: A Scoping Review.", "abstract": "Machine-learning (ML) and deep-learning (DL) algorithms are part of a group of modeling algorithms that grasp the hidden patterns in data based on a training process, enabling them to extract complex information from the input data. In the past decade, these algorithms have been increasingly used for image processing, specifically in the medical domain. Cardiothoracic imaging is one of the early adopters of ML/DL research, and the COVID-19 pandemic resulted in more research focus on the feasibility and applications of ML/DL in cardiothoracic imaging. In this scoping review, we systematically searched available peer-reviewed medical literature on cardiothoracic imaging and quantitatively extracted key data elements in order to get a big picture of how ML/DL have been used in the rapidly evolving cardiothoracic imaging field. During this report, we provide insights on different applications of ML/DL and some nuances pertaining to this specific field of research. Finally, we provide general suggestions on how researchers can make their research more than just a proof-of-concept and move toward clinical adoption.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-10-28", "authors": ["BardiaKhosravi", "PouriaRouzrokh", "ShahriarFaghani", "ManaMoassefi", "SanazVahdati", "ElhamMahmoudi", "HamidChalian", "Bradley JErickson"], "doi": "10.3390/diagnostics12102512\n10.1007/s12525-021-00475-2\n10.4997/jrcpe.2020.309\n10.1093/eurheartj/ehw302\n10.3389/fdata.2018.00006\n10.3348/kjr.2017.18.4.570\n10.1001/jama.2018.1150\n10.1007/s00256-021-03876-8\n10.1038/s42256-021-00305-2\n10.1016/j.acra.2021.12.032\n10.1007/s00330-021-07781-5\n10.11622/smedj.2019141\n10.1016/j.jacr.2017.12.021\n10.1097/RTI.0000000000000453\n10.1038/s41598-021-84698-5\n10.1016/j.acra.2018.10.007\n10.1259/bjro.20200037\n10.1007/s00247-021-05177-7\n10.1016/j.media.2017.06.015\n10.1007/978-981-16-3783-4_15\n10.7326/M18-0850\n10.1007/s10462-021-10106-z\n10.1145/3065386\n10.5555/1593511\n10.1186/s12874-021-01404-9\n10.1007/s00330-020-07450-z\n10.1148/radiol.2020200432\n10.1148/radiol.2020204226\n10.1016/j.compbiomed.2021.104304\n10.1148/ryai.2019180041\n10.1118/1.3528204\n10.1016/j.cmpb.2021.106373\n10.3348/kjr.2021.0148\n10.3390/s21217059\n10.1007/s00330-019-06628-4\n10.1609/aaai.v33i01.3301590\n10.1186/s41747-018-0068-z\n10.1186/s13244-020-00887-2\n10.1148/ryai.220010\n10.1016/j.cmpb.2019.05.020\n10.1016/j.compbiomed.2022.105466\n10.1007/s00431-021-04061-8\n10.1155/2021/6050433\n10.1016/j.cmpb.2022.106815\n10.1155/2022/4185835\n10.1016/j.ejmp.2019.11.026\n10.1002/mp.15019\n10.3390/tomography7040054\n10.1055/a-1717-2703\n10.1016/j.cmpb.2019.105288\n10.1016/j.media.2020.101823\n10.1016/j.compbiomed.2021.104689\n10.1109/TMI.2018.2833385\n10.1016/j.artmed.2020.101975\n10.1016/j.morpho.2019.09.002\n10.1002/mp.14066\n10.1016/j.media.2022.102362\n10.1109/TMI.2021.3053008\n10.1088/1361-6560/ab1cee\n10.1371/journal.pone.0244745\n10.1016/j.media.2022.102491\n10.1088/1361-6560/ab18db\n10.3233/XST-17358\n10.1148/ryai.220061\n10.1148/radiol.2018182294\n10.1148/rg.2020200040\n10.1016/j.ejmp.2021.08.011\n10.1016/j.lungcan.2021.01.027\n10.1016/j.media.2022.102389\n10.1097/RLI.0000000000000707\n10.1148/ryai.2020190043\n10.1148/ryai.2021200267\n10.1007/s00530-022-00960-4\n10.1148/ryai.210290\n10.1016/j.acra.2021.09.007\n10.1097/RLI.0000000000000763\n10.1145/3531146.3533193"}
{"title": "Pseudo-Label Guided Image Synthesis for Semi-Supervised COVID-19 Pneumonia Infection Segmentation.", "abstract": "Coronavirus disease 2019 (COVID-19) has become a severe global pandemic. Accurate pneumonia infection segmentation is important for assisting doctors in diagnosing COVID-19. Deep learning-based methods can be developed for automatic segmentation, but the lack of large-scale well-annotated COVID-19 training datasets may hinder their performance. Semi-supervised segmentation is a promising solution which explores large amounts of unlabelled data, while most existing methods focus on pseudo-label refinement. In this paper, we propose a new perspective on semi-supervised learning for COVID-19 pneumonia infection segmentation, namely pseudo-label guided image synthesis. The main idea is to keep the pseudo-labels and synthesize new images to match them. The synthetic image has the same COVID-19 infected regions as indicated in the pseudo-label, and the reference style extracted from the style code pool is added to make it more realistic. We introduce two representative methods by incorporating the synthetic images into model training, including single-stage Synthesis-Assisted Cross Pseudo Supervision (SA-CPS) and multi-stage Synthesis-Assisted Self-Training (SA-ST), which can work individually as well as cooperatively. Synthesis-assisted methods expand the training data with high-quality synthetic data, thus improving the segmentation performance. Extensive experiments on two COVID-19 CT datasets for segmenting the infections demonstrate our method is superior to existing schemes for semi-supervised segmentation, and achieves the state-of-the-art performance on both datasets. Code is available at: https://github.com/FeiLyu/SASSL.", "journal": "IEEE transactions on medical imaging", "date": "2022-10-27", "authors": ["FeiLyu", "MangYe", "Jonathan FrederikCarlsen", "KennyErleben", "SuneDarkner", "Pong CYuen"], "doi": "10.1109/TMI.2022.3217501"}
{"title": "[The role of artificial intelligence in assessing the progression of fibrosing lung diseases].", "abstract": "The widespread use of artificial intelligence (AI) programs during the COVID-19 pandemic to assess the exact volume of lung tissue damage has allowed them to train a large number of radiologists. The simplicity of the program for determining the volume of the affected lung tissue in acute interstitial pneumonia, which has density indicators in the range from -200 HU to -730 HU, which includes the density indicators of \"ground glass\" and reticulation (the main radiation patterns in COVID-19) allows you to accurately determine the degree of prevalence process. The characteristics of chronic interstitial pneumonia, which are progressive in nature, fit into the same density framework. \u0410im. To \u0430ssess AI's ability to assess the progression of fibrosing lung disease using lung volume counting programs used for COVID-19 and chronic obstructive pulmonary disease.\nRetrospective analysis of computed tomography data during follow-up of 75 patients with progressive fibrosing lung disease made it possible to assess the prevalence and growth of interstitial lesions.\nUsing the experience of using AI programs to assess acute interstitial pneumonia in COVID-19 can be applied to chronic interstitial pneumonia.\n\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0432 \u043f\u0435\u0440\u0438\u043e\u0434 \u043f\u0430\u043d\u0434\u0435\u043c\u0438\u0438 COVID-19 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0430 (\u0418\u0418) \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0442\u043e\u0447\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043c\u0430 \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0439 \u0442\u043a\u0430\u043d\u0438 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u043b\u043e \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0438\u043c \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u0440\u0435\u043d\u0442\u0433\u0435\u043d\u043e\u043b\u043e\u0433\u043e\u0432. \u041f\u0440\u043e\u0441\u0442\u043e\u0442\u0430 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u043d\u043e\u0439 \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0439 \u0442\u043a\u0430\u043d\u0438 \u043f\u0440\u0438 \u043e\u0441\u0442\u0440\u043e\u0439 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043f\u043d\u0435\u0432\u043c\u043e\u043d\u0438\u0438, \u0438\u043c\u0435\u044e\u0449\u0435\u0439 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u0432 \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043a\u0435 \u043e\u0442 -200 \u0435\u0434\u0438\u043d\u0438\u0446 \u0425\u0430\u0443\u043d\u0441\u0444\u0438\u043b\u0434\u0430 (HU) \u0434\u043e -730 HU, \u043a\u0443\u0434\u0430 \u0432\u0445\u043e\u0434\u044f\u0442 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u043c\u0430\u0442\u043e\u0432\u043e\u0433\u043e \u0441\u0442\u0435\u043a\u043b\u0430 \u0438 \u0440\u0435\u0442\u0438\u043a\u0443\u043b\u044f\u0446\u0438\u0438 (\u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u043b\u0443\u0447\u0435\u0432\u044b\u0445 \u043f\u0430\u0442\u0442\u0435\u0440\u043d\u043e\u0432 \u043f\u0440\u0438 COVID-19), \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0441\u0442\u0435\u043f\u0435\u043d\u044c \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430. \u0412 \u0442\u0435 \u0436\u0435 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u0440\u0430\u043c\u043a\u0438 \u0443\u043a\u043b\u0430\u0434\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u0445\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0444\u0438\u0431\u0440\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u0445 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u0430\u0431\u043e\u043b\u0435\u0432\u0430\u043d\u0438\u0439 \u043b\u0435\u0433\u043a\u0438\u0445, \u0438\u043c\u0435\u044e\u0449\u0438\u0445 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0438\u0440\u0443\u044e\u0449\u0438\u0439 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440. \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043a\u0438\u0441\u0442\u043e\u0437\u043d\u043e\u0433\u043e \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u043e\u0431\u0441\u0447\u0435\u0442\u0430 \u044d\u043c\u0444\u0438\u0437\u0435\u043c\u044b \u0434\u0430\u0435\u0442 \u043f\u043e\u043b\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0431 \u043e\u0431\u044a\u0435\u043c\u0435 \u0443\u0442\u0440\u0430\u0447\u0435\u043d\u043d\u043e\u0439 \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0439 \u0442\u043a\u0430\u043d\u0438. \u0426\u0435\u043b\u044c. \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0418\u0418 \u0432 \u043e\u0446\u0435\u043d\u043a\u0435 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0444\u0438\u0431\u0440\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u0445 \u0431\u043e\u043b\u0435\u0437\u043d\u0435\u0439 \u043b\u0435\u0433\u043a\u0438\u0445. \u041c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u044b \u0438 \u043c\u0435\u0442\u043e\u0434\u044b. \u0420\u0435\u0442\u0440\u043e\u0441\u043f\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u043e\u0439 \u0442\u043e\u043c\u043e\u0433\u0440\u0430\u0444\u0438\u0438 \u043f\u0440\u0438 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0438 75 \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u0441 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0438\u0440\u0443\u044e\u0449\u0438\u043c\u0438 \u0444\u0438\u0431\u0440\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u043c\u0438 \u0431\u043e\u043b\u0435\u0437\u043d\u044f\u043c\u0438 \u043b\u0435\u0433\u043a\u0438\u0445 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u043b \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0430\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u0435 \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b. \u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043e\u0431\u044a\u0435\u043c\u0430 \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0439 \u0442\u043a\u0430\u043d\u0438 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u043b\u0438 \u0442\u043e\u0447\u043d\u043e \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u0441\u0442\u0435\u043f\u0435\u043d\u044c \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u043f\u0440\u0438 \u0444\u0438\u0431\u0440\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u0445 \u0431\u043e\u043b\u0435\u0437\u043d\u044f\u0445 \u043b\u0435\u0433\u043a\u0438\u0445 \u0438 \u043a\u043e\u0440\u0440\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0441 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0433\u043e \u0440\u0435\u0441\u0443\u0440\u0441\u0430 \u0434\u0438\u0444\u0444\u0443\u0437\u0438\u043e\u043d\u043d\u043e\u0439 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c\u044e \u043b\u0435\u0433\u043a\u0438\u0445. \u0421\u0442\u0435\u043f\u0435\u043d\u044c \u043d\u0430\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u043f\u0440\u0438 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u0438 \u043e\u0431\u043e\u0441\u0442\u0440\u0435\u043d\u0438\u0438 \u0438\u043c\u0435\u043b\u0430 \u0432\u044b\u0441\u043e\u043a\u0443\u044e \u0441\u0432\u044f\u0437\u044c \u043f\u043e \u0448\u043a\u0430\u043b\u0435 \u0427\u0435\u0434\u0434\u043e\u043a\u0430 (p0,05; r=0,72) \u0441\u043e \u0441\u0442\u0435\u043f\u0435\u043d\u044c\u044e \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044f \u0434\u0438\u0444\u0444\u0443\u0437\u0438\u043e\u043d\u043d\u043e\u0439 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u0438 \u043b\u0435\u0433\u043a\u0438\u0445. \u0421\u043e\u0447\u0435\u0442\u0430\u043d\u0438\u0435 \u0438\u0434\u0438\u043e\u043f\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0433\u043e \u0444\u0438\u0431\u0440\u043e\u0437\u0430 \u0441 \u0445\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043e\u0431\u0441\u0442\u0440\u0443\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u0431\u043e\u043b\u0435\u0437\u043d\u044c\u044e \u043b\u0435\u0433\u043a\u0438\u0445, \u0432\u044b\u044f\u0432\u043b\u0435\u043d\u043d\u043e\u0435 \u0443 23 (30,6%) \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u043e\u0432, \u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u044c \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u044f\u0436\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0441\u043e\u0442\u043e\u0432\u043e\u0433\u043e \u043b\u0435\u0433\u043a\u043e\u0433\u043e \u0441 \u043a\u0440\u0443\u043f\u043d\u044b\u043c \u0434\u0438\u0430\u043c\u0435\u0442\u0440\u043e\u043c \u0441\u043e\u0442, \u043e\u0442\u043c\u0435\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0443 42 (56%) \u0431\u043e\u043b\u044c\u043d\u044b\u0445, \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043b\u043e \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0441\u043e\u0447\u0435\u0442\u0430\u043d\u043d\u043e\u0433\u043e \u043f\u043e\u0434\u0441\u0447\u0435\u0442\u0430 \u043e\u0431\u044a\u0435\u043c\u0430 \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0439 \u0442\u043a\u0430\u043d\u0438 \u0441 \u0432\u044b\u0447\u043b\u0435\u043d\u0435\u043d\u0438\u0435\u043c \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u0435\u0439, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0445 \u043a\u0430\u043a \u0434\u043b\u044f COVID-19-\u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043b\u0435\u0433\u043a\u0438\u0445, \u0442\u0430\u043a \u0438 \u0434\u043b\u044f \u043a\u0438\u0441\u0442\u043e\u0437\u043d\u043e-\u044d\u043c\u0444\u0438\u0437\u0435\u043c\u0430\u0442\u043e\u0437\u043d\u043e\u0433\u043e \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044f \u0442\u043e\u0447\u043d\u043e \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u0441\u0442\u0435\u043f\u0435\u043d\u044c \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0438 \u043a\u0438\u0441\u0442\u043e\u0437\u043d\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u043f\u0440\u0438 \u0438\u0434\u0438\u043e\u043f\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u043c \u0444\u0438\u0431\u0440\u043e\u0437\u0435. \u0417\u0430\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435. \u041e\u043f\u044b\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c \u0418\u0418 \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043e\u0441\u0442\u0440\u043e\u0439 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043f\u043d\u0435\u0432\u043c\u043e\u043d\u0438\u0438 \u043f\u0440\u0438 COVID-19 \u0438 \u044d\u043c\u0444\u0438\u0437\u0435\u043c\u044b \u043f\u0440\u0438 \u0445\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043e\u0431\u0441\u0442\u0440\u0443\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u0431\u043e\u043b\u0435\u0437\u043d\u0438 \u043b\u0435\u0433\u043a\u0438\u0445 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0445\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0444\u0438\u0431\u0440\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u0445 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u0430\u0431\u043e\u043b\u0435\u0432\u0430\u043d\u0438\u0439 \u043b\u0435\u0433\u043a\u0438\u0445, \u0447\u0442\u043e \u0432\u0430\u0436\u043d\u043e \u0434\u043b\u044f \u0441\u0432\u043e\u0435\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0430\u043d\u0442\u0438\u0444\u0438\u0431\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0442\u0435\u0440\u0430\u043f\u0438\u0438.", "journal": "Terapevticheskii arkhiv", "date": "2022-10-27", "authors": ["A ASperanskaia"], "doi": "10.26442/00403660.2022.03.201407"}
{"title": "Comprehensive Survey of Machine Learning Systems for COVID-19 Detection.", "abstract": "The last two years are considered the most crucial and critical period of the COVID-19 pandemic affecting most life aspects worldwide. This virus spreads quickly within a short period, increasing the fatality rate associated with the virus. From a clinical perspective, several diagnosis methods are carried out for early detection to avoid virus propagation. However, the capabilities of these methods are limited and have various associated challenges. Consequently, many studies have been performed for COVID-19 automated detection without involving manual intervention and allowing an accurate and fast decision. As is the case with other diseases and medical issues, Artificial Intelligence (AI) provides the medical community with potential technical solutions that help doctors and radiologists diagnose based on chest images. In this paper, a comprehensive review of the mentioned AI-based detection solution proposals is conducted. More than 200 papers are reviewed and analyzed, and 145 articles have been extensively examined to specify the proposed AI mechanisms with chest medical images. A comprehensive examination of the associated advantages and shortcomings is illustrated and summarized. Several findings are concluded as a result of a deep analysis of all the previous works using machine learning for COVID-19 detection, segmentation, and classification.", "journal": "Journal of imaging", "date": "2022-10-27", "authors": ["BayanAlsaaidah", "Moh'd RasoulAl-Hadidi", "HebaAl-Nsour", "RajaMasadeh", "NaelAlZubi"], "doi": "10.3390/jimaging8100267\n10.1177/2347631120983481\n10.1016/j.jbusres.2020.05.030\n10.1080/16549716.2020.1788263\n10.11591/ijece.v10i5.pp4738-4744\n10.1148/rg.2017160130\n10.1016/j.jmir.2019.09.005\n10.1147/rd.33.0210\n10.1109/MCE.2016.2640698\n10.25046/aj030435\n10.1007/s10514-022-10039-8\n10.1016/j.crad.2018.05.015\n10.1038/nature14539\n10.1021/acs.chas.0c00075\n10.1561/2000000039\n10.1016/j.neunet.2014.09.003\n10.1109/TSMC.1971.4308320\n10.1109/5.726791\n10.1016/j.knosys.2018.10.034\n10.1145/2347736.2347755\n10.1016/j.jinf.2020.04.004\n10.1016/j.imu.2020.100449\n10.1038/s41598-021-93719-2\n10.3390/s22062224\n10.1093/clinchem/hvaa200\n10.1016/j.chaos.2020.110120\n10.1007/s10462-021-10106-z\n10.1016/j.eswa.2022.116540\n10.34306/ajri.v3i2.659\n10.1111/exsy.12759\n10.1007/s11042-020-09894-3\n10.1109/TIP.2021.3058783\n10.1109/JBHI.2021.3074893\n10.1007/s12559-020-09785-7\n10.1007/s13246-020-00865-4\n10.1109/ACCESS.2020.3010287\n10.1016/j.cmpb.2020.105608\n10.1109/JBHI.2020.3037127\n10.1016/j.patrec.2020.09.010\n10.1007/s11042-021-10783-6\n10.3389/fmed.2020.00427\n10.1007/s10489-020-01826-w\n10.1038/s41598-020-74539-2\n10.1109/ACCESS.2020.2994762\n10.1145/3422622\n10.1016/j.compbiomed.2012.12.004\n10.1002/mp.14676\n10.1109/TMI.2020.2996645\n10.1016/j.patcog.2021.108109\n10.1155/2021/5544742\n10.3390/s21217116\n10.1016/j.compbiomed.2020.103869\n10.1016/j.compbiomed.2020.104066\n10.1016/j.compbiomed.2021.104348\n10.1016/j.compbiomed.2021.104781\n10.1016/j.chaos.2020.110170\n10.1016/j.eswa.2020.114054\n10.1016/j.aej.2021.06.024\n10.1016/j.compbiomed.2021.104572\n10.1016/j.imu.2020.100412\n10.20944/preprints202003.0300.v1\n10.1007/s11042-021-11299-9\n10.1101/2020.04.11.20054643\n10.1016/j.asoc.2021.108190\n10.1101/2020.05.10.20097063\n10.1101/2020.07.02.20136721\n10.1016/j.imu.2022.100945\n10.1101/2020.11.08.20228080\n10.1016/j.compmedimag.2021.102008\n10.1007/s40846-020-00529-4\n10.1101/2020.03.30.20047787\n10.1080/07391102.2020.1788642\n10.1080/07391102.2021.1875049\n10.1109/TII.2021.3057524\n10.1016/j.radi.2020.10.018\n10.1016/j.imu.2020.100505\n10.1016/j.inffus.2021.04.008\n10.1007/s42600-020-00091-7\n10.1007/s10489-020-02010-w\n10.3390/ijerph18158052\n10.1002/ima.22525\n10.1002/ima.22527\n10.31661/jbpe.v0i0.2008-1153\n10.1002/jemt.23713\n10.1371/journal.pone.0242899\n10.3390/jpm11010028\n10.1007/s40747-020-00199-4\n10.1007/s11760-020-01820-2\n10.1016/j.patcog.2021.107848\n10.1016/j.compbiomed.2020.104037\n10.1007/s10044-021-00984-y\n10.1152/physiolgenomics.00084.2020\n10.14299/ijser.2020.03.02\n10.20944/preprints202009.0524.v1\n10.1148/radiol.2020200905\n10.1007/s12539-020-00403-6\n10.1007/s00530-021-00826-1\n10.1186/s12938-020-00807-x\n10.1186/s12938-020-00831-x\n10.1038/s41598-020-76141-y\n10.1007/s40747-020-00216-6\n10.1007/s10096-020-03901-z\n10.1007/s10489-020-02149-6\n10.1007/s00259-020-04929-1\n10.1007/s00521-021-06219-9\n10.3390/sym12040651\n10.1109/JAS.2020.1003393\n10.1016/j.patcog.2020.107747\n10.1109/ACCESS.2021.3058854\n10.1016/j.bbe.2021.01.002\n10.18517/ijaseit.10.2.11446\n10.1109/TMI.2020.2995965\n10.1007/s10489-020-01829-7\n10.1007/s42600-020-00110-7\n10.1080/02664763.2020.1849057\n10.1016/j.compbiomed.2020.103792\n10.3390/bioengineering8020026\n10.1016/j.asoc.2022.108966\n10.1155/2022/7377502\n10.3389/fdgth.2021.662343\n10.1049/ipr2.12474"}
{"title": "Deep Transfer Learning for COVID-19 Detection and Lesion Recognition Using Chest CT Images.", "abstract": "Starting from December 2019, the global pandemic of coronavirus disease 2019 (COVID-19) is continuously expanding and has caused several millions of deaths worldwide. Fast and accurate diagnostic methods for COVID-19 detection play a vital role in containing the plague. Chest computed tomography (CT) is one of the most commonly used diagnosis methods. However, a complete CT-scan has hundreds of slices, and it is time-consuming for radiologists to check each slice to diagnose COVID-19. This study introduces a novel method for fast and automated COVID-19 diagnosis using the chest CT scans. The proposed models are based on the state-of-the-art deep convolutional neural network (CNN) architecture, and a 2D global max pooling (globalMaxPool2D) layer is used to improve the performance. We compare the proposed models to the existing state-of-the-art deep learning models such as CNN based models and vision transformer (ViT) models. Based off of metric such as area under curve (AUC), sensitivity, specificity, accuracy, and false discovery rate (FDR), experimental results show that the proposed models outperform the previous methods, and the best model achieves an area under curve of 0.9744 and accuracy 94.12% on our test datasets. It is also shown that the accuracy is improved by around 1% by using the 2D global max pooling layer. Moreover, a heatmap method to highlight the lesion area on COVID-19 chest CT images is introduced in the paper. This heatmap method is helpful for a radiologist to identify the abnormal pattern of COVID-19 on chest CT images. In addition, we also developed a freely accessible online simulation software for automated COVID-19 detection using CT images. The proposed deep learning models and software tool can be used by radiologist to diagnose COVID-19 more accurately and efficiently.", "journal": "Computational and mathematical methods in medicine", "date": "2022-10-27", "authors": ["SaiZhang", "Guo-ChangYuan"], "doi": "10.1155/2022/4509394\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1111/all.14238\n10.1016/j.jhin.2020.03.001\n10.1093/cid/ciaa344\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1007/s00330-020-06975-7\n10.1016/S1473-3099(20)30086-4\n10.1007/s00330-020-06731-x\n10.1021/acssensors.0c02042\n10.1146/annurev-bioeng-071516-044442\n10.1007/978-1-4471-4929-3\n10.1155/2018/5157020\n10.1016/S2213-2600(18)30286-8\n10.1148/radiol.2020192154\n10.1136/gutjnl-2017-314547\n10.1093/mind/LIX.236.433\n10.1162/089976602760128018\n10.1155/2017/8314740\n10.1109/FG.2017.42\n10.1109/HealthCom.2017.8210843\n10.1016/j.compbiomed.2021.104306\n10.1016/j.compbiomed.2020.103795\n10.1038/s41591-020-0931-3\n10.1016/j.ejrad.2020.109402\n10.1016/j.compbiomed.2021.104348\n10.1016/j.imu.2020.100427\n10.7717/peerj.10086\n10.1016/j.patrec.2020.10.001\n10.3390/healthcare10010085\n10.1109/TENSYMP52854.2021.9550819\n10.1007/978-1-4842-2766-4\n10.1109/ICEngTechnol.2017.8308186\n10.1145/3065386\n10.1609/aaai.v31i1.11231\n10.3390/s22082988\n10.1007/s11263-019-01228-7"}
{"title": "Chest radiograph classification and severity of suspected COVID-19 by different radiologist groups and attending clinicians: multi-reader, multi-case study.", "abstract": "To quantify reader agreement for the British Society of Thoracic Imaging (BSTI) diagnostic and severity classification for COVID-19 on chest radiographs (CXR), in particular agreement for an indeterminate CXR that could instigate CT imaging, from single and paired images.\nTwenty readers (four groups of five individuals)-consultant chest (CCR), general consultant (GCR), and specialist registrar (RSR) radiologists, and infectious diseases clinicians (IDR)-assigned BSTI categories and severity in addition to modified Covid-Radiographic Assessment of Lung Edema Score (Covid-RALES), to 305 CXRs (129 paired; 2 time points) from 176 guideline-defined COVID-19 patients. Percentage agreement with a consensus of two chest radiologists was calculated for (1) categorisation to those needing CT (indeterminate) versus those that did not (classic/probable, non-COVID-19); (2) severity; and (3) severity change on paired CXRs using the two scoring systems.\nAgreement with consensus for the indeterminate category was low across all groups (28-37%). Agreement for other BSTI categories was highest for classic/probable for the other three reader groups (66-76%) compared to GCR (49%). Agreement for normal was similar across all radiologists (54-61%) but lower for IDR (31%). Agreement for a severe CXR was lower for GCR (65%), compared to the other three reader groups (84-95%). For all groups, agreement for changes across paired CXRs was modest.\nAgreement for the indeterminate BSTI COVID-19 CXR category is low, and generally moderate for the other BSTI categories and for severity change, suggesting that the test, rather than readers, is limited in utility for both deciding disposition and serial monitoring.\n\u2022 Across different reader groups, agreement for COVID-19 diagnostic categorisation on CXR varies widely. \u2022 Agreement varies to a degree that may render CXR alone ineffective for triage, especially for indeterminate cases. \u2022 Agreement for serial CXR change is moderate, limiting utility in guiding management.", "journal": "European radiology", "date": "2022-10-26", "authors": ["ArjunNair", "AlexanderProcter", "SteveHalligan", "ThomasParry", "AsiaAhmed", "MarkDuncan", "MagaliTaylor", "ManilChouhan", "TrevorGaunt", "JamesRoberts", "Nielsvan Vucht", "AlanCampbell", "Laura MayDavis", "JosephJacob", "RachelHubbard", "ShankarKumar", "AmmaarahSaid", "XinhuiChan", "TimCutfield", "AkishLuintel", "MichaelMarks", "NeilStone", "SueMallet"], "doi": "10.1007/s00330-022-09172-w\n10.1016/j.crad.2020.03.008\n10.1148/radiol.2020201160\n10.1001/jamainternmed.2020.2033\n10.1097/RTI.0000000000000533\n10.1007/s11547-020-01272-1\n10.4269/ajtmh.20-0535\n10.1148/radiol.2020201754\n10.1007/s00330-020-07270-1\n10.1148/radiol.2020201874\n10.1038/s41598-020-79470-0\n10.1371/journal.pone.0242759\n10.1016/j.crad.2020.06.005\n10.1016/j.ejrad.2020.109272\n10.1186/s12916-020-01810-8\n10.1186/s43055-021-00541-x\n10.1183/13993003.01809-2020"}
{"title": "Diagnostic performance of corona virus disease 2019 chest computer tomography image recognition based on deep learning: Systematic review and meta-analysis.", "abstract": "To analyze the diagnosis performance of deep learning model used in corona virus disease 2019 (COVID-19) computer tomography(CT) chest scans. The included sample contains healthy people, confirmed COVID-19 patients and unconfirmed suspected patients with corresponding symptoms.\nPubMed, Web of Science, Wiley, China National Knowledge Infrastructure, WAN FANG DATA, and Cochrane Library were searched for articles. Three researchers independently screened the literature, extracted the data. Any differences will be resolved by consulting the third author to ensure that a highly reliable and useful research paper is produced. Data were extracted from the final articles, including: authors, country of study, study type, sample size, participant demographics, type and name of AI software, results (accuracy, sensitivity, specificity, ROC, and predictive values), other outcome(s) if applicable.\nAmong the 3891 searched results, 32 articles describing 51,392 confirmed patients and 7686 non-infected individuals met the inclusion criteria. The pooled sensitivity, the pooled specificity, positive likelihood ratio, negative likelihood ratio and the pooled diagnostic odds ratio (OR) is 0.87(95%CI [confidence interval]: 0.85, 0.89), 0.85(95%CI: 0.82, 0.87), 6.7(95%CI: 5.7, 7.8), 0.14(95%CI: 0.12, 0.16), and 49(95%CI: 38, 65). Further, the AUROC (area under the receiver operating characteristic curve) is 0.94(95%CI: 0.91, 0.96). Secondary outcomes are specific sensitivity and specificity within subgroups defined by different models. Resnet has the best diagnostic performance, which has the highest sensitivity (0.91[95%CI: 0.87, 0.94]), specificity (0.90[95%CI: 0.86, 0.93]) and AUROC (0.96[95%CI: 0.94, 0.97]), according to the AUROC, we can get the rank Resnet > Densenet > VGG > Mobilenet > Inception > Effficient > Alexnet.\nOur study findings show that deep learning models have immense potential in accurately stratifying COVID-19 patients and in correctly differentiating them from patients with other types of pneumonia and normal patients. Implementation of deep learning-based tools can assist radiologists in correctly and quickly detecting COVID-19 and, consequently, in combating the COVID-19 pandemic.", "journal": "Medicine", "date": "2022-10-26", "authors": ["QiaolanWang", "JingxuanMa", "LuoningZhang", "LinshenXie"], "doi": "10.1097/MD.0000000000031346"}
{"title": "Automated diagnosis of COVID-19 using radiological modalities and Artificial Intelligence functionalities: A retrospective study based on chest HRCT database.", "abstract": "The spread of coronavirus has been challenging for the healthcare system's proper management and diagnosis during the rapid spread and control of the infection. Real-time reverse transcription-polymerase chain reaction (RT-PCR), though considered the standard testing measure, has low sensitivity and is time-consuming, which restricts the fast screening of individuals. Therefore, computer tomography (CT) is used to complement the traditional approaches and provide fast and effective screening over other diagnostic methods. This work aims to appraise the importance of chest CT findings of COVID-19 and post-COVID in the diagnosis and prognosis of infected patients and to explore the ways and means to integrate CT findings for the development of advanced Artificial Intelligence (AI) tool-based predictive diagnostic techniques.\nThe retrospective study includes a 188 patient database with COVID-19 infection confirmed by RT-PCR testing, including post-COVID patients. Patients underwent chest high-resolution computer tomography (HRCT), where the images were evaluated for common COVID-19 findings and involvement of the lung and its lobes based on the coverage region. The radiological modalities analyzed in this study may help the researchers in generating a predictive model based on AI tools for further classification with a high degree of reliability.\nMild to moderate ground glass opacities (GGO) with or without consolidation, crazy paving patterns, and halo signs were common COVID-19 related findings. A CT score is assigned to every patient based on the severity of lung lobe involvement.\nTypical multifocal, bilateral, and peripheral distributions of GGO are the main characteristics related to COVID-19 pneumonia. Chest HRCT can be considered a standard method for timely and efficient assessment of disease progression and management severity. With its fusion with AI tools, chest HRCT can be used as a one-stop platform for radiological investigation and automated diagnosis system.", "journal": "Biomedical signal processing and control", "date": "2022-10-25", "authors": ["UpasanaBhattacharjya", "Kandarpa KumarSarma", "Jyoti PrakashMedhi", "Binoy KumarChoudhury", "GeetanjaliBarman"], "doi": "10.1016/j.bspc.2022.104297\n10.1101/2020.02\n10.1101/2020.03.12.20027185"}
{"title": "The Deep Learning-Based Framework for Automated Predicting COVID-19 Severity Score.", "abstract": "With the COVID-19 pandemic sweeping the globe, an increasing number of people are working on pandemic research, but there is less effort on predicting its severity. Diagnostic chest imaging is thought to be a quick and reliable way to identify the severity of COVID-19. We describe a deep learning method to automatically predict the severity score of patients by analyzing chest X-rays, with the goal of collaborating with doctors to create corresponding treatment measures for patients and can also be used to track disease change. Our model consists of a feature extraction phase and an outcome prediction phase. The feature extraction phase uses a DenseNet backbone network to extract 18 features related to lung diseases from CXRs; the outcome prediction phase, which employs the MLP regression model, selects several important features for prediction from the features extracted in the previous phase and demonstrates the effectiveness of our model by comparing it with several commonly used regression models. On a dataset of 2373 CXRs, our model predicts the geographic extent score with 1.02 MAE and the lung opacity score with 0.85 MAE.", "journal": "Procedia computer science", "date": "2022-10-25", "authors": ["YongchangZheng", "HongweiDong"], "doi": "10.1016/j.procs.2022.09.165"}
{"title": "Effects of long-term COVID-19 confinement and music stimulation on mental state and brain activity of young people.", "abstract": "The Corona Virus Disease 2019 (COVID-19) pandemic may have had a negative emotional impact on individuals. This study investigated the effect of long-term lockdown and music on young people's mood and neurophysiological responses in the prefrontal cortex (PFC). Fifteen healthy young adults were recruited and PFC activation was acquired using functional near-infrared spectroscopy during the conditions of resting, Stroop and music stimulation. The Depression Anxiety Stress Scales mental scale scores were simultaneously recorded. Mixed effect models, paired t-tests, one-way ANOVAs and Spearman analyses were adopted to analyse the experimental parameters. Stress, anxiety and depression levels increased significantly from Day 30 to Day 40. In terms of reaction time, both Stroop1 and Stroop2 were faster on Day 40 than on Day 30 (P\u00a0=\u00a00.01, P\u00a0=\u00a00.003). The relative concentration changes of oxyhemoglobin were significantly higher during premusic conditions than music stimulation and postmusic Stroop. The intensity of functional connectivity shifted from inter- to intracerebral over time. In conclusion, the reduced hemodynamic response of the PFC in healthy young adults is associated with negative emotions, especially anxiety, during lockdown. Immediate music stimulation appears to improve efficiency by altering the pattern of connections in PFC.", "journal": "Neuroscience letters", "date": "2022-10-23", "authors": ["LinaLuo", "MianjiaShan", "YangminZu", "YufangChen", "LingguoBu", "LejunWang", "MingNi", "WenxinNiu"], "doi": "10.1016/j.neulet.2022.136922"}
{"title": "Automatic deep learning-based consolidation/collapse classification in lung ultrasound images for COVID-19 induced pneumonia.", "abstract": "Our automated deep learning-based approach identifies consolidation/collapse in LUS images to aid in the identification of late stages of COVID-19 induced pneumonia, where consolidation/collapse is one of the possible associated pathologies. A common challenge in training such models is that annotating each frame of an ultrasound video requires high labelling effort. This effort in practice becomes prohibitive for large ultrasound datasets. To understand the impact of various degrees of labelling precision, we compare labelling strategies to train fully supervised models (frame-based method, higher labelling effort) and inaccurately supervised models (video-based methods, lower labelling effort), both of which yield binary predictions for LUS videos on a frame-by-frame level. We moreover introduce a novel sampled quaternary method which randomly samples only 10% of the LUS video frames and subsequently assigns (ordinal) categorical labels to all frames in the video based on the fraction of positively annotated samples. This method outperformed the inaccurately supervised video-based method and more surprisingly, the supervised frame-based approach with respect to metrics such as precision-recall area under curve (PR-AUC) and F1 score, despite being a form of inaccurate learning. We argue that our video-based method is more robust with respect to label noise and mitigates overfitting in a manner similar to label smoothing. The algorithm was trained using a ten-fold cross validation, which resulted in a PR-AUC score of 73% and an accuracy of 89%. While the efficacy of our classifier using the sampled quaternary method significantly lowers the labelling effort, it must be verified on a larger consolidation/collapse dataset, our proposed classifier using the sampled quaternary video-based method is clinically comparable with trained experts' performance.", "journal": "Scientific reports", "date": "2022-10-21", "authors": ["NabeelDurrani", "DamjanVukovic", "Jeroenvan der Burgt", "MariaAntico", "Ruud J Gvan Sloun", "DavidCanty", "MarianSteffens", "AndrewWang", "AlistairRoyse", "ColinRoyse", "KaviHaji", "JasonDowling", "GirijaChetty", "DavideFontanarosa"], "doi": "10.1038/s41598-022-22196-y\n10.1016/j.compbiomed.2021.104742\n10.1186/s12931-020-01504-y\n10.1016/j.hrtlng.2021.02.015\n10.1007/s12630-020-01704-6\n10.1111/1742-6723.13546\n10.1186/s13089-018-0103-6\n10.1016/j.crad.2020.05.001\n10.1109/TMI.2021.3117246\n10.1109/TUFFC.2021.3070696\n10.1109/TMI.2020.2994459\n10.1109/TUFFC.2022.3161716\n10.1016/j.imu.2021.100687\n10.1109/TUFFC.2020.3002249\n10.1109/TUFFC.2020.3005512\n10.1002/jum.16052\n10.1038/s41598-021-90153-2\n10.1016/j.ejmp.2021.02.023\n10.1186/s13063-019-4003-2\n10.1002/jum.15548\n10.1097/00005382-199621000-00002\n10.1093/nsr/nwx106\n10.1118/1.3611983\n10.2307/2095465\n10.1016/j.ipl.2005.11.003\n10.1371/journal.pone.0118432\n10.1007/978-3-642-40994-3_29\n10.11613/BM.2012.031"}
{"title": "COVID19 Diagnosis Using Chest X-rays and Transfer Learning.", "abstract": "A pandemic of respiratory illnesses from a novel coronavirus known as Sars-CoV-2 has swept across the globe since December of 2019. This is calling upon the research community including medical imaging to provide effective tools for use in combating this virus. Research in biomedical imaging of viral patients is already very active with machine learning models being created for diagnosing Sars-CoV-2 infections in patients using CT scans and chest x-rays. We aim to build upon this research. Here we used a transfer-learning approach to develop models capable of diagnosing COVID19 from chest x-ray. For this work we compiled a dataset of 112120 negative images from the Chest X-Ray 14 and 2725 positive images from public repositories. We tested multiple models, including logistic regression and random forest and XGBoost with and without principal components analysis, using five-fold cross-validation to evaluate recall, precision, and f1-score. These models were compared to a pre-trained deep-learning model for evaluating chest x-rays called COVID-Net. Our best model was XGBoost with principal components with a recall, precision, and f1-score of 0.692, 0.960, 0.804 respectively. This model greatly outperformed COVID-Net which scored 0.987, 0.025, 0.048. This model, with its high precision and reasonable sensitivity, would be most useful as \"rule-in\" test for COVID19. Though it outperforms some chemical assays in sensitivity, this model should be studied in patients who would not ordinarily receive a chest x-ray before being used for screening.", "journal": "medRxiv : the preprint server for health sciences", "date": "2022-10-21", "authors": ["JonathanStubblefield", "JasonCausey", "DakotaDale", "JakeQualls", "EmilyBellis", "JenniferFowler", "KarlWalker", "XiuzhenHuang"], "doi": "10.1101/2022.10.09.22280877\n10.1145/2939672.2939785\n10.1109/CVPR.2017.369\n10.1101/2020.02.25.20021568\n10.1101/2020.04.11.20062091\n10.3390/diagnostics10090669\n10.7937/91ah-v663\n10.1148/radiol.2021203957\n10.1007/s10278-013-9622-7\n10.1613/jair.953"}
{"title": "Role of air pollutants in dengue fever incidence: evidence from two southern cities in Taiwan.", "abstract": "Air pollution may be involved in spreading dengue fever (DF) besides rainfalls and warmer temperatures. While particulate matter (PM), especially those with diameter of 10\u2009\u03bcm (PM10) or 2.5\u2009\u03bcm or less (PM25), and NO2 increase the risk of coronavirus 2 infection, their roles in triggering DF remain unclear. We explored if air pollution factors predict DF incidence in addition to the classic climate factors. Public databases and DF records of two southern cities in Taiwan were used in regression analyses. Month order, PM10 minimum, PM2.5 minimum, and precipitation days were retained in the enter mode model, and SO2 minimum, O3 maximum, and CO minimum were retained in the stepwise forward mode model in addition to month order, PM10 minimum, PM2.5 minimum, and precipitation days. While PM2.5 minimum showed a negative contribution to the monthly DF incidence, other variables showed the opposite effects. The sustain of month order, PM10 minimum, PM2.5 minimum, and precipitation days in both regression models confirms the role of classic climate factors and illustrates a potential biological role of the air pollutants in the life cycle of mosquito vectors and dengue virus and possibly human immune status. Future DF prevention should concern the contribution of air pollution besides the classic climate factors.", "journal": "Pathogens and global health", "date": "2022-10-21", "authors": ["Hao-ChunLu", "Fang-YuLin", "Yao-HueiHuang", "Yu-TungKao", "El-WuiLoh"], "doi": "10.1080/20477724.2022.2135711"}
{"title": "Automated system for classification of COVID-19 infection from lung CT images based on machine learning and deep learning techniques.", "abstract": "The objectives of our proposed study were as follows: First objective is to segment the CT images using a k-means clustering algorithm for extracting the region of interest and to extract textural features using gray level co-occurrence matrix (GLCM). Second objective is to implement machine learning classifiers such as Na\u00efve bayes, bagging and Reptree to classify the images into two image classes namely COVID and non-COVID and to compare the performance of the three pre-trained CNN models such as AlexNet, ResNet50 and SqueezeNet with that of the proposed machine learning classifiers. Our dataset consists of 100 COVID and non-COVID images which are pre-processed and segmented with our proposed algorithm. Following the feature extraction process, three machine learning classifiers (Naive Bayes, Bagging, and REPTree) were used to classify the normal and covid patients. We had implemented the three pre-trained CNN models such as AlexNet, ResNet50 and SqueezeNet for comparing their performance with machine learning classifiers. In machine learning, the Naive Bayes classifier achieved the highest accuracy of 97%, whereas the ResNet50 CNN model attained the highest accuracy of 99%. Hence the deep learning networks outperformed well compared to the machine learning techniques in the classification of Covid-19 images.", "journal": "Scientific reports", "date": "2022-10-19", "authors": ["BhargaveeGuhan", "LailaAlmutairi", "SSowmiya", "USnekhalatha", "TRajalakshmi", "Shabnam MohamedAslam"], "doi": "10.1038/s41598-022-20804-5\n10.1016/j.ajem.2020.04.048\n10.1126/scitranslmed.abc1931\n10.1136/bmj.m1403\n10.1002/jmv.25721\n10.1080/14737159.2020.1757437\n10.1148/radiol.2021204522\n10.1136/bmjopen-2020-04294\n10.1016/j.ibmed.2020.100013\n10.1007/s10489-020-01902-1\n10.1007/s13246-020-00865-4\n10.1007/s10140-020-01886-y\n10.1007/s10096-020-03901-z\n10.1007/s10044-021-00984-y\n10.1007/s40009-020-01009-8\n10.1155/2022/5329014\n10.1016/j.jnlest.2022.100161\n10.1016/j.compbiomed.2020.104037\n10.1038/s41598-021-95537-y\n10.1016/j.media.2020.101794\n10.1016/j.bspc.2020.102365\n10.1016/j.compbiomed.2020.103795\n10.1148/radiol.2020191145\n10.1148/rg.2017160130\n10.18201/ijisae.2019252786\n10.30534/ijatcse/2020/221932020\n10.1109/TMI.2018.2806309\n10.1016/j.jocs.2018.11.008"}
{"title": "Active deep learning from a noisy teacher for semi-supervised 3D image segmentation: Application to COVID-19 pneumonia infection in CT.", "abstract": "Supervised deep learning has become a standard approach to solving medical image segmentation tasks. However, serious difficulties in attaining pixel-level annotations for sufficiently large volumetric datasets in real-life applications have highlighted the critical need for alternative approaches, such as semi-supervised learning, where model training can leverage small expert-annotated datasets to enable learning from much larger datasets without laborious annotation. Most of the semi-supervised approaches combine expert annotations and machine-generated annotations with equal weights within deep model training, despite the latter annotations being relatively unreliable and likely to affect model optimization negatively. To overcome this, we propose an active learning approach that uses an example re-weighting strategy, where machine-annotated samples are weighted (i) based on the similarity of their gradient directions of descent to those of expert-annotated data, and (ii) based on the gradient magnitude of the last layer of the deep model. Specifically, we present an active learning strategy with a query function that enables the selection of reliable and more informative samples from machine-annotated batch data generated by a noisy teacher. When validated on clinical COVID-19 CT benchmark data, our method improved the performance of pneumonia infection segmentation compared to the state of the art.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2022-10-19", "authors": ["Mohammad ArafatHussain", "ZahraMirikharaji", "MohammadMomeny", "MahmoudMarhamati", "Ali AsgharNeshat", "RafeefGarbi", "GhassanHamarneh"], "doi": "10.1016/j.compmedimag.2022.102127\n10.7937/tcia.2020.gqry-nc81"}
{"title": "Attention induction for a CT volume classification of COVID-19.", "abstract": "This study proposes a method to draw attention toward the specific radiological findings of coronavirus disease 2019 (COVID-19) in CT images, such as bilaterality of ground glass opacity (GGO) and/or consolidation, in order to improve the classification accuracy of input CT images.\nWe propose an induction mask that combines a similarity and a bilateral mask. A similarity mask guides attention to regions with similar appearances, and a bilateral mask induces attention to the opposite side of the lung to capture bilaterally distributed lesions. An induction mask for pleural effusion is also proposed in this study. ResNet18 with nonlocal blocks was trained by minimizing the loss function defined by the induction mask.\nThe four-class classification accuracy of the CT images of 1504 cases was 0.6443, where class 1 was the typical appearance of COVID-19 pneumonia, class 2 was the indeterminate appearance of COVID-19 pneumonia, class 3 was the atypical appearance of COVID-19 pneumonia, and class 4 was negative for pneumonia. The four classes were divided into two subgroups. The accuracy of COVID-19 and pneumonia classifications was evaluated, which were 0.8205 and 0.8604, respectively. The accuracy of the four-class and COVID-19 classifications improved when attention was paid to pleural effusion.\nThe proposed attention induction method was effective for the classification of CT images of COVID-19 patients. Improvement of the classification accuracy of class 3 by focusing on features specific to the class remains a topic for future work.", "journal": "International journal of computer assisted radiology and surgery", "date": "2022-10-18", "authors": ["YusukeTakateyama", "TakahitoHaruishi", "MasahiroHashimoto", "YoshitoOtake", "ToshiakiAkashi", "AkinobuShimizu"], "doi": "10.1007/s11548-022-02769-y\n10.1148/radiol.2020200432\n10.1148/radiol.2020200343\n10.1148/radiol.2020200905\n10.1016/j.compbiomed.2020.103795\n10.1007/s11356-020-10133-3\n10.1109/ACCESS.2020.3016780\n10.1109/TMI.2020.2994908\n10.1016/j.patrec.2021.06.021\n10.1016/j.media.2012.08.002\n10.1007/s11263-015-0816-y\n10.1007/BF02295996"}
{"title": "The correlation between COVID-19 segmentation volume based on artificial intelligence technology and gastric wall edema: a multi-center study in Wuhan.", "abstract": "This study aimed to investigate manifestations of the gastric wall and related risk factors in COVID-19 patients with gastrointestinal symptoms by CT.\nTwo hundred and forty patients diagnosed with COVID-19 by RT-PCR were enrolled from January 2020 to April 2020. Patients showed gastrointestinal symptoms, including nausea, vomiting, or diarrhea. Results of the initial laboratory examination were performed after admission. Chest CT was performed for all patients, with the lower bound including the gastric antrum. The volume of COVID-19 and lungs was segmented, and the ratio was calculated as follows: PV/LV\u2009=\u2009Volume\nAmong the 240 patients, 109 presented with gastric wall edema (edema group), and 131 showed no gastric wall edema (non-edema group); the PV/LV values between the two groups were significantly different (\nSARS-CoV-2 invades the gastrointestinal tract, gastric wall edema is the primary CT manifestation, and gastric wall edema is more likely to occur with a shorter APTT and severe pneumonia, with a slightly longer hospitalization time. Patients with gastric wall edema observed by CT should intervene early, which may improve digestive function, and further strengthen immune potency against COVID-19.", "journal": "Chinese journal of academic radiology", "date": "2022-10-18", "authors": ["XiaomingLi", "FengxiChen", "JieCheng", "YimanLi", "JunWang", "JianWang", "ChenLiu"], "doi": "10.1007/s42058-022-00104-7\n10.1038/s41564-020-0695-z\n10.1186/s12967-020-02324-w\n10.1038/s41591-020-0968-3\n10.1001/jama.2020.1585\n10.1016/j.cgh.2020.04.032\n10.1136/gutjnl-2020-320832\n10.1016/j.ijid.2020.04.027\n10.1111/apt.15731\n10.1056/NEJMoa2001191\n10.1053/j.gastro.2020.04.006\n10.1126/science.abc1669\n10.1016/j.surg.2020.04.035\n10.1007/s00384-020-03627-6\n10.1016/j.jns.2020.116824\n10.15252/msb.20209610\n10.1007/s00247-020-04616-1\n10.1038/nrmicro.2016.142\n10.3389/fmicb.2020.00301\n10.3390/ijms21145168\n10.1016/j.cell.2020.02.052\n10.1016/S0140-6736(20)30937-5\n10.1056/NEJMcibr1007320\n10.1111/jth.14820\n10.1053/j.gastro.2020.02.055\n10.1007/s00134-020-06079-2\n10.1038/s41586-020-2008-3\n10.1016/S0140-6736(20)30211-7\n10.1007/s11427-020-1733-4"}
{"title": "Multi-texture features and optimized DeepNet for COVID-19 detection using chest x-ray images.", "abstract": "The corona virus disease 2019 (COVID-19) pandemic has a severe influence on population health all over the world. Various methods are developed for detecting the COVID-19, but the process of diagnosing this problem from radiology and radiography images is one of the effective procedures for diagnosing the affected patients. Therefore, a robust and effective multi-local texture features (MLTF)-based feature extraction approach and Improved Weed Sea-based DeepNet (IWS-based DeepNet) approach is proposed for detecting the COVID-19 at an earlier stage. The developed IWS-based DeepNet is developed for detecting COVID-19to optimize the structure of the Deep Convolutional Neural Network (Deep CNN). The IWS is devised by incorporating the Improved Invasive Weed Optimization (IIWO) and Sea Lion Optimization (SLnO), respectively. The noises present in the input chest x-ray (CXR) image are discarded using Region of Interest (RoI) extraction by adaptive thresholding technique. For feature extraction, the proposed MLFT is newly developed by considering various texture features for extracting the best features. Finally, the COVID-19 detection is performed using the proposed IWS-based DeepNet. Furthermore, the proposed technique achieved effective performance in terms of True Positive Rate (TPR), True Negative Rate (TNR), and accuracy with the maximum values of 0.933%, 0.890%, and 0.919%.", "journal": "Concurrency and computation : practice & experience", "date": "2022-10-18", "authors": ["AnandbabuGopatoti", "VijayalakshmiP"], "doi": "10.1002/cpe.7157\n10.1007/s12559-021-09848-3\n10.1109/LSP.2018.2817176"}
{"title": "NSCGCN: A novel deep GCN model to diagnosis COVID-19.", "abstract": "Corona Virus Disease 2019 (COVID-19) was a lung disease with high mortality and was highly contagious. Early diagnosis of COVID-19 and distinguishing it from pneumonia was beneficial for subsequent treatment.\nRecently, Graph Convolutional Network (GCN) has driven a significant contribution to disease diagnosis. However, limited by the nature of the graph convolution algorithm, deep GCN has an over-smoothing problem. Most of the current GCN models are shallow neural networks, which do not exceed five layers. Furthermore, the objective of this study is to develop a novel deep GCN model based on the DenseGCN and the pre-trained model of deep Convolutional Neural Network (CNN) to complete the diagnosis of chest X-ray (CXR) images.\nWe apply the pre-trained model of deep CNN to perform feature extraction on the data to complete the extraction of pixel-level features in the image. And then, to extract the potential relationship between the obtained features, we propose Neighbourhood Feature Reconstruction Algorithm to reconstruct them into graph-structured data. Finally, we design a deep GCN model that exploits the graph-structured data to diagnose COVID-19 effectively. In the deep GCN model, we propose a Node-Self Convolution Algorithm (NSC) based on feature fusion to construct a deep GCN model called NSCGCN (Node-Self Convolution Graph Convolutional Network).\nExperiments were carried out on the Computed Tomography (CT) and CXR datasets. The results on the CT dataset confirmed that: compared with the six state-of-the-art (SOTA) shallow GCN models, the accuracy and sensitivity of the proposed NSCGCN had improve 8% as sensitivity (Sen.)\u00a0=\u00a087.50%, F1 score\u00a0=\u00a097.37%, precision (Pre.)\u00a0=\u00a089.10%, accuracy (Acc.)\u00a0=\u00a097.50%, area under the ROC curve (AUC)\u00a0=\u00a097.09%. Moreover, the results on the CXR dataset confirmed that: compared with the fourteen SOTA GCN models, sixteen SOTA CNN transfer learning models and eight SOTA COVID-19 diagnosis methods on the COVID-19 dataset. Our proposed method had best performances as Sen.\u00a0=\u00a096.45%, F1 score\u00a0=\u00a096.45%, Pre.\u00a0=\u00a096.61%, Acc.\u00a0=\u00a096.45%, AUC\u00a0=\u00a099.22%.\nOur proposed NSCGCN model is effective and performed better than the thirty-eight SOTA methods. Thus, the proposed NSC could help build deep GCN models. Our proposed COVID-19 diagnosis method based on the NSCGCN model could help radiologists detect pneumonia from CXR images and distinguish COVID-19 from Ordinary Pneumonia (OPN). The source code of this work will be publicly available at https://github.com/TangChaosheng/NSCGCN.", "journal": "Computers in biology and medicine", "date": "2022-10-17", "authors": ["ChaoshengTang", "ChaochaoHu", "JundingSun", "Shui-HuaWang", "Yu-DongZhang"], "doi": "10.1016/j.compbiomed.2022.106151\n10.1109/TMI.2022.3187141"}
{"title": "Computer-aided diagnostic for classifying chest X-ray images using deep ensemble learning.", "abstract": "Nowadays doctors and radiologists are overwhelmed with a huge amount of work. This led to the effort to design different Computer-Aided Diagnosis systems (CAD system), with the aim of accomplishing a faster and more accurate diagnosis. The current development of deep learning is a big opportunity for the development of new CADs. In this paper, we propose a novel architecture for a convolutional neural network (CNN) ensemble for classifying chest X-ray (CRX) images into four classes: viral Pneumonia, Tuberculosis, COVID-19, and Healthy. Although Computed tomography (CT) is the best way to detect and diagnoses pulmonary issues, CT is more expensive than CRX. Furthermore, CRX is commonly the first step in the diagnosis, so it's very important to be accurate in the early stages of diagnosis and treatment.\nWe applied the transfer learning technique and data augmentation to all CNNs for obtaining better performance. We have designed and evaluated two different CNN-ensembles: Stacking and Voting. This system is ready to be applied in a CAD system to automated diagnosis such a second or previous opinion before the doctors or radiology's. Our results show a great improvement, 99% accuracy of the Stacking Ensemble and 98% of accuracy for the the Voting Ensemble.\nTo minimize missclassifications, we included six different base CNN models in our architecture (VGG16, VGG19, InceptionV3, ResNet101V2, DenseNet121 and CheXnet) and it could be extended to any number as well as we expect extend the number of diseases to detected. The proposed method has been validated using a large dataset created by mixing several public datasets with different image sizes and quality. As we demonstrate in the evaluation carried out, we reach better results and generalization compared with previous works. In addition, we make a first approach to explainable deep learning with the objective of providing professionals more information that may be valuable when evaluating CRXs.", "journal": "BMC medical imaging", "date": "2022-10-16", "authors": ["LaraVisu\u00f1a", "DandiYang", "JavierGarcia-Blas", "JesusCarretero"], "doi": "10.1186/s12880-022-00904-4\n10.1007/s10489-020-01888-w\n10.1016/j.eswa.2020.114054\n10.1007/s10489-020-01902-1\n10.3389/fmed.2020.00427\n10.1016/j.imu.2020.100405\n10.1007/s10462-020-09825-6\n10.1016/j.media.2021.102121\n10.3233/XST-200831\n10.1055/s-0039-1677911\n10.1186/s43055-021-00524-y\n10.1016/j.cmpb.2020.105608\n10.1016/j.eswa.2021.115141\n10.1109/ACCESS.2020.3031384\n10.1016/j.eswa.2021.115401\n10.1109/TII.2021.3057683\n10.1007/s13246-020-00966-0\n10.1016/j.compeleceng.2019.08.004\n10.1016/j.eswa.2020.113909\n10.1007/s11263-015-0816-y\n10.1148/rg.2017160032\n10.3390/s21051742\n10.1007/s10044-021-00970-4\n10.1109/ACCESS.2020.2971257"}
{"title": "Deep learning of longitudinal chest X-ray and clinical variables predicts duration on ventilator and mortality in COVID-19 patients.", "abstract": "To use deep learning of serial portable chest X-ray (pCXR) and clinical variables to predict mortality and duration on invasive mechanical ventilation (IMV) for Coronavirus disease 2019 (COVID-19) patients.\nThis is a retrospective study. Serial pCXR and serial clinical variables were analyzed for data from day 1, day 5, day 1-3, day 3-5, or day 1-5 on IMV (110 IMV survivors and 76 IMV non-survivors). The outcome variables were duration on IMV and mortality. With fivefold cross-validation, the performance of the proposed deep learning system was evaluated by receiver operating characteristic (ROC) analysis and correlation analysis.\nPredictive models using 5-consecutive-day data outperformed those using 3-consecutive-day and 1-day data. Prediction using data closer to the outcome was generally better (i.e., day 5 data performed better than day 1 data, and day 3-5 data performed better than day 1-3 data). Prediction performance was generally better for the combined pCXR and non-imaging clinical data than either alone. The combined pCXR and non-imaging data of 5 consecutive days predicted mortality with an accuracy of 85\u2009\u00b1\u20093.5% (95% confidence interval (CI)) and an area under the curve (AUC) of 0.87\u2009\u00b1\u20090.05 (95% CI) and predicted the duration needed to be on IMV to within 2.56\u2009\u00b1\u20090.21 (95% CI) days on the validation dataset.\nDeep learning of longitudinal pCXR and clinical data have the potential to accurately predict mortality and duration on IMV in COVID-19 patients. Longitudinal pCXR could have prognostic value if these findings can be validated in a large, multi-institutional cohort.", "journal": "Biomedical engineering online", "date": "2022-10-15", "authors": ["HongyiDuanmu", "ThomasRen", "HaifangLi", "NeilMehta", "Adam JSinger", "Jeffrey MLevsky", "Michael LLipton", "Tim QDuong"], "doi": "10.1186/s12938-022-01045-z\n10.1056/NEJMoa2001017\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200642\n10.1148/radiol.2020200463\n10.1148/radiol.2020200432\n10.1148/radiol.2020200370\n10.1016/j.clinimag.2020.04.001\n10.1007/s10140-020-01808-y\n10.1097/RTI.0000000000000533\n10.1148/radiol.2020201160\n10.1001/jama.2020.6775\n10.1177/08850666211033836\n10.1056/NEJMp2006141\n10.1161/CIRCULATIONAHA.115.001593\n10.1590/0100-3984.2019.0049\n10.1016/S1470-2045(19)30333-X\n10.1371/journal.pone.0213653\n10.1371/journal.pone.0221339\n10.1148/radiol.2020201754\n10.1007/s11547-020-01232-9\n10.1371/journal.pone.0236621\n10.7717/peerj.10309\n10.1186/s12938-020-00831-x\n10.1371/journal.pone.0236618\n10.1093/infdis/jiaa447\n10.1161/CIRCRESAHA.120.317134\n10.32604/cmc.2020.010691\n10.1093/cid/ciaa414\n10.1056/NEJMoa2001316\n10.1002/emp2.12205\n10.3389/fmed.2021.661940\n10.7150/ijms.51235\n10.7717/peerj.10337\n10.7717/peerj.11205\n10.1109/TPAMI.2016.2572683"}
{"title": "Dynamic feature learning for COVID-19 segmentation and classification.", "abstract": "Since December 2019, coronavirus SARS-CoV-2 (COVID-19) has rapidly developed into a global epidemic, with millions of patients affected worldwide. As part of the diagnostic pathway, computed tomography (CT) scans are used to help patient management. However, parenchymal imaging findings in COVID-19 are non-specific and can be seen in other diseases. In this work, we propose to first segment lesions from CT images, and further, classify COVID-19 patients from healthy persons and common pneumonia patients. In detail, a novel Dynamic Fusion Segmentation Network (DFSN) that automatically segments infection-related pixels is first proposed. Within this network, low-level features are aggregated to high-level ones to effectively capture context characteristics of infection regions, and high-level features are dynamically fused to model multi-scale semantic information of lesions. Based on DFSN, Dynamic Transfer-learning Classification Network (DTCN) is proposed to distinguish COVID-19 patients. Within DTCN, a pre-trained DFSN is transferred and used as the backbone to extract pixel-level information. Then the pixel-level information is dynamically selected and used to make a diagnosis. In this way, the pre-trained DFSN is utilized through transfer learning, and clinical significance of segmentation results is comprehensively considered. Thus DTCN becomes more sensitive to typical signs of COVID-19. Extensive experiments are conducted to demonstrate effectiveness of the proposed DFSN and DTCN frameworks. The corresponding results indicate that these two models achieve state-of-the-art performance in terms of segmentation and classification.", "journal": "Computers in biology and medicine", "date": "2022-10-15", "authors": ["XiaoqinZhang", "RunhuaJiang", "PengchengHuang", "TaoWang", "MingjunHu", "Andrew FScarsbrook", "Alejandro FFrangi"], "doi": "10.1016/j.compbiomed.2022.106136"}
{"title": "Research on multi-modal autonomous diagnosis algorithm of COVID-19 based on whale optimized support vector machine and improved D-S evidence fusion.", "abstract": "Aiming at the problem that the single CT image signal feature recognition method in the self-diagnosis of diseases cannot accurately and reliably classify COVID-19, and it is easily confused with suspected cases. The collected CT signals and experimental indexes are extracted to construct different feature vectors. The support vector machine is optimized by the improved whale algorithm for the preliminary diagnosis of COVID-19, and the basic probability distribution function of each evidence is calculated by the posterior probability modeling method. Then the similarity measure is introduced to optimize the basic probability distribution function. Finally, the multi-domain feature fusion prediction model is established by using the weighted D-S evidence theory. The experimental results show that the fusion of multi-domain feature information by whale optimized support vector machine and improved D-S evidence theory can effectively improve the accuracy and the precision of COVID-19 autonomous diagnosis. The method of replacing a single feature parameter with multi-modal indicators (CT, routine laboratory indexes, serum cytokines and chemokines) provides a more reliable signal source for the diagnosis model, which can effectively distinguish COVID-19 from the suspected cases.", "journal": "Computers in biology and medicine", "date": "2022-10-15", "authors": ["GuoweiWang", "ShuliGuo", "LinaHan", "XiaoweiSong", "YuanyuanZhao"], "doi": "10.1016/j.compbiomed.2022.106181"}
{"title": "A novel abnormality annotation database for COVID-19 affected frontal lung X-rays.", "abstract": "Consistent clinical observations of characteristic findings of COVID-19 pneumonia on chest X-rays have attracted the research community to strive to provide a fast and reliable method for screening suspected patients. Several machine learning algorithms have been proposed to find the abnormalities in the lungs using chest X-rays specific to COVID-19 pneumonia and distinguish them from other etiologies of pneumonia. However, despite the enormous magnitude of the pandemic, there are very few instances of public databases of COVID-19 pneumonia, and to the best of our knowledge, there is no database with annotation of abnormalities on the chest X-rays of COVID-19 affected patients. Annotated databases of X-rays can be of significant value in the design and development of algorithms for disease prediction. Further, explainability analysis for the performance of existing or new deep learning algorithms will be enhanced significantly with access to ground-truth abnormality annotations. The proposed COVID Abnormality Annotation for X-Rays (CAAXR) database is built upon the BIMCV-COVID19+ database which is a large-scale dataset containing COVID-19+ chest X-rays. The primary contribution of this study is the annotation of the abnormalities in over 1700 frontal chest X-rays. Further, we define protocols for semantic segmentation as well as classification for robust evaluation of algorithms. We provide benchmark results on the defined protocols using popular deep learning models such as DenseNet, ResNet, MobileNet, and VGG for classification, and UNet, SegNet, and Mask-RCNN for semantic segmentation. The classwise accuracy, sensitivity, and AUC-ROC scores are reported for the classification models, and the IoU and DICE scores are reported for the segmentation models.", "journal": "PloS one", "date": "2022-10-15", "authors": ["SurbhiMittal", "Vasantha KumarVenugopal", "Vikash KumarAgarwal", "ManuMalhotra", "Jagneet SinghChatha", "SavinayKapur", "AnkurGupta", "VikasBatra", "PuspitaMajumdar", "AakarshMalhotra", "KartikThakral", "SahebChhabra", "MayankVatsa", "RichaSingh", "SantanuChaudhury"], "doi": "10.1371/journal.pone.0271931\n10.1016/j.tmaid.2020.101623\n10.2214/AJR.20.22969\n10.1007/s13246-020-00865-4\n10.3390/sym12040651\n10.1016/j.compbiomed.2020.103869\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2020.103792\n10.1097/RTI.0000000000000533\n10.1016/j.mehy.2020.109761\n10.1038/s41598-020-76550-z\n10.1371/journal.pone.0247176\n10.1109/JBHI.2021.3111415\n10.1007/s13278-021-00731-5\n10.1016/j.media.2021.102046\n10.1007/s10489-020-01900-3\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1109/TPAMI.2016.2644615\n10.1371/journal.pmed.1002683\n10.1126/science.aax2342\n10.1109/TCYB.2019.2905157"}
{"title": "An Intelligent Sensor Based Decision Support System for Diagnosing Pulmonary Ailment through Standardized Chest X-ray Scans.", "abstract": "Academics and the health community are paying much attention to developing smart remote patient monitoring, sensors, and healthcare technology. For the analysis of medical scans, various studies integrate sophisticated deep learning strategies. A smart monitoring system is needed as a proactive diagnostic solution that may be employed in an epidemiological scenario such as COVID-19. Consequently, this work offers an intelligent medicare system that is an IoT-empowered, deep learning-based decision support system (DSS) for the automated detection and categorization of infectious diseases (COVID-19 and pneumothorax). The proposed DSS system was evaluated using three independent standard-based chest X-ray scans. The suggested DSS predictor has been used to identify and classify areas on whole X-ray scans with abnormalities thought to be attributable to COVID-19, reaching an identification and classification accuracy rate of 89.58% for normal images and 89.13% for COVID-19 and pneumothorax. With the suggested DSS system, a judgment depending on individual chest X-ray scans may be made in approximately 0.01 s. As a result, the DSS system described in this study can forecast at a pace of 95 frames per second (FPS) for both models, which is near to real-time.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-10-15", "authors": ["ShivaniBatra", "HarshSharma", "WadiiBoulila", "VaishaliArya", "PrakashSrivastava", "Mohammad ZubairKhan", "MoezKrichen"], "doi": "10.3390/s22197474\n10.1152/physiolgenomics.00029.2020\n10.1038/s41586-020-2008-3\n10.1126/science.aba9757\n10.1016/j.compbiomed.2020.103670\n10.1016/j.ijid.2020.01.050\n10.3390/e24040533\n10.1016/j.cmpb.2020.105532\n10.1016/j.compbiomed.2020.103792\n10.1016/j.diii.2020.11.008\n10.1148/radiol.2020203465\n10.1016/j.diii.2021.05.006\n10.1007/s10044-021-00958-0\n10.1016/j.cmpb.2018.01.017\n10.1016/j.ijmedinf.2018.06.003\n10.1155/2021/6657533\n10.1016/j.rmcr.2020.101265\n10.1378/chest.125.6.2345\n10.1007/s10140-020-01806-0\n10.1148/radiol.2020202439\n10.3390/s22166312\n10.1016/j.ijmedinf.2022.104791\n10.3390/jpm11100993\n10.1109/TMI.2020.2993291\n10.1109/TMI.2020.2996645\n10.1038/s41598-020-76550-z\n10.1007/s13246-020-00865-4\n10.1007/s10489-020-01826-w\n10.1016/j.cmpb.2020.105581\n10.1007/s10044-021-00984-y\n10.1007/s10489-020-01770-9\n10.3390/s22155738\n10.1016/j.cmpb.2020.105584\n10.1002/jmv.25891\n10.1148/radiol.2020202352\n10.1183/13993003.02697-2020\n10.1038/s41598-020-74164-z\n10.1016/j.media.2021.102216\n10.1016/j.cell.2018.02.010\n10.1016/j.ecoinf.2016.11.006\n10.1016/j.jocs.2017.10.006"}
{"title": "COVID-19 Detection on Chest X-ray and CT Scan: A Review of the Top-100 Most Cited Papers.", "abstract": "Since the beginning of the COVID-19 pandemic, many works have been published proposing solutions to the problems that arose in this scenario. In this vein, one of the topics that attracted the most attention is the development of computer-based strategies to detect COVID-19 from thoracic medical imaging, such as chest X-ray (CXR) and computerized tomography scan (CT scan). By searching for works already published on this theme, we can easily find thousands of them. This is partly explained by the fact that the most severe worldwide pandemic emerged amid the technological advances recently achieved, and also considering the technical facilities to deal with the large amount of data produced in this context. Even though several of these works describe important advances, we cannot overlook the fact that others only use well-known methods and techniques without a more relevant and critical contribution. Hence, differentiating the works with the most relevant contributions is not a trivial task. The number of citations obtained by a paper is probably the most straightforward and intuitive way to verify its impact on the research community. Aiming to help researchers in this scenario, we present a review of the top-100 most cited papers in this field of investigation according to the Google Scholar search engine. We evaluate the distribution of the top-100 papers taking into account some important aspects, such as the type of medical imaging explored, learning settings, segmentation strategy, explainable artificial intelligence (XAI), and finally, the dataset and code availability.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-10-15", "authors": ["Yandre M GCosta", "Sergio ASilva", "Lucas OTeixeira", "Rodolfo MPereira", "DiegoBertolini", "Alceu SBritto", "Luiz SOliveira", "George D CCavalcanti"], "doi": "10.3390/s22197303\n10.22038/abjs.2020.51846.2556\n10.1177/1178633720962935\n10.32744/pse.2022.3.8\n10.1007/978-3-319-24574-4_28\n10.1109/TPAMI.2016.2644615\n10.1145/2939672.2939778\n10.1371/journal.pone.0130140\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1007/s10044-021-00984-y\n10.1038/s41551-021-00704-1\n10.1016/j.eng.2020.04.010\n10.1016/j.cmpb.2020.105581\n10.1007/s10489-020-01829-7\n10.1109/TCBB.2021.3065361\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2020.103795\n10.1038/s41598-020-76282-0\n10.1016/j.mehy.2020.109761\n10.1016/j.patrec.2020.09.010\n10.1016/j.chaos.2020.109944\n10.1148/ryct.2020200075\n10.1016/j.compbiomed.2020.103805\n10.1016/j.cmpb.2020.105532\n10.1183/13993003.00775-2020\n10.1016/j.cmpb.2020.105608\n10.3390/sym12040651\n10.1016/j.imu.2020.100412\n10.1016/j.eswa.2020.114054\n10.1016/j.compbiomed.2020.104037\n10.1016/j.bbe.2021.05.013\n10.1016/j.compbiomed.2021.104319\n10.1016/j.compbiomed.2020.104115\n10.1145/3450439.3451867\n10.3390/s21217116\n10.1007/s11042-022-12156-z\n10.1016/j.inffus.2021.04.008\n10.1007/s00259-020-04953-1\n10.1101/2020.03.12.20027185\n10.1101/2020.04.13.20063941\n10.1101/2020.04.08.20057679\n10.1101/2020.03.30.20047787\n10.1101/2020.04.13.20063461\n10.1101/2020.04.08.20040907\n10.1109/ACCESS.2020.3005510\n10.3233/XST-200715\n10.1016/j.compbiomed.2020.103869\n10.3390/app10134640\n10.1007/s10489-020-01902-1\n10.1016/j.chaos.2020.110245\n10.1109/ACCESS.2020.3016780\n10.1016/j.imu.2020.100427\n10.1007/s40846-020-00529-4\n10.1016/j.chemolab.2020.104054\n10.1016/j.chaos.2020.110071\n10.1007/s10916-021-01745-4\n10.1109/JAS.2020.1003393\n10.1109/ACCESS.2020.3003810\n10.1016/j.asoc.2020.106885\n10.1080/07391102.2020.1767212\n10.3390/e22050517\n10.1007/s10489-020-02055-x\n10.1007/s42600-021-00151-6\n10.1016/j.asoc.2020.106859\n10.1016/j.chaos.2020.110190\n10.1007/s00500-020-05275-y\n10.1007/s10489-020-01826-w\n10.1007/s10489-020-01888-w\n10.2196/19569\n10.1177/2472630320958376\n10.1016/j.bspc.2020.102365\n10.1155/2020/8828855\n10.1016/j.ejrad.2020.109041\n10.1007/s13755-020-00135-3\n10.7759/cureus.9448\n10.1038/s41551-020-00633-5\n10.1016/j.compbiomed.2021.104348\n10.1016/j.chaos.2020.110170\n10.1007/s12559-020-09787-5\n10.3389/fmed.2020.00427\n10.1038/s41598-020-74164-z\n10.1186/s12880-020-00529-5\n10.1016/j.bbe.2020.08.008\n10.1002/ima.22469\n10.3390/ijerph17186933\n10.1016/j.bbe.2020.08.005\n10.1016/j.chaos.2020.110495\n10.1016/j.eswa.2020.113909\n10.3390/s21020455\n10.1007/s00330-020-07044-9\n10.1007/s12539-020-00403-6\n10.1007/s10140-020-01886-y\n10.1109/JSEN.2021.3076767\n10.1007/s00330-020-07042-x\n10.1007/s00500-020-05424-3\n10.3390/v12070769\n10.1038/s41467-020-20657-4\n10.1038/s41598-021-88807-2\n10.1038/s41598-020-74539-2\n10.1016/j.bspc.2021.102588\n10.1007/s10044-021-00970-4\n10.1007/s10489-020-01900-3\n10.1007/s40747-020-00216-6\n10.1007/s00521-020-05410-8\n10.1016/j.knosys.2020.106647\n10.1007/s00264-020-04609-7\n10.1016/j.imu.2020.100505\n10.1109/ACCESS.2020.3025010\n10.1007/s10489-020-01867-1"}
{"title": "Optical Monitoring of Breathing Patterns and Tissue Oxygenation: A Potential Application in COVID-19 Screening and Monitoring.", "abstract": "The worldwide outbreak of the novel Coronavirus (COVID-19) has highlighted the need for a screening and monitoring system for infectious respiratory diseases in the acute and chronic phase. The purpose of this study was to examine the feasibility of using a wearable near-infrared spectroscopy (NIRS) sensor to collect respiratory signals and distinguish between normal and simulated pathological breathing. Twenty-one healthy adults participated in an experiment that examined five separate breathing conditions. Respiratory signals were collected with a continuous-wave NIRS sensor (PortaLite, Artinis Medical Systems) affixed over the sternal manubrium. Following a three-minute baseline, participants began five minutes of imposed difficult breathing using a respiratory trainer. After a five minute recovery period, participants began five minutes of imposed rapid and shallow breathing. The study concluded with five additional minutes of regular breathing. NIRS signals were analyzed using a machine learning model to distinguish between normal and simulated pathological breathing. Three features: breathing interval, breathing depth, and O", "journal": "Sensors (Basel, Switzerland)", "date": "2022-10-15", "authors": ["Aaron JamesMah", "ThienNguyen", "LeiliGhazi Zadeh", "AtrinaShadgan", "KosarKhaksari", "MehdiNourizadeh", "AliZaidi", "SoonghoPark", "Amir HGandjbakhche", "BabakShadgan"], "doi": "10.3390/s22197274\n10.3389/fmed.2020.00311\n10.1377/hlthaff.9.3.66\n10.1080/14737159.2020.1757437\n10.1016/S1473-3099(21)00048-7\n10.4037/aacnacc2022448\n10.1513/AnnalsATS.202005-418FR\n10.1183/13993003.01217-2020\n10.1007/s001340101064\n10.1177/0885066608324380\n10.1016/S0952-8180(97)00039-1\n10.1136/bmjresp-2020-000778\n10.1371/journal.pone.0241955\n10.1007/s15010-021-01603-y\n10.1080/10408363.2020.1860895\n10.1016/S2213-2600(20)30349-0\n10.1016/j.rmed.2021.106541\n10.1155/2021/6692409\n10.1155/2009/719604\n10.1016/j.jtcvs.2005.02.058\n10.1093/bja/aep299\n10.1255/jnirs.963\n10.1155/2005/951895\n10.1097/HCR.0000000000000185\n10.1007/s00421-016-3334-x\n10.1016/j.resp.2011.06.001\n10.1259/bjr/31200593\n10.1007/s10877-019-00410-z\n10.1016/j.jaut.2020.102433\n10.1148/radiol.2020200236\n10.1056/NEJMoa2001316\n10.1080/14767058.2018.1489535\n10.1186/cc11146\n10.1093/ehjci/jeaa163\n10.1001/jama.2020.22813\n10.1002/jmv.27101\n10.1186/s12931-020-01429-6\n10.1016/j.bios.2021.113486\n10.1021/acssensors.1c00312\n10.1016/j.bios.2020.112454\n10.1021/acsnano.0c05657"}
{"title": "Explanatory classification of CXR images into COVID-19, Pneumonia and Tuberculosis using deep learning and XAI.", "abstract": "Chest X-ray (CXR) images are considered useful to monitor and investigate a variety of pulmonary disorders such as COVID-19, Pneumonia, and Tuberculosis (TB). With recent technological advancements, such diseases may now be recognized more precisely using computer-assisted diagnostics. Without compromising the classification accuracy and better feature extraction, deep learning (DL) model to predict four different categories is proposed in this study. The proposed model is validated with publicly available datasets of 7132 chest x-ray (CXR) images. Furthermore, results are interpreted and explained using Gradient-weighted Class Activation Mapping (Grad-CAM), Local Interpretable Modelagnostic Explanation (LIME), and SHapley Additive exPlanation (SHAP) for better understandably. Initially, convolution features are extracted to collect high-level object-based information. Next, shapely values from SHAP, predictability results from LIME, and heatmap from Grad-CAM are used to explore the black-box approach of the DL model, achieving average test accuracy of 94.31\u00a0\u00b1\u00a01.01% and validation accuracy of 94.54\u00a0\u00b1\u00a01.33 for 10-fold cross validation. Finally, in order to validate the model and qualify medical risk, medical sensations of classification are taken to consolidate the explanations generated from the eXplainable Artificial Intelligence (XAI) framework. The results suggest that XAI and DL models give clinicians/medical professionals persuasive and coherent conclusions related to the detection and categorization of COVID-19, Pneumonia, and TB.", "journal": "Computers in biology and medicine", "date": "2022-10-14", "authors": ["MohanBhandari", "Tej BahadurShahi", "BiratSiku", "ArjunNeupane"], "doi": "10.1016/j.compbiomed.2022.106156\n10.1145/3527174\n10.1001/jamanetworkopen.2021.41096\n10.1109/ICIP.2017.8296695\n10.1080/03007995.2017.1385450\n10.1109/EIT.2019.8833768\n10.1016/j.media.2022.102470"}
{"title": "Multilevel segmentation of 2D and volumetric medical images using hybrid Coronavirus Optimization Algorithm.", "abstract": "Medical image segmentation is a crucial step in Computer-Aided Diagnosis systems, where accurate segmentation is vital for perfect disease diagnoses. This paper proposes a multilevel thresholding technique for 2D and 3D medical image segmentation using Otsu and Kapur's entropy methods as fitness functions to determine the optimum threshold values. The proposed algorithm applies the hybridization concept between the recent Coronavirus Optimization Algorithm (COVIDOA) and Harris Hawks Optimization Algorithm (HHOA) to benefit from both algorithms' strengths and overcome their limitations. The improved performance of the proposed algorithm over COVIDOA and HHOA algorithms is demonstrated by solving 5 test problems from IEEE CEC 2019 benchmark problems. Medical image segmentation is tested using two groups of images, including 2D medical images and volumetric (3D) medical images, to demonstrate its superior performance. The utilized test images are from different modalities such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), and X-ray images. The proposed algorithm is compared with seven well-known metaheuristic algorithms, where the performance is evaluated using four different metrics, including the best fitness values, Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Normalized Correlation Coefficient (NCC). The experimental results demonstrate the superior performance of the proposed algorithm in terms of convergence to the global optimum and making a good balance between exploration and exploitation properties. Moreover, the quality of the segmented images using the proposed algorithm at different threshold levels is better than the other methods according to PSNR, SSIM, and NCC values. Additionally, the Wilcoxon rank-sum test is conducted to prove the statistical significance of the proposed algorithm.", "journal": "Computers in biology and medicine", "date": "2022-10-14", "authors": ["Khalid MHosny", "Asmaa MKhalid", "Hanaa MHamza", "SeyedaliMirjalili"], "doi": "10.1016/j.compbiomed.2022.106003\n10.21203/rs.3.rs-1592094/v1"}
{"title": "The natural language processing of radiology requests and reports of chest imaging: Comparing five transformer models' multilabel classification and a proof-of-concept study.", "abstract": "Radiology requests and reports contain valuable information about diagnostic findings and indications, and transformer-based language models are promising for more accurate text classification.\nIn a retrospective study, 2256 radiologist-annotated radiology requests (8 classes) and reports (10 classes) were divided into training and testing datasets (90% and 10%, respectively) and used to train 32 models. Performance metrics were compared by model type (LSTM, Bertje, RobBERT, BERT-clinical, BERT-multilingual, BERT-base), text length, data prevalence, and training strategy. The best models were used to predict the remaining 40,873 cases' categories of the datasets of requests and reports.\nThe RobBERT model performed the best after 4000 training iterations, resulting in AUC values ranging from 0.808 [95% CI (0.757-0.859)] to 0.976 [95% CI (0.956-0.996)] for the requests and 0.746 [95% CI (0.689-0.802)] to 1.0 [95% CI (1.0-1.0)] for the reports. The AUC for the classification of normal reports was 0.95 [95% CI (0.922-0.979)]. The predicted data demonstrated variability of both diagnostic yield for various request classes and request patterns related to COVID-19 hospital admission data.\nTransformer-based natural language processing is feasible for the multilabel classification of chest imaging request and report items. Diagnostic yield varies with the information in the requests.", "journal": "Health informatics journal", "date": "2022-10-14", "authors": ["Allard WOlthof", "Peter Mavan Ooijen", "Ludo JCornelissen"], "doi": "10.1177/14604582221131198"}
{"title": "Role of Imaging and AI in the Evaluation of COVID-19 Infection: A Comprehensive Survey.", "abstract": "Coronavirus disease 2019 (COVID-19) is a respiratory illness that started and rapidly became the pandemic of the century, as the number of people infected with it globally exceeded 253.4 million. Since the beginning of the pandemic of COVID-19, over two years have passed. During this hard period, several defies have been coped by the scientific society to know this novel disease, evaluate it, and treat affected patients. All these efforts are done to push back the spread of the virus. This article provides a comprehensive review to learn about the COVID-19 virus and its entry mechanism, its main repercussions on many organs and tissues of the body, identify its symptoms in the short and long terms, in addition to recognize the role of diagnosis imaging in COVID-19. Principally, the quick evolution of active vaccines act an exceptional accomplishment where leaded to decrease rate of death worldwide. However, some hurdels still have to be overcome. Many proof referrers that infection with CoV-19 causes neurological dis function in a substantial ratio of influenced patients, where these symptoms appear severely during the infection and still less is known about the potential long term consequences for the brain, where Loss of smell is a neurological sign and rudimentary symptom of COVID-19. Hence, we review the causes of olfactory bulb dysfunction and Anosmia associated with COVID-19, the latest appropriate therapeutic strategies for the COVID-19 treatment (e.g., the ACE2 strategy and the Ang II receptor), and the tests through the follow-up phases. Additionally, we discuss the long-term complications of the virus and thus the possibility of improving therapeutic strategies. Moreover, the main steps of artificial intelligence that have been used to foretell and early diagnose COVID-19 are presented, where Artificial intelligence, especially machine learning is emerging as an effective approach for diagnostic image analysis with performance in the discriminate diagnosis of injuries of COVID-19 on multiple organs, comparable to that of human practitioners. The followed methodology to prepare the current survey is to search the related work concerning the mentioned topic from different journals, such as Springer, Wiley, and Elsevier. Additionally, different studies have been compared, the results are collected and then reported as shown. The articles are selected based on the year (i.e., the last three years). Also, different keywords were checked (e.g., COVID-19, COVID-19 Treatment, COVID-19 Symptoms, and COVID-19 and Anosmia).", "journal": "Frontiers in bioscience (Landmark edition)", "date": "2022-10-13", "authors": ["MayadaElgendy", "Hossam MagdyBalaha", "MohamedShehata", "AhmedAlksas", "MahitabGhoneim", "FatmaSherif", "AliMahmoud", "AhmedElgarayhi", "FatmaTaher", "MohammedSallah", "MohammedGhazal", "AymanEl-Baz"], "doi": "10.31083/j.fbl2709276"}
{"title": "Predicting necessity of daily online adaptive replanning based on wavelet image features for MRI guided adaptive radiation therapy.", "abstract": "Online adaptive replanning (OLAR) is generally labor-intensive and time-consuming during MRI-guided adaptive radiation therapy (MRgART). This work aims to develop a method to determine OLAR necessity during MRgART.\nA machine learning classifier was developed to predict OLAR necessity based on wavelet multiscale texture features extracted from daily MRIs and was trained and tested with data from 119 daily MRI datasets acquired during MRgART for 24 pancreatic cancer patients treated on a 1.5\u00a0T MR-Linac. Spearman correlations, interclass correlation (ICC), coefficient of variance (COV), t-test (p\u00a0<\u00a00.05), self-organized map (SOM) and maximum stable extremal region (MSER) algorithm were used to determine candidate features, which were used to build the prediction models using Bayesian classifiers. The model performance was judged using the AUC of the ROC curve.\nSpearman correlation identified 123 features that were not redundant (r\u00a0<\u00a00.9). Of them 82 showed high ICC for repositioning\u00a0>\u00a00.6, 67 had a COV greater than 9% for OLAR. Among the 38 features passed the t-test, 25 passed the SOM and 12 passed the MSER. These final 12 features were used to build the classifier model. The combination of 2-3 features at a time was used to build the classifier models. The best performing model was a 3-feature combination, which can predict OLAR necessity with a CV-AUC of 0.98.\nA machine learning classifier model based on the wavelet features extracted from daily MRI for pancreatic cancer was developed to automatically and objectively determine if OLAR is necessary for a treatment fraction avoiding unnecessary effort during MRgART.", "journal": "Radiotherapy and oncology : journal of the European Society for Therapeutic Radiology and Oncology", "date": "2022-10-11", "authors": ["Haidy GNasief", "Abdul KParchur", "EenasOmari", "YingZhang", "XinfengChen", "EricPaulson", "William AHall", "BethErickson", "X AllenLi"], "doi": "10.1016/j.radonc.2022.10.001"}
{"title": "Two-step machine learning to diagnose and predict involvement of lungs in COVID-19 and pneumonia using CT radiomics.", "abstract": "To develop a two-step machine learning (ML) based model to diagnose and predict involvement of lungs in COVID-19 and non COVID-19 pneumonia patients using CT chest radiomic features.\nThree hundred CT scans (3-classes: 100 COVID-19, 100 pneumonia, and 100 healthy subjects) were enrolled in this study. Diagnostic task included 3-class classification. Severity prediction score for COVID-19 and pneumonia was considered as mild (0-25%), moderate (26-50%), and severe (>50%). Whole lungs were segmented utilizing deep learning-based segmentation. Altogether, 107 features including shape, first-order histogram, second and high order texture features were extracted. Pearson correlation coefficient (PCC\u226590%) followed by different features selection algorithms were employed. ML-based supervised algorithms (Na\u00efve Bays, Support Vector Machine, Bagging, Random Forest, K-nearest neighbors, Decision Tree and Ensemble Meta voting) were utilized. The optimal model was selected based on precision, recall and area-under-curve (AUC) by randomizing the training/validation, followed by testing using the test set.\nNine pertinent features (2 shape, 1 first-order, and 6 second-order) were obtained after features selection for both phases. In diagnostic task, the performance of 3-class classification using Random Forest was 0.909\u00b10.026, 0.907\u00b10.056, 0.902\u00b10.044, 0.939\u00b10.031, and 0.982\u00b10.010 for precision, recall, F1-score, accuracy, and AUC, respectively. The severity prediction task using Random Forest achieved 0.868\u00b10.123 precision, 0.865\u00b10.121 recall, 0.853\u00b10.139 F1-score, 0.934\u00b10.024 accuracy, and 0.969\u00b10.022 AUC.\nThe two-phase ML-based model accurately classified COVID-19 and pneumonia patients using CT radiomics, and adequately predicted severity of lungs involvement. This 2-steps model showed great potential in assessing COVID-19 CT images towards improved management of patients.", "journal": "Computers in biology and medicine", "date": "2022-10-11", "authors": ["PegahMoradi Khaniabadi", "YassineBouchareb", "HumoudAl-Dhuhli", "IsaacShiri", "FaizaAl-Kindi", "BitaMoradi Khaniabadi", "HabibZaidi", "ArmanRahmim"], "doi": "10.1016/j.compbiomed.2022.106165\n10.1016/j.pdpdt.2021.102287\n10.1016/j.compbiomed.2021.104665\n10.1148/radiol.2020201160\n10.1148/radiol.2021202553\n10.1016/S1120-1797(22)00087-4\n10.1016/j.compbiomed.2021.104304\n10.1038/s41598-021-88807-2\n10.1007/s42979-020-00394-7\n10.48550/arXiv.2003.11988\n10.1186/s12967-020-02692-3\n10.1007/s11432-020-2849-3\n10.1016/j.acra.2020.09.004\n10.1016/j.compbiomed.2022.105467\n10.1038/s41598-022-18994-z\n10.1002/ima.22672\n10.1148/radiol.2020201473\n10.1148/radiol.2020200370\n10.1148/radiol.2462070712\n10.1148/radiol.11092149\n10.1148/ryct.2020200322\n10.1148/radiol.2020200463\n10.30476/ijms.2021.88036.1858.20\n10.1148/radiol.2020191145\n10.1186/s40644-020-00311-4\n10.1002/mp.13649\n10.1016/j.ejro.2020.100271\n10.1101/2021.12.07.21267367\n10.2967/jnumed.121.262567\n10.1007/s00330-020-06829-2\n10.1088/1361-6560/abe838\n10.1109/ICSPIS51611.2020.9349605\n10.21037/atm-20-3026\n10.1155/2021/2263469\n10.1016/j.media.2020.10182\n10.1007/s12539-020-00410-7\n10.1186/s12880-021-00564-w\n10.1016/j.smhl.2020.100178\n10.1016/j.csbj.2021.06.022\n10.1038/s41598-021-96755-0\n10.1016/j.bspc.2022.103662\n10.21037/qims.2020.02.21\n10.3389/fdgth.2021.662343\n10.1186/s43055-021-00592-0\n10.1007/s00259-020-05075-4\n10.1038/s41598-021-99015-3\n10.1016/j.ijid.2021.03.008\n10.1016/j.compbiomed.2021.104531\n10.1148/ryct.2020200047"}
{"title": "Evaluation of federated learning variations for COVID-19 diagnosis using chest radiographs from 42 US and European hospitals.", "abstract": "Federated learning (FL) allows multiple distributed data holders to collaboratively learn a shared model without data sharing. However, individual health system data are heterogeneous. \"Personalized\" FL variations have been developed to counter data heterogeneity, but few have been evaluated using real-world healthcare data. The purpose of this study is to investigate the performance of a single-site versus a 3-client federated model using a previously described Coronavirus Disease 19 (COVID-19) diagnostic model. Additionally, to investigate the effect of system heterogeneity, we evaluate the performance of 4 FL variations.\nWe leverage a FL healthcare collaborative including data from 5 international healthcare systems (US and Europe) encompassing 42 hospitals. We implemented a COVID-19 computer vision diagnosis system using the Federated Averaging (FedAvg) algorithm implemented on Clara Train SDK 4.0. To study the effect of data heterogeneity, training data was pooled from 3 systems locally and federation was simulated. We compared a centralized/pooled model, versus FedAvg, and 3 personalized FL variations (FedProx, FedBN, and FedAMP).\nWe observed comparable model performance with respect to internal validation (local model: AUROC 0.94 vs FedAvg: 0.95, P\u2009=\u2009.5) and improved model generalizability with the FedAvg model (P\u2009<\u2009.05). When investigating the effects of model heterogeneity, we observed poor performance with FedAvg on internal validation as compared to personalized FL algorithms. FedAvg did have improved generalizability compared to personalized FL algorithms. On average, FedBN had the best rank performance on internal and external validation.\nFedAvg can significantly improve the generalization of the model compared to other personalization FL algorithms; however, at the cost of poor internal validity. Personalized FL may offer an opportunity to develop both internal and externally validated algorithms.", "journal": "Journal of the American Medical Informatics Association : JAMIA", "date": "2022-10-11", "authors": ["LePeng", "GaoxiangLuo", "AndrewWalker", "ZacharyZaiman", "Emma KJones", "HemantGupta", "KristopherKersten", "John LBurns", "Christopher AHarle", "TanjaMagoc", "BenjaminShickel", "Scott DSteenburg", "TylerLoftus", "Genevieve BMelton", "Judy WawiraGichoya", "JuSun", "Christopher JTignanelli"], "doi": "10.1093/jamia/ocac188"}
{"title": "Development and validation of chest CT-based imaging biomarkers for early stage COVID-19 screening.", "abstract": "Coronavirus Disease 2019 (COVID-19) is currently a global pandemic, and early screening is one of the key factors for COVID-19 control and treatment. Here, we developed and validated chest CT-based imaging biomarkers for COVID-19 patient screening from two independent hospitals with 419 patients. We identified the vasculature-like signals from CT images and found that, compared to healthy and community acquired pneumonia (CAP) patients, COVID-19 patients display a significantly higher abundance of these signals. Furthermore, unsupervised feature learning led to the discovery of clinical-relevant imaging biomarkers from the vasculature-like signals for accurate and sensitive COVID-19 screening that have been double-blindly validated in an independent hospital (sensitivity: 0.941, specificity: 0.920, AUC: 0.971, accuracy 0.931, F1 score: 0.929). Our findings could open a new avenue to assist screening of COVID-19 patients.", "journal": "Frontiers in public health", "date": "2022-10-11", "authors": ["Xiao-PingLiu", "XuYang", "MiaoXiong", "XuanyuMao", "XiaoqingJin", "ZhiqiangLi", "ShuangZhou", "HangChang"], "doi": "10.3389/fpubh.2022.1004117\n10.1111/tmi.13383\n10.1002/jmv.25722\n10.1080/14737159.2020.1757437\n10.1148/radiol.2020200343\n10.1111/eci.13706\n10.1371/journal.pone.0242958\n10.1016/j.radi.2020.09.010\n10.1016/j.ijid.2020.04.023\n10.1007/s11604-020-00948-y\n10.2214/AJR.20.22961\n10.1148/radiol.2020200642\n10.1148/radiol.2020200330\n10.1148/radiol.2020200432\n10.1177/0846537120913033\n10.1371/journal.pone.0263916\n10.1016/j.compbiomed.2022.105298\n10.2196/27468\n10.1155/2021/9868517\n10.1200/CCI.19.00155\n10.1016/j.patcog.2014.10.005\n10.1109/83.902291\n10.1007/s11263-014-0790-9\n10.1016/j.acra.2020.03.003\n10.1097/TP.0000000000003412\n10.1056/NEJMoa2015432\n10.1038/s41591-020-0822-7\n10.1016/j.tjem.2018.08.001\n10.1089/ars.2012.5149\n10.1093/cvr/cvaa078\n10.1016/j.jcv.2020.104362\n10.1146/annurev-virology-031413-085548\n10.1016/0891-5849(96)00131-1\n10.1371/journal.ppat.1008536\n10.1038/s41577-020-0311-8\n10.1016/S0140-6736(20)30937-5\n10.1016/j.bcp.2009.04.029\n10.1148/radiol.2020200905\n10.1016/j.cell.2020.04.045\n10.1038/s41591-020-0931-3\n10.1109/TMI.2020.2996256\n10.21037/qims.2020.04.02\n10.1016/S1473-3099(20)30241-3\n10.1148/radiol.2020201845\n10.1148/radiol.2020201365\n10.1038/d41586-020-01001-8\n10.1136/bmj.m1367"}
{"title": "SVD-CLAHE boosting and balanced loss function for Covid-19 detection from an imbalanced Chest X-Ray dataset.", "abstract": "Covid-19 disease has had a disastrous effect on the health of the global population, for the last two years. Automatic early detection of Covid-19 disease from Chest X-Ray (CXR) images is a very crucial step for human survival against Covid-19. In this paper, we propose a novel data-augmentation technique, called SVD-CLAHE Boosting and a novel loss function Balanced Weighted Categorical Cross Entropy (BWCCE), in order to detect Covid 19 disease efficiently from a highly class-imbalanced Chest X-Ray image dataset. Our proposed SVD-CLAHE Boosting method is comprised of both oversampling and under-sampling methods. First, a novel Singular Value Decomposition (SVD) based contrast enhancement and Contrast Limited Adaptive Histogram Equalization (CLAHE) methods are employed for oversampling the data in minor classes. Simultaneously, a Random Under Sampling (RUS) method is incorporated in major classes, so that the number of images per class will be more balanced. Thereafter, Balanced Weighted Categorical Cross Entropy (BWCCE) loss function is proposed in order to further reduce small class imbalance after SVD-CLAHE Boosting. Experimental results reveal that ResNet-50 model on the augmented dataset (by SVD-CLAHE Boosting), along with BWCCE loss function, achieved 95% F1 score, 94% accuracy, 95% recall, 96% precision and 96% AUC, which is far better than the results by other conventional Convolutional Neural Network (CNN) models like InceptionV3, DenseNet-121, Xception etc. as well as other existing models like Covid-Lite and Covid-Net. Hence, our proposed framework outperforms other existing methods for Covid-19 detection. Furthermore, the same experiment is conducted on VGG-19 model in order to check the validity of our proposed framework. Both ResNet-50 and VGG-19 model are pre-trained on the ImageNet dataset. We publicly shared our proposed augmented dataset on Kaggle website (https://www.kaggle.com/tr1gg3rtrash/balanced-augmented-covid-cxr-dataset), so that any research community can widely utilize this dataset. Our code is available on GitHub website online (https://github.com/MrinalTyagi/SVD-CLAHE-and-BWCCE).", "journal": "Computers in biology and medicine", "date": "2022-10-09", "authors": ["SantanuRoy", "MrinalTyagi", "VibhutiBansal", "VikasJain"], "doi": "10.1016/j.compbiomed.2022.106092"}
{"title": "A self-supervised COVID-19 CT recognition system with multiple regularizations.", "abstract": "The diagnosis of Coronavirus Disease 2019 (COVID-19) exploiting machine learning algorithms based on chest computed tomography (CT) images has become an important technology. Though many excellent computer-aided methods leveraging CT images have been designed, they do not possess sufficiently high recognition accuracy. Besides, these methods entail vast amounts of training data, which might be difficult to be satisfied in some real-world applications. To address these two issues, this paper proposes a novel COVID-19 recognition system based on CT images, which has high recognition accuracy, while only requiring a small amount of training data. Specifically, the system possesses the following three improvements: 1) Data: a novel redesigned BCELoss that incorporates Label Smoothing, Focal Loss, and Label Weighting Regularization (LSFLLW-R) technique for optimizing the solution space and preventing overfitting, 2) Model: a backbone network processed by two-phase contrastive self-supervised learning for classifying multiple labels, and 3) Method: a decision-fusing ensemble learning method for getting a more stable system, with balanced metric values. Our proposed system is evaluated on the small-scale expanded COVID-CT dataset, achieving an accuracy of 94.3%, a precision of 94.1%, a recall (sensitivity) of 93.4%, an F1-score of 94.7%, and an Area Under the Curve (AUC) of 98.9%, for COVID-19 diagnosis, respectively. These experimental results verify that our system can not only identify pathological locations effectively, but also achieve better performance in terms of accuracy, generalizability, and stability, compared with several other state-of-the-art COVID-19 diagnosis methods.", "journal": "Computers in biology and medicine", "date": "2022-10-08", "authors": ["HanLu", "QunDai"], "doi": "10.1016/j.compbiomed.2022.106149\n10.1101/2020.04.13.20063941v1"}
{"title": "RadImageNet: An Open Radiologic Deep Learning Research Dataset for Effective Transfer Learning.", "abstract": "To demonstrate the value of pretraining with millions of radiologic images compared with ImageNet photographic images on downstream medical applications when using transfer learning.\nThis retrospective study included patients who underwent a radiologic study between 2005 and 2020 at an outpatient imaging facility. Key images and associated labels from the studies were retrospectively extracted from the original study interpretation. These images were used for RadImageNet model training with random weight initiation. The RadImageNet models were compared with ImageNet models using the area under the receiver operating characteristic curve (AUC) for eight classification tasks and using Dice scores for two segmentation problems.\nThe RadImageNet database consists of 1.35 million annotated medical images in 131\u2009872 patients who underwent CT, MRI, and US for musculoskeletal, neurologic, oncologic, gastrointestinal, endocrine, abdominal, and pulmonary pathologic conditions. For transfer learning tasks on small datasets-thyroid nodules (US), breast masses (US), anterior cruciate ligament injuries (MRI), and meniscal tears (MRI)-the RadImageNet models demonstrated a significant advantage (\nRadImageNet pretrained models demonstrated better interpretability compared with ImageNet models, especially for smaller radiologic datasets.", "journal": "Radiology. Artificial intelligence", "date": "2022-10-08", "authors": ["XueyanMei", "ZelongLiu", "Philip MRobson", "BrettMarinelli", "MingqianHuang", "AmishDoshi", "AdamJacobi", "ChendiCao", "Katherine ELink", "ThomasYang", "YingWang", "HayitGreenspan", "TimothyDeyer", "Zahi AFayad", "YangYang"], "doi": "10.1148/ryai.210315"}
{"title": "Deep learning models for COVID-19 chest x-ray classification: Preventing shortcut learning using feature disentanglement.", "abstract": "In response to the COVID-19 global pandemic, recent research has proposed creating deep learning based models that use chest radiographs (CXRs) in a variety of clinical tasks to help manage the crisis. However, the size of existing datasets of CXRs from COVID-19+ patients are relatively small, and researchers often pool CXR data from multiple sources, for example, using different x-ray machines in various patient populations under different clinical scenarios. Deep learning models trained on such datasets have been shown to overfit to erroneous features instead of learning pulmonary characteristics in a phenomenon known as shortcut learning. We propose adding feature disentanglement to the training process. This technique forces the models to identify pulmonary features from the images and penalizes them for learning features that can discriminate between the original datasets that the images come from. We find that models trained in this way indeed have better generalization performance on unseen data; in the best case we found that it improved AUC by 0.13 on held out data. We further find that this outperforms masking out non-lung parts of the CXRs and performing histogram equalization, both of which are recently proposed methods for removing biases in CXR datasets.", "journal": "PloS one", "date": "2022-10-07", "authors": ["AnusuaTrivedi", "CalebRobinson", "MarianBlazes", "AnthonyOrtiz", "JocelynDesbiens", "SunilGupta", "RahulDodhia", "Pavan KBhatraju", "W ConradLiles", "JayashreeKalpathy-Cramer", "Aaron YLee", "Juan MLavista Ferres"], "doi": "10.1371/journal.pone.0274098\n10.1016/j.dsx.2020.04.012\n10.1016/j.compbiomed.2020.103792\n10.3389/fmed.2020.00427\n10.1007/s13246-020-00865-4\n10.1136/bmj.m1328\n10.1016/j.cmpb.2020.105532\n10.1101/2020.09.13.20193565\n10.1016/j.cell.2020.04.045"}
{"title": "Exploration of the Potential Link, Hub Genes, and Potential Drugs for Coronavirus Disease 2019 and Lung Cancer Based on Bioinformatics Analysis.", "abstract": "The ongoing pandemic of coronavirus disease 2019 (COVID-19) has a huge influence on global public health and the economy. Lung cancer is one of the high-risk factors of COVID-19, but the molecular mechanism of lung cancer and COVID-19 is still unclear, and further research is needed. Therefore, we used the transcriptome information of the public database and adopted bioinformatics methods to identify the common pathways and molecular biomarkers of lung cancer and COVID-19 to further understand the connection between them. The two RNA-seq data sets in this study-GSE147507 (COVID-19) and GSE33532 (lung cancer)-were both derived from the Gene Expression Omnibus (GEO) database and identified differentially expressed genes (DEGs) for lung cancer and COVID-19 patients. We conducted Gene Ontology (GO) functions and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways enrichment analysis and found some common features between lung cancer and COVID-19. We also performed TFs-gene, miRNAs-gene, and gene-drug analyses. In total, 32 DEGs were found. A protein-protein interaction (PPI) network was constructed by DEGs, and 10 hub genes were screened. Finally, the identified drugs may be helpful for COVID-19 treatment.", "journal": "Journal of oncology", "date": "2022-10-07", "authors": ["YeWang", "QingLi", "JianfangZhang", "HuiXie"], "doi": "10.1155/2022/8124673\n10.1111/ahg.12418\n10.1016/j.biopha.2017.03.018\n10.1016/j.arcmed.2020.11.006\n10.1177/1073274820960467\n10.1016/s1470-2045(20)30096-6\n10.1186/s12885-020-07418-8\n10.1016/s2352-3018(16)30215-6\n10.1186/s12890-019-0910-y\n10.1002/jmv.25685\n10.1111/bjh.16659\n10.1016/j.ebiom.2020.102763\n10.1001/jamaoncol.2020.0980\n10.1111/bjh.17116\n10.1016/j.cell.2020.04.026\n10.18632/aging.203159\n10.1007/s11033-021-06149-8\n10.1159/000444125\n10.21037/atm-21-927\n10.1042/bsr20182377\n10.1084/jem.20201129\n10.1038/s41467-018-05928-5\n10.1111/1759-7714.13303\n10.1038/s41598-020-61424-1\n10.3390/nu12113448\n10.1002/mgg3.1115\n10.2217/bmm-2021-0337\n10.1155/2020/8931419\n10.1242/dev.174037\n10.3390/cells10040882\n10.1167/iovs.61.14.12\n10.1021/acscentsci.0c01537\n10.1369/0022155417753363\n10.1038/s41591-019-0750-6\n10.1038/nrd4099\n10.4274/tjh.galenos.2019.2018.0420\n10.1002/ibd.20850\n10.3389/fneur.2020.540156\n10.3390/ijms20143401\n10.1242/jcs.239715\n10.3390/ijms21165795\n10.3892/etm.2014.1829\n10.1002/stem.1969\n10.3892/mmr.2019.10543\n10.1038/s41598-020-66469-w\n10.1186/s12967-020-02561-z\n10.1016/j.biopha.2018.08.040\n10.1042/BSR20200884\n10.1017/S2045796021000597\n10.1016/S2215-0366(20)30564-2\n10.1038/s41598-020-58207-z\n10.1016/j.eclinm.2020.100688\n10.1016/j.jhep.2020.06.006\n10.3389/fped.2020.602083\n10.3748/wjg.v26.i31.4694\n10.1186/s12929-020-00703-5\n10.1158/2159-8290.CD-20-0941\n10.1016/j.xcrm.2021.100288\n10.1001/jamapsychiatry.2018.3412\n10.3389/fpsyt.2021.653662\n10.1210/me.2015-1274\n10.1136/hrt.2006.096412\n10.1158/2159-8290.CD-15-1177\n10.1038/s41593-019-0372-9"}
{"title": "Digital culturally tailored marketing for enrolling Latino participants in a web-based registry: Baseline metrics from the Brain Health Registry.", "abstract": "This culturally tailored enrollment effort aims to determine the feasibility of enrolling 5000 older Latino adults from California into the Brain Health Registries (BHR) over 2.25 years.\nThis paper describes (1) the development and deployment of culturally tailored BHR websites and digital ads, in collaboration with a Latino community science partnership board and a marketing company; (2) an interim feasibility analysis of the enrollment efforts and numbers, and participant characteristics (primary aim); as well as (3) an exploration of module completion and a preliminary efficacy evaluation of the culturally tailored digital efforts compared to BHR's standard non-culturally tailored efforts (secondary aim).\nIn 12.5 months, 3603 older Latino adults were enrolled (71% of the total California Latino BHR initiative enrollment goal). Completion of all BHR modules was low (6%).\nTargeted ad placement, culturally tailored enrollment messaging, and culturally tailored BHR websites increased enrollment of Latino participants in BHR, but did not translate to increased module completion.\nCulturally tailored social marketing and website improvements were implemented. The efforts enrolled 5662 Latino individuals in 12.5 months. The number of Latino Brain Health Registry (BHR) participants increased by 122.7%. We failed to adequately enroll female Latinos and Latinos with lower education. Future work will evaluate effects of a newly released Spanish-language BHR website.", "journal": "Alzheimer's & dementia : the journal of the Alzheimer's Association", "date": "2022-10-05", "authors": ["Miriam TAshford", "Monica RCamacho", "ChengshiJin", "JosephEichenbaum", "AaronUlbricht", "RoxanneAlaniz", "LesleyVan De Mortel", "JenneferSorce", "AnnaAaronson", "ShivamParmar", "DerekFlenniken", "JulietFockler", "DianaTruran", "R ScottMackin", "MonicaRivera Mindt", "AlejandraMorlett-Paredes", "Hector MGonz\u00e1lez", "Elizabeth RoseMayeda", "Michael WWeiner", "Rachel LNosheny"], "doi": "10.1002/alz.12805\n10.15585/mmwr.mm6650a3\n10.1093/geront/gnab107\n10.18865/ed.25.3.245\n10.1016/j.neubiorev.2019.03.020\n10.2174/1567205016666190321161901\n10.1016/j.jalz.2017.04.005\n10.1016/j.jalz.2018.02.021"}
{"title": "Artificial intelligence-based model for COVID-19 prognosis incorporating chest radiographs and clinical data; a retrospective model development and validation study.", "abstract": "The purpose of this study was to develop an artificial intelligence-based model to prognosticate COVID-19 patients at admission by combining clinical data and chest radiographs.\nThis retrospective study used the Stony Brook University COVID-19 dataset of 1384 inpatients. After exclusions, 1356 patients were randomly divided into training (1083) and test datasets (273). We implemented three artificial intelligence models, which classified mortality, ICU admission, or ventilation risk. Each model had three submodels with different inputs: clinical data, chest radiographs, and both. We showed the importance of the variables using SHapley Additive exPlanations (SHAP) values.\nThe mortality prediction model was best overall with area under the curve, sensitivity, specificity, and accuracy of 0.79 (0.72-0.86), 0.74 (0.68-0.79), 0.77 (0.61-0.88), and 0.74 (0.69-0.79) for the clinical data-based model; 0.77 (0.69-0.85), 0.67 (0.61-0.73), 0.81 (0.67-0.92), 0.70 (0.64-0.75) for the image-based model, and 0.86 (0.81-0.91), 0.76 (0.70-0.81), 0.77 (0.61-0.88), 0.76 (0.70-0.81) for the mixed model. The mixed model had the best performance (\nThese results suggest that prognosis models become more accurate if AI-derived chest radiograph features and clinical data are used together.\nThis AI model evaluates chest radiographs together with clinical data in order to classify patients as having high or low mortality risk. This work shows that chest radiographs taken at admission have significant COVID-19 prognostic information compared to clinical data other than age and sex.", "journal": "The British journal of radiology", "date": "2022-10-05", "authors": ["Shannon LWalston", "ToshimasaMatsumoto", "YukioMiki", "DaijuUeda"], "doi": "10.1259/bjr.20220058\n10.1136/bmj.m1328\n10.1080/23744235.2020.1784457\n10.1016/j.jcrc.2020.04.004\n10.1001/jama.2016.0287\n10.1148/radiol.2020201433\n10.1148/radiol.2020201160\n10.1183/13993003.01104-2020\n10.1016/S2589-7500(21)00039-X\n10.1038/s41467-020-17280-8\n10.1093/jamia/ocab029\n10.1001/jama.2018.11100\n10.1148/radiol.2020200905\n10.1007/s00330-020-07269-8\n10.1186/s12938-020-00807-x\n10.1148/ryai.2020200029\n10.1007/s10278-013-9622-7\n10.1016/j.jiph.2021.09.023\n10.1371/journal.pone.0241955\n10.1109/CVPR.2015.7298594\n10.1109/ICCV.2017.74\n10.1002/1097-0142(1950)3:1<32::aid-cncr2820030106>3.0.co;2-3\n10.1148/ryai.2020190043\n10.1001/jamainternmed.2020.2033\n10.1016/j.atherosclerosis.2021.05.015\n10.1007/s11739-020-02475-0\n10.3390/jcm9061668\n10.1172/jci.insight.139024\n10.1017/S0950268820001727\n10.1088/1361-6560/abbf9e\n10.1183/13993003.03498-2020\n10.1016/j.media.2021.102216\n10.1038/s41746-021-00446-z\n10.1109/JBHI.2021.3103389\n10.1038/s41746-021-00546-w\n10.1148/ryai.2020200098\n10.1038/s41746-021-00453-0\n10.1038/s41467-020-20657-4\n10.1016/j.rxeng.2021.09.004\n10.1186/s12879-022-07617-7\n10.1038/s41598-022-10136-9\n10.3390/app12083903\n10.1007/s10278-022-00691-y\n10.1007/s00330-021-08049-8\n10.1007/s40477-021-00559-x\n10.1016/j.ejca.2016.03.081\n10.1002/9780471420194\n10.1038/s41591-021-01506-3"}
{"title": "Fully Automatic Dual-Probe Lung Ultrasound Scanning Robot for Screening Triage.", "abstract": "Two-dimensional lung ultrasound (LUS) has widely emerged as a rapid and noninvasive imaging tool for the detection and diagnosis of coronavirus disease 2019 (COVID-19). However, image differences will be magnified due to changes in ultrasound (US) imaging experience, such as US probe attitude control and force control, which will directly affect the diagnosis results. In addition, the risk of virus transmission between sonographer and patients is increased due to frequent physical contact. In this study, a fully automatic dual-probe US scanning robot for the acquisition of LUS images is proposed and developed. Furthermore, the trajectory was optimized based on the velocity look-ahead strategy, the stability of contact force of the system and the scanning efficiency were improved by 24.13% and 29.46%, respectively. Also, the control ability of the contact force of robotic automatic scanning was 34.14 times higher than that of traditional manual scanning, which significantly improves the smoothness of scanning. Importantly, there was no significant difference in image quality obtained by robotic automatic scanning and manual scanning. Furthermore, the scanning time for a single person is less than 4 min, which greatly improves the efficiency of screening triage of group COVID-19 diagnosis and suspected patients and reduces the risk of virus exposure and spread.", "journal": "IEEE transactions on ultrasonics, ferroelectrics, and frequency control", "date": "2022-10-04", "authors": ["JiyongTan", "BingLi", "YuquanLeng", "YuanweiLi", "JunhuaPeng", "JiayiWu", "BaomingLuo", "XinxingChen", "YimingRong", "ChenglongFu"], "doi": "10.1109/TUFFC.2022.3211532"}
{"title": "Unraveling the mystery of efficacy in Chinese medicine formula: New approaches and technologies for research on pharmacodynamic substances.", "abstract": "Traditional Chinese medicine (TCM) is the key to unlock treasures of Chinese civilization. TCM and its compound play a beneficial role in medical activities to cure diseases, especially in major public health events such as novel coronavirus epidemics across the globe. The chemical composition in Chinese medicine formula is complex and diverse, but their effective substances resemble \"mystery boxes\". Revealing their active ingredients and their mechanisms of action has become focal point and difficulty of research for herbalists. Although the existing research methods are numerous and constantly updated iteratively, there is remain a lack of prospective reviews. Hence, this paper provides a comprehensive account of existing new approaches and technologies based on previous studies with an ", "journal": "Arabian journal of chemistry", "date": "2022-10-04", "authors": ["YaoleiLi", "ZhijianLin", "YuWang", "ShanshanJu", "HaoWu", "HongyuJin", "ShuangchengMa", "BingZhang"], "doi": "10.1016/j.arabjc.2022.104302\n10.1039/c4lc00999a\n10.1016/j.jtice.2020.08.028\n10.1021/acs.analchem.7b04833\n10.1016/j.talanta.2020.121411\n10.1016/j.trac.2020.115923\n10.3390/cancers14010190\n10.1016/j.jddst.2021.102794\n10.1177/2472555219830087\n10.1186/s13073-016-0303-2\n10.1039/c8cc08530g\n10.4268/cjcmm20151720\n10.1016/j.indcrop.2020.112673\n10.1038/ncomms8489\n10.1039/D0NJ03221B\n10.3389/fphar.2017.00228\n10.1016/j.jpba.2017.06.001\n10.1016/j.aca.2019.11.042\n10.1016/j.apsb.2020.10.002\n10.1016/j.jep.2021.114172\n10.1016/j.talanta.2019.120554\n10.1021/acs.analchem.6b03557\n10.1016/j.scib.2021.01.005\n10.1016/j.yexcr.2020.112139\n10.1016/j.talanta.2020.121453\n10.1093/jmcb/mjaa036\n10.1007/s11101-019-09635-x\n10.3390/antiox11040658\n10.1002/pca.2888\n10.1038/ncb3312\n10.1016/j.chroma.2021.462237\n10.1007/s10911-016-9359-2\n10.1016/j.chroma.2018.11.024\n10.3390/pharmaceutics13091373\n10.1016/j.jep.2020.112886\n10.1021/ac00276a003\n10.1016/j.jpba.2018.02.005\n10.1080/13880209.2020.1822421\n10.1016/j.apsb.2020.01.019\n10.11842/wst.2015.05.025\n10.1016/j.ijms.2019.03.001\n10.1016/j.apsb.2021.11.007\n10.1016/j.phrs.2020.105077\n10.1016/j.phymed.2018.01.019\n10.1016/j.chroma.2021.462178\n10.1016/j.apsb.2021.08.030\n10.1021/acs.analchem.0c05277\n10.1038/s41467-018-07594-z\n10.3321/j.issn:0513-4870.2009.03.008\n10.3390/biom9100577\n10.1016/j.freeradbiomed.2021.11.023\n10.1016/j.jep.2019.111900\n10.1016/j.jchromb.2019.04.032\n10.1021/acs.analchem.0c03956\n10.1126/science.abn6158\n10.1186/s40580-021-00270-x\n10.1038/s41589-020-0555-4\n10.1126/science.1247125\n10.1021/acs.analchem.0c00227\n10.1016/j.jchromb.2018.03.046\n10.1016/j.apsb.2019.10.001\n10.1021/acsnano.9b01346\n10.1016/j.foodchem.2021.131528\n10.1016/j.procbio.2013.02.005\n10.1016/j.jpba.2019.112795\n10.1016/j.talanta.2021.122873\n10.1007/s00216-013-7612-8\n10.11669/cpj.2019.13.001\n10.1073/pnas.1706778114\n10.1016/j.jff.2018.09.024\n10.1016/j.jep.2021.114439\n10.1016/j.jchromb.2013.02.009\n10.1016/j.ijbiomac.2021.08.231\n10.1007/s11095-013-1283-1\n10.1016/j.jep.2020.112687\n10.1038/s41467-019-11370-y\n10.1016/j.apsb.2020.04.001\n10.1038/s41419-019-1638-6\n10.1016/j.jchromb.2021.122793\n10.1016/j.chroma.2019.03.053\n10.1016/j.phymed.2021.153790\n10.1002/anie.202002151\n10.1016/j.microc.2020.104949\n10.1038/srep35544\n10.1007/s00204-022-03234-0\n10.1038/s41587-021-01055-7\n10.3390/molecules25143256\n10.1016/j.copbio.2018.03.011\n10.1016/j.ijbiomac.2020.07.297\n10.19540/j.cnki.cjcmm.20210129.601\n10.3389/fphar.2018.00622\n10.1016/j.aca.2021.338452\n10.3969/j.issn.1001-859X.1999.03.001\n10.1136/gutjnl-2019-319114\n10.1007/s42242-020-00124-1\n10.1038/523266a\n10.1080/07391102.2020.1715260\n10.1016/j.biopha.2022.112895\n10.1016/j.chroma.2021.462021\n10.1016/j.ijbiomac.2020.10.018\n10.1016/j.chroma.2017.08.037\n10.1039/c4cc08728c\n10.1016/j.chroma.2019.460501\n10.1021/jasms.9b00097\n10.1016/j.aca.2018.12.020\n10.1155/2019/1983780\n10.16438/j.0513-4870.2019-0413\n10.1016/j.ijantimicag.2017.10.001\n10.1007/s00210-019-01786-0\n10.3969/j.issn.1674-3849.2002.02.001\n10.1021/acsami.1c06968\n10.1016/j.apsb.2019.12.004\n10.1016/j.jep.2020.112733\n10.1021/acs.jafc.7b05280\n10.1039/c9fo02475a\n10.1016/j.phrs.2021.105913\n10.1016/j.pharmthera.2021.107824\n10.1016/j.talanta.2019.120367\n10.1155/2019/5135692\n10.1039/c4ay01809e\n10.1016/j.chroma.2017.08.072\n10.1073/pnas.0712365105\n10.1016/j.jchromb.2021.122889\n10.1016/j.jpba.2021.113999\n10.1039/d0tb02661a\n10.1136/gutjnl-2017-315458\n10.1016/j.talanta.2019.03.047\n10.1038/srep37471\n10.16438/j.0513-4870.2018-1008\n10.1039/d0fo01262a\n10.1016/j.chroma.2018.09.027\n10.1038/d41586-018-01079-1\n10.3389/fphar.2020.00508\n10.1186/s13045-018-0662-9\n10.1016/j.talanta.2019.02.011\n10.1016/j.patcog.2020.107558\n10.7501/j.issn.0253-2670.2015.08.001\n10.4268/cjcmm20151719\n10.1016/j.phrs.2020.105034\n10.1016/j.jff.2017.11.049\n10.1039/c004590j\n10.1016/j.apsb.2021.02.017\n10.1038/s41374-018-0025-8\n10.19540/j.cnki.cjcmm.20190222.011\n10.1038/s41467-022-28494-3\n10.16438/j.0513-4870.2018-0912\n10.7501/j.issn.1674-6376.2019.07.036\n10.1016/j.eng.2019.03.009\n10.3389/fphar.2019.00650\n10.1016/j.eng.2018.11.008\n10.1186/s13007-020-00571-y\n10.1016/j.phymed.2018.03.003\n10.1021/acscentsci.9b01125\n10.1016/j.jpba.2020.113419\n10.1016/j.apsb.2019.12.011\n10.19540/j.cnki.cjcmm.20210222.201\n10.1016/j.phrs.2020.104935\n10.1016/j.jchromb.2021.122979\n10.3390/pharmaceutics13081280\n10.3390/molecules24193431"}
{"title": "Automatic Detection of Cases of COVID-19 Pneumonia from Chest X-ray Images and Deep Learning Approaches.", "abstract": "Machine learning has already been used as a resource for disease detection and health care as a complementary tool to help with various daily health challenges. The advancement of deep learning techniques and a large amount of data-enabled algorithms to outperform medical teams in certain imaging tasks, such as pneumonia detection, skin cancer classification, hemorrhage detection, and arrhythmia detection. Automated diagnostics, which are enabled by images extracted from patient examinations, allow for interesting experiments to be conducted. This research differs from the related studies that were investigated in the experiment. These works are capable of binary categorization into two categories. COVID-Net, for example, was able to identify a positive case of COVID-19 or a healthy person with 93.3% accuracy. Another example is CHeXNet, which has a 95% accuracy rate in detecting cases of pneumonia or a healthy state in a patient. Experiments revealed that the current study was more effective than the previous studies in detecting a greater number of categories and with a higher percentage of accuracy. The results obtained during the model's development were not only viable but also excellent, with an accuracy of nearly 96% when analyzing a chest X-ray with three possible diagnoses in the two experiments conducted.", "journal": "Computational intelligence and neuroscience", "date": "2022-10-04", "authors": ["FahimaHajjej", "SarraAyouni", "MalekHasan", "TanvirAbir"], "doi": "10.1155/2022/7451551\n10.1186/s41256-020-00135-6\n10.1186/s13643-021-01648-y\n10.21203/rs.3.rs-198847/v1\n10.1080/22221751.2020.1772678\n10.1016/j.media.2020.101794\n10.1155/2021/8148772\n10.21931/RB/2020.05.03.19\n10.1109/access.2020.3010226\n10.3233/XST-200715\n10.2991/ijcis.d.210518.001\n10.1016/j.cmpb.2020.105532\n10.1155/2022/4569879\n10.1504/ijcsm.2022.122146\n10.1016/j.matpr.2021.05.553\n10.1016/j.ijin.2020.12.002\n10.1007/s00521-022-06918-x\n10.3390/s22031211\n10.1155/2021/5759184\n10.32604/csse.2022.022014\n10.1016/j.bbe.2020.08.008"}
{"title": "Augmentation of literature review of COVID-19 radiology.", "abstract": "We suggest an augmentation of the excellent comprehensive review article titled \"Comprehensive literature review on the radiographic findings, imaging modalities, and the role of radiology in the coronavirus disease 2019 (COVID-19) pandemic\" under the following categories: (1) \"Inclusion of additional radiological features, related to pulmonary infarcts and to COVID-19 pneumonia\"; (2) \"Amplified discussion of cardiovascular COVID-19 manifestations and the role of cardiac magnetic resonance imaging in monitoring and prognosis\"; (3) \"Imaging findings related to fluorodeoxyglucose positron emission tomography, optical, thermal and other imaging modalities/devices, including 'intelligent edge' and other remote monitoring devices\"; (4) \"Artificial intelligence in COVID-19 imaging\"; (5) \"Additional annotations to the radiological images in the manuscript to illustrate the additional signs discussed\"; and (6) \"A minor correction to a passage on pulmonary destruction\".", "journal": "World journal of radiology", "date": "2022-10-04", "authors": ["Suleman AdamMerchant", "PrakashNadkarni", "Mohd Javed SaifullahShaikh"], "doi": "10.4329/wjr.v14.i9.342"}
{"title": "Wavelet transformation can enhance computed tomography texture features: a multicenter radiomics study for grade assessment of COVID-19 pulmonary lesions.", "abstract": "This study set out to develop a computed tomography (CT)-based wavelet transforming radiomics approach for grading pulmonary lesions caused by COVID-19 and to validate it using real-world data.\nThis retrospective study analyzed 111 patients with 187 pulmonary lesions from 16 hospitals; all patients had confirmed COVID-19 and underwent non-contrast chest CT. Data were divided into a training cohort (72 patients with 127 lesions from nine hospitals) and an independent test cohort (39 patients with 60 lesions from seven hospitals) according to the hospital in which the CT was performed. In all, 73 texture features were extracted from manually delineated lesion volumes, and 23 three-dimensional (3D) wavelets with eight decomposition modes were implemented to compare and validate the value of wavelet transformation for grade assessment. Finally, the optimal machine learning pipeline, valuable radiomic features, and final radiomic models were determined. The area under the receiver operating characteristic (ROC) curve (AUC), calibration curve, and decision curve were used to determine the diagnostic performance and clinical utility of the models.\nOf the 187 lesions, 108 (57.75%) were diagnosed as mild lesions and 79 (42.25%) as moderate/severe lesions. All selected radiomic features showed significant correlations with the grade of COVID-19 pulmonary lesions (P<0.05). Biorthogonal 1.1 (bior1.1) LLL was determined as the optimal wavelet transform mode. The wavelet transforming radiomic model had an AUC of 0.910 in the test cohort, outperforming the original radiomic model (AUC =0.880; P<0.05). Decision analysis showed the radiomic model could add a net benefit at any given threshold of probability.\nWavelet transformation can enhance CT texture features. Wavelet transforming radiomics based on CT images can be used to effectively assess the grade of pulmonary lesions caused by COVID-19, which may facilitate individualized management of patients with this disease.", "journal": "Quantitative imaging in medicine and surgery", "date": "2022-10-04", "authors": ["ZekunJiang", "JinYin", "PeilunHan", "NanChen", "QingboKang", "YueQiu", "YiyueLi", "QichengLao", "MiaoSun", "DanYang", "ShanHuang", "JiajunQiu", "KangLi"], "doi": "10.21037/qims-22-252\n10.1056/NEJMoa2002032\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(21)02143-7\n10.1148/radiol.2020200823\n10.1148/radiol.2020200642\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020201160\n10.1038/s41551-021-00704-1\n10.1007/s12539-020-00410-7\n10.1016/j.chaos.2020.109944\n10.1038/s41467-020-17971-2\n10.2196/19569\n10.21037/qims-20-782\n10.1109/TMI.2020.3021254\n10.1371/journal.pone.0263916\n10.1016/j.compbiomed.2022.105298\n10.1016/j.media.2020.101910\n10.1148/ryct.2020200047\n10.1109/RBME.2020.2987975\n10.1016/j.acra.2020.09.004\n10.1155/2021/2263469\n10.1186/s40644-021-00406-6\n10.3322/caac.21552\n10.1016/j.ejrad.2020.108991\n10.21037/qims.2020.03.10\n10.1038/s41598-021-01470-5\n10.1186/s12885-020-6523-2\n10.3389/fonc.2018.00096\n10.1016/j.neuroimage.2006.01.015\n10.1158/0008-5472.CAN-17-0339\n10.1148/radiol.2020191145\n10.3390/cancers11121937\n10.18452/2487\n10.18452/2487\n10.1007/s40618-021-01672-8\n10.1017/S0033291719003027\n10.3390/diagnostics11081317\n10.1016/j.compbiomed.2021.104665\n10.1049/ipr2.12153\n10.1080/23808993.2019.1585805\n10.1109/MSP.2018.2877123\n10.1016/j.jksuci.2019.06.012\n10.1148/rg.2017170056\n10.1590/1678-9199-jvatitd-2020-0011\n10.1007/s00330-021-08368-w\n10.1007/s10278-021-00484-9\n10.2967/jnumed.118.222893\n10.1038/s41416-021-01381-2\n10.1038/nrclinonc.2017.141\n10.1007/s00330-019-06360-z"}
{"title": "Biomarkers extracted by fully automated body composition analysis from chest CT correlate with SARS-CoV-2 outcome severity.", "abstract": "The complex process of manual biomarker extraction from body composition analysis (BCA) has far restricted the analysis of SARS-CoV-2 outcomes to small patient cohorts and a limited number of tissue types. We investigate the association of two BCA-based biomarkers with the development of severe SARS-CoV-2 infections for 918 patients (354 female, 564 male) regarding disease severity and mortality (186 deceased). Multiple tissues, such as muscle, bone, or adipose tissue are used and acquired with a deep-learning-based, fully-automated BCA from computed tomography images of the chest. The BCA features and markers were univariately analyzed with a Shapiro-Wilk and two-sided Mann-Whitney-U test. In a multivariate approach, obtained markers were adjusted by a defined set of laboratory parameters promoted by other studies. Subsequently, the relationship between the markers and two endpoints, namely severity and mortality, was investigated with regard to statistical significance. The univariate approach showed that the muscle volume was significant for female (p", "journal": "Scientific reports", "date": "2022-10-01", "authors": ["Ren\u00e9Hosch", "SimoneKattner", "Marc MoritzBerger", "ThorstenBrenner", "JohannesHaubold", "JensKleesiek", "SvenKoitka", "LennardKroll", "AnisaKureishi", "NilsFlaschel", "FelixNensa"], "doi": "10.1038/s41598-022-20419-w\n10.1016/j.dsx.2020.06.060\n10.2196/26075\n10.1038/s41586-020-2521-4\n10.1016/S2213-8587(21)00089-9\n10.1016/j.clnesp.2020.09.018\n10.1186/s12911-021-01576-w\n10.1016/j.imu.2021.100564\n10.1016/j.isci.2021.103523\n10.1016/j.metabol.2020.154378\n10.1093/bja/aev541\n10.1007/s00261-020-02693-2\n10.1186/s12933-021-01327-1\n10.1002/oby.22971\n10.1080/17512433.2017.1347503\n10.1016/j.ejca.2015.12.030\n10.1002/jcsm.12379\n10.1097/RTI.0000000000000428\n10.1002/jcsm.12573\n10.1007/s00330-020-07147-3\n10.1016/j.ejrad.2021.110031\n10.1038/s41598-021-00161-5\n10.1111/nyas.12842\n10.3390/jcm10020356\n10.1038/oby.2008.575\n10.1038/ncpcardio0319\n10.1016/j.numecd.2013.11.010\n10.3389/fphys.2021.651167\n10.1007/s11906-019-0939-6\n10.1093/ehjci/jeu006\n10.1016/j.jcct.2017.11.007\n10.1016/j.amjcard.2016.01.033\n10.5812/ijem.3505\n10.1038/s41592-019-0686-2\n10.1016/j.metabol.2020.154436\n10.1148/radiol.2021204141"}
{"title": "AI in Health Science: A Perspective.", "abstract": "By helping practitioners understand complicated and varied types of data, Artificial Intelligence (AI) has influenced medical practice deeply. It is the use of a computer to mimic intelligent behaviour. Many medical professions, particularly those reliant on imaging or surgery, are progressively developing AI. While AI cognitive component outperforms human intellect, it lacks awareness, emotions, intuition, and adaptability. With minimum human participation, AI is quickly growing in healthcare, and numerous AI applications have been created to address current issues. This article explains AI, its various elements and how to utilize them in healthcare. It also offers practical suggestions for developing an AI strategy to assist the digital healthcare transition.", "journal": "Current pharmaceutical biotechnology", "date": "2022-10-01", "authors": ["RaghavMishra", "KajalChaudhary", "IshaMishra"], "doi": "10.2174/1389201023666220929145220"}
{"title": "COVID-19 Semantic Pneumonia Segmentation and Classification Using Artificial Intelligence.", "abstract": "Coronavirus 2019 (COVID-19) has become a pandemic. The seriousness of COVID-19 can be realized from the number of victims worldwide and large number of deaths. This paper presents an efficient deep semantic segmentation network (DeepLabv3Plus). Initially, the dynamic adaptive histogram equalization is utilized to enhance the images. Data augmentation techniques are then used to augment the enhanced images. The second stage builds a custom convolutional neural network model using several pretrained ImageNet models and compares them to repeatedly trim the best-performing models to reduce complexity and improve memory efficiency. Several experiments were done using different techniques and parameters. Furthermore, the proposed model achieved an average accuracy of 99.6% and an area under the curve of 0.996 in the COVID-19 detection. This paper will discuss how to train a customized smart convolutional neural network using various parameters on a set of chest X-rays with an accuracy of 99.6%.", "journal": "Contrast media & molecular imaging", "date": "2022-10-01", "authors": ["Mohammed JAbdulaal", "Ibrahim MMehedi", "Abdullah MAbusorrah", "Abdulah JezaAljohani", "Ahmad HMilyani", "Md MasudRana", "MohamedMahmoud"], "doi": "10.1155/2022/5297709\n10.1109/tnnls.2020.2995800\n10.1109/tnnls.2017.2766168\n10.1016/j.media.2020.101836\n10.1109/access.2020.3016780\n10.1016/j.mehy.2020.109761\n10.1016/j.imu.2020.100360\n10.1016/j.knosys.2020.106270\n10.1016/j.asoc.2020.106580\n10.1109/ICNN.1993.298572\n10.1109/tmi.2016.2528162\n10.1007/s13246-020-00888-x\n10.1016/j.compbiomed.2020.103792\n10.1001/jama.2020.3786\n10.1007/978-981-16-7618-5_3\n10.1016/j.eswa.2020.114054\n10.7717/peerj-cs.358\n10.1109/tgrs.2022.3155765\n10.1504/ijhm.2021.114174\n10.1109/access.2020.2971257\n10.1038/s41598-020-76550-z\n10.1101/2020.03.26.20044610\n10.1016/j.cmpb.2020.105532\n10.3389/fpubh.2020.00437\n10.1109/access.2020.3003810\n10.1016/j.patrec.2020.04.016\n10.1109/access.2019.2941937\n10.1504/ijhm.2021.114173\n10.1109/access.2021.3120717\n10.1504/ijhm.2021.120616\n10.1109/access.2021.3101142"}
{"title": "A novel multimodal fusion framework for early diagnosis and accurate classification of COVID-19 patients using X-ray images and speech signal processing techniques.", "abstract": "COVID-19 outbreak has become one of the most challenging problems for human being. It is a communicable disease caused by a new coronavirus strain, which infected over 375 million people already and caused almost 6 million deaths. This paper aims to develop and design a framework for early diagnosis and fast classification of COVID-19 symptoms using multimodal Deep Learning techniques.\nwe collected chest X-ray and cough sample data from open source datasets, Cohen and datasets and local hospitals. The features are extracted from the chest X-ray images are extracted from chest X-ray datasets. We also used cough audio datasets from Coswara project and local hospitals. The publicly available Coughvid DetectNow and Virufy datasets are used to evaluate COVID-19 detection based on speech sounds, respiratory, and cough. The collected audio data comprises slow and fast breathing, shallow and deep coughing, spoken digits, and phonation of sustained vowels. Gender, geographical location, age, preexisting medical conditions, and current health status (COVID-19 and Non-COVID-19) are recorded.\nThe proposed framework uses the selection algorithm of the pre-trained network to determine the best fusion model characterized by the pre-trained chest X-ray and cough models. Third, deep chest X-ray fusion by discriminant correlation analysis is used to fuse discriminatory features from the two models. The proposed framework achieved recognition accuracy, specificity, and sensitivity of 98.91%, 96.25%, and 97.69%, respectively. With the fusion method we obtained 94.99% accuracy.\nThis paper examines the effectiveness of well-known ML architectures on a joint collection of chest-X-rays and cough samples for early classification of COVID-19. It shows that existing methods can effectively used for diagnosis and suggesting that the fusion learning paradigm could be a crucial asset in diagnosing future unknown illnesses. The proposed framework supports health informatics basis on early diagnosis, clinical decision support, and accurate prediction.", "journal": "Computer methods and programs in biomedicine", "date": "2022-09-30", "authors": ["SantoshKumar", "Mithilesh KumarChaube", "Saeed HamoodAlsamhi", "Sachin KumarGupta", "MohsenGuizani", "RaffaeleGravina", "GiancarloFortino"], "doi": "10.1016/j.cmpb.2022.107109\n10.1109/TDSC.2022.3144657"}
{"title": "A Deep Learning based Solution (Covi-DeteCT) Amidst COVID-19.", "abstract": "The whole world has been severely affected due to the COVID-19 pandemic. The rapid and large-scale spread has caused immense pressure on the medical sector hence increasing the chances of false detection due to human errors and mishandling of reports. At the time of outbreaks of COVID-19, there is a crucial shortage of test kits as well. Quick diagnostic testing has become one of the main challenges. For the detection of COVID-19, many Artificial Intelligence based methodologies have been proposed, a few had suggested integration of the model on a public usable platform, but none had executed this on a working application as per our knowledge.\nKeeping the above comprehension in mind, the objective is to provide an easy-to-use platform for COVID-19 identification. This work would be a contribution to the digitization of health facilities. This work is a fusion of deep learning classifiers and medical images to provide a speedy and accurate identification of the COVID-19 virus by analyzing the user's CT scan images of the lungs. It will assist healthcare workers in reducing their workload and decreasing the possibility of false detection.\nIn this work, various models like Resnet50V2 and Resnet101V2, an adjusted rendition of ResNet101V2 with Feature Pyramid Network, have been applied for classifying the CT scan images into the categories: normal or COVID-19 positive.\nA detailed analysis of all three models' performances have been done on the SARS-CoV-2 dataset with various metrics like precision, recall, F1-score, ROC curve, etc. It was found that Resnet50V2 achieves an accuracy of 96.79%, whereas Resnet101V2 achieves an accuracy of 97.79%. An accuracy of 98.19% has been obtained by ResNet101V2 with Feature Pyramid Network. As Res- Net101V2 with Feature Pyramid Network is showing better results, thus, it is further incorporated into a working application that takes CT images as input from the user and feeds into the trained model and detects the presence of COVID-19 infection.\nA mobile application integrated with the deeper variant of ResNet, i.e., ResNet101V2 with FPN checks the presence of COVID-19 in a faster and accurate manner. People can use this application on their smart mobile devices. This automated system would assist healthcare workers as well, which ultimately reduces their workload and decreases the possibility of false detection.", "journal": "Current medical imaging", "date": "2022-09-30", "authors": ["KavitaPandey"], "doi": "10.2174/1573405618666220928145344"}
{"title": "The 2021 SIIM-FISABIO-RSNA Machine Learning COVID-19 Challenge: Annotation and Standard Exam Classification of COVID-19 Chest Radiographs.", "abstract": "We describe the curation, annotation methodology, and characteristics of the dataset used in an artificial intelligence challenge for detection and localization of COVID-19 on chest radiographs. The chest radiographs were annotated by an international group of radiologists into four mutually exclusive categories, including \"typical,\" \"indeterminate,\" and \"atypical appearance\" for COVID-19, or \"negative for pneumonia,\" adapted from previously published guidelines, and bounding boxes were placed on airspace opacities. This dataset and respective annotations are available to researchers for academic and noncommercial use.", "journal": "Journal of digital imaging", "date": "2022-09-29", "authors": ["ParasLakhani", "JMongan", "CSinghal", "QZhou", "K PAndriole", "W FAuffermann", "P MPrasanna", "T XPham", "MichaelPeterson", "P JBergquist", "T SCook", "S FFerraciolli", "G C ACorradi", "M STakahashi", "C SWorkman", "MParekh", "S IKamel", "JGalant", "AMas-Sanchez", "E CBen\u00edtez", "MS\u00e1nchez-Valverde", "LJaques", "MPanadero", "MVidal", "MCulia\u00f1ez-Casas", "DAngulo-Gonzalez", "S GLanger", "Mar\u00edade la Iglesia-Vay\u00e1", "GShih"], "doi": "10.1007/s10278-022-00706-8\n10.1016/S2213-2600(20)30527-0\n10.1016/j.jped.2020.08.001\n10.1177/074823378500100206\n10.1007/s00134-012-2682-1\n10.1148/radiol.2020201365\n10.1148/radiol.2017162326\n10.1371/journal.pmed.1002697\n10.1148/radiol.2020201874\n10.1007/s00330-020-07269-8\n10.2214/AJR.20.24801\n10.1007/s11547-020-01202-1\n10.1016/j.acra.2021.01.016\n10.1148/radiol.2021203957\n10.1148/radiol.2018180736\n10.1007/s10278-019-00299-9\n10.1037/1040-3590.6.4.284\n10.1148/radiol.2020201160"}
{"title": "IEViT: An enhanced vision transformer architecture for chest X-ray image classification.", "abstract": "Chest X-ray imaging is a relatively cheap and accessible diagnostic tool that can assist in the diagnosis of various conditions, including pneumonia, tuberculosis, COVID-19, and others. However, the requirement for expert radiologists to view and interpret chest X-ray images can be a bottleneck, especially in remote and deprived areas. Recent advances in machine learning have made possible the automated diagnosis of chest X-ray scans. In this work, we examine the use of a novel Transformer-based deep learning model for the task of chest X-ray image classification.\nWe first examine the performance of the Vision Transformer (ViT) state-of-the-art image classification machine learning model for the task of chest X-ray image classification, and then propose and evaluate the Input Enhanced Vision Transformer (IEViT), a novel enhanced Vision Transformer model that can achieve improved performance on chest X-ray images associated with various pathologies.\nExperiments on four chest X-ray image data sets containing various pathologies (tuberculosis, pneumonia, COVID-19) demonstrated that the proposed IEViT\u00a0model outperformed ViT for all the data sets and variants examined, achieving an F1-score between 96.39% and 100%, and an improvement over ViT of up to +5.82% in terms of F1-score across the four examined data sets. IEViT's maximum sensitivity (recall) ranged between 93.50% and 100% across the four data sets, with an improvement over ViT of up to +3%, whereas IEViT's maximum precision ranged between 97.96% and 100% across the four data sets, with an improvement over ViT of up to +6.41%.\nResults showed that the proposed IEViT\u00a0model outperformed all ViT's variants for all the examined chest X-ray image data sets, demonstrating its superiority and generalisation ability. Given the relatively low cost and the widespread accessibility of chest X-ray imaging, the use of the proposed IEViT\u00a0model can potentially offer a powerful, but relatively cheap and accessible method for assisting diagnosis using chest X-ray images.", "journal": "Computer methods and programs in biomedicine", "date": "2022-09-27", "authors": ["Gabriel IluebeOkolo", "StamosKatsigiannis", "NaeemRamzan"], "doi": "10.1016/j.cmpb.2022.107141"}
{"title": "Machine learning techniques for CT imaging diagnosis of novel coronavirus pneumonia: a review.", "abstract": "Since 2020, novel coronavirus pneumonia has been spreading rapidly around the world, bringing tremendous pressure on medical diagnosis and treatment for hospitals. Medical imaging methods, such as computed tomography (CT), play a crucial role in diagnosing and treating COVID-19. A large number of CT images (with large volume) are produced during the CT-based medical diagnosis. In such a situation, the diagnostic judgement by human eyes on the thousands of CT images is inefficient and time-consuming. Recently, in order to improve diagnostic efficiency, the machine learning technology is being widely used in computer-aided diagnosis and treatment systems (i.e., CT Imaging) to help doctors perform accurate analysis and provide them with effective diagnostic decision support. In this paper, we comprehensively review these frequently used machine learning methods applied in the CT Imaging Diagnosis for the COVID-19, discuss the machine learning-based applications from the various kinds of aspects including the image acquisition and pre-processing, image segmentation, quantitative analysis and diagnosis, and disease follow-up and prognosis. Moreover, we also discuss the limitations of the up-to-date machine learning technology in the context of CT imaging computer-aided diagnosis.", "journal": "Neural computing & applications", "date": "2022-09-27", "authors": ["JingjingChen", "YixiaoLi", "LinglingGuo", "XiaokangZhou", "YihanZhu", "QingfengHe", "HaijunHan", "QilongFeng"], "doi": "10.1007/s00521-022-07709-0\n10.1016/j.ijid.2020.01.009\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2001316\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.1148/radiol.2020200343\n10.3906/sag-2004-331\n10.1183/13993003.00334-2020\n10.1183/13993003.00398-2020\n10.1007/s00330-020-06731-x\n10.1145/3411760\n10.1007/s12194-017-0406-5\n10.1038/s41591-018-0177-5\n10.1183/13993003.00986-2018\n10.1016/S2213-2600(18)30286-8\n10.1183/13993003.01216-2019\n10.1016/j.media.2017.06.014\n10.1016/j.bspc.2019.101586\n10.1016/j.cell.2020.04.045\n10.1117/1.JMI.5.3.036501\n10.1016/j.media.2010.02.004\n10.1007/s10278-012-9539-6\n10.1016/j.ijleo.2015.06.011\n10.1016/j.media.2012.02.001\n10.1145/3065386\n10.1007/978-3-030-00889-5_1\n10.1109/TPAMI.2017.2699184\n10.1109/JIOT.2021.3130434\n10.1109/TII.2021.3116085\n10.1109/JIOT.2021.3077449\n10.1109/TMI.2016.2536809\n10.1186/s12880-018-0286-0\n10.1038/s41591-020-0931-3\n10.1016/j.compbiomed.2020.103795\n10.1109/TMI.2020.2996256\n10.2196/19569\n10.1155/2020/8843664\n10.1016/j.ejrad.2020.109041\n10.21037/atm.2020.03.132\n10.1148/radiol.2020200905\n10.1016/j.eng.2020.04.010\n10.1016/j.patrec.2020.10.001\n10.1109/TCBB.2020.2994780\n10.1038/s41598-021-83424-5\n10.1148/radiol.2020201491\n10.1109/TMI.2020.2995508\n10.1109/TMI.2020.2994908\n10.1016/j.chaos.2020.110153\n10.1038/s41598-021-84561-7\n10.1088/1361-6560/abbf9e\n10.1016/j.jpha.2020.03.004\n10.1109/TII.2021.3061419\n10.1186/s12938-020-00807-x\n10.1109/RBME.2020.2987975\n10.1183/13993003.00775-2020\n10.32604/cmc.2020.010691\n10.1109/TCSS.2020.2987846\n10.1007/s11739-020-02475-0\n10.1007/s00330-020-07042-x\n10.1038/s41467-020-17280-8\n10.3389/fpubh.2020.00357\n10.2196/20259\n10.3390/jcm9061668\n10.1017/S0950268820001727\n10.1038/s42256-021-00307-0\n10.1007/s00330-020-07156-2\n10.1109/ACCESS.2018.2870052\n10.1016/j.inffus.2019.12.012\n10.1038/s41467-020-18685-1\n10.1016/j.radonc.2018.10.019\n10.48550/arXiv.2005.06465\n10.1186/s41747-020-00173-2\n10.1016/j.ijleo.2015.06.011"}
{"title": "E-GCS: Detection of COVID-19 through classification by attention bottleneck residual network.", "abstract": "Recently,\u2009 the coronavirus disease 2019 (COVID-19)\u2009 has caused mortality of many people globally. Thus, there existed a need to detect this disease to prevent its further spread. Hence, the study aims to predict COVID-19 infected patients based on deep learning (DL) and image processing.\nThe study intends to classify the normal and abnormal cases of COVID-19 by considering three different medical imaging modalities namely ultrasound imaging, X-ray images and CT scan images through introduced attention bottleneck residual network (AB-ResNet). It also aims to segment the abnormal infected area from normal images for\u2009localizing localising the disease infected area through the proposed edge based graph cut segmentation (E-GCS).\nAB-ResNet is used for classifying images whereas E-GCS segment the abnormal images. The study possess various advantages as it rely on DL and possess capability for accelerating the training speed of deep networks. It also enhance the network depth leading to minimum parameters, minimising the impact of vanishing gradient issue and attaining effective network performance with respect to better accuracy.\nPerformance and comparative analysis is undertaken to evaluate the efficiency of the introduced system and results explores the efficiency of the proposed system in COVID-19 detection with high accuracy (99%).", "journal": "Engineering applications of artificial intelligence", "date": "2022-09-27", "authors": ["TAhila", "A CSubhajini"], "doi": "10.1016/j.engappai.2022.105398"}
{"title": "Application of Deep Learning Techniques in Diagnosis of Covid-19 (Coronavirus): A Systematic Review.", "abstract": "Covid-19 is now one of the most incredibly intense and severe illnesses of the twentieth century. Covid-19 has already endangered the lives of millions of people worldwide due to its acute pulmonary effects. Image-based diagnostic techniques like X-ray, CT, and ultrasound are commonly employed to get a quick and reliable clinical condition. Covid-19 identification out of such clinical scans is exceedingly time-consuming, labor-intensive, and susceptible to silly intervention. As a result, radiography imaging approaches using Deep Learning (DL) are consistently employed to achieve great results. Various artificial intelligence-based systems have been developed for the early prediction of coronavirus using radiography pictures. Specific DL methods such as CNN and RNN noticeably extract extremely critical characteristics, primarily in diagnostic imaging. Recent coronavirus studies have used these techniques to utilize radiography image scans significantly. The disease, as well as the present pandemic, was studied using public and private data. A total of 64 pre-trained and custom DL models concerning imaging modality as taxonomies are selected from the studied articles. The constraints relevant to DL-based techniques are the sample selection, network architecture, training with minimal annotated database, and security issues. This includes evaluating causal agents, pathophysiology, immunological reactions, and epidemiological illness. DL-based Covid-19 detection systems are the key focus of this review article. Covid-19 work is intended to be accelerated as a result of this study.", "journal": "Neural processing letters", "date": "2022-09-27", "authors": ["Yogesh HBhosale", "K SridharPatnaik"], "doi": "10.1007/s11063-022-11023-0\n10.1101/2020.02.25.20021568\n10.1016/j.scs.2020.102571\n10.1109/ACCESS.2021.3058066\n10.1002/jmv.26709\n10.1109/ACCESS.2020.3003810\n10.33889/IJMEMS.2020.5.4.052\n10.1080/14737159.2021.1962708\n10.1016/j.matpr.2020.06.245\n10.1016/j.jiph.2020.03.019\n10.1080/14737159.2020.1757437\n10.1109/ACCESS.2021.3054484\n10.1080/14737159.2020.1816466\n10.1109/JBHI.2020.3030224\n10.1007/s12559-020-09787-5\n10.1080/07391102.2020.1767212\n10.1016/j.scs.2021.102777\n10.1007/s10489-020-01831-z\n10.1109/RBME.2020.2990959\n10.1007/s00521-020-05437-x\n10.1097/RLI.0000000000000748\n10.1148/radiol.2020200905\n10.1109/ACCESS.2021.3058537\n10.1016/j.csbj.2021.02.016\n10.1016/j.chaos.2020.110120\n10.2196/19569\n10.1007/s00500-020-05424-3\n10.1038/s41746-021-00399-3\n10.1016/j.ijleo.2021.166405\n10.1109/TII.2021.3057683\n10.1016/j.chaos.2020.110245\n10.1109/JBHI.2020.3037127\n10.1016/j.asoc.2021.107184\n10.1016/j.irbm.2021.01.004\n10.1016/j.ejrad.2020.109041\n10.1016/j.aej.2021.01.011\n10.1016/j.asoc.2020.106859\n10.1016/j.chaos.2020.110190\n10.1016/j.asoc.2020.106744\n10.1016/j.asoc.2020.106885\n10.1007/s10489-020-01902-1\n10.1007/s13246-020-00865-4\n10.1038/s41598-020-76550-z\n10.7937/tcia.2020.6c7y-gq39\n10.7910/DVN/6ACUZJ\n10.1016/j.ejrad.2020.109402\n10.21037/atm.2020.03.132\n10.1016/j.chaos.2020.110495\n10.1016/j.imu.2020.100505\n10.1016/j.eswa.2020.114054\n10.1016/j.asoc.2021.107160\n10.1109/ACCESS.2020.3016780\n10.1016/j.mehy.2020.109761\n10.1016/j.imu.2020.100427\n10.1007/s10140-020-01886-y\n10.1016/j.media.2020.101913\n10.1016/j.iot.2021.100377\n10.1002/ima.22706,32,2,(419-434)\n10.1007/s00264-020-04609-7\n10.1016/j.bspc.2021.102490\n10.1016/j.knosys.2020.106647\n10.1016/j.ibmed.2020.100013\n10.1016/j.imu.2020.100506\n10.1016/j.scs.2020.102589\n10.3390/app11020672\n10.1016/j.advms.2020.06.005\n10.1109/TMI.2020.2994459\n10.1016/j.media.2021.101993\n10.1016/j.compeleceng.2020.106960\n10.1109/ACCESS.2020.3005510\n10.1109/ACCESS.2020.2994762\n10.3348/kjr.2020.0536\n10.3390/electronics9091439\n10.1007/s42600-021-00132-9\n10.1186/s40537-020-00392-9\n10.1016/j.compbiomed.2020.103792"}
{"title": "Rapid artificial intelligence solutions in a pandemic-The COVID-19-20 Lung CT Lesion Segmentation Challenge.", "abstract": "Artificial intelligence (AI) methods for the automatic detection and quantification of COVID-19 lesions in chest computed tomography (CT) might play an important role in the monitoring and management of the disease. We organized an international challenge and competition for the development and comparison of AI algorithms for this task, which we supported with public data and state-of-the-art benchmark methods. Board Certified Radiologists annotated 295 public images from two sources (A and B) for algorithms training (n=199, source A), validation (n=50, source A) and testing (n=23, source A; n=23, source B). There were 1,096 registered teams of which 225 and 98 completed the validation and testing phases, respectively. The challenge showed that AI models could be rapidly designed by diverse teams with the potential to measure disease or facilitate timely and patient-specific interventions. This paper provides an overview and the major outcomes of the COVID-19 Lung CT Lesion Segmentation Challenge - 2020.", "journal": "Medical image analysis", "date": "2022-09-27", "authors": ["Holger RRoth", "ZiyueXu", "CarlosTor-D\u00edez", "RamonSanchez Jacob", "JonathanZember", "JoseMolto", "WenqiLi", "ShengXu", "BarisTurkbey", "EvrimTurkbey", "DongYang", "AhmedHarouni", "NicolaRieke", "ShishuaiHu", "FabianIsensee", "ClaireTang", "QinjiYu", "JanS\u00f6lter", "TongZheng", "VitaliLiauchuk", "ZiqiZhou", "Jan HendrikMoltz", "BrunoOliveira", "YongXia", "Klaus HMaier-Hein", "QikaiLi", "AndreasHusch", "LuyangZhang", "VassiliKovalev", "LiKang", "AlessaHering", "Jo\u00e3o LVila\u00e7a", "MonaFlores", "DaguangXu", "BradfordWood", "Marius GeorgeLinguraru"], "doi": "10.1016/j.media.2022.102605"}
{"title": "Domain and Content Adaptive Convolution Based Multi-Source Domain Generalization for Medical Image Segmentation.", "abstract": "The domain gap caused mainly by variable medical image quality renders a major obstacle on the path between training a segmentation model in the lab and applying the trained model to unseen clinical data. To address this issue, domain generalization methods have been proposed, which however usually use static convolutions and are less flexible. In this paper, we propose a multi-source domain generalization model based on the domain and content adaptive convolution (DCAC) for the segmentation of medical images across different modalities. Specifically, we design the domain adaptive convolution (DAC) module and content adaptive convolution (CAC) module and incorporate both into an encoder-decoder backbone. In the DAC module, a dynamic convolutional head is conditioned on the predicted domain code of the input to make our model adapt to the unseen target domain. In the CAC module, a dynamic convolutional head is conditioned on the global image features to make our model adapt to the test image. We evaluated the DCAC model against the baseline and four state-of-the-art domain generalization methods on the prostate segmentation, COVID-19 lesion segmentation, and optic cup/optic disc segmentation tasks. Our results not only indicate that the proposed DCAC model outperforms all competing methods on each segmentation task but also demonstrate the effectiveness of the DAC and CAC modules. Code is available at https://git.io/DCAC.", "journal": "IEEE transactions on medical imaging", "date": "2022-09-27", "authors": ["ShishuaiHu", "ZehuiLiao", "JianpengZhang", "YongXia"], "doi": "10.1109/TMI.2022.3210133"}
{"title": "State of the Art in Lung Ultrasound, Shifting from Qualitative to Quantitative Analyses.", "abstract": "Lung ultrasound (LUS) has been increasingly expanding since the 1990s, when the clinical relevance of vertical artifacts was first reported. However, the massive spread of LUS is only recent and is associated with the coronavirus disease 2019 (COVID-19) pandemic, during which semi-quantitative computer-aided techniques were proposed to automatically classify LUS data. In this review, we discuss the state of the art in LUS, from semi-quantitative image analysis approaches to quantitative techniques involving the analysis of radiofrequency data. We also discuss recent in vitro and in silico studies, as well as research on LUS safety. Finally, conclusions are drawn highlighting the potential future of LUS.", "journal": "Ultrasound in medicine & biology", "date": "2022-09-27", "authors": ["FedericoMento", "UmairKhan", "FrancescoFaita", "AndreaSmargiassi", "RiccardoInchingolo", "TizianoPerrone", "LibertarioDemi"], "doi": "10.1016/j.ultrasmedbio.2022.07.007"}
{"title": "Advancements in COVID-19 Testing: An In-depth Overview.", "abstract": "COVID-19 rapidly evolved as a pandemic, killing and hospitalising millions of people, and creating unprecedented hurdles for communities and health care systems worldwide. The rapidly evolving pandemic prompted the head of the World Health Organisation to deliver a critical message: \"test, test, test.\" The response from the diagnostic industry and researchers worldwide was overwhelming, resulting in more than a thousand commercial tests being available worldwide. Several sampling approaches and diagnostic techniques have been employed from the early stages of the pandemic, such as SARS-CoV-2 detection by targeting the viral RNA or protein indirectly via antibody testing, biochemical estimation, and various imaging techniques, and many are still in the various stages of development and yet to be marketed. Accurate testing techniques and appropriate sampling are the need of the hour to manage, diagnose and treat the pandemic, especially in the current crisis where SARS-CoV-2 undergoes constant mutation, evolving into various strains, which are pretty challenging. The article discusses various testing techniques as well as screening methods for detection, treatment, and management of COVID-19 transmissions, such as NAAT, PCR, isothermal detection including RT-LAMP, RPA, NASBA, RCA, SDA, NEAR, and TMA, CRISPR strategy, nanotechnology approach, metagenomic profiling, point of care tests, virus neutralization test, ELISA, biomarker estimation, utilization of imaging techniques such as CT, ultrasonography, brain MRI in COVID-19 complications, and other novel strategies including microarray methods, microfluidic methods and artificial intelligence with an emphasis on advancements in the testing strategies for the diagnosis, management, and prevention of COVID-19.", "journal": "Current pharmaceutical biotechnology", "date": "2022-09-27", "authors": ["RajeshKumar", "SeethaHarilal", "Abdullah GAl-Sehemi", "MehboobaliPannipara", "Githa ElizabethMathew", "BijoMathew"], "doi": "10.2174/1389201023666220921144150"}
{"title": "Analysis of the Causes of Solitary Pulmonary Nodule Misdiagnosed as Lung Cancer by Using Artificial Intelligence: A Retrospective Study at a Single Center.", "abstract": "Artificial intelligence (AI) adopting deep learning technology has been widely used in the med-ical imaging domain in recent years. It realized the automatic judgment of benign and malig-nant solitary pulmonary nodules (SPNs) and even replaced the work of doctors to some extent. However, misdiagnoses can occur in certain cases. Only by determining the causes can AI play a larger role. A total of 21 Coronavirus disease 2019 (COVID-19) patients were diagnosed with SPN by CT imaging. Their Clinical data, including general condition, imaging features, AI re-ports, and outcomes were included in this retrospective study. Although they were confirmed COVID-19 by testing reverse transcription-polymerase chain reaction (RT-PCR) with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), their CT imaging data were misjudged by AI to be high-risk nodules for lung cancer. Imaging characteristics included burr sign (76.2%), lobulated sign (61.9%), pleural indentation (42.9%), smooth edges (23.8%), and cavity (14.3%). The accuracy of AI was different from that of radiologists in judging the nature of be-nign SPNs (p < 0.001, \u03ba = 0.036 < 0.4, means the two diagnosis methods poor fit). COVID-19 patients with SPN might have been misdiagnosed using the AI system, suggesting that the AI system needs to be further optimized, especially in the event of a new disease outbreak.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-09-24", "authors": ["Xiong-YingWu", "FanDing", "KunLi", "Wen-CaiHuang", "YongZhang", "JianZhu"], "doi": "10.3390/diagnostics12092218\n10.1016/j.chest.2017.01.018\n10.1148/radiol.2017161659\n10.1109/TMI.2016.2629462\n10.1016/j.media.2017.06.015\n10.1002/mp.12846\n10.1109/TBME.2016.2613502\n10.1038/srep24454\n10.1016/j.cell.2020.04.045\n10.3390/cancers12082211\n10.1056/NEJMoa2001316\n10.1155/2017/4067832\n10.1038/srep46479\n10.1371/journal.pone.0248957\n10.1016/j.jtho.2020.04.030\n10.1016/j.jinf.2020.03.033\n10.1007/s00330-020-07042-x\n10.1097/CM9.0000000000000634\n10.3390/cancers11111673\n10.1111/1759-7714.13185\n10.1016/j.cell.2018.02.010\n10.1136/thoraxjnl-2019-214104\n10.1038/s41586-020-2012-7\n10.1016/j.compbiomed.2012.12.004\n10.1038/nbt.4233\n10.1136/bmj.m606"}
{"title": "Segmentation-Based Classification Deep Learning Model Embedded with Explainable AI for COVID-19 Detection in Chest X-ray Scans.", "abstract": "Background and Motivation: COVID-19 has resulted in a massive loss of life during the last two years. The current imaging-based diagnostic methods for COVID-19 detection in multiclass pneumonia-type chest X-rays are not so successful in clinical practice due to high error rates. Our hypothesis states that if we can have a segmentation-based classification error rate <5%, typically adopted for 510 (K) regulatory purposes, the diagnostic system can be adapted in clinical settings. Method: This study proposes 16 types of segmentation-based classification deep learning-based systems for automatic, rapid, and precise detection of COVID-19. The two deep learning-based segmentation networks, namely UNet and UNet+, along with eight classification models, namely VGG16, VGG19, Xception, InceptionV3, Densenet201, NASNetMobile, Resnet50, and MobileNet, were applied to select the best-suited combination of networks. Using the cross-entropy loss function, the system performance was evaluated by Dice, Jaccard, area-under-the-curve (AUC), and receiver operating characteristics (ROC) and validated using Grad-CAM in explainable AI framework. Results: The best performing segmentation model was UNet, which exhibited the accuracy, loss, Dice, Jaccard, and AUC of 96.35%, 0.15%, 94.88%, 90.38%, and 0.99 (p-value <0.0001), respectively. The best performing segmentation-based classification model was UNet+Xception, which exhibited the accuracy, precision, recall, F1-score, and AUC of 97.45%, 97.46%, 97.45%, 97.43%, and 0.998 (p-value <0.0001), respectively. Our system outperformed existing methods for segmentation-based classification models. The mean improvement of the UNet+Xception system over all the remaining studies was 8.27%. Conclusion: The segmentation-based classification is a viable option as the hypothesis (error rate <5%) holds true and is thus adaptable in clinical practice.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-09-24", "authors": ["NoneNillmani", "NeerajSharma", "LucaSaba", "Narendra NKhanna", "Mannudeep KKalra", "Mostafa MFouda", "Jasjit SSuri"], "doi": "10.3390/diagnostics12092132\n10.1371/journal.pone.0249788\n10.1016/j.compbiomed.2020.103960\n10.1186/s13244-022-01176-w\n10.1007/s10554-020-02089-9\n10.1001/jama.2020.3786\n10.1016/j.acra.2015.12.010\n10.1148/radiol.2015150425\n10.1118/1.2836950\n10.1016/j.ejrad.2019.02.038\n10.1038/s42256-020-0186-1\n10.1016/j.zemedi.2018.11.002\n10.1016/j.compbiomed.2018.05.014\n10.3390/cancers11010111\n10.1007/s10916-021-01707-w\n10.1007/s11548-021-02317-0\n10.1016/j.cmpb.2020.105581\n10.1016/j.chaos.2020.110495\n10.1007/s10489-020-01902-1\n10.1016/j.bspc.2020.102365\n10.1148/radiol.2020203511\n10.3390/diagnostics12061482\n10.1016/j.cmpb.2017.07.011\n10.3390/biomedicines9070720\n10.1016/j.compbiomed.2016.06.010\n10.1016/j.compbiomed.2020.103847\n10.1016/j.compbiomed.2021.104721\n10.23736/S0392-9590.21.04771-4\n10.3390/diagnostics12051283\n10.3390/diagnostics11112109\n10.1016/j.compbiomed.2020.103958\n10.1109/ACCESS.2021.3086020\n10.2352/J.ImagingSci.Technol.2020.64.2.020508\n10.1109/TIM.2022.3174270\n10.1007/s11222-009-9153-8\n10.1109/TKDE.2019.2912815\n10.1016/j.neucom.2018.05.011\n10.1109/JBHI.2022.3177854\n10.1109/ACCESS.2020.3031384\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.17632/rscbjbr9sj.2\n10.1016/j.cell.2018.02.010\n10.1016/j.compbiomed.2022.105571\n10.1016/j.compbiomed.2017.10.022\n10.1109/ACCESS.2019.2962617\n10.1007/s13755-021-00166-4\n10.3390/diagnostics12030652\n10.1016/j.eswa.2021.116288\n10.1007/s11277-018-5777-3\n10.1007/s11277-018-5702-9\n10.1186/s12880-020-00514-y\n10.1109/ACCESS.2020.3017915\n10.3390/s21217116\n10.1016/j.cmpb.2019.06.005\n10.1038/s41598-021-99015-3\n10.1016/j.asoc.2020.106912\n10.3390/s22031211\n10.1109/TMI.2020.2993291\n10.1007/s00330-021-08050-1\n10.1016/j.bspc.2021.103182\n10.1016/j.bea.2022.100041\n10.1016/j.compbiomed.2022.105244\n10.1016/j.neucom.2021.03.034\n10.1007/s10916-016-0504-7\n10.1016/j.ejrad.2022.110164\n10.1142/S0219467801000402\n10.1016/j.mri.2012.04.021\n10.1007/s10554-020-02124-9\n10.3390/sym14071310"}
{"title": "Optometrist's perspectives of Artificial Intelligence in eye care.", "abstract": "The application of artificial intelligence (AI) in diagnosing and managing ocular disease has gained popularity as research highlights the utilization of AI to improve personalized medicine and healthcare outcomes. The objective of this study is to describe current optometric perspectives of AI in eye care.\nMembers of the American Academy of Optometry were sent an electronic invitation to complete a 17-item survey. Survey items assessed perceived advantages and concerns regarding AI using a 5-point Likert scale ranging from \"strongly agree\" to \"strongly disagree.\"\nA total of 400 optometrists completed the survey. The mean number of years since optometry school completion was 25 \u00b1 15.1. Most respondents reported familiarity with AI (66.8%). Though half of optometrists had concerns about the diagnostic accuracy of AI (53.0%), most believed it would improve the practice of optometry (72.0%). Optometrists reported their willingness to incorporate AI into practice increased from 53.3% before the COVID-19 pandemic to 65.5% after onset of the pandemic (p<0.001).\nIn this study, optometrists are optimistic about the use of AI in eye care, and willingness to incorporate AI in clinical practice also increased after the onset of the COVID-19 pandemic.", "journal": "Journal of optometry", "date": "2022-09-23", "authors": ["Angelica CScanzera", "EllenShorter", "CharlesKinnaird", "NitaValikodath", "TalaAl-Khaled", "EmilyCole", "SashaKravets", "Joelle AHallak", "TimothyMcMahon", "R V PaulChan"], "doi": "10.1016/j.optom.2022.06.006"}
{"title": "Identification of micro- and nanoplastics released from medical masks using hyperspectral imaging and deep learning.", "abstract": "Apart from other severe consequences, the COVID-19 pandemic has inflicted a surge in personal protective equipment usage, some of which, such as medical masks, have a short effective protection time. Their misdisposition and subsequent natural degradation make them huge sources of micro- and nanoplastic particles. To better understand the consequences of the direct influence of microplastic pollution on biota, there is an urgent need to develop a reliable and high-throughput analytical tool for sub-micrometre plastic identification and visualisation in environmental and biological samples. This study evaluated the application of a combined technique based on dark-field enhanced microscopy and hyperspectral imaging augmented with deep learning data analysis for the visualisation, detection and identification of microplastic particles released from commercially available medical masks after 192 hours of UV-C irradiation. The analysis was performed using a separated blue-coloured spunbond outer layer and white-coloured meltblown interlayer that allowed us to assess the influence of the structure and pigmentation of intact and UV-exposed samples on classification performance. Microscopy revealed strong fragmentation of both layers and the formation of microparticles and fibres of various shapes after UV exposure. Based on the spectral signatures of both layers, it was possible to identify intact materials using a convolutional neural network successfully. However, the further classification of UV-exposed samples demonstrated that the spectral characteristics of samples in the visible to near-infrared range are disrupted, causing a decreased performance of the CNN. Despite this, the application of a deep learning algorithm in hyperspectral analysis outperformed the conventional spectral angle mapper technique in classifying both intact and UV-exposed samples, confirming the potential of the proposed approach in secondary microplastic analysis.", "journal": "The Analyst", "date": "2022-09-21", "authors": ["IlnurIshmukhametov", "SvetlanaBatasheva", "RawilFakhrullin"], "doi": "10.1039/d2an01139e"}
{"title": "Endoscopic capsule robot-based diagnosis, navigation and localization in the gastrointestinal tract.", "abstract": "The proliferation of video capsule endoscopy (VCE) would not have been possible without continued technological improvements in imaging and locomotion. Advancements in imaging include both software and hardware improvements but perhaps the greatest software advancement in imaging comes in the form of artificial intelligence (AI). Current research into AI in VCE includes the diagnosis of tumors, gastrointestinal bleeding, Crohn's disease, and celiac disease. Other advancements have focused on the improvement of both camera technologies and alternative forms of imaging. Comparatively, advancements in locomotion have just started to approach clinical use and include onboard controlled locomotion, which involves miniaturizing a motor to incorporate into the video capsule, and externally controlled locomotion, which involves using an outside power source to maneuver the capsule itself. Advancements in locomotion hold promise to remove one of the major disadvantages of VCE, namely, its inability to obtain targeted diagnoses. Active capsule control could in turn unlock additional diagnostic and therapeutic potential, such as the ability to obtain targeted tissue biopsies or drug delivery. With both advancements in imaging and locomotion has come a corresponding need to be better able to process generated images and localize the capsule's position within the gastrointestinal tract. Technological advancements in computation performance have led to improvements in image compression and transfer, as well as advancements in sensor detection and alternative methods of capsule localization. Together, these advancements have led to the expansion of VCE across a number of indications, including the evaluation of esophageal and colon pathologies including esophagitis, esophageal varices, Crohn's disease, and polyps after incomplete colonoscopy. Current research has also suggested a role for VCE in acute gastrointestinal bleeding throughout the gastrointestinal tract, as well as in urgent settings such as the emergency department, and in resource-constrained settings, such as during the COVID-19 pandemic. VCE has solidified its role in the evaluation of small bowel bleeding and earned an important place in the practicing gastroenterologist's armamentarium. In the next few decades, further improvements in imaging and locomotion promise to open up even more clinical roles for the video capsule as a tool for non-invasive diagnosis of lumenal gastrointestinal pathologies.", "journal": "Frontiers in robotics and AI", "date": "2022-09-20", "authors": ["MarkHanscom", "David RCave"], "doi": "10.3389/frobt.2022.896028\n10.1016/j.gie.2018.10.027\n10.1111/jgh.14941\n10.1159/000525314\n10.1016/j.dld.2013.01.025\n10.1016/j.gie.2020.05.066\n10.1016/j.cgh.2007.12.029\n10.1109/TBME.2007.894729\n10.1109/TBME.2010.2087332\n10.1109/TBME.2009.2013336\n10.1109/IEMBS.2010.5627090\n10.1016/j.cmpb.2011.10.004\n10.1097/MCG.0b013e318288a2cd\n10.1055/a-0856-6845\n10.1055/a-0750-5682\n10.1016/j.gie.2019.04.248\n10.1016/j.gie.2013.02.039\n10.1007/s12213-016-0087-x\n10.1055/s-0029-1243808\n10.1109/TBME.2013.2290018\n10.1055/a-1546-8727\n10.1111/j.1572-0241.2007.01117.x\n10.2310/7290.2012.00014\n10.1053/j.gastro.2019.06.025\n10.1055/s-0032-1310158\n10.1097/01.mcg.0000170764.29202.24\n10.1016/S2589-7500(19)30108-6\n10.1016/j.giec.2003.10.020\n10.1038/ajg.2015.246\n10.1117/1.JBO.21.10.104001\n10.3748/wjg.v17.i41.4590\n10.1053/j.gastro.2017.07.049\n10.1016/j.giec.2020.12.011\n10.1001/jamanetworkopen.2021.18796\n10.14740/gr949w\n10.1155/2013/304723\n10.1016/j.gie.2010.10.016\n10.1016/j.gie.2020.01.027\n10.1109/TBME.2014.2352493\n10.1016/j.gie.2010.08.053\n10.1007/s00464-021-09007-7\n10.3390/jpm12040644\n10.1016/j.gie.2009.12.058\n10.3390/diagnostics11101792\n10.1007/s00261-016-1026-y\n10.1016/j.gie.2019.11.012\n10.4253/wjge.v4.i2.33\n10.1243/09544119JEIM134\n10.14309/ajg.0000000000001245\n10.1109/TBCAS.2010.2079932\n10.1016/j.gie.2018.06.036\n10.1109/TITB.2012.2185807\n10.1111/j.1572-0241.2003.08731.x\n10.4161/jig.23751\n10.1186/s12876-022-02302-0\n10.1016/j.gie.2013.11.022\n10.1016/j.gie.2018.06.016\n10.3390/diagnostics12061445\n10.1002/emp2.12579\n10.1109/IEMBS.2006.260385\n10.1007/s10916-009-9424-0\n10.1109/IEMBS.2006.259668\n10.1016/j.gie.2011.02.011\n10.1007/s00464-021-08689-3\n10.1016/j.ultrasmedbio.2019.12.003\n10.1016/j.gie.2007.11.052\n10.1109/IEMBS.2007.4352917\n10.1016/j.gie.2015.09.015\n10.1016/j.gie.2011.09.030\n10.1016/j.gie.2020.01.054\n10.1186/1471-230X-12-83\n10.1016/j.cgh.2018.07.019\n10.1007/s11938-017-0115-5\n10.1016/j.bios.2015.11.073\n10.1109/TBME.2008.915680\n10.1002/mp.12299\n10.1109/EMBC.2013.6610847\n10.1016/s0016-5085(10)63081-8\n10.3748/wjg.v21.i37.10528\n10.1016/j.gie.2016.04.043\n10.1016/j.gie.2010.01.064\n10.3748/wjg.v28.i20.2227\n10.1055/a-1790-5996\n10.1109/TBME.2012.2201715\n10.1080/13645700903201167\n10.1111/den.13896\n10.3748/wjg.v17.i11.1462\n10.1111/den.13507\n10.1038/s41598-021-90523-w\n10.1055/a-1308-1297\n10.1016/j.gie.2013.06.026\n10.1007/s10544-005-1588-x\n10.1016/j.lanwpc.2020.100072\n10.1021/js970185g\n10.1109/EMBC.2018.8513012\n10.1109/IEMBS.2011.6091642\n10.1109/TBME.2022.3157451\n10.1002/mp.12147\n10.1055/s-0042-122015\n10.1080/17474124.2017.1257384\n10.3390/diagnostics12061333\n10.1016/j.gie.2008.02.023\n10.1016/j.compbiomed.2017.03.031"}
{"title": "Application of machine learning and medical imaging in the detection of COVID-19 patients: A review article.", "abstract": "In the present study, a particular technique of artificial intelligence (AI) is applied for diagnosis and classifying medical images of patients with coronavirus disease (COVID-19). Chest radiography and laboratory-based tests are two of the most important diagnostic approaches for the detection of people with the coronavirus. Recently, a lot of studies have been carried out on using AI techniques for achieving appropriate diagnosis of COVID-19 patients using computed tomography (CT) of the chest. The present study is reviewing all available literature that have investigated the role of chest CT toward AI in the detection of COVID-19. As a novel field of computer science, AI focuses on teaching computers to be capable of learning complex tasks and decide about their solution methods. In this study, we used Matlab, Payton, and Fortran software as well as other software which are suitable for this research. In this regard, the present review study is aimed to collect the information from all the studies conducted on the role of AI as a decisive and comprehensive technology for the detection of coronavirus in patients to have a more accurate diagnosis and investigate its epidemiology.", "journal": "Journal of family medicine and primary care", "date": "2022-09-20", "authors": ["SepidehYadollahi", "SetarehYadollahi", "ElmiraZanjani", "FatemehKhaleghi"], "doi": "10.4103/jfmpc.jfmpc_1715_21"}
{"title": "Automated Lung Segmentation from Computed Tomography Images of Normal and COVID-19 Pneumonia Patients.", "abstract": "Automated image segmentation is an essential step in quantitative image analysis. This study assesses the performance of a deep learning-based model for lung segmentation from computed tomography (CT) images of normal and COVID-19 patients.\nA descriptive-analytical study was conducted from December 2020 to April 2021 on the CT images of patients from various educational hospitals affiliated with Mashhad University of Medical Sciences (Mashhad, Iran). Of the selected images and corresponding lung masks of 1,200 confirmed COVID-19 patients, 1,080 were used to train a residual neural network. The performance of the residual network (ResNet) model was evaluated on two distinct external test datasets, namely the remaining 120 COVID-19 and 120 normal patients. Different evaluation metrics such as Dice similarity coefficient (DSC), mean absolute error (MAE), relative mean Hounsfield unit (HU) difference, and relative volume difference were calculated to assess the accuracy of the predicted lung masks. The Mann-Whitney U test was used to assess the difference between the corresponding values in the normal and COVID-19 patients. P<0.05 was considered statistically significant.\nThe ResNet model achieved a DSC of 0.980 and 0.971 and a relative mean HU difference of -2.679% and -4.403% for the normal and COVID-19 patients, respectively. Comparable performance in lung segmentation of normal and COVID-19 patients indicated the model's accuracy for identifying lung tissue in the presence of COVID-19-associated infections. Although a slightly better performance was observed in normal patients.\nThe ResNet model provides an accurate and reliable automated lung segmentation of COVID-19 infected lung tissue.A preprint version of this article was published on arXiv before formal peer review (https://arxiv.org/abs/2104.02042).", "journal": "Iranian journal of medical sciences", "date": "2022-09-20", "authors": ["FaezeGholamiankhah", "SamanehMostafapour", "NouraddinAbdi Goushbolagh", "SeyedjafarShojaerazavi", "ParvanehLayegh", "Seyyed MohammadTabatabaei", "HosseinArabi"], "doi": "10.30476/IJMS.2022.90791.2178\n10.30476/ijms.2020.85869.1549\n10.30476/ijms.2020.87233.1730\n10.1109/TMI.2020.3001810\n10.1109/TBDATA.2021.3056564\n10.1109/TMI.2020.2996645\n10.1016/j.media.2020.101794\n10.1109/ICBME.2010.5704968\n10.1016/j.radphyschem.2021.109666\n10.1148/radiol.2020200642\n10.1101/2020.03.12.20027185\n10.1016/j.cell.2020.04.045\n10.1148/radiol.2020202439\n10.1186/s41747-020-00173-2\n10.1016/j.media.2016.11.003\n10.1038/s41598-020-80936-4\n10.1016/j.procs.2018.01.104\n10.1186/s41824-020-00086-8\n10.1002/ima.22527\n10.1002/mp.14418\n10.1016/j.media.2020.101759\n10.1016/j.media.2016.02.002\n10.1109/TMI.2021.3066161\n10.1109/TMI.2020.2995965\n10.1016/j.patcog.2021.108109\n10.1016/j.patcog.2020.107747\n10.1515/cdbme-2016-0114\n10.1016/j.cmpb.2018.01.025\n10.1016/j.media.2020.101718\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020200432\n10.1002/mp.14676\n10.1109/RBME.2020.2987975\n10.3390/sym12040651\n10.3892/etm.2020.9210\n10.1016/j.imu.2021.100681\n10.3390/diagnostics11081405\n10.1016/j.patcog.2021.108071\n10.1136/neurintsurg-2015-011697"}
{"title": "The role of artificial intelligence in plain chest radiographs interpretation during the Covid-19 pandemic.", "abstract": "Artificial intelligence (AI) plays a crucial role in the future development of all healthcare sectors ranging from clinical assistance of physicians by providing accurate diagnosis, prognosis and treatment to the development of vaccinations and aiding in the combat against the Covid-19 global pandemic. AI has an important role in diagnostic radiology where the algorithms can be trained by large datasets to accurately provide a timely diagnosis of the radiological images given. This has led to the development of several AI algorithms that can be used in regions of scarcity of radiologists during the current pandemic by simply denoting the presence or absence of Covid-19 pneumonia in PCR positive patients on plain chest radiographs as well as in helping to levitate the over-burdened radiology departments by accelerating the time for report delivery. Plain chest radiography is the most common radiological study in the emergency department setting and is readily available, fast and a cheap method that can be used in triaging patients as well as being portable in the medical wards and can be used as the initial radiological examination in Covid-19 positive patients to detect pneumonic changes. Numerous studies have been done comparing several AI algorithms to that of experienced thoracic radiologists in plain chest radiograph reports measuring accuracy of each in Covid-19 patients. The majority of studies have reported performance equal or higher to that of the well-experienced thoracic radiologist in predicting the presence or absence of Covid-19 pneumonic changes in the provided chest radiographs.", "journal": "BJR open", "date": "2022-09-16", "authors": ["DanaAlNuaimi", "ReemAlKetbi"], "doi": "10.1259/bjro.20210075\n10.1016/j.ijsu.2020.02.034\n10.1016/j.dsx.2020.04.012\n10.1016/j.carj.2018.02.002\n10.1038/s41568-018-0016-5\n10.1016/j.jmir.2019.09.005\n10.3390/diagnostics11030530\n10.1148/radiol.2020204226\n10.1016/j.opresp.2020.100078\n10.1007/s12553-021-00520-2\n10.1016/S2589-7500(20)30079-0\n10.1016/j.ibmed.2020.100014\n10.1148/radiol.2020202944\n10.2147/RMI.S292314\n10.1148/radiol.2020201874\n10.1148/radiol.2020204238\n10.1145/3466690\n10.1007/s00146-020-00978-0\n10.1016/S2589-7500(21)00039-X\n10.1101/2020.05.25.20113084\n10.4103/ijri.IJRI_777_20\n10.1148/radiol.2020203511\n10.1038/s42256-021-00307-0"}
{"title": "TOPSIS aided ensemble of CNN models for screening COVID-19 in chest X-ray images.", "abstract": "The novel coronavirus (COVID-19), has undoubtedly imprinted our lives with its deadly impact. Early testing with isolation of the individual is the best possible way to curb the spread of this deadly\u00a0virus. Computer aided diagnosis (CAD) provides an alternative and cheap option for screening of the said virus. In this paper, we propose a convolution neural network (CNN)-based CAD method for COVID-19 and pneumonia detection from chest X-ray images. We consider three input types for three identical base classifiers. To capture maximum possible complementary features, we consider the original RGB image, Red channel image and the original image stacked with Robert's edge information. After that we develop an ensemble strategy based on the technique for order preference by similarity to an ideal solution (TOPSIS) to aggregate the outcomes of base classifiers. The overall framework, called TOPCONet, is very light in comparison with standard CNN models in terms of the number of trainable\u00a0parameters required. TOPCONet achieves state-of-the-art results when evaluated on the three publicly available datasets: (1) IEEE COVID-19 dataset + Kaggle Pneumonia Dataset, (2) Kaggle Radiography dataset and (3) COVIDx.", "journal": "Scientific reports", "date": "2022-09-15", "authors": ["RishavPramanik", "SubhrajitDey", "SamirMalakar", "SeyedaliMirjalili", "RamSarkar"], "doi": "10.1038/s41598-022-18463-7\n10.1016/S0140-6736(21)02000-6\n10.1148/radiol.2020200432\n10.1038/s41563-020-00906-z\n10.1007/s10916-019-1451-x\n10.1016/j.cmpb.2022.106776\n10.1007/s10916-021-01747-2\n10.1148/radiol.2020200642\n10.1007/s10489-020-01943-6\n10.1016/j.neunet.2018.07.011\n10.1016/j.asoc.2021.107160\n10.1109/TNNLS.2019.2906867\n10.1109/ACCESS.2020.2995597\n10.1038/s41598-020-76550-z\n10.1007/s00521-020-05410-8\n10.1016/j.eswa.2022.117812\n10.1109/JBHI.2021.3069798\n10.1016/j.eswa.2020.113909\n10.1016/j.acra.2020.04.034\n10.1109/TMI.2020.2993291\n10.1007/s00521-021-06737-6\n10.1016/j.bdr.2021.100233\n10.1016/j.bbe.2021.12.001\n10.1016/j.rinp.2021.105045\n10.1016/j.chaos.2020.110245\n10.1016/j.compbiomed.2021.104834\n10.1038/s41598-021-88807-2\n10.1109/ACCESS.2021.3061058\n10.1016/j.compbiomed.2021.105047\n10.3390/diagnostics11111972\n10.1016/j.cmpb.2020.105581\n10.1016/j.chaos.2020.110495\n10.1007/s10489-020-01904-z\n10.1016/j.asoc.2020.106912\n10.1016/j.asoc.2021.107918\n10.1016/j.cmpb.2020.105532\n10.1007/s00521-021-06629-9\n10.1038/nature14539\n10.1109/TCSVT.2020.3019293\n10.1109/MCI.2010.938364\n10.1016/j.asoc.2021.107698\n10.1109/ACCESS.2020.3010287\n10.1007/s10489-020-01902-1\n10.1016/j.eswa.2020.114054\n10.1016/j.asoc.2021.107184\n10.1016/j.compbiomed.2021.104401\n10.1038/s41598-020-79139-8"}
{"title": "Detection of COVID-19 Infection in CT and X-ray images using transfer learning approach.", "abstract": "The infection caused by the SARS-CoV-2 (COVID-19) pandemic is a threat to human lives. An early and accurate diagnosis is necessary for treatment.\nThe study presents an efficient classification methodology for precise identification of infection caused by COVID-19 using CT and X-ray images.\nThe depthwise separable convolution-based model of MobileNet V2 was exploited for feature extraction. The features of infection were supplied to the SVM classifier for training which produced accurate classification results.\nThe accuracies for CT and X-ray images are 99.42% and 98.54% respectively. The MCC score was used to avoid any mislead caused by accuracy and F1 score as it is more mathematically balanced metric. The MCC scores obtained for CT and X-ray were 0.9852 and 0.9657, respectively. The Youden's index showed a significant improvement of more than 2% for both imaging techniques.\nThe proposed transfer learning-based approach obtained the best results for all evaluation metrics and produced reliable results for the accurate identification of COVID-19 symptoms. This study can help in reducing the time in diagnosis of the infection.", "journal": "Technology and health care : official journal of the European Society for Engineering and Medicine", "date": "2022-09-13", "authors": ["AlokTiwari", "SumitTripathi", "Dinesh ChandraPandey", "NeerajSharma", "ShiruSharma"], "doi": "10.3233/THC-220114"}
{"title": "A Novel Method for COVID-19 Detection Based on DCNNs and Hierarchical Structure.", "abstract": "The worldwide outbreak of the new coronavirus disease (COVID-19) has been declared a pandemic by the World Health Organization (WHO). It has a devastating impact on daily life, public health, and global economy. Due to the highly infectiousness, it is urgent to early screening of suspected cases quickly and accurately. Chest X-ray medical image, as a diagnostic basis for COVID-19, arouses attention from medical engineering. However, due to small lesion difference and lack of training data, the accuracy of detection model is insufficient. In this work, a transfer learning strategy is introduced to hierarchical structure to enhance high-level features of deep convolutional neural networks. The proposed framework consisting of asymmetric pretrained DCNNs with attention networks integrates various information into a wider architecture to learn more discriminative and complementary features. Furthermore, a novel cross-entropy loss function with a penalty term weakens misclassification. Extensive experiments are implemented on the COVID-19 dataset. Compared with the state-of-the-arts, the effectiveness and high performance of the proposed method are demonstrated.", "journal": "Computational and mathematical methods in medicine", "date": "2022-09-13", "authors": ["YuqinLi", "KeZhang", "WeiliShi", "ZhengangJiang"], "doi": "10.1155/2022/2484435\n10.1016/j.bbe.2020.08.008\n10.1016/j.irbm.2020.05.003\n10.1007/s10044-021-00984-y\n10.7717/peerj-cs.313\n10.1080/07391102.2020.1788642\n10.1016/j.compbiomed.2020.103792\n10.1101/2020.05.12.20099937\n10.1117/12.2581496\n10.1155/2022/6185013\n10.1016/j.mehy.2020.109761\n10.1016/j.patrec.2021.11.020\n10.1016/B978-0-12-824536-1.00003-4\n10.1007/s10489-020-01900-3\n10.1007/s10489-020-01826-w\n10.1007/s11517-020-02299-2\n10.1016/j.patrec.2020.09.010\n10.20944/preprints202005.0151.v1\n10.1016/j.bspc.2022.103595\n10.1016/j.compbiomed.2021.105134\n10.1016/j.bspc.2019.04.031\n10.1109/cvpr.2016.90\n10.1109/cvpr.2018.00745\n10.1109/ACCESS.2020.3010287\n10.1186/s40537-019-0197-0\n10.1049/ipr2.12090\n10.1007/s40846-020-00529-4\n10.3390/ijerph18063056\n10.3390/healthcare9050522\n10.1016/j.irbm.2020.07.001"}
{"title": "Hotspots and trends in ophthalmology in recent 5 years: Bibliometric analysis in 2017-2021.", "abstract": "The purpose of this study was to investigate the hotspots and research trends of ophthalmology research.\nOphthalmology research literature published between 2017 and 2021 was obtained in the Web of Science Core Collection database. The bibliometric analysis and network visualization were performed with the VOSviewer and CiteSpace. Publication-related information, including publication volume, citation counts, countries, journals, keywords, subject categories, and publication time, was analyzed.\nA total of 10,469 included ophthalmology publications had been cited a total of 7,995 times during the past 5 years. The top countries and journals for the number of publications were the United States and the Ophthalmology. The top 25 global high-impact documents had been identified using the citation ranking. Keyword co-occurrence analysis showed that the hotspots in ophthalmology research were epidemiological characteristics and treatment modalities of ocular diseases, artificial intelligence and fundus imaging technology, COVID-19-related telemedicine, and screening and prevention of ocular diseases. Keyword burst analysis revealed that \"neural network,\" \"pharmacokinetics,\" \"geographic atrophy,\" \"implementation,\" \"variability,\" \"adverse events,\" \"automated detection,\" and \"retinal images\" were the research trends of research in the field of ophthalmology through 2021. The analysis of the subject categories demonstrated the close cooperation relationships that existed between different subject categories, and collaborations with non-ophthalmology-related subject categories were increasing over time in the field of ophthalmology research.\nThe hotspots in ophthalmology research were epidemiology, prevention, screening, and treatment of ocular diseases, as well as artificial intelligence and fundus imaging technology and telemedicine. Research trends in ophthalmology research were artificial intelligence, drug development, and fundus diseases. Knowledge from non-ophthalmology fields is likely to be more involved in ophthalmology research.", "journal": "Frontiers in medicine", "date": "2022-09-13", "authors": ["YuanTan", "WeiningZhu", "YingshiZou", "BowenZhang", "YinglinYu", "WeiLi", "GuangmingJin", "ZhenzhenLiu"], "doi": "10.3389/fmed.2022.988133\n10.1097/WNO.0000000000001375\n10.1016/j.actbio.2021.02.017\n10.1016/j.exer.2016.10.013\n10.1186/s13643-019-1113-6\n10.1016/j.cbi.2021.109679\n10.1038/s41586-020-2975-4\n10.1016/j.preteyeres.2021.100970\n10.1073/pnas.2117038119\n10.1016/j.bioactmat.2022.02.019\n10.1177/1534735420959442\n10.1080/09737766.2008.10700853\n10.4103/0301-4738.64117\n10.1055/s-0029-1245134\n10.3126/nepjoph.v4i2.6548\n10.1007/s00347-008-1849-1\n10.1046/j.1442-9071.2003.00663.x\n10.12688/f1000research.20673.1\n10.4103/0301-4738.151471\n10.1016/j.clineuro.2020.105740\n10.1016/j.ophtha.2017.05.035\n10.1016/j.ophtha.2019.04.017\n10.1016/j.oret.2016.12.009\n10.1016/j.ophtha.2017.09.028\n10.1136/bjophthalmol-2018-313173\n10.1016/j.ophtha.2018.01.023\n10.1016/j.ophtha.2017.08.015\n10.1016/j.ophtha.2018.04.007\n10.1007/s00592-017-0974-1\n10.1016/j.ophtha.2017.12.011\n10.1016/j.ophtha.2017.03.036\n10.1016/j.ophtha.2017.08.027\n10.1136/annrheumdis-2018-213225\n10.1007/s00417-020-04641-8\n10.1016/j.ophtha.2017.10.031\n10.1016/j.cmpb.2018.02.001\n10.1016/j.preteyeres.2018.07.004\n10.1016/j.ophtha.2017.02.008\n10.1126/sciadv.aat4388\n10.1016/j.preteyeres.2016.12.001\n10.3390/polym10070701\n10.1016/j.ophtha.2016.10.008\n10.1364/BOE.8.001056\n10.1016/j.preteyeres.2017.07.002\n10.1056/NEJMoa1609583\n10.1016/j.diabres.2021.108880\n10.1167/iovs.62.6.5\n10.18240/ijo.2017.09.16\n10.2147/OPTH.S235751\n10.2147/CIA.S297494\n10.1016/j.ophtha.2021.12.017\n10.1016/j.ajo.2020.04.029"}
{"title": "Rapid quantification of COVID-19 pneumonia burden from computed tomography with convolutional long short-term memory networks.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2022-09-13", "authors": ["AdityaKillekar", "KajetanGrodecki", "AndrewLin", "SebastienCadet", "PriscillaMcElhinney", "AryabodRazipour", "CatoChan", "Barry DPressman", "PeterJulien", "PeterChen", "JuditSimon", "PalMaurovich-Horvat", "NicolaGaibazzi", "UditThakur", "ElisabettaMancini", "CeciliaAgalbato", "JiroMunechika", "HidenariMatsumoto", "RobertoMen\u00e8", "GianfrancoParati", "FrancoCernigliaro", "NiteshNerlekar", "CamillaTorlasco", "GianlucaPontone", "DaminiDey", "PiotrSlomka"], "doi": "10.1117/1.JMI.9.5.054001\n10.1016/j.ajic.2020.07.011\n10.1007/s00330-020-07033-y\n10.1038/s41598-020-80061-2\n10.1148/rg.2020200159\n10.1148/radiol.2020200463\n10.1148/radiol.2020200843\n10.1148/radiol.2020200370\n10.1007/s00330-020-06817-6\n10.1148/ryct.2020200047\n10.1148/ryai.2020200048\n10.1148/ryct.2020200441\n10.1148/ryct.2020200389\n10.1001/archinternmed.2009.440\n10.2967/jnumed.120.246256\n10.1097/RLU.0000000000003135\n10.1016/j.diii.2020.05.011\n10.1007/s00259-020-05014-3\n10.4103/ijri.IJRI_479_20\n10.1016/j.cell.2020.04.045\n10.1148/radiol.2020201491\n10.1109/TMI.2020.2996645\n10.1016/j.media.2020.101836\n10.1007/978-3-319-46723-8_49\n10.1109/3DV.2016.79\n10.1162/neco.1997.9.8.1735\n10.1148/radiol.10091808\n10.1109/CVPR.2017.243\n10.1109/IVS.2019.8813852\n10.1109/CVPR.2017.19\n10.1109/CVPR.2009.5206848\n10.1007/BF02295996\n10.1148/radiol.2020200642\n10.1148/radiol.2020200905\n10.1016/j.metabol.2020.154436\n10.1109/TMI.2020.3000314\n10.1186/s12880-020-00529-5\n10.1155/2020/4706576\n10.1007/s00521-020-05514-1\n10.1002/int.22586\n10.1117/12.2613272"}
{"title": "Lung image segmentation based on DRD U-Net and combined WGAN with Deep Neural Network.", "abstract": "COVID-19 is a hot issue right now, and it's causing a huge number of infections in people, posing a grave threat to human life. Deep learning-based image diagnostic technology can effectively enhance the deficiencies of the current main detection method. This paper proposes a multi-classification model diagnosis based on segmentation and classification multi-task.\nIn the segmentation task, the end-to-end DRD U-Net model is used to segment the lung lesions to improve the ability of feature reuse and target segmentation. In the classification task, the model combined with WGAN and Deep Neural Network classifier is used to effectively solve the problem of multi-classification of COVID-19 images with small samples, to achieve the goal of effectively distinguishing COVID-19 patients, other pneumonia patients, and normal subjects.\nExperiments are carried out on common X-ray image and CT image data sets. The results display that in the segmentation task, the model is optimal in the key indicators of DSC and HD, and the error is increased by 0.33% and reduced by 3.57 mm compared with the original network U-Net. In the classification task, compared with SMOTE oversampling method, accuracy increased from 65.32% to 73.84%, F-measure increased from 67.65% to 74.65%, G-mean increased from 66.52% to 74.37%. At the same time, compared with other classical multi-task models, the results also have some advantages.\nThis study provides new possibilities for COVID-19 image diagnosis methods, improves the accuracy of diagnosis, and hopes to provide substantial help for COVID-19 diagnosis.", "journal": "Computer methods and programs in biomedicine", "date": "2022-09-12", "authors": ["LuoyuLian", "XinLuo", "CanyuPan", "JinlongHuang", "WenshanHong", "ZhendongXu"], "doi": "10.1016/j.cmpb.2022.107097"}
{"title": "Unmet needs in pneumonia research: a comprehensive approach by the CAPNETZ study group.", "abstract": "Despite improvements in medical science and public health, mortality of community-acquired pneumonia (CAP) has barely changed throughout the last 15\u00a0years. The current SARS-CoV-2 pandemic has once again highlighted the central importance of acute respiratory infections to human health. The \"network of excellence on Community Acquired Pneumonia\" (CAPNETZ) hosts the most comprehensive CAP database worldwide including more than 12,000 patients. CAPNETZ connects physicians, microbiologists, virologists, epidemiologists, and computer scientists throughout Europe. Our aim was to summarize the current situation in CAP research and identify the most pressing unmet needs in CAP research.\nTo identify areas of future CAP research, CAPNETZ followed a multiple-step procedure. First, research members of CAPNETZ were individually asked to identify unmet needs. Second, the top 100 experts in the field of CAP research were asked for their insights about the unmet needs in CAP (Delphi approach). Third, internal and external experts discussed unmet needs in CAP at a scientific retreat.\nEleven topics for future CAP research were identified: detection of causative pathogens, next generation sequencing for antimicrobial treatment guidance, imaging diagnostics, biomarkers, risk stratification, antiviral and antibiotic treatment, adjunctive therapy, vaccines and prevention, systemic and local immune response, comorbidities, and long-term cardio-vascular complications.\nPneumonia is a complex disease where the interplay between pathogens, immune system and comorbidities not only impose an immediate risk of mortality but also affect the patients' risk of developing comorbidities as well as mortality for up to a decade after pneumonia has resolved. Our review of unmet needs in CAP research has shown that there are still major shortcomings in our knowledge of CAP.", "journal": "Respiratory research", "date": "2022-09-11", "authors": ["Mathias WPletz", "Andreas VestergaardJensen", "ChristinaBahrs", "ClaudiaDavenport", "JanRupp", "MartinWitzenrath", "GritBarten-Neiner", "MartinKolditz", "SabineDettmer", "James DChalmers", "DaianaStolz", "NorbertSuttorp", "StefanoAliberti", "Wolfgang MKuebler", "GernotRohde"], "doi": "10.1186/s12931-022-02117-3\n10.1016/S0140-6736(17)31833-0\n10.1016/S0140-6736(21)00630-9\n10.1164/rccm.201908-1581ST\n10.1136/thx.2009.121434\n10.1055/s-0042-101873\n10.1186/s12890-017-0404-8\n10.1136/thoraxjnl-2013-204282\n10.1136/thx.2008.109785\n10.1016/j.rmed.2014.05.004\n10.1007/s10096-018-3224-8\n10.1086/526526\n10.1093/cid/cit254\n10.1186/1471-2334-14-61\n10.1001/jama.1996.03530260048030\n10.7326/0003-4819-142-3-200502010-00006\n10.1136/thorax.58.5.377\n10.1016/j.rmed.2013.04.003\n10.1378/chest.11-2393\n10.1001/archinte.159.9.970\n10.1001/jama.2012.384\n10.1093/cid/cix647\n10.1111/j.1469-0691.2010.03296.x\n10.1001/archinte.159.14.1550\n10.1016/0140-6736(93)91887-R\n10.1086/379712\n10.1164/rccm.200712-1777OC\n10.1097/MD.0b013e318190f444\n10.1007/s15010-004-3107-z\n10.1016/j.chest.2019.10.006\n10.1093/infdis/jiv323\n10.1016/j.cmi.2018.12.037\n10.1016/j.ccm.2011.05.011\n10.1183/13993003.01144-2016\n10.1093/cid/cit734\n10.1136/thoraxjnl-2013-203384\n10.1183/13993003.02086-2016\n10.1038/s41587-019-0156-5\n10.1183/09031936.01.00213501\n10.1016/j.ajem.2012.08.041\n10.1164/rccm.201501-0017OC\n10.2169/internalmedicine.55.5556\n10.1016/j.ajem.2015.01.035\n10.1378/chest.12-0364\n10.1097/RTI.0000000000000524\n10.1016/j.chest.2017.08.028\n10.1183/13993003.00434-2017\n10.1136/bmj.f2450\n10.1007/s00134-007-0895-5\n10.1186/1471-2334-12-90\n10.1136/bmj.e3397\n10.1513/AnnalsATS.201901-007OC\n10.1183/13993003.01389-2017\n10.1164/rccm.201003-0415OC\n10.1164/rccm.201708-1733OC\n10.3390/ijms20082004\n10.1155/2018/7028267\n10.1111/resp.12996\n10.1513/AnnalsATS.201804-286OC\n10.1371/journal.pone.0191750\n10.1007/s001340051129\n10.1055/s-0032-1315637\n10.1111/joim.12349\n10.1136/thoraxjnl-2015-206881\n10.1016/j.cmi.2017.07.007\n10.1016/j.rmed.2016.10.015\n10.1001/jama.2016.0287\n10.1007/s00134-016-4517-y\n10.1097/MD.0000000000012634\n10.1164/rccm.201807-1419OC\n10.1016/j.annemergmed.2018.11.036\n10.1001/jama.2018.19232\n10.1016/j.cmi.2019.02.022\n10.1001/jama.2016.0115\n10.1002/14651858.CD002109.pub4\n10.1097/MCP.0000000000000557\n10.1136/jim-2018-000712\n10.1183/13993003.00048-2021\n10.1038/d41586-021-03379-5\n10.1097/QCO.0000000000000347\n10.3390/toxins11120734\n10.1097/MCC.0000000000000435\n10.1016/j.cmi.2020.07.016\n10.1186/1471-2334-14-13\n10.3389/fmicb.2016.00693\n10.1093/jac/dkp160\n10.1128/JCM.02597-14\n10.1186/s13054-016-1430-2\n10.1056/NEJMp1905589\n10.1007/s11908-017-0565-x\n10.1055/s-0036-1593538\n10.1161/01.RES.0000036603.61868.F9\n10.1183/13993003.01329-2016\n10.1016/S2213-2600(21)00331-3\n10.1056/NEJMoa2101643\n10.1038/s41591-021-01499-z\n10.1001/jama.2021.9508\n10.1016/S2213-2600(20)30556-7\n10.1128/CMR.00078-09\n10.1056/NEJMoa1406330\n10.1038/s41598-018-37186-2\n10.1111/resp.13232\n10.1055/s-0032-1315643\n10.1186/s12879-018-3630-7\n10.1183/13993003.00824-2019\n10.1007/s00134-018-5143-7\n10.1016/j.rmed.2013.09.005\n10.1136/bmjresp-2016-000152\n10.3390/v11030295\n10.1056/NEJMoa2102685\n10.1101/2021.06.15.21258542v1\n10.1183/09031936.00133510\n10.1001/jama.2013.279206\n10.1016/j.ahj.2017.07.020\n10.12688/f1000research.7657.1\n10.7326/M19-0735\n10.1016/S1473-3099(17)30049-X\n10.1016/S0140-6736(00)02377-1\n10.1086/648593\n10.1371/journal.pone.0169370\n10.1016/j.vaccine.2016.03.052\n10.1183/09031936.00183614\n10.1016/j.vaccine.2019.11.026\n10.1016/j.vaccine.2019.05.003\n10.1371/journal.ppat.1007438\n10.1016/j.ijantimicag.2008.01.021\n10.4103/jgid.jgid_88_17\n10.3934/publichealth.2020004\n10.3390/vaccines7010009\n10.1159/000504477\n10.1086/374604\n10.1093/bmb/61.1.1\n10.1016/j.cytogfr.2020.04.002\n10.3389/fimmu.2018.01421\n10.1183/16000617.0034-2015\n10.1038/srep35021\n10.1016/j.gene.2018.08.080\n10.1186/s12920-019-0572-x\n10.1164/rccm.201701-0104OC\n10.1097/MCP.0000000000000584\n10.1183/23120541.00020-2015\n10.1093/cid/cix703\n10.1093/cid/cir840\n10.1093/cid/ciy723\n10.1183/13993003.01520-2016\n10.1016/j.ejim.2013.12.001\n10.1007/s40520-014-0297-9\n10.1111/resp.13233\n10.1016/S0140-6736(12)61266-5\n10.1164/rccm.201501-0140OC\n10.1136/bmj.j413\n10.1183/13993003.00972-2015\n10.1001/jama.2014.18229\n10.1164/rccm.201801-0139WS"}
{"title": "Detection of COVID-19 in Point of Care Lung Ultrasound.", "abstract": "The coronavirus disease 2019 (COVID-19) evolved into a global pandemic, responsible for a significant number of infections and deaths. In this scenario, point-of-care ultrasound (POCUS) has emerged as a viable and safe imaging modality. Computer vision (CV) solutions have been proposed to aid clinicians in POCUS image interpretation, namely detection/segmentation of structures and image/patient classification but relevant challenges still remain. As such, the aim of this study is to develop CV algorithms, using Deep Learning techniques, to create tools that can aid doctors in the diagnosis of viral and bacterial pneumonia (VP and BP) through POCUS exams. To do so, convolutional neural networks were designed to perform in classification tasks. The architectures chosen to build these models were the VGG16, ResNet50, DenseNet169 e MobileNetV2. Patients images were divided in three classes: healthy (HE), BP and VP (which includes COVID-19). Through a comparative study, which was based on several performance metrics, the model based on the DenseNet169 architecture was designated as the best performing model, achieving 78% average accuracy value of the five iterations of 5- Fold Cross-Validation. Given that the currently available POCUS datasets for COVID-19 are still limited, the training of the models was negatively affected by such and the models were not tested in an independent dataset. Furthermore, it was also not possible to perform lesion detection tasks. Nonetheless, in order to provide explainability and understanding of the models, Gradient-weighted Class Activation Mapping (GradCAM) were used as a tool to highlight the most relevant classification regions. Clinical relevance - Reveals the potential of POCUS to support COVID-19 screening. The results are very promising although the dataset is limite.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["JoanaMaximino", "MiguelCoimbra", "JoaoPedrosa"], "doi": "10.1109/EMBC48229.2022.9871235"}
{"title": "Dynamic Classification of Imageless Bioelectrical Impedance Tomography Features with Attention-Driven Spatial Transformer Neural Network.", "abstract": "Point-of-Care monitoring devices have proven to be pivotal in the timely screening and intervention of critical care patients. The urgent demands for their deployment in the COVID-19 pandemic era has translated into the escalation of rapid, reliable, and low-cost monitoring systems research and development. Electrical Impedance Tomography (EIT) is a highly promising modality in providing deep tissue imaging that aids in patient bedside diagnosis and treatment. Motivated to bring forth an accurate and intelligent EIT screening system, we bypassed the complexity and challenges typically associated with its image reconstruction and feature identification processes by solely focusing on the raw data output to extract the embedded knowledge. We developed a novel machine learning architecture based on an attention-driven spatial transformer neural network to specifically accommodate for the patterns and dependencies within EIT raw data. Through elaborate precision-mapped phantom experiments, we validated the reproduction and recognition of features with systemically controlled changes. We demonstrated over 95% accuracy via state-of-the-art machine learning models, and an enhanced performance using our adapted transformer pipeline with shorter training time and greater computational efficiency. Our approach of using imageless EIT driven by a novel attention-focused feature learning algorithm is highly promising in revolutionizing conventional EIT operations and augmenting its practical usage in medicine and beyond.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["MingdeZheng", "HassanJahanandish", "HongweiLi"], "doi": "10.1109/EMBC48229.2022.9870921"}
{"title": "Automated Quantification of Inflamed Lung Regions in Chest CT by UNet++ and SegCaps: A Comparative Analysis in COVID-19 Cases.", "abstract": "During the current COVID-19 pandemic, a high volume of lung imaging has been generated in the aid of the treating clinician. Importantly, lung inflammation severity, associated with the disease outcome, needs to be precisely quantified. Producing consistent and accurate reporting in high-demand scenarios can be a challenge that can compromise patient care with significant inter- or intra-observer variability in quantifying lung inflammation in a chest CT scan. In this backdrop, automated segmentation has recently been attempted using UNet++, a convolutional neural network (CNN), and results comparable to manual methods have been reported. In this paper, we hypothesize that the desired task can be performed with comparable efficiency using capsule networks with fewer parameters that make use of an advanced vector representation of information and dynamic routing. In this paper, we validate this hypothesis using SegCaps, a capsule network, by direct comparison, individual comparison with CT severity score, and comparing the relative effect on a ML(machine learning)-based prognosis model developed elsewhere. We further provide a scenario, where a combination of UNet++ and SegCaps achieves improved performance compared to individual models.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["PriyaBhatia", "AbhisharSinha", "Swati PurohitJoshi", "RahuldebSarkar", "RajeshGhosh", "SoumyaJana"], "doi": "10.1109/EMBC48229.2022.9870901"}
{"title": "Teleoperated Probe Manipulator for Prone-Position Echocardiography Examination.", "abstract": "Echocardiography probe manipulation is a strenuous task. During a procedure, the operator must hold the probe, extend their arm, bend their elbow, and monitor the resulting image simultaneously, which causes strain and introduces variability to the measurement. We propose a teleoperated probe manipulation robot to reduce the burden of handling the probe and minimize the infection risk during the COVID pandemic. The proposed robot utilizes prone position scanning that could enlarge the cardiac windows for easier scanning and eliminate the risk of the robot pressing down on the patient. We derived the robot's requirements based on a real clinical scenario. Initial evaluation showed that the robot could achieve the required range of motion, force, and control. The robot's functionality was tested by a non-clinician, in which the tester could obtain heart images of a volunteer in under one minute.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["Muhammad WildanGifari", "ModarHassan", "KenjiSuzuki"], "doi": "10.1109/EMBC48229.2022.9871021"}
{"title": "Transfer Learning for Automated COVID-19 B-Line Classification in Lung Ultrasound.", "abstract": "Lung ultrasound (LUS) as a diagnostic tool is gaining support for its role in the diagnosis and management of COVID-19 and a number of other lung pathologies. B-lines are a predominant feature in COVID-19, however LUS requires a skilled clinician to interpret findings. To facilitate the interpretation, our main objective was to develop automated methods to classify B-lines as pathologic vs. normal. We developed transfer learning models based on ResNet networks to classify B-lines as pathologic (at least 3 B-lines per lung field) vs. normal using COVID-19 LUS data. Assessment of B-line severity on a 0-4 multi-class scale was also explored. For binary B-line classification, at the frame-level, all ResNet models pretrained with ImageNet yielded higher performance than the baseline nonpretrained ResNet-18. Pretrained ResNet-18 has the best Equal Error Rate (EER) of 9.1% vs the baseline of 11.9%. At the clip-level, all pretrained network models resulted in better Cohen's kappa agreement (linear-weighted) and clip score accuracy, with the pretrained ResNet-18 having the best Cohen's kappa of 0.815 [95% CI: 0.804-0.826], and ResNet-101 the best clip scoring accuracy of 93.6%. Similar results were shown for multi-class scoring, where pretrained network models outperformed the baseline model. A class activation map is also presented to guide clinicians in interpreting LUS findings. Future work aims to further improve the multi-class assessment for severity of B-lines with a more diverse LUS dataset.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["Joseph RPare", "Lars AGjesteby", "Brian ATelfer", "Melinda MTonelli", "Megan MLeo", "EhabBillatos", "JonathanScalera", "Laura JBrattain"], "doi": "10.1109/EMBC48229.2022.9871894"}
{"title": "Data-Efficient Training of Pure Vision Transformers for the Task of Chest X-ray Abnormality Detection Using Knowledge Distillation.", "abstract": "It is generally believed that vision transformers (ViTs) require a huge amount of data to generalize well, which limits their adoption. The introduction of data-efficient algorithms such as data-efficient image transformers (DeiT) provided an opportunity to explore the application of ViTs in medical imaging, where data scarcity is a limiting factor. In this work, we investigated the possibility of using pure transformers for the task of chest x-ray abnormality detection on a small dataset. Our proposed framework is built on a DeiT structure benefiting from a teacher-student scheme for training, with a DenseNet with strong classification performance as the teacher and an adapted ViT as the student. The results show that the performance of transformers is on par with that of convolutional neural networks (CNNs). We achieved a test accuracy of 92.2% for the task of classifying chest x-ray images (normal/pneumonia/COVID-19) on a carefully selected dataset using pure transformers. The results show the capability of transformers to accompany or replace CNNs for achieving state-of-the-art in medical imaging applications. The code and models of this work are available at https://github.com/Ouantimb-Lab/DeiTCovid.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["Seyed AliJalalifar", "AliSadeghi-Naini"], "doi": "10.1109/EMBC48229.2022.9871372"}
{"title": "Wasserstein GAN based Chest X-Ray Dataset Augmentation for Deep Learning Models: COVID-19 Detection Use-Case.", "abstract": "The novel coronavirus infection (COVID-19) is still continuing to be a concern for the entire globe. Since early detection of COVID-19 is of particular importance, there have been multiple research efforts to supplement the current standard RT-PCR tests. Several deep learning models, with varying effectiveness, using Chest X-Ray images for such diagnosis have also been proposed. While some of the models are quite promising, there still remains a dearth of training data for such deep learning models. The present paper attempts to provide a viable solution to the problem of data deficiency in COVID-19 CXR images. We show that the use of a Wasserstein Generative Adversarial Network (WGAN) could lead to an effective and lightweight solution. It is demonstrated that the WGAN generated images are at par with the original images using inference tests on an already proposed COVID-19 detection model.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["B ZahidHussain", "IfrahAndleeb", "Mohammad SamarAnsari", "Amit MaheshJoshi", "NadiaKanwal"], "doi": "10.1109/EMBC48229.2022.9871519"}
{"title": "Distance-based detection of out-of-distribution silent failures for Covid-19 lung lesion segmentation.", "abstract": "Automatic segmentation of ground glass opacities and consolidations in chest computer tomography (CT) scans can potentially ease the burden of radiologists during times of high resource utilisation. However, deep learning models are not trusted in the clinical routine due to failing silently on out-of-distribution (OOD) data. We propose a lightweight OOD detection method that leverages the Mahalanobis distance in the feature space and seamlessly integrates into state-of-the-art segmentation pipelines. The simple approach can even augment pre-trained models with clinically relevant uncertainty quantification. We validate our method across four chest CT distribution shifts and two magnetic resonance imaging applications, namely segmentation of the hippocampus and the prostate. Our results show that the proposed method effectively detects far- and near-OOD samples across all explored scenarios.", "journal": "Medical image analysis", "date": "2022-09-10", "authors": ["CamilaGonz\u00e1lez", "KarolGotkowski", "MoritzFuchs", "AndreasBucher", "ArminDadras", "RicardaFischbach", "Isabel JasminKaltenborn", "AnirbanMukhopadhyay"], "doi": "10.1016/j.media.2022.102596\n10.7937/K9/TCIA.2015.zF0vlOPv\n10.5281/zenodo.3757476\n10.1016/j.cmpb.2021.106236\n10.1055/a-1544-2240"}
{"title": "COVID-19 diagnosis via chest X-ray image classification based on multiscale class residual attention.", "abstract": "Aiming at detecting COVID-19 effectively, a multiscale class residual attention (MCRA) network is proposed via chest X-ray (CXR) image classification. First, to overcome the data shortage and improve the robustness of our network, a pixel-level image mixing of local regions was introduced to achieve data augmentation and reduce noise. Secondly, multi-scale fusion strategy was adopted to extract global contextual information at different scales and enhance semantic representation. Last but not least, class residual attention was employed to generate spatial attention for each class, which can avoid inter-class interference and enhance related features to further improve the COVID-19 detection. Experimental results show that our network achieves superior diagnostic performance on COVIDx dataset, and its accuracy, PPV, sensitivity, specificity and F1-score are 97.71%, 96.76%, 96.56%, 98.96% and 96.64%, respectively; moreover, the heat maps can endow our deep model with somewhat interpretability.", "journal": "Computers in biology and medicine", "date": "2022-09-10", "authors": ["ShangwangLiu", "TongboCai", "XiufangTang", "YangyangZhang", "ChanggengWang"], "doi": "10.1016/j.compbiomed.2022.106065\n10.1016/j.compbiomed.2022.105350\n10.1109/TIP.2021.3058783\n10.1007/s00330-020-07268-9\n10.1109/TNNLS.2021.3086570\n10.1109/TBDATA.2017.2717439\n10.1007/s11063-021-10569-9\n10.32604/cmes.2020.09463\n10.1016/j.micpro.2020.103282\n10.2174/1574893615666200207094357\n10.1016/j.compbiomed.2022.105383\n10.1007/s10489-021-02393-4\n10.1109/TMI.2021.3117564\n10.1016/j.asoc.2021.108041\n10.1016/j.compbiomed.2021.105002\n10.1016/j.media.2020.101839\n10.1007/s11063-022-10742-8\n10.1016/j.compbiomed.2021.105127\n10.1007/s10489-021-02572-3\n10.1109/TMI.2021.3127074\n10.1007/s00521-021-06806-w\n10.1016/j.compbiomed.2022.105604\n10.1016/j.compbiomed.2022.105244\n10.1016/j.compbiomed.2022.105210\n10.1007/s10489-021-02691-x\n10.1016/j.compbiomed.2022.105335\n10.1109/TIM.2021.3128703\n10.1109/TGRS.2021.3080580\n10.1109/TIP.2021.3124668\n10.1109/TIP.2021.3127851\n10.1109/TGRS.2021.3056624\n10.1016/j.neucom.2021.12.077\n10.1109/TIP.2022.3144017\n10.1109/TMI.2021.3140120\n10.1016/j.media.2021.102345\n10.1109/TIP.2021.3139232\n10.1109/TIP.2022.3154931\n10.1016/j.neucom.2021.11.104\n10.1016/j.media.2022.102381\n10.1109/TPAMI.2020.3040258\n10.1109/TPAMI.2020.3026069\n10.1109/CVPR.2016.90"}
{"title": "Machine Learning Model Based on Radiomic Features for Differentiation between COVID-19 and Pneumonia on Chest X-ray.", "abstract": "Machine learning approaches are employed to analyze differences in real-time reverse transcription polymerase chain reaction scans to differentiate between COVID-19 and pneumonia. However, these methods suffer from large training data requirements, unreliable images, and uncertain clinical diagnosis. Thus, in this paper, we used a machine learning model to differentiate between COVID-19 and pneumonia via radiomic features using a bias-minimized dataset of chest X-ray scans. We used logistic regression (LR), naive Bayes (NB), support vector machine (SVM), k-nearest neighbor (KNN), bagging, random forest (RF), extreme gradient boosting (XGB), and light gradient boosting machine (LGBM) to differentiate between COVID-19 and pneumonia based on training data. Further, we used a grid search to determine optimal hyperparameters for each machine learning model and 5-fold cross-validation to prevent overfitting. The identification performances of COVID-19 and pneumonia were compared with separately constructed test data for four machine learning models trained using the maximum probability, contrast, and difference variance of the gray level co-occurrence matrix (GLCM), and the skewness as input variables. The LGBM and bagging model showed the highest and lowest performances; the GLCM difference variance showed a high overall effect in all models. Thus, we confirmed that the radiomic features in chest X-rays can be used as indicators to differentiate between COVID-19 and pneumonia using machine learning.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-09-10", "authors": ["Young JaeKim"], "doi": "10.3390/s22176709\n10.1007/s00253-020-11061-5\n10.1016/j.prp.2021.153443\n10.1148/radiol.2020203173\n10.1007/s11547-020-01232-9\n10.1016/S0140-6736(20)30183-5\n10.1016/j.diii.2020.03.014\n10.1016/j.ejmp.2021.04.016\n10.1016/j.rinp.2021.105045\n10.1016/j.bbe.2021.05.013\n10.1007/s10489-020-01902-1\n10.1007/s10489-020-02055-x\n10.1007/s13347-021-00477-0\n10.1158/0008-5472.CAN-17-0339\n10.2967/jnumed.118.222893\n10.4236/jsip.2012.32019\n10.1016/j.ejmp.2017.05.071\n10.3390/tomography7020022\n10.1007/s00521-020-05017-z\n10.1002/jmri.26524\n10.1016/j.procs.2018.05.057\n10.1186/s40644-019-0243-3\n10.1007/s00521-018-3754-0\n10.4236/ojs.2015.57075\n10.1016/j.inffus.2018.11.008\n10.1016/S1532-0464(03)00034-0\n10.1155/2021/5594899\n10.1007/s41870-018-0233-x\n10.1177/1536867X20909688\n10.3390/diagnostics11091714\n10.1016/j.cpc.2018.02.018\n10.1109/TKDE.2019.2912815"}
{"title": "Fine-Grained Assessment of COVID-19 Severity Based on Clinico-Radiological Data Using Machine Learning.", "abstract": "The severe and critical cases of COVID-19 had high mortality rates. Clinical features, laboratory data, and radiological features provided important references for the assessment of COVID-19 severity. The machine learning analysis of clinico-radiological features, especially the quantitative computed tomography (CT) image analysis results, may achieve early, accurate, and fine-grained assessment of COVID-19 severity, which is an urgent clinical need.\nTo evaluate if machine learning algorithms using CT-based clinico-radiological features could achieve the accurate fine-grained assessment of COVID-19 severity.\nThe clinico-radiological features were collected from 78 COVID-19 patients with different severities. A neural network was developed to automatically measure the lesion volume from CT images. The severity was clinically diagnosed using two-type (severe and non-severe) and fine-grained four-type (mild, regular, severe, critical) classifications, respectively. To investigate the key features of COVID-19 severity, statistical analyses were performed between patients' clinico-radiological features and severity. Four machine learning algorithms (decision tree, random forest, SVM, and XGBoost) were trained and applied in the assessment of COVID-19 severity using clinico-radiological features.\nThe CT imaging features (CTscore and lesion volume) were significantly related with COVID-19 severity (\nCT-based clinico-radiological features can provide an important reference for the accurate fine-grained assessment of illness severity using machine learning to achieve the early triage of COVID-19 patients.", "journal": "International journal of environmental research and public health", "date": "2022-09-10", "authors": ["HaipengLiu", "JiangtaoWang", "YayuanGeng", "KunweiLi", "HanWu", "JianChen", "XiangfeiChai", "ShaolinLi", "DingchangZheng"], "doi": "10.3390/ijerph191710665\n10.1016/S0140-6736(20)30566-3\n10.1002/jmv.25770\n10.1186/s13613-020-00650-2\n10.1002/jmv.25748\n10.1101/2020.03.24.20042283\n10.1038/s41392-020-0148-4\n10.18632/aging.103372\n10.1016/j.kint.2020.03.005\n10.34133/2020/2402961\n10.2214/AJR.20.22954\n10.1148/radiol.2020200463\n10.7150/thno.45985\n10.2147/IDR.S264541\n10.21037/atm-20-2464\n10.1109/RBME.2020.2987975\n10.2139/ssrn.3564426\n10.1016/j.acra.2020.09.004\n10.1097/IM9.0000000000000022\n10.1007/s00330-020-06817-6\n10.1080/01605682.2020.1865846\n10.1186/s12967-020-02692-3\n10.1002/mp.14609\n10.1007/s00330-020-07013-2\n10.1503/cmaj.200648\n10.1148/radiol.2020200490\n10.1056/NEJMsb2005114\n10.1016/j.dsx.2020.12.029\n10.7150/thno.51471"}
{"title": "Semantic-Powered Explainable Model-Free Few-Shot Learning Scheme of Diagnosing COVID-19 on Chest X-Ray.", "abstract": "Chest X-ray (CXR) is commonly performed as an initial investigation in COVID-19, whose fast and accurate diagnosis is critical. Recently, deep learning has a great potential in detecting people who are suspected to be infected with COVID-19. However, deep learning resulting with black-box models, which often breaks down when forced to make predictions about data for which limited supervised information is available and lack inter-pretability, still is a major barrier for clinical integration. In this work, we hereby propose a semantic-powered explainable model-free few-shot learning scheme to quickly and precisely diagnose COVID-19 with higher reliability and transparency. Specifically, we design a Report Image Explanation Cell (RIEC) to exploit clinically indicators derived from radiology reports as interpretable driver to introduce prior knowledge at training. Meanwhile, multi-task collaborative diagnosis strategy (MCDS) is developed to construct N-way K-shot tasks, which adopts a cyclic and collaborative training approach for producing better generalization performance on new tasks. Extensive experiments demonstrate that the proposed scheme achieves competitive results (accuracy of 98.91%, precision of 98.95%, recall of 97.94% and F1-score of 98.57%) to diagnose COVID-19 and other pneumonia infected categories, even with only 200 paired CXR images and radiology reports for training. Furthermore, statistical results of comparative experiments show that our scheme provides an interpretable window into the COVID-19 diagnosis to improve the performance of the small sample size, the reliability and transparency of black-box deep learning models. Our source codes will be released on https://github.com/AI-medical-diagnosis-team-of-JNU/SPEMFSL-Diagnosis-COVID-19.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-09-09", "authors": ["YihangWang", "ChunjuanJiang", "YouqingWu", "TianxuLv", "HengSun", "YuanLiu", "LihuaLi", "XiangPan"], "doi": "10.1109/JBHI.2022.3205167"}
{"title": "Imaging of Intimate Partner Violence, From the ", "abstract": "Intimate partner violence (IPV) is a highly prevalent public health issue with multiple adverse health effects. Radiologists are well suited to assessing a patient's likelihood of IPV. Recognition of common IPV injury mechanisms and resulting target and defensive injury patterns on imaging and understanding of differences between patients who have experienced IPV and those who have not with respect to use of imaging will aid radiologists in accurate IPV diagnosis. Target injuries often involve the face and neck as a result of blunt trauma or strangulation; defensive injuries often involve an extremity. Awareness of differences in injury patterns resulting from IPV-related and accidental trauma can aid radiologists in detecting a mismatch between the provided clinical history and imaging findings to support suspicion of IPV. Radiologists should consider all available current and prior imaging in assessing the likelihood of IPV; this process may be aided by machine learning methods. Even if correctly suspecting IPV on the basis of imaging, radiologists face challenges in acting on that suspicion, including appropriately documenting the findings, without compromising the patient's confidentiality and safety. However, through a multidisciplinary approach with appropriate support mechanisms, radiologists may serve as effective frontline physicians for raising suspicion of IPV.", "journal": "AJR. American journal of roentgenology", "date": "2022-09-08", "authors": ["AnjiTang", "AndrewWong", "BhartiKhurana"], "doi": "10.2214/AJR.22.27973"}
{"title": "Ensemble of Deep Neural Networks based on Condorcet's Jury Theorem for screening Covid-19 and Pneumonia from radiograph images.", "abstract": "COVID-19 detection using Artificial Intelligence and Computer-Aided Diagnosis has been the subject of several studies. Deep Neural Networks with hundreds or even millions of parameters (weights) are referred to as \"black boxes\" because their behavior is difficult to comprehend, even when the model's structure and weights are visible. On the same dataset, different Deep Convolutional Neural Networks perform differently. So, we do not necessarily have to rely on just one model; instead, we can evaluate our final score by combining multiple models. While including multiple models in the voter pool, it is not always true that the accuracy will improve. So, In this regard, the authors proposed a novel approach to determine the voting ensemble score of individual classifiers based on Condorcet's Jury Theorem (CJT). The authors demonstrated that the theorem holds while ensembling the N number of classifiers in Neural Networks. With the help of CJT, the authors proved that a model's presence in the voter pool would improve the likelihood that the majority vote will be accurate if it is more accurate than the other models. Besides this, the authors also proposed a Domain Extended Transfer Learning (DETL) ensemble model as a soft voting ensemble method and compared it with CJT based ensemble method. Furthermore, as deep learning models typically fail in real-world testing, a novel dataset has been used with no duplicate images. Duplicates in the dataset are quite problematic since they might affect the training process. Therefore, having a dataset devoid of duplicate images is considered to prevent data leakage problems that might impede the thorough assessment of the trained models. The authors also employed an algorithm for faster training to save computational efforts. Our proposed method and experimental results outperformed the state-of-the-art with the DETL-based ensemble model showing an accuracy of 97.26%, COVID-19, sensitivity of 98.37%, and specificity of 100%. CJT-based ensemble model showed an accuracy of 98.22%, COVID-19, sensitivity of 98.37%, and specificity of 99.79%.", "journal": "Computers in biology and medicine", "date": "2022-09-06", "authors": ["GauravSrivastava", "NiteshPradhan", "YashwinSaini"], "doi": "10.1016/j.compbiomed.2022.105979\n10.1109/TII.2021.3057683\n10.1109/TII.2021.3057524"}
{"title": "Deep learning framework for prediction of infection severity of COVID-19.", "abstract": "With the onset of the COVID-19 pandemic, quantifying the condition of positively diagnosed patients is of paramount importance. Chest CT scans can be used to measure the severity of a lung infection and the isolate involvement sites in order to increase awareness of a patient's disease progression. In this work, we developed a deep learning framework for lung infection severity prediction. To this end, we collected a dataset of 232 chest CT scans and involved two public datasets with an additional 59 scans for our model's training and used two external test sets with 21 scans for evaluation. On an input chest Computer Tomography (CT) scan, our framework, in parallel, performs a lung lobe segmentation utilizing a pre-trained model and infection segmentation using three distinct trained ", "journal": "Frontiers in medicine", "date": "2022-09-06", "authors": ["MehdiYousefzadeh", "MasoudHasanpour", "MozhdehZolghadri", "FatemehSalimi", "AvaYektaeian Vaziri", "AbolfazlMahmoudi Aqeel Abadi", "RamezanJafari", "ParsaEsfahanian", "Mohammad-RezaNazem-Zadeh"], "doi": "10.3389/fmed.2022.940960\n10.1016/j.idm.2020.02.002\n10.1101/2020.02.27.20028027\n10.1101/2020.02.07.937862\n10.1148/radiol.2020200642\n10.1148/radiol.2020200343\n10.1371/journal.pone.0250952\n10.1038/nature14539\n10.1038/s41598-019-51503-3\n10.1038/s41591-019-0447-x\n10.1038/s41598-019-56589-3\n10.1109/TNNLS.2019.2892409\n10.1038/s41591-020-0931-3\n10.1016/j.patcog.2018.07.031\n10.1016/j.dsx.2020.04.012\n10.1109/RBME.2020.2987975\n10.1016/S2589-7500(20)30054-6\n10.1148/ryct.2020200075\n10.1101/2020.03.24.20041020\n10.1136/bmj.m1328\n10.1007/s42979-020-00216-w\n10.1007/s10916-020-01582-x\n10.1109/TAI.2020.3020521\n10.1007/s10278-019-00227-x\n10.1038/srep46479\n10.1109/JBHI.2017.2725903\n10.1002/mp.14676\n10.1007/s00330-020-07042-x\n10.1016/j.knosys.2020.106647\n10.1109/TMI.2020.2995108\n10.1109/ISBI.2019.8759468\n10.1007/s10278-019-00223-1\n10.1186/s41747-020-00173-2\n10.1038/s41598-020-76282-0\n10.48550/arXiv.2003.11988\n10.1007/s10489-020-01829-7\n10.1016/j.patcog.2020.107747\n10.3389/fpubh.2020.00357\n10.48550/arXiv.2006.05018\n10.1101/2020.05.20.20108159\n10.1101/2020.05.20.20100362\n10.1007/978-3-319-24574-4_28\n10.48550/arXiv.1511.07122\n10.1109/TPAMI.2017.2699184\n10.1118/1.3611983\n10.1016/j.imu.2022.100935"}
{"title": "A novel adaptive cubic quasi-Newton optimizer for deep learning based medical image analysis tasks, validated on detection of COVID-19 and segmentation for COVID-19 lung infection, liver tumor, and optic disc/cup.", "abstract": "Most of existing deep learning research in medical image analysis is focused on networks with stronger performance. These networks have achieved success, while their architectures are complex and even contain massive parameters ranging from thousands to millions in numbers. The nature of high dimension and nonconvex makes it easy to train a suboptimal model through the popular stochastic first-order optimizers, which only use gradient\u00a0information.\nOur purpose is to design an adaptive cubic quasi-Newton optimizer, which could help to escape from suboptimal solution and improve the performance of deep neural networks on four medical image analysis tasks including: detection of COVID-19, COVID-19 lung infection segmentation, liver tumor segmentation, optic disc/cup\u00a0segmentation.\nIn this work, we introduce a novel adaptive cubic quasi-Newton optimizer with high-order moment (termed ACQN-H) for medical image analysis. The optimizer dynamically captures the curvature of the loss function by diagonally approximated Hessian and the norm of difference between previous two estimates, which helps to escape from saddle points more efficiently. In addition, to reduce the variance introduced by the stochastic nature of the problem, ACQN-H hires high-order moment through exponential moving average on iteratively calculated approximated Hessian matrix. Extensive experiments are performed to access the performance of ACQN-H. These include detection of COVID-19 using COVID-Net on dataset COVID-chestxray, which contains 16\u00a0565 training samples and 1841 test samples; COVID-19 lung infection segmentation using Inf-Net on COVID-CT, which contains 45, 5, and 5 computer tomography (CT) images for training, validation, and testing, respectively; liver tumor segmentation using ResUNet on LiTS2017, which consists of 50\u00a0622 abdominal scan images for training and 26\u00a0608 images for testing; optic disc/cup segmentation using MRNet on RIGA, which has 655 color fundus images for training and 95 for testing. The results are compared with commonly used stochastic first-order optimizers such as Adam, SGD, and AdaBound, and recently proposed stochastic quasi-Newton optimizer Apollo. In task detection of COVID-19, we use classification accuracy as the evaluation metric. For the other three medical image segmentation tasks, seven commonly used evaluation metrics are utilized, that is, Dice, structure measure, enhanced-alignment measure (EM), mean absolute error (MAE), intersection over union (IoU), true positive rate (TPR), and true negative rate.\nExperiments on four tasks show that ACQN-H achieves improvements over other stochastic optimizers: (1) comparing with AdaBound, ACQN-H achieves 0.49%, 0.11%, and 0.70% higher accuracy on the COVID-chestxray dataset using network COVID-Net with VGG16, ResNet50 and DenseNet121 as backbones, respectively; (2) ACQN-H has the best scores in terms of evaluation metrics Dice, TPR, EM, and MAE on COVID-CT dataset using network Inf-Net. Particularly, ACQN-H achieves 1.0% better Dice as compared to Apollo; (3) ACQN-H achieves the best results on LiTS2017 dataset using network ResUNet, and outperforms Adam in terms of Dice by 2.3%; (4) ACQN-H improves the performance of network MRNet on RIGA dataset, and achieves 0.5% and 1.0% better scores on cup segmentation for Dice and IoU, respectively, compared with SGD. We also present fivefold validation results of four tasks. It can be found that the results on detection of COVID-19, liver tumor segmentation and optic disc/cup segmentation can achieve high performance with low variance. For COVID-19 lung infection segmentation, the variance on test set is much larger than on validation set, which may due to small size of\u00a0dataset.\nThe proposed optimizer ACQN-H has been validated on four medical image analysis tasks including: detection of COVID-19 using COVID-Net on COVID-chestxray, COVID-19 lung infection segmentation using Inf-Net on COVID-CT, liver tumor segmentation using ResUNet on LiTS2017, optic disc/cup segmentation using MRNet on RIGA. Experiments show that ACQN-H can achieve some performance improvement. Moreover, the work is expected to boost the performance of existing deep learning networks in medical image\u00a0analysis.", "journal": "Medical physics", "date": "2022-09-05", "authors": ["YanLiu", "MaojunZhang", "ZhiweiZhong", "XiangrongZeng"], "doi": "10.1002/mp.15969"}
{"title": "A Deep Ensemble Dynamic Learning Network for Corona Virus Disease 2019 Diagnosis.", "abstract": "Corona virus disease 2019 is an extremely fatal pandemic around the world. Intelligently recognizing X-ray chest radiography images for automatically identifying corona virus disease 2019 from other types of pneumonia and normal cases provides clinicians with tremendous conveniences in diagnosis process. In this article, a deep ensemble dynamic learning network is proposed. After a chain of image preprocessing steps and the division of image dataset, convolution blocks and the final average pooling layer are pretrained as a feature extractor. For classifying the extracted feature samples, two-stage bagging dynamic learning network is trained based on neural dynamic learning and bagging algorithms, which diagnoses the presence and types of pneumonia successively. Experimental results manifest that using the proposed deep ensemble dynamic learning network obtains 98.7179% diagnosis accuracy, which indicates more excellent diagnosis effect than existing state-of-the-art models on the open image dataset. Such accurate diagnosis effects provide convincing evidences for further detections and treatments.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2022-09-03", "authors": ["ZhijunZhang", "BozhaoChen", "YameiLuo"], "doi": "10.1109/TNNLS.2022.3201198"}
{"title": "Deep Convolutional Neural Network Mechanism Assessment of COVID-19 Severity.", "abstract": "As an epidemic, COVID-19's core test instrument still has serious flaws. To improve the present condition, all capabilities and tools available in this field are being used to combat the pandemic. Because of the contagious characteristics of the unique coronavirus (COVID-19) infection, an overwhelming comparison with patients queues up for pulmonary X-rays, overloading physicians and radiology and significantly impacting the quality of care, diagnosis, and outbreak prevention. Given the scarcity of clinical services such as intensive care and motorized ventilation systems in the aspect of this vastly transmissible ailment, it is critical to categorize patients as per their risk categories. This research describes a novel use of the deep convolutional neural network (CNN) technique to COVID-19 illness assessment seriousness. Utilizing chest X-ray images as contribution, an unsupervised DCNN model is constructed and suggested to split COVID-19 individuals into four seriousness classrooms: low, medium, serious, and crucial with an accuracy level of 96 percent. The efficiency of the DCNN model developed with the proposed methodology is demonstrated by empirical findings on a suitably huge sum of chest X-ray scans. To the evidence relating, it is the first COVID-19 disease incidence evaluation research with four different phases, to use a reasonably high number of X-ray images dataset and a DCNN with nearly all hyperparameters dynamically adjusted by the variable selection optimization task.", "journal": "BioMed research international", "date": "2022-09-03", "authors": ["JNirmaladevi", "MVidhyalakshmi", "E BijolinEdwin", "NVenkateswaran", "VinayAvasthi", "Abdullah AAlarfaj", "Abdurahman HajinurHirad", "R KRajendran", "TegegneAyalewHailu"], "doi": "10.1155/2022/1289221\n10.1109/ISMSIT50672.2020.9255149\n10.3390/electronics10141677\n10.1101/2020.06.25.20140004\n10.1002/pa.2537\n10.1016/j.asoc.2020.106912\n10.1109/SMART-TECH49988.2020.00041\n10.1155/2022/4352730\n10.3390/a13100249\n10.1016/j.idm.2020.03.002\n10.1155/2021/5709257\n10.1007/s42600-020-00105-4\n10.4066/biomedicalresearch.29-18-886\n10.1007/s40747-021-00312-1\n10.1155/2020/8856801\n10.1007/s10489-020-01829-7\n10.1155/2021/6927985\n10.1155/2021/5587188\n10.1016/j.chaos.2020.110056\n10.3390/jpm11050343\n10.3390/jcm9061668\n10.1016/j.cmpb.2019.06.023\n10.3390/info12030109\n10.1016/j.chaos.2020.110059\n10.1016/j.ipm.2021.102809\n10.1371/journal.pone.0241332\n10.3389/fimmu.2020.01581\n10.1109/ACCESS.2020.2997311\n10.1016/j.patter.2020.100074\n10.1148/radiol.2021204531\n10.1016/S2589-7500(20)30162-X"}
{"title": "High-dimensional multinomial multiclass severity scoring of COVID-19 pneumonia using CT radiomics features and machine learning algorithms.", "abstract": "We aimed to construct a prediction model based on computed tomography (CT) radiomics features to classify COVID-19 patients into severe-, moderate-, mild-, and non-pneumonic. A total of 1110 patients were studied from a publicly available dataset with 4-class severity scoring performed by a radiologist (based on CT images and clinical features). The entire lungs were segmented and followed by resizing, bin discretization and radiomic features extraction. We utilized two feature selection algorithms, namely bagging random forest (BRF) and multivariate adaptive regression splines (MARS), each coupled to a classifier, namely multinomial logistic regression (MLR), to construct multiclass classification models. The dataset was divided into 50% (555 samples), 20% (223 samples), and 30% (332 samples) for training, validation, and untouched test datasets, respectively. Subsequently, nested cross-validation was performed on train/validation to select the features and tune the models. All predictive power indices were reported based on the testing set. The performance of multi-class models was assessed using precision, recall, F1-score, and accuracy based on the 4\u2009\u00d7\u20094 confusion matrices. In addition, the areas under the receiver operating characteristic curves (AUCs) for multi-class classifications were calculated and compared for both models. Using BRF, 23 radiomic features were selected, 11 from first-order, 9 from GLCM, 1 GLRLM, 1 from GLDM, and 1 from shape. Ten features were selected using the MARS algorithm, namely 3 from first-order, 1 from GLDM, 1 from GLRLM, 1 from GLSZM, 1 from shape, and 3 from GLCM features. The mean absolute deviation, skewness, and variance from first-order and flatness from shape, and cluster prominence from GLCM features and Gray Level Non Uniformity Normalize from GLRLM were selected by both BRF and MARS algorithms. All selected features by BRF or MARS were significantly associated with four-class outcomes as assessed within MLR (All p values\u2009<\u20090.05). BRF\u2009+\u2009MLR and MARS\u2009+\u2009MLR resulted in pseudo-R", "journal": "Scientific reports", "date": "2022-09-02", "authors": ["IsaacShiri", "ShayanMostafaei", "AtlasHaddadi Avval", "YazdanSalimi", "AmirhosseinSanaat", "AzadehAkhavanallaf", "HosseinArabi", "ArmanRahmim", "HabibZaidi"], "doi": "10.1038/s41598-022-18994-z\n10.1016/j.ijid.2020.09.1464\n10.1016/j.rbmo.2020.06.001\n10.1289/ehp.120-a118\n10.2214/ajr.20.22954\n10.1016/j.ejrad.2020.108961\n10.1186/s12904-016-0105-8\n10.1016/j.contraception.2010.08.022\n10.1093/bjaceaccp/mkn033\n10.1038/s41591-020-0979-0\n10.1016/j.jaci.2020.04.006\n10.1164/rccm.201105-0816OC\n10.4103/ijri.IJRI_300_16\n10.1007/s00261-007-9315-0\n10.1148/radiol.202020147310.1148/radiol.2020201473\n10.1186/s13244-020-00901-7\n10.1007/s00330-020-07033-y\n10.2214/ajr.20.22976\n10.1097/rli.0000000000000672\n10.1148/radiol.2015151169\n10.1053/j.semnuclmed.2022.04.004\n10.7150/thno.30309\n10.1088/0031-9155/61/13/R150\n10.1016/j.ijrobp.2014.11.030\n10.1016/j.compbiomed.2021.104304\n10.1016/j.compbiomed.2021.105145\n10.1016/j.ejro.2020.100271\n10.1016/j.media.2020.101910\n10.1016/j.compbiomed.2022.105467\n10.1097/rct.0000000000001094\n10.1148/radiol.2020200843\n10.1038/s41467-020-18685-1\n10.1002/ima.22672\n10.1148/radiol.2020191145\n10.1158/0008-5472.Can-17-0339\n10.1186/s13244-021-01105-3\n10.1016/j.compbiomed.2021.104752\n10.1155/2021/6919483\n10.1007/s12559-021-09848-310.1007/s12559-021-09848-3\n10.1155/2021/9996737\n10.1155/2022/4694567\n10.1016/j.compbiomed.2021.104834\n10.1186/s12938-020-00831-x10.1186/s12938-020-00831-x\n10.1148/ryct.2020200322\n10.1007/s00330-020-07012-3\n10.1148/ryai.2020200048\n10.1016/j.clon.2021.11.014\n10.1016/j.compbiomed.2022.105230\n10.1007/s10278-021-00500-y"}
{"title": "Deep learning-based patient re-identification is able to exploit the biometric nature of medical chest X-ray data.", "abstract": "With the rise and ever-increasing potential of deep learning techniques in recent years, publicly available medical datasets became a key factor to enable reproducible development of diagnostic algorithms in the medical domain. Medical data contains sensitive patient-related information and is therefore usually anonymized by removing patient identifiers, e.g., patient names before publication. To the best of our knowledge, we are the first to show that a well-trained deep learning system is able to recover the patient identity from chest X-ray data. We demonstrate this using the publicly available large-scale ChestX-ray14 dataset, a collection of 112,120 frontal-view chest X-ray images from 30,805 unique patients. Our verification system is able to identify whether two frontal chest X-ray images are from the same person with an AUC of 0.9940 and a classification accuracy of 95.55%. We further highlight that the proposed system is able to reveal the same person even ten and more years after the initial scan. When pursuing a retrieval approach, we observe an mAP@R of 0.9748 and a precision@1 of 0.9963. Furthermore, we achieve an AUC of up to 0.9870 and a precision@1 of up to 0.9444 when evaluating our trained networks on external datasets such as CheXpert and the COVID-19 Image Data Collection. Based on this high identification rate, a potential attacker may leak patient-related information and additionally cross-reference images to obtain more information. Thus, there is a great risk of sensitive content falling into unauthorized hands or being disseminated against the will of the concerned patients. Especially during the COVID-19 pandemic, numerous chest X-ray datasets have been published to advance research. Therefore, such data may be vulnerable to potential attacks by deep learning-based re-identification algorithms.", "journal": "Scientific reports", "date": "2022-09-02", "authors": ["KaiPackh\u00e4user", "SebastianG\u00fcndel", "NicolasM\u00fcnster", "ChristopherSyben", "VincentChristlein", "AndreasMaier"], "doi": "10.1038/s41598-022-19045-3\n10.1378/chest.10-1302\n10.1038/s41598-019-56847-4\n10.2214/AJR.12.10375\n10.1038/nature14539\n10.1016/j.zemedi.2018.12.003\n10.1016/j.acra.2019.10.006\n10.1016/S0197-2456(00)00097-0\n10.1007/s40256-020-00420-2\n10.1148/radiol.2020192224\n10.1007/s10278-006-1051-4\n10.1142/S0218488502001648\n10.1016/j.csl.2019.06.001\n10.1145/1866739.1866758\n10.1561/0400000042\n10.1038/s42256-020-0186-1\n10.1109/MSP.2013.2259911\n10.1038/s41746-020-00323-1\n10.1038/s42256-021-00337-8\n10.1126/science.aab3050\n10.1109/5.726791\n10.1016/j.patrec.2005.10.010"}
{"title": "Point-of-care SARS-CoV-2 sensing using lens-free imaging and a deep learning-assisted quantitative agglutination assay.", "abstract": "The persistence of the global COVID-19 pandemic caused by the SARS-CoV-2 virus has continued to emphasize the need for point-of-care (POC) diagnostic tests for viral diagnosis. The most widely used tests, lateral flow assays used in rapid antigen tests, and reverse-transcriptase real-time polymerase chain reaction (RT-PCR), have been instrumental in mitigating the impact of new waves of the pandemic, but fail to provide both sensitive and rapid readout to patients. Here, we present a portable lens-free imaging system coupled with a particle agglutination assay as a novel biosensor for SARS-CoV-2. This sensor images and quantifies individual microbeads undergoing agglutination through a combination of computational imaging and deep learning as a way to detect levels of SARS-CoV-2 in a complex sample. SARS-CoV-2 pseudovirus in solution is incubated with acetyl cholinesterase 2 (ACE2)-functionalized microbeads then loaded into an inexpensive imaging chip. The sample is imaged in a portable in-line lens-free holographic microscope and an image is reconstructed from a pixel superresolved hologram. Images are analyzed by a deep-learning algorithm that distinguishes microbead agglutination from cell debris and viral particle aggregates, and agglutination is quantified based on the network output. We propose an assay procedure using two images which results in the accurate determination of viral concentrations greater than the limit of detection (LOD) of 1.27 \u00d7 10", "journal": "Lab on a chip", "date": "2022-09-02", "authors": ["Colin JPotter", "YanmeiHu", "ZhenXiong", "JunWang", "EuanMcLeod"], "doi": "10.1039/d2lc00289b"}
{"title": "Automated COVID-19 Classification Using Heap-Based Optimization with the Deep Transfer Learning Model.", "abstract": "The outbreak of the COVID-19 pandemic necessitates prompt identification of affected persons to restrict the spread of the COVID-19 epidemic. Radiological imaging such as computed tomography (CT) and chest X-rays (CXR) is considered an effective way to diagnose COVID-19. However, it needs an expert's knowledge and consumes more time. At the same time, artificial intelligence (AI) and medical images are discovered to be helpful in effectively assessing and providing treatment for COVID-19 infected patients. In particular, deep learning (DL) models act as a vital part of a high-performance classification model for COVID-19 recognition on CXR images. This study develops a heap-based optimization with the deep transfer learning model for detection and classification (HBODTL-DC) of COVID-19. The proposed HBODTL-DC system majorly focuses on the identification of COVID-19 on CXR images. To do so, the presented HBODTL-DC model initially exploits the Gabor filtering (GF) technique to enhance the image quality. In addition, the HBO algorithm with a neural architecture search network (NasNet) large model is employed for the extraction of feature vectors. Finally, Elman Neural Network (ENN) model gets the feature vectors as input and categorizes the CXR images into distinct classes. The experimental validation of the HBODTL-DC model takes place on the benchmark CXR image dataset from the Kaggle repository, and the outcomes are checked in numerous dimensions. The experimental outcomes stated the supremacy of the HBODTL-DC model over recent approaches with a maximum accuracy of 0.9992.", "journal": "Computational intelligence and neuroscience", "date": "2022-09-02", "authors": ["BahjatFakieh", "MahmoudRagab"], "doi": "10.1155/2022/7508836\n10.3390/jpm12020309\n10.3390/s21217286\n10.3390/ijerph18063056\n10.1016/j.patrec.2021.08.018\n10.3390/app11199023\n10.1016/j.bbe.2020.08.008\n10.1155/2020/8828855\n10.1016/j.eswa.2020.114054\n10.1007/s11760-020-01820-2\n10.1016/j.cmpb.2020.105581\n10.1109/SSCI47803.2020.9308571\n10.3390/s21041480\n10.1109/access.2020.3025010\n10.3390/diagnostics11050895\n10.1016/j.patcog.2014.01.006\n10.1016/j.eswa.2020.113702\n10.1016/j.asej.2022.101728\n10.1177/0361198120967943\n10.1016/j.compbiomed.2021.104816\n10.1155/2022/6074538\n10.1155/2022/6185013"}
{"title": "Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Image Segmentation.", "abstract": "Automatic tumor or lesion segmentation is a crucial step in medical image analysis for computer-aided diagnosis. Although the existing methods based on convolutional neural networks (CNNs) have achieved the state-of-the-art performance, many challenges still remain in medical tumor segmentation. This is because, although the human visual system can detect symmetries in 2-D images effectively, regular CNNs can only exploit translation invariance, overlooking further inherent symmetries existing in medical images, such as rotations and reflections. To solve this problem, we propose a novel group equivariant segmentation framework by encoding those inherent symmetries for learning more precise representations. First, kernel-based equivariant operations are devised on each orientation, which allows it to effectively address the gaps of learning symmetries in existing approaches. Then, to keep segmentation networks globally equivariant, we design distinctive group layers with layer-wise symmetry constraints. Finally, based on our novel framework, extensive experiments conducted on real-world clinical data demonstrate that a group equivariant Res-UNet (called GER-UNet) outperforms its regular CNN-based counterpart and the state-of-the-art segmentation methods in the tasks of hepatic tumor segmentation, COVID-19 lung infection segmentation, and retinal vessel detection. More importantly, the newly built GER-UNet also shows potential in reducing the sample complexity and the redundancy of filters, upgrading current segmentation CNNs, and delineating organs on other medical imaging modalities.", "journal": "IEEE transactions on cybernetics", "date": "2022-09-01", "authors": ["ShuchaoPang", "AnanDu", "Mehmet AOrgun", "YanWang", "Quan ZSheng", "ShoujinWang", "XiaoshuiHuang", "ZhenmeiYu"], "doi": "10.1109/TCYB.2022.3195447"}
{"title": "Multithreshold Segmentation and Machine Learning Based Approach to Differentiate COVID-19 from Viral Pneumonia.", "abstract": "Coronavirus disease (COVID-19) has created an unprecedented devastation and the loss of millions of lives globally. Contagious nature and fatalities invariably pose challenges to physicians and healthcare support systems. Clinical diagnostic evaluation using reverse transcription-polymerase chain reaction and other approaches are currently in use. The Chest X-ray (CXR) and CT images were effectively utilized in screening purposes that could provide relevant data on localized regions affected by the infection. A step towards automated screening and diagnosis using CXR and CT could be of considerable importance in these turbulent times. The main objective is to probe a simple threshold-based segmentation approach to identify possible infection regions in CXR images and investigate intensity-based, wavelet transform (WT)-based, and Laws based texture features with statistical measures. Further feature selection strategy using Random Forest (RF) then selected features used to create Machine Learning (ML) representation with Support Vector Machine (SVM) and a Random Forest (RF) to make different COVID-19 from viral pneumonia (VP). The results obtained clearly indicate that the intensity and WT-based features vary in the two pathologies that are better differentiated with the combined features trained using SVM and RF classifiers. Classifier performance measures like an Area Under the Curve (AUC) of 0.97 and by and large classification accuracy of 0.9 using the RF model clearly indicate that the methodology implemented is useful in characterizing COVID-19 and Viral Pneumonia.", "journal": "Computational intelligence and neuroscience", "date": "2022-08-31", "authors": ["ShaikMahaboob Basha", "Alo\u00edsio VieiraLira Neto", "SamahAlshathri", "Mohamed AbdElaziz", "ShaikHashmitha Mohisin", "Victor Hugo CDe Albuquerque"], "doi": "10.1155/2022/2728866\n10.23750/abm.v91i1.9397\n10.1038/s41551-021-00704-1\n10.1186/s12938-018-0544-y\n10.1038/s41598-020-74539-2\n10.1007/s13755-021-00146-8\n10.1016/j.measurement.2019.05.076\n10.1016/j.crad.2018.12.015\n10.1109/access.2020.3010287\n10.1007/s12559-020-09787-5\n10.1109/access.2020.2994762\n10.1186/s12938-020-00831-x\n10.1007/978-3-030-87196-3_54\n10.1109/tmi.2020.2993291\n10.1016/j.jart.2017.07.005\n10.1109/JIOT.2020.3038009\n10.1109/anziis.2001.974061\n10.1007/s12065-019-00327-1\n10.1109/42.141636\n10.3390/jimaging7110245\n10.32604/iasc.2021.014369\n10.1109/access.2020.2980942\n10.1016/j.aei.2017.09.007\n10.1002/ima.22393\n10.1007/s10334-018-0674-z\n10.1155/2015/457906\n10.22266/ijies2016.0930.09\n10.1109/jsac.2020.3020598\n10.1002/ima.22518\n10.1109/jas.2020.1003393\n10.1016/j.rinp.2021.105045\n10.5573/ieiespc.2017.6.6.401\n10.1016/j.bspc.2014.01.008\n10.1515/jisys-2016-0111\n10.1016/j.eswa.2016.04.029\n10.1016/j.scs.2020.102589\n10.1155/2022/3167717\n10.1007/978-981-15-1286-5_66\n10.1109/primeasia.2012.6458659\n10.1109/sibircon.2010.5555323\n10.3390/electronics9020274\n10.1007/s12652-020-01963-7\n10.1007/s13042-020-01248-7\n10.1109/tsmc.1979.4310076\n10.1007/978-3-030-74575-2_14\n10.1109/tmi.2020.2993291\n10.1177/2472630320958376\n10.1080/01431160412331269698\n10.1155/2021/7517313\n10.1007/978-3-030-87196-3_54\n10.1109/access.2018.2817614\n10.1002/cpa.3160450502\n10.1016/j.ipm.2009.03.002"}
{"title": "COV-RadNet: A Deep Convolutional Neural Network for Automatic Detection of COVID-19 from Chest X-Rays and CT Scans.", "abstract": "With the increase in severity of COVID-19 pandemic situation, the world is facing a critical fight to cope up with the impacts on human health, education and economy. The ongoing battle with the novel corona virus, is showing much priority to diagnose and provide rapid treatment to the patients. The rapid growth of COVID-19 has broken the healthcare system of the affected countries, creating a shortage in ICUs, test kits, ventilation support system. etc. This paper aims at finding an automatic COVID-19 detection approach which will assist the medical practitioners to diagnose the disease quickly and effectively. In this paper, a deep convolutional neural network, 'COV-RadNet' is proposed to detect COVID positive, viral pneumonia, lung opacity and normal, healthy people by analyzing their Chest Radiographic (X-ray and CT scans) images. Data augmentation technique is applied to balance the dataset 'COVID 19 Radiography Dataset' to make the classifier more robust to the classification task. We have applied transfer learning approach using four deep learning based models: VGG16, VGG19, ResNet152 and ResNext 101 to detect COVID-19 from chest X-ray images. We have achieved 97% classification accuracy using our proposed COV-RadNet model for COVID/Viral Pneumonia/Lungs Opacity/Normal, 99.5% accuracy to detect COVID/Viral Pneumonia/Normal and 99.72% accuracy to detect COVID and non-COVID people. Using chest CT scan images, we have found 99.25% accuracy to classify between COVID and non-COVID classes. Among the performance of the pre-trained models, ResNext 101 has shown the highest accuracy of 98.5% for multiclass classification (COVID, viral pneumonia, Lungs opacity and normal).", "journal": "Computer methods and programs in biomedicine update", "date": "2022-08-31", "authors": ["Md KhairulIslam", "Sultana UmmeHabiba", "Tahsin AhmedKhan", "FarzanaTasnim"], "doi": "10.1016/j.cmpbup.2022.100064"}
{"title": "Constructing custom-made radiotranscriptomic signatures of vascular inflammation from routine CT angiograms: a prospective outcomes validation study in COVID-19.", "abstract": "Direct evaluation of vascular inflammation in patients with COVID-19 would facilitate more efficient trials of new treatments and identify patients at risk of long-term complications who might respond to treatment. We aimed to develop a novel artificial intelligence (AI)-assisted image analysis platform that quantifies cytokine-driven vascular inflammation from routine CT angiograms, and sought to validate its prognostic value in COVID-19.\nFor this prospective outcomes validation study, we developed a radiotranscriptomic platform that uses RNA sequencing data from human internal mammary artery biopsies to develop novel radiomic signatures of vascular inflammation from CT angiography images. We then used this platform to train a radiotranscriptomic signature (C19-RS), derived from the perivascular space around the aorta and the internal mammary artery, to best describe cytokine-driven vascular inflammation. The prognostic value of C19-RS was validated externally in 435 patients (331 from study arm 3 and 104 from study arm 4) admitted to hospital with or without COVID-19, undergoing clinically indicated pulmonary CT angiography, in three UK National Health Service (NHS) trusts (Oxford, Leicester, and Bath). We evaluated the diagnostic and prognostic value of C19-RS for death in hospital due to COVID-19, did sensitivity analyses based on dexamethasone treatment, and investigated the correlation of C19-RS with systemic transcriptomic changes.\nPatients with COVID-19 had higher C19-RS than those without (adjusted odds ratio [OR] 2\u00b797 [95% CI 1\u00b743-6\u00b727], p=0\u00b70038), and those infected with the B.1.1.7 (alpha) SARS-CoV-2 variant had higher C19-RS values than those infected with the wild-type SARS-CoV-2 variant (adjusted OR 1\u00b789 [95% CI 1\u00b717-3\u00b720] per SD, p=0\u00b7012). C19-RS had prognostic value for in-hospital mortality in COVID-19 in two testing cohorts (high [\u22656\u00b799] vs low [<6\u00b799] C19-RS; hazard ratio [HR] 3\u00b731 [95% CI 1\u00b749-7\u00b733], p=0\u00b70033; and 2\u00b758 [1\u00b710-6\u00b705], p=0\u00b7028), adjusted for clinical factors, biochemical biomarkers of inflammation and myocardial injury, and technical parameters. The adjusted HR for in-hospital mortality was 8\u00b724 (95% CI 2\u00b716-31\u00b736, p=0\u00b70019) in patients who received no dexamethasone treatment, but 2\u00b727 (0\u00b769-7\u00b755, p=0\u00b718) in those who received dexamethasone after the scan, suggesting that vascular inflammation might have been a therapeutic target of dexamethasone in COVID-19. Finally, C19-RS was strongly associated (r=0\u00b761, p=0\u00b700031) with a whole blood transcriptional module representing dysregulation of coagulation and platelet aggregation pathways.\nRadiotranscriptomic analysis of CT angiography scans introduces a potentially powerful new platform for the development of non-invasive imaging biomarkers. Application of this platform in routine CT pulmonary angiography scans done in patients with COVID-19 produced the radiotranscriptomic signature C19-RS, a marker of cytokine-driven inflammation driving systemic activation of coagulation and responsible for adverse clinical outcomes, which predicts in-hospital mortality and might allow targeted therapy.\nEngineering and Physical Sciences Research Council, British Heart Foundation, Oxford BHF Centre of Research Excellence, Innovate UK, NIHR Oxford Biomedical Research Centre, Wellcome Trust, Onassis Foundation.", "journal": "The Lancet. Digital health", "date": "2022-08-30", "authors": ["Christos PKotanidis", "ChengXie", "DonnaAlexander", "Jonathan C LRodrigues", "KatieBurnham", "AlexanderMentzer", "DanielO'Connor", "JulianKnight", "MuhammadSiddique", "HelenLockstone", "SheenaThomas", "RafailKotronias", "Evangelos KOikonomou", "IleanaBadi", "MariaLyasheva", "CheeragShirodaria", "Sheila FLumley", "BedeConstantinides", "NicholasSanderson", "GillianRodger", "Kevin KChau", "ArchieLodge", "MariaTsakok", "FergusGleeson", "DavidAdlam", "PraveenRao", "DasIndrajeet", "AparnaDeshpande", "AmritaBajaj", "Benjamin JHudson", "VivekSrivastava", "ShakilFarid", "GeorgeKrasopoulos", "RanaSayeed", "Ling-PeiHo", "StefanNeubauer", "David ENewby", "Keith MChannon", "JohnDeanfield", "CharalambosAntoniades", "NoneNone", "NoneNone"], "doi": "10.1016/S2589-7500(22)00132-7"}
{"title": "Chest X-ray analysis empowered with deep learning: A systematic review.", "abstract": "Chest radiographs are widely used in the medical domain and at present, chest X-radiation particularly plays an important role in the diagnosis of medical conditions such as pneumonia and COVID-19 disease. The recent developments of deep learning techniques led to a promising performance in medical image classification and prediction tasks. With the availability of chest X-ray datasets and emerging trends in data engineering techniques, there is a growth in recent related publications. Recently, there have been only a few survey papers that addressed chest X-ray classification using deep learning techniques. However, they lack the analysis of the trends of recent studies. This systematic review paper explores and provides a comprehensive analysis of the related studies that have used deep learning techniques to analyze chest X-ray images. We present the state-of-the-art deep learning based pneumonia and COVID-19 detection solutions, trends in recent studies, publicly available datasets, guidance to follow a deep learning process, challenges and potential future research directions in this domain. The discoveries and the conclusions of the reviewed work have been organized in a way that researchers and developers working in the same domain can use this work to support them in taking decisions on their research.", "journal": "Applied soft computing", "date": "2022-08-30", "authors": ["DulaniMeedeniya", "HasharaKumarasinghe", "ShammiKolonne", "ChamodiFernando", "Isabel De la TorreD\u00edez", "Gon\u00e7aloMarques"], "doi": "10.1016/j.asoc.2022.109319\n10.1038/s41392-020-00243-2\n10.1148/ryct.2020200028\n10.1016/j.compmedimag.2019.05.005\n10.1109/42.974918\n10.1109/ACCESS.2021.3065965\n10.3390/app10020559\n10.1007/s13246-020-00865-4\n10.1016/B978-0-12-819061-6.00013-6\n10.1007/s11633-020-1231-6\n10.3390/jimaging6120131\n10.1016/j.media.2021.102125\n10.30534/ijeter/2021/09972021\n10.1016/j.scs.2020.102589\n10.1109/MCI.2020.3019873\n10.1109/ic-ETITE47903.2020.152\n10.1016/j.compbiomed.2020.103898\n10.1097/01.NAJ.0000444496.24228.2c\n10.1016/j.chaos.2020.110337\n10.1007/978-981-15-7219-7_22\n10.1109/EMBC44109.2020.9175594\n10.3390/diagnostics10060417\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.243\n10.48550/arXiv.1602.07360\n10.1109/CVPR.2017.195\n10.1007/978-3-319-24574-4_28\n10.1016/j.compmedimag.2017.04.001\n10.1016/j.asoc.2020.106580\n10.1016/j.asoc.2020.106691\n10.1017/9781139061773\n10.1109/IES50839.2020.9231540\n10.1109/ICOSEC49089.2020.9215257\n10.1145/3431804\n10.1109/KSE.2018.8573404\n10.1038/s41598-020-76550-z\n10.1016/j.chaos.2020.109944\n10.1109/INDIACom51348.2021.00137\n10.1109/ICCCNT49239.2020.9225543\n10.1109/ISRITI51436.2020.9315478\n10.1007/s10044-021-00970-4\n10.1155/2021/5513679\n10.1002/ima.22566\n10.1016/j.compbiomed.2020.103805\n10.1016/j.knosys.2020.106062\n10.1117/12.2547635\n10.1109/TMI.2013.2290491\n10.1109/TMI.2013.2284099\n10.1007/978-3-030-32254-0_74\n10.1109/ICVEE50212.2020.9243290\n10.1109/EMBC44109.2020.9176517\n10.1109/EBBT.2019.8741582\n10.1007/s00264-020-04609-7\n10.1007/s10489-020-01829-7\n10.1016/j.imu.2020.100405\n10.1109/ICECOCS50124.2020.9314567\n10.1016/j.jjimei.2021.100020\n10.1101/2020.03.26.20044610\n10.1109/ACCESS.2021.3086229\n10.1117/12.2581314\n10.1109/RIVF48685.2020.9140733\n10.1109/ACCESS.2020.2974242\n10.31661/jbpe.v0i0.2008-1153\n10.1109/ICISS49785.2020.9316100\n10.1109/DASA53625.2021.9682248\n10.48550/arXiv.1711.05225\n10.1109/ICECCT.2019.8869364\n10.1016/j.chaos.2020.110122\n10.1016/j.patrec.2020.09.010\n10.1007/s42600-021-00151-6\n10.1016/j.eswa.2021.114883\n10.1016/j.mehy.2020.109761\n10.1109/ICECCE49384.2020.9179404\n10.1155/2020/8828855\n10.48550/arXiv.1409.1556\n10.1109/CVPR.2018.00474\n10.5614/itbj.ict.res.appl.2019.13.3.5\n10.1117/12.2581882\n10.17632/rscbjbr9sj.3\n10.1155/2019/4180949\n10.5220/0007404301120119\n10.1007/s12559-020-09795-5\n10.1016/j.cmpb.2020.105581\n10.1177/2472630320958376\n10.3390/s21041480\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1016/j.chaos.2020.110245\n10.1109/Confluence47617.2020.9057809\n10.1109/CCECE.2019.8861969\n10.1007/978-981-15-3369-3_36\n10.1142/S0218001421510046\n10.1109/EBBT.2019.8742050\n10.34740/kaggle/dsv/1019469\n10.17632/2fxz4px6d8.4\n10.1016/j.compbiomed.2020.103792\n10.1016/j.measurement.2019.05.076\n10.1109/EIT48999.2020.9208232\n10.12928/TELKOMNIKA.v18i3.14751\n10.5220/0007346600760083\n10.32604/cmc.2021.018514\n10.1016/j.sysarc.2019.101635\n10.1007/978-3-642-21219-2_1\n10.1007/978-3-030-32248-9_45\n10.1109/ICARC54489.2022.9753811\n10.3991/ijoe.v18i07.30807\n10.48550/arXiv.1701.03757\n10.1016/j.simpa.2022.100340\n10.1148/ryai.2020190043\n10.1007/s00521-021-06396-7"}
{"title": "SEL-COVIDNET: An intelligent application for the diagnosis of COVID-19 from chest X-rays and CT-scans.", "abstract": "COVID-19 detection from medical imaging is a difficult challenge that has piqued the interest of experts worldwide. Chest X-rays and computed tomography (CT) scanning are the essential imaging modalities for diagnosing COVID-19. All researchers focus their efforts on developing viable methods and rapid treatment procedures for this pandemic. Fast and accurate automated detection approaches have been devised to alleviate the need for medical professionals. Deep Learning (DL) technologies have successfully recognized COVID-19 situations. This paper proposes a developed set of nine deep learning models for diagnosing COVID-19 based on transfer learning and implementation in a novel architecture (SEL-COVIDNET). We include a global average pooling layer, flattening, and two dense layers that are fully connected. The model's effectiveness is evaluated using balanced and unbalanced COVID-19 radiography datasets. After that, our model's performance is analyzed using six evaluation measures: accuracy, sensitivity, specificity, precision, F1-score, and Matthew's correlation coefficient (MCC). Experiments demonstrated that the proposed SEL-COVIDNET with tuned DenseNet121, InceptionResNetV2, and MobileNetV3Large models outperformed the results of comparative SOTA for multi-class classification (COVID-19 vs. No-finding vs. Pneumonia) in terms of accuracy (98.52%), specificity (98.5%), sensitivity (98.5%), precision (98.7%), F1-score (98.7%), and MCC (97.5%). For the COVID-19 vs. No-finding classification, our method had an accuracy of 99.77%, a specificity of 99.85%, a sensitivity of 99.85%, a precision of 99.55%, an F1-score of 99.7%, and an MCC of 99.4%. The proposed model offers an accurate approach for detecting COVID-19 patients, which aids in the containment of the COVID-19 pandemic.", "journal": "Informatics in medicine unlocked", "date": "2022-08-30", "authors": ["Ahmad AlSmadi", "AhedAbugabah", "Ahmad MohammadAl-Smadi", "SultanAlmotairi"], "doi": "10.1016/j.imu.2022.101059\n10.1145/3447450.3447458\n10.1109/ACCESS.2020.3010287\n10.48550/ARXIV.1512.03385\n10.48550/ARXIV.1801.04381\n10.1109/ICCV.2019.00140\n10.1109/CVPRW50498.2020.00183"}
{"title": "HADCNet: Automatic segmentation of COVID-19 infection based on a hybrid attention dense connected network with dilated convolution.", "abstract": "the automatic segmentation of lung infections in CT slices provides a rapid and effective strategy for diagnosing, treating, and assessing COVID-19 cases. However, the segmentation of the infected areas presents several difficulties, including high intraclass variability and interclass similarity among infected areas, as well as blurred edges and low contrast. Therefore, we propose HADCNet, a deep learning framework that segments lung infections based on a dual hybrid attention strategy. HADCNet uses an encoder hybrid attention module to integrate feature information at different scales across the peer hierarchy to refine the feature map. Furthermore, a decoder hybrid attention module uses an improved skip connection to embed the semantic information of higher-level features into lower-level features by integrating multi-scale contextual structures and assigning the spatial information of lower-level features to higher-level features, thereby capturing the contextual dependencies of lesion features across levels and refining the semantic structure, which reduces the semantic gap between feature maps at different levels and improves the model segmentation performance. We conducted fivefold cross-validations of our model on four publicly available datasets, with final mean Dice scores of 0.792, 0.796, 0.785, and 0.723. These results show that the proposed model outperforms popular state-of-the-art semantic segmentation methods and indicate its potential use in the diagnosis and treatment of COVID-19.", "journal": "Computers in biology and medicine", "date": "2022-08-28", "authors": ["YingChen", "TaohuiZhou", "YiChen", "LongfengFeng", "ChengZheng", "LanLiu", "LipingHu", "BujianPan"], "doi": "10.1016/j.compbiomed.2022.105981"}
{"title": "The role of artificial intelligence technology analysis of high-resolution computed tomography images in predicting the severity of COVID-19 pneumonia.", "abstract": "High-resolution computed tomography (HRCT) is usually used only for qualitative analysis of COVID-19 pneumonia. However, when coupled with artificial intelligence (AI) it can also automatically provide quantitative data.\nThe purpose of the study was to analyze the role of automatic assessment of COVID\u201119 pneumonia severity on HRCT images by AI technology.\nWe retrospectively studied medical records of consecutive patients admitted to the Krakow University Hospital due to COVID\u201119. Of the 1729 patients, 804 underwent HRCT with automatic analysis of such radiological parameters as absolute inflammation volume, absolute ground glass volume, absolute consolidation volume (ACV), percentage inflammation volume, percentage ground glass volume, percentage consolidation volume (PCV), and severity of pneumonia classified as none, mild, moderate, or critical.\nThe automatically assessed radiological parameters correlated with the clinical parameters that reflected the severity of pneumonia (P <0.05). The patients with critical pneumonia, as compared with mild or moderate one, were more frequently men, had significantly lower oxygen saturation, higher respiratory rate, higher levels of inflammatory markers, as well as more common need for mechanical ventilation and admission to the intensive care unit. They were also more likely to die during hospitalization. Notably, as determined by the receiver operating characteristic curve analysis, radiological parameters above or equal to the cutoff points were independently associated with in\u2011hospital mortality (ACV odds ratio [OR], 4.08; 95% CI, 2.62-6.35; PCV OR, 4.05; 95% CI, 2.60-6.30).\nUsing AI to analyze HRCT images is a simple and valuable approach to predict the severity of COVID\u201119 pneumonia.", "journal": "Polish archives of internal medicine", "date": "2022-08-27", "authors": ["RobertChrzan", "WiktoriaWojciechowska", "Micha\u0142Terlecki", "MarekKlocek", "MarekRajzer", "TadeuszPopiela"], "doi": "10.20452/pamw.16332"}
{"title": "Novel Coronavirus and Common Pneumonia Detection from CT Scans Using Deep Learning-Based Extracted Features.", "abstract": "COVID-19 which was announced as a pandemic on 11 March 2020, is still infecting millions to date as the vaccines that have been developed do not prevent the disease but rather reduce the severity of the symptoms. Until a vaccine is developed that can prevent COVID-19 infection, the testing of individuals will be a continuous process. Medical personnel monitor and treat all health conditions; hence, the time-consuming process to monitor and test all individuals for COVID-19 becomes an impossible task, especially as COVID-19 shares similar symptoms with the common cold and pneumonia. Some off-the-counter tests have been developed and sold, but they are unreliable and add an additional burden because false-positive cases have to visit hospitals and perform specialized diagnostic tests to confirm the diagnosis. Therefore, the need for systems that can automatically detect and diagnose COVID-19 automatically without human intervention is still an urgent priority and will remain so because the same technology can be used for future pandemics and other health conditions. In this paper, we propose a modified machine learning (ML) process that integrates deep learning (DL) algorithms for feature extraction and well-known classifiers that can accurately detect and diagnose COVID-19 from chest CT scans. Publicly available datasets were made available by the China Consortium for Chest CT Image Investigation (CC-CCII). The highest average accuracy obtained was 99.9% using the modified ML process when 2000 features were extracted using GoogleNet and ResNet18 and using the support vector machine (SVM) classifier. The results obtained using the modified ML process were higher when compared to similar methods reported in the extant literature using the same datasets or different datasets of similar size; thus, this study is considered of added value to the current body of knowledge. Further research in this field is required to develop methods that can be applied in hospitals and can better equip mankind to be prepared for any future pandemics.", "journal": "Viruses", "date": "2022-08-27", "authors": ["GhazanfarLatif", "HamdyMorsy", "AsmaaHassan", "JaafarAlghazo"], "doi": "10.3390/v14081667\n10.1007/s10044-021-00984-y\n10.1016/j.idm.2020.02.002\n10.3201/eid1212.060401\n10.1136/bmj.m641\n10.1001/jama.2020.2565\n10.1016/j.dsx.2020.04.012\n10.1109/JBHI.2020.3037127\n10.1016/j.media.2020.101797\n10.1007/s10489-020-02029-z\n10.1016/j.susoc.2021.08.001\n10.1109/ACCESS.2020.3016780\n10.1016/j.eswa.2020.114054\n10.1109/ACCESS.2020.2994762\n10.1016/j.chaos.2020.110495\n10.1007/s10489-020-01902-1\n10.3390/s21020455\n10.3390/s21041480\n10.3390/diagnostics11111972\n10.1016/j.compbiomed.2022.105233\n10.1007/s11042-022-11913-4\n10.2174/1573405614666180402150218\n10.1016/j.procs.2019.12.110\n10.1155/2022/2665283\n10.3390/math10050796\n10.1002/mp.14609\n10.3390/diagnostics11071155\n10.1016/j.jcp.2020.110010\n10.3233/JIFS-189132\n10.1109/ictcs.2019.8923092\n10.3390/app10134523\n10.1007/s40846-021-00656-6\n10.2139/ssrn.3754116\n10.4316/AECE.2014.01010\n10.1145/3277104.3278311\n10.3390/diagnostics12041018\n10.1007/978-1-4471-7296-3_21\n10.1016/j.cell.2020.04.045\n10.1016/j.media.2020.101913\n10.1002/mp.15044\n10.1016/j.compbiomed.2021.104857"}
{"title": "COVID-19 and Virtual Nutrition: A Pilot Study of Integrating Digital Food Models for Interactive Portion Size Education.", "abstract": "Background and aims: Digital food viewing is a vital skill for connecting dieticians to e-health. The aim of this study was to integrate a novel pedagogical framework that combines interactive three- (3-D) and two-dimensional (2-D) food models into a formal dietetic training course. The level of agreement between the digital food models (first semester) and the effectiveness of educational integration of digital food models during the school closure due to coronavirus disease 2019 (COVID-19) (second semester) were evaluated. Method: In total, 65 second-year undergraduate dietetic students were enrolled in a nutritional practicum course at the School of Nutrition and Health Sciences, Taipei Medical University (Taipei, Taiwan). A 3-D food model was created using Agisoft Metashape. Students\u2019 digital food viewing skills and receptiveness towards integrating digital food models were evaluated. Results: In the first semester, no statistical differences were observed between 2-D and 3-D food viewing skills in food identification (2-D: 89% vs. 3-D: 85%) and quantification (within \u00b110% difference in total calories) (2-D: 19.4% vs. 3-D: 19.3%). A Spearman correlation analysis showed moderate to strong correlations of estimated total calories (0.69~0.93; all p values < 0.05) between the 3-D and 2-D models. Further analysis showed that students who struggled to master both 2-D and 3-D food viewing skills had lower estimation accuracies than those who did not (equal performers: 28% vs. unequal performers:16%, p = 0.041), and interactive 3-D models may help them perform better than 2-D models. In the second semester, the digital food viewing skills significantly improved (food identification: 91.5% and quantification: 42.9%) even for those students who struggled to perform digital food viewing skills equally in the first semester (equal performers: 44% vs. unequal performers: 40%). Conclusion: Although repeated training greatly enhanced students\u2019 digital food viewing skills, a tailored training program may be needed to master 2-D and 3-D digital food viewing skills. Future study is needed to evaluate the effectiveness of digital food models for future \u201ceHealth\u201d care.", "journal": "Nutrients", "date": "2022-08-27", "authors": ["Dang Khanh NganHo", "Yu-ChiehLee", "Wan-ChunChiu", "Yi-TaShen", "Chih-YuanYao", "Hung-KuoChu", "Wei-TaChu", "Nguyen Quoc KhanhLe", "Hung TrongNguyen", "Hsiu-YuehSu", "Jung-SuChang"], "doi": "10.3390/nu14163313\n10.1016/j.appet.2019.104522\n10.1080/09637486.2017.1309521\n10.1017/S1368980012003655\n10.1001/archpedi.156.9.867\n10.1111/jhn.12063\n10.1038/oby.2011.344\n10.3390/nu9010073\n10.1258/jtt.2011.100906\n10.1016/j.clnu.2020.08.002\n10.3390/nu13010175\n10.3390/nu9020114\n10.3390/nu10080984\n10.1079/BJN19960007\n10.1007/s10055-020-00484-0\n10.1515/libri-2017-0024\n10.1186/s12966-017-0516-9\n10.1111/acem.13972\n10.1109/ism.2010.50\n10.1017/S1368980018000344\n10.1111/j.1365-277X.2010.01042.x\n10.3390/nu10060741\n10.1038/s41366-020-00693-2\n10.1089/tmj.2009.0174"}
{"title": "Vascular Implications of COVID-19: Role of Radiological Imaging, Artificial Intelligence, and Tissue Characterization: A Special Report.", "abstract": "The SARS-CoV-2 virus has caused a pandemic, infecting nearly 80 million people worldwide, with mortality exceeding six million. The average survival span is just 14 days from the time the symptoms become aggressive. The present study delineates the deep-driven vascular damage in the pulmonary, renal, coronary, and carotid vessels due to SARS-CoV-2. This special report addresses an important gap in the literature in understanding (i) the pathophysiology of vascular damage and the role of medical imaging in the visualization of the damage caused by SARS-CoV-2, and (ii) further understanding the severity of COVID-19 using artificial intelligence (AI)-based tissue characterization (TC). PRISMA was used to select 296 studies for AI-based TC. Radiological imaging techniques such as magnetic resonance imaging (MRI), computed tomography (CT), and ultrasound were selected for imaging of the vasculature infected by COVID-19. Four kinds of hypotheses are presented for showing the vascular damage in radiological images due to COVID-19. Three kinds of AI models, namely, machine learning, deep learning, and transfer learning, are used for TC. Further, the study presents recommendations for improving AI-based architectures for vascular studies. We conclude that the process of vascular damage due to COVID-19 has similarities across vessel types, even though it results in multi-organ dysfunction. Although the mortality rate is ~2% of those infected, the long-term effect of COVID-19 needs monitoring to avoid deaths. AI seems to be penetrating the health care industry at warp speed, and we expect to see an emerging role in patient care, reduce the mortality and morbidity rate.", "journal": "Journal of cardiovascular development and disease", "date": "2022-08-26", "authors": ["Narendra NKhanna", "MaheshMaindarkar", "AnudeepPuvvula", "SudipPaul", "MrinaliniBhagawati", "PuneetAhluwalia", "ZoltanRuzsa", "AdityaSharma", "SmikshaMunjral", "RaghuKolluri", "Padukone RKrishnan", "Inder MSingh", "John RLaird", "MostafaFatemi", "AzraAlizad", "Surinder KDhanjil", "LucaSaba", "AntonellaBalestrieri", "GavinoFaa", "Kosmas IParaskevas", "Durga PrasannaMisra", "VikasAgarwal", "AmanSharma", "JagjitTeji", "MustafaAl-Maini", "AndrewNicolaides", "VijayRathore", "SubbaramNaidu", "KieraLiblik", "Amer MJohri", "MonikaTurk", "David WSobel", "GyanPareek", "MartinMiner", "KlaudijaViskovic", "GeorgeTsoulfas", "Athanasios DProtogerou", "SophieMavrogeni", "George DKitas", "Mostafa MFouda", "Manudeep KKalra", "Jasjit SSuri"], "doi": "10.3390/jcdd9080268\n10.1007/s10072-021-05756-4\n10.3233/JPD-202038\n10.3389/fpsyt.2020.590134\n10.1001/jama.2020.1585\n10.1016/j.compbiomed.2020.103960\n10.1186/s13054-020-03062-7\n10.1161/CIRCULATIONAHA.112.093245\n10.1161/01.ATV.0000051384.43104.FC\n10.1016/S0140-6736(20)30937-5\n10.1038/s41577-020-0343-0\n10.1016/j.atherosclerosis.2020.10.014\n10.1016/j.cell.2020.04.004\n10.21037/cdt-20-561\n10.1016/j.ejrad.2019.02.038\n10.1007/s10916-017-0797-1\n10.1016/j.compbiomed.2020.103804\n10.1007/s10554-020-02099-7\n10.1007/s00296-020-04691-5\n10.4239/wjd.v12.i3.215\n10.1007/s11517-018-1897-x\n10.1016/j.ultras.2011.11.003\n10.7863/ultra.33.2.245\n10.1007/s10278-012-9553-8\n10.3109/02652048.2013.879932\n10.1016/j.cmpb.2017.07.011\n10.1016/j.eswa.2015.03.014\n10.1109/TIM.2011.2174897\n10.1007/s10916-017-0745-0\n10.1007/s11517-006-0119-0\n10.1016/j.cmpb.2016.02.004\n10.21037/cdt.2016.03.08\n10.3390/diagnostics11122367\n10.3390/jcm9072146\n10.1007/s11517-012-1019-0\n10.1016/j.cmpb.2012.09.008\n10.1007/s11883-018-0736-8\n10.3390/diagnostics11112109\n10.1016/j.compbiomed.2021.105131\n10.1007/s10916-021-01707-w\n10.1109/JBHI.2021.3103839\n10.1016/j.mehy.2020.109603\n10.1007/s13721-017-0155-8\n10.3389/fnagi.2021.633752\n10.1016/j.virol.2015.03.043\n10.1038/s41579-018-0118-9\n10.1038/nrmicro2090\n10.1001/jama.2020.6019\n10.1038/d41573-020-00016-0\n10.1056/NEJMoa2001282\n10.1086/375233\n10.1001/jama.2020.2648\n10.1038/nri2171\n10.1161/CIRCULATIONAHA.104.510461\n10.1152/ajplung.00498.2016\n10.1016/j.cell.2020.02.058\n10.1038/s41598-022-07918-6\n10.1177/03009858221079665\n10.1007/s12250-020-00207-4\n10.1128/JVI.01248-09\n10.1002/rth2.12175\n10.2147/COPD.S329783\n10.1128/mBio.00638-15\n10.1016/S0140-6736(20)30628-0\n10.1002/jmv.23354\n10.1007/s12035-021-02457-z\n10.1002/path.1440\n10.1002/jmv.25709\n10.1056/NEJMoa2015432\n10.1080/20009666.2021.1921908\n10.1136/bmj-2021-069590\n10.1186/s12959-020-00255-6\n10.7326/L20-1275\n10.1148/ryct.2020200277\n10.1148/rg.282075705\n10.1155/2022/4640788\n10.1097/CM9.0000000000000774\n10.1007/s11684-020-0754-0\n10.1159/000342483\n10.1038/s41467-021-22781-1\n10.1016/j.kint.2020.04.003\n10.1681/ASN.2020040419\n10.2139/ssrn.3559601\n10.1016/j.tcm.2020.10.005\n10.1016/j.avsg.2020.07.013\n10.1016/j.acra.2020.07.019\n10.1186/s13054-020-02931-5\n10.1007/s00330-020-07300-y\n10.1016/j.ajem.2020.07.054\n10.1093/eurheartj/ehab314\n10.1016/j.ijcard.2020.04.028\n10.1128/MMBR.05015-11\n10.1016/j.tox.2022.153104\n10.1080/15384101.2019.1662678\n10.1016/j.ebiom.2020.102763\n10.1002/jmv.25900\n10.1016/j.ebiom.2020.102789\n10.1038/s41584-020-0474-5\n10.37899/journallamedihealtico.v3i3.647\n10.1016/j.juro.2015.03.119\n10.1016/j.idcr.2020.e00968\n10.1093/ckj/sfaa141\n10.1007/s00134-020-06153-9\n10.1016/j.biopha.2021.111966\n10.4103/iju.IJU_76_21\n10.1016/j.xkme.2020.07.010\n10.1097/SHK.0000000000001659\n10.1016/j.clinimag.2020.11.011\n10.1148/radiol.2020201623\n10.4269/ajtmh.20-0869\n10.1016/S0140-6736(20)30566-3\n10.1016/j.jacc.2020.04.031\n10.1016/j.immuni.2020.06.017\n10.1016/S0304-4157(98)00018-5\n10.1038/nri3345\n10.2174/138920111798281171\n10.12688/f1000research.9692.1\n10.1111/j.1365-2362.2009.02153.x\n10.1046/j.1523-1755.62.s82.4.x\n10.3390/molecules27072048\n10.1161/01.RES.84.9.1043\n10.1111/ijlh.13829\n10.1161/01.ATV.20.8.2019\n10.1016/j.jacc.2020.06.080\n10.1002/ccd.29056\n10.1111/jocs.14538\n10.1016/j.clinimag.2021.02.016\n10.1007/s00259-021-05375-3\n10.1186/s13244-021-00973-z\n10.3174/ajnr.A6674\n10.1161/CIRCULATIONAHA.120.047525\n10.1016/j.wneu.2020.08.154\n10.1016/j.neurad.2020.04.003\n10.3389/fcvm.2021.671669\n10.1016/j.clinimag.2020.07.007\n10.1111/jon.12803\n10.1155/2020/7397480\n10.1016/j.jvs.2021.11.064\n10.1016/j.cmpb.2017.12.016\n10.1007/s10916-015-0214-6\n10.7785/tcrt.2012.500381\n10.1038/nature14539\n10.1016/j.nicl.2022.103065\n10.1016/j.cmpb.2021.106332\n10.1016/j.ejrad.2020.109041\n10.1007/s00330-020-07044-9\n10.3348/kjr.2020.0536\n10.1109/TMI.2020.2994459\n10.1002/jmri.26887\n10.1148/ryai.2020200048\n10.1148/radiol.2020200905\n10.1007/s10096-020-03901-z\n10.21037/atm.2020.03.132\n10.1109/ACCESS.2020.3005510\n10.1016/j.cell.2020.04.045\n10.1109/TUFFC.2020.3005512\n10.1109/ACCESS.2020.3003810\n10.1016/j.compbiomed.2020.103869\n10.1016/j.cmpb.2020.105608\n10.1080/07391102.2020.1788642\n10.1111/nan.12667\n10.3390/cancers14040987\n10.1016/j.cmpb.2017.09.004\n10.1002/mp.14193\n10.1016/j.ekir.2017.11.002\n10.1038/s41746-019-0104-2\n10.1007/s11517-005-0016-y\n10.7863/jum.2009.28.11.1561\n10.1080/03772063.2019.1604176\n10.1093/ajcn/65.4.1000\n10.1007/s11883-016-0635-9\n10.5853/jos.2017.02922\n10.1016/j.cmpb.2013.07.012\n10.1016/j.bspc.2013.08.008\n10.1109/TMI.2016.2528162\n10.1515/itms-2017-0007\n10.1016/j.compbiomed.2020.103958\n10.1016/j.cmpb.2012.05.008\n10.1016/j.compbiomed.2018.05.014\n10.1177/1544316718806421\n10.1016/j.compbiomed.2020.103847\n10.3390/diagnostics11122257\n10.23736/S0392-9590.21.04771-4\n10.1016/j.compbiomed.2016.11.011\n10.1109/JBHI.2016.2631401\n10.3934/mbe.2022229\n10.1504/IJCSE.2022.120788\n10.1016/j.jacr.2017.12.028\n10.1016/j.ejrad.2017.01.031\n10.21037/atm-20-7676\n10.1007/s10554-020-02124-9\n10.1109/TIM.2021.3052577\n10.3390/jcm11133721\n10.1049/el.2020.2102\n10.3390/electronics11111800\n10.1016/j.compbiomed.2022.105273\n10.1109/RBME.2020.2990959\n10.1016/j.preteyeres.2021.100965\n10.1038/s41467-020-17971-2\n10.1056/NEJMc2011400\n10.1016/j.jacc.2020.05.001\n10.1186/s12882-020-02150-8\n10.7759/cureus.9540\n10.1007/s13204-021-01868-7\n10.1007/s11548-021-02317-0\n10.3389/fphys.2022.832457\n10.1002/cyto.a.24274\n10.1038/s41598-019-54244-5\n10.1148/radiol.2021211483\n10.1016/j.jcmg.2021.10.013\n10.1681/ASN.2020050589\n10.1016/j.ijantimicag.2020.105949\n10.1016/j.mayocp.2020.03.024\n10.1148/radiol.2020203511\n10.1186/s13054-020-03179-9\n10.1055/a-1775-8633\n10.1155/2021/6761364\n10.1681/ASN.2020050597\n10.1093/ejcts/ezac289\n10.36660/abc.20200302\n10.1016/j.compbiomed.2021.104721\n10.1016/j.ejvs.2009.03.013\n10.3348/kjr.2021.0148\n10.1007/s40520-021-01985-x\n10.1007/s00296-021-05062-4\n10.1056/NEJM199412013312202\n10.1016/j.ultrasmedbio.2014.12.024\n10.1227/01.NEU.0000239895.00373.E4\n10.1681/ASN.V92231\n10.1165/rcmb.2007-0441OC\n10.1097/MCA.0000000000000934\n10.3390/biology11020221\n10.2214/AJR.11.6955\n10.1097/MCA.0000000000000914\n10.1097/CCM.0000000000004899\n10.1002/14651858.CD003186.pub3\n10.3390/diagnostics12010166\n10.1016/j.bspc.2016.03.001\n10.1016/j.compbiomed.2021.105204\n10.1109/TIM.2022.3174270\n10.1016/S0140-6736(96)07492-2\n10.1016/j.compbiomed.2022.105571\n10.3390/app10238623\n10.1016/j.clim.2020.108509\n10.1016/j.jpeds.2020.07.039\n10.1007/BF01907940\n10.1200/GO.20.00064\n10.1016/j.irbm.2020.05.003\n10.2214/AJR.20.23034\n10.1007/s11604-021-01120-w\n10.1148/radiol.2020200343\n10.1016/j.asoc.2020.106912\n10.1109/TIP.2021.3058783\n10.1093/eurheartj/ehaa399"}
{"title": "Artificial intelligence model on chest imaging to diagnose COVID-19 and other pneumonias: A systematic review and meta-analysis.", "abstract": "When diagnosing Coronavirus disease 2019(COVID-19), radiologists cannot make an accurate judgments because the image characteristics of COVID-19 and other pneumonia are similar. As machine learning advances, artificial intelligence(AI) models show promise in diagnosing COVID-19 and other pneumonias. We performed a systematic review and meta-analysis to assess the diagnostic accuracy and methodological quality of the models.\nWe searched PubMed, Cochrane Library, Web of Science, and Embase, preprints from medRxiv and bioRxiv to locate studies published before December 2021, with no language restrictions. And a quality assessment (QUADAS-2), Radiomics Quality Score (RQS) tools and CLAIM checklist were used to assess the quality of each study. We used random-effects models to calculate pooled sensitivity and specificity, I\nWe screened 32 studies from the 2001 retrieved articles for inclusion in the meta-analysis. We included 6737 participants in the test or validation group. The meta-analysis revealed that AI models based on chest imaging distinguishes COVID-19 from other pneumonias: pooled area under the curve (AUC) 0.96 (95 % CI, 0.94-0.98), sensitivity 0.92 (95 % CI, 0.88-0.94), pooled specificity 0.91 (95 % CI, 0.87-0.93). The average RQS score of 13 studies using radiomics was 7.8, accounting for 22 % of the total score. The 19 studies using deep learning methods had an average CLAIM score of 20, slightly less than half (48.24 %) the ideal score of 42.00.\nThe AI model for chest imaging could well diagnose COVID-19 and other pneumonias. However, it has not been implemented as a clinical decision-making tool. Future researchers should pay more attention to the quality of research methodology and further improve the generalizability of the developed predictive models.", "journal": "European journal of radiology open", "date": "2022-08-24", "authors": ["Lu-LuJia", "Jian-XinZhao", "Ni-NiPan", "Liu-YanShi", "Lian-PingZhao", "Jin-HuiTian", "GangHuang"], "doi": "10.1016/j.ejro.2022.100438"}
{"title": "A dual-stage deep convolutional neural network for automatic diagnosis of COVID-19 and pneumonia from chest CT images.", "abstract": "In the Coronavirus disease-2019 (COVID-19) pandemic, for fast and accurate diagnosis of a large number of patients, besides traditional methods, automated diagnostic tools are now extremely required. In this paper, a deep convolutional neural network (CNN) based scheme is proposed for automated accurate diagnosis of COVID-19 from lung computed tomography (CT) scan images. First, for the automated segmentation of lung regions in a chest CT scan, a modified CNN architecture, namely SKICU-Net is proposed by incorporating additional skip interconnections in the U-Net model that overcome the loss of information in dimension scaling. Next, an agglomerative hierarchical clustering is deployed to eliminate the CT slices without significant information. Finally, for effective feature extraction and diagnosis of COVID-19 and pneumonia from the segmented lung slices, a modified DenseNet architecture, namely P-DenseCOVNet is designed where parallel convolutional paths are introduced on top of the conventional DenseNet model for getting better performance through overcoming the loss of positional arguments. Outstanding performances have been achieved with an F", "journal": "Computers in biology and medicine", "date": "2022-08-23", "authors": ["FarhanSadik", "Ankan GhoshDastider", "Mohseu RashidSubah", "TanvirMahmud", "Shaikh AnowarulFattah"], "doi": "10.1016/j.compbiomed.2022.105806\n10.1101/2020.02.14.20023028"}
{"title": "New International Guidelines and Consensus on the Use of Lung Ultrasound.", "abstract": "Following the innovations and new discoveries of the last 10\u2009years in the field of lung ultrasound (LUS), a multidisciplinary panel of international LUS experts from six countries and from different fields (clinical and technical) reviewed and updated the original international consensus for point-of-care LUS, dated 2012. As a result, a total of 20 statements have been produced. Each statement is complemented by guidelines and future developments proposals. The statements are furthermore classified based on their nature as technical (5), clinical (11), educational (3), and safety (1) statements.", "journal": "Journal of ultrasound in medicine : official journal of the American Institute of Ultrasound in Medicine", "date": "2022-08-23", "authors": ["LibertarioDemi", "FrankWolfram", "CatherineKlersy", "AnnalisaDe Silvestri", "Virginia ValeriaFerretti", "MarieMuller", "DouglasMiller", "FrancescoFeletti", "MarcinWe\u0142nicki", "NataliaBuda", "AgnieszkaSkoczylas", "AndrzejPomiecko", "DomagojDamjanovic", "RobertOlszewski", "Andrew WKirkpatrick", "RaoulBreitkreutz", "GebhartMathis", "GinoSoldati", "AndreaSmargiassi", "RiccardoInchingolo", "TizianoPerrone"], "doi": "10.1002/jum.16088\n10.1002/jum.16052\n10.1002/jum.15284\n10.1136/postgradmedj-2020-138137\n10.1159/000430483\n10.1159/000368086\n10.1121/1.4988990\n10.1121/10.0001797\n10.1002/ppul.24920\n10.1016/j.media.2021.101975\n10.1109/TUFFC.2020.3010299\n10.1136/thx.2010.137026\n10.1007/s10396-019-00953-3\n10.1002/jum.15852\n10.1024/1661-8157/a001690\n10.1007/s10741-021-10085-x\n10.5281/zenodo.4147317"}
{"title": "Study on the correlations of different clinical types with imaging findings at initial diagnosis and clinical laboratory indexes in COVID-19 patients.", "abstract": "To investigate the correlations of initial lab and imaging findings in COVID-19 patients of different clinical types.\nWe retrospective analyzed patients confirmed with COVID-19 in the Fifth Medical Center of the People's Liberation Army (PLA) General Hospital between February to April 2020, selected a total of 58 (N) patients with lab and imaging examinations that met the study criteria, using Artificial intelligence (AI) software to calculate the percentage of COVID-19 lesions in the volume of the whole lung, then the correlations of general information, initial chest CT examination after admission and laboratory examinations were analyzed.\nThe 58 (N) COVID-19 patients were divided into mild group [41(n) cases]: and severe group [17(n) cases]: according to patient's condition. CT findings of the severe group and mild group mainly included single or multiple ground glass opacity (GGO), with lesions mainly distributed in the periphery of lungs or GGO mixed with consolidation, with lesions involved in peripheral and central areas of both lungs, accompanied other signs. A significant difference in CRP, IL-6, D-D, GGT was observed between the two groups (p < 0.05). The ratios regarding lymphocyte abnormality and neutrophil abnormality in the severe group were higher than those in the mild group (p < 0.05).\nThe CT features at initial diagnosis of COVID-19 were mainly characterized by multiple GGO with or without partial consolidation in both lungs, with the lesions mainly distributed at the subpleural regions. Some lab test indexes were correlated with the clinical types of COVID-19.", "journal": "Pakistan journal of medical sciences", "date": "2022-08-23", "authors": ["HongweiRen", "XiaoboZhang", "YuTang", "TaoYan", "YuanLiu"], "doi": "10.12669/pjms.38.6.5091"}
{"title": "Transforming healthcare through a digital revolution: A review of digital healthcare technologies and solutions.", "abstract": "The COVID-19 pandemic has put a strain on the entire global healthcare infrastructure. The pandemic has necessitated the re-invention, re-organization, and transformation of the healthcare system. The resurgence of new COVID-19 virus variants in several countries and the infection of a larger group of communities necessitate a rapid strategic shift. Governments, non-profit, and other healthcare organizations have all proposed various digital solutions. It's not clear whether these digital solutions are adaptable, functional, effective, or reliable. With the disease becoming more and more prevalent, many countries are looking for assistance and implementation of digital technologies to combat COVID-19. Digital health technologies for COVID-19 pandemic management, surveillance, contact tracing, diagnosis, treatment, and prevention will be discussed in this paper to ensure that healthcare is delivered effectively. Artificial Intelligence (AI), big data, telemedicine, robotic solutions, Internet of Things (IoT), digital platforms for communication (DC), computer vision, computer audition (CA), digital data management solutions (blockchain), digital imaging are premiering to assist healthcare workers (HCW's) with solutions that include case base surveillance, information dissemination, disinfection, and remote consultations, along with many other such interventions.", "journal": "Frontiers in digital health", "date": "2022-08-23", "authors": ["NitheshNaik", "B M ZeeshanHameed", "NilakshmanSooriyaperakasam", "ShankeethVinayahalingam", "VathsalaPatil", "KomalSmriti", "JanhaviSaxena", "MilapShah", "SufyanIbrahim", "AnshumanSingh", "HadisKarimi", "KarthickeyanNaganathan", "Dasharathraj KShetty", "Bhavan PrasadRai", "PiotrChlosta", "Bhaskar KSomani"], "doi": "10.3389/fdgth.2022.919985\n10.1016/S2213-2600(20)30079-5\n10.1016/j.jcrc.2020.04.012\n10.1016/j.tacc.2020.05.002\n10.1371/journal.pone.0116949\n10.1016/S2214-109X(20)30366-1\n10.1016/j.worlddev.2020.105318\n10.1002/wps.20764\n10.3389/fpsyt.2020.00790\n10.7189/jogh.10.05002\n10.1016/j.ijsu.2020.04.018\n10.2471/BLT.20.021120\n10.1038/s41746-021-00412-9\n10.1016/j.ihj.2020.04.001\n10.7554/eLife.57309\n10.4178/epih.e2020027\n10.1016/S0140-6736(20)32651-9\n10.7326/M20-3012\n10.1101/2020.02.29.20029421v1.full.pdf\n10.2139/ssrn.3568314\n10.3389/frai.2020.00065\n10.1016/j.aiopen.2020.07.001\n10.1016/S2589-7500(20)30192-8\n10.7326/M20-1495\n10.1186/s12967-020-02324-w\n10.3389/fbloc.2021.830459\n10.1155/2022/7786441\n10.1109/ACCESS.2020.3032450\n10.1016/S2589-7500(21)00210-7\n10.1002/ett.4255\n10.1002/hbe2.226\n10.1007/s13312-020-1840-8"}
{"title": "Multicenter Study on COVID-19 Lung Computed Tomography Segmentation with varying Glass Ground Opacities using Unseen Deep Learning Artificial Intelligence Paradigms: COVLIAS 1.0 Validation.", "abstract": "Variations in COVID-19 lesions such as glass ground opacities (GGO), consolidations, and crazy paving can compromise the ability of solo-deep learning (SDL) or hybrid-deep learning (HDL) artificial intelligence (AI) models in predicting automated COVID-19 lung segmentation in Computed Tomography (CT) from unseen data leading to poor clinical manifestations. As the first study of its kind, \"COVLIAS 1.0-Unseen\" proves two hypotheses, (i) contrast adjustment is vital for AI, and (ii) HDL is superior to SDL. In a multicenter study, 10,000 CT slices were collected from 72 Italian (ITA) patients with low-GGO, and 80 Croatian (CRO) patients with high-GGO. Hounsfield Units (HU) were automatically adjusted to train the AI models and predict from test data, leading to four combinations-two Unseen sets: (i) train-CRO:test-ITA, (ii) train-ITA:test-CRO, and two Seen sets: (iii) train-CRO:test-CRO, (iv) train-ITA:test-ITA. COVILAS used three SDL models: PSPNet, SegNet, UNet and six HDL models: VGG-PSPNet, VGG-SegNet, VGG-UNet, ResNet-PSPNet, ResNet-SegNet, and ResNet-UNet. Two trained, blinded senior radiologists conducted ground truth annotations. Five types of performance metrics were used to validate COVLIAS 1.0-Unseen which was further benchmarked against MedSeg, an open-source web-based system. After HU adjustment for DS and JI, HDL (Unseen AI)\u2009>\u2009SDL (Unseen AI) by 4% and 5%, respectively. For CC, HDL (Unseen AI)\u2009>\u2009SDL (Unseen AI) by 6%. The COVLIAS-MedSeg difference was\u2009<\u20095%, meeting regulatory guidelines.Unseen AI was successfully demonstrated using automated HU adjustment. HDL was found to be superior to SDL.", "journal": "Journal of medical systems", "date": "2022-08-22", "authors": ["Jasjit SSuri", "SushantAgarwal", "LucaSaba", "Gian LucaChabert", "AlessandroCarriero", "AlessioPasch\u00e8", "PietroDanna", "ArminMehmedovi\u0107", "GavinoFaa", "TanayJujaray", "Inder MSingh", "Narendra NKhanna", "John RLaird", "Petros PSfikakis", "VikasAgarwal", "Jagjit STeji", "RajanikantR Yadav", "FerencNagy", "Zsigmond Tam\u00e1sKincses", "ZoltanRuzsa", "KlaudijaViskovic", "Mannudeep KKalra"], "doi": "10.1007/s10916-022-01850-y\n10.23750/abm.v91i1.9397\n10.26355/eurrev_202012_24058\n10.1016/j.compbiomed.2020.103960\n10.1007/s10554-020-02089-9\n10.4239/wjd.v12.i3.215\n10.26355/eurrev_202108_26464\n10.1016/j.clinimag.2021.05.016\n10.23750/abm.v92i5.10418\n10.52586/5026\n10.1148/radiol.2020200432\n10.1016/j.ejrad.2020.109041\n10.1007/s00330-020-06920-8\n10.21037/atm-2020-cass-13\n10.1016/j.neurad.2017.09.007\n10.21037/atm-20-7676\n10.5152/dir.2020.20304\n10.1007/s00330-020-06915-5\n10.1007/s13244-010-0060-5\n10.1080/07853890.2020.1851044\n10.2214/AJR.20.23034\n10.1148/radiol.2020200343\n10.1007/s11548-020-02286-w\n10.1007/s11548-021-02317-0\n10.1007/s10916-021-01707-w\n10.3390/diagnostics12010166\n10.1109/JBHI.2021.3103839\n10.3390/diagnostics11122257\n10.1016/j.wneu.2016.05.069\n10.1016/j.ejmp.2017.11.036\n10.1016/j.compbiomed.2021.104721\n10.1016/j.compbiomed.2021.104803\n10.23736/S0392-9590.21.04771-4\n10.1016/j.compbiomed.2021.105131\n10.1109/TMI.2020.3002417\n10.11613/BM.2015.015\n10.1080/10408340500526766\n10.1177/875647939000600106\n10.2741/4725\n10.1016/j.ejrad.2019.02.038\n10.1007/s10916-018-0940-7\n10.1016/j.cmpb.2017.09.004\n10.1016/j.cmpb.2019.04.008\n10.1007/s10916-010-9645-2\n10.1016/j.cmpb.2012.09.008\n10.1007/s11517-012-1019-0\n10.1177/0954411913483637\n10.1016/j.cmpb.2017.12.016\n10.1016/j.compbiomed.2017.10.019\n10.7785/tcrt.2012.500346\n10.1016/j.compbiomed.2015.07.021\n10.1016/j.cmpb.2017.07.011\n10.1007/s11517-021-02322-0\n10.1007/s00330-020-06829-2\n10.1109/TNNLS.2021.3054746\n10.1109/TMI.2019.2959609\n10.1186/s12880-020-00529-5\n10.1016/j.acra.2020.09.004\n10.1109/TPAMI.2016.2644615\n10.1006/jmps.1999.1279\n10.1007/978-0-387-39940-9_565\n10.1109/TIM.2011.2174897\n10.1007/s10916-015-0214-6\n10.1016/j.cmpb.2016.02.004\n10.1007/s10916-017-0797-1\n10.1016/j.eswa.2015.03.014\n10.1055/s-0032-1330336\n10.1016/j.bspc.2016.03.001\n10.1109/4233.992158\n10.31083/j.rcm.2020.04.236"}
{"title": "Psoas muscle metastatic disease mimicking a psoas abscess on imaging.", "abstract": "Here, we report a case of malignant psoas syndrome presented to us during the second peak of the COVID-19 pandemic. Our patient had a medical history of hypertension, recently diagnosed with left iliac deep vein thrombosis and previous breast and endometrial cancers. She presented with exquisite pain and a fixed flexion deformity of the left hip. A rim-enhancing lesion was seen within the left psoas muscle and was initially deemed to be a psoas abscess. This failed to respond to medical management and attempts at drainage. Subsequent further imaging revealed the mass was of a malignant nature; histology revealing a probable carcinomatous origin. Following diagnosis, palliative input was obtained and, unfortunately, our patient passed away in a hospice shortly after discharge. We discuss the aetiology, radiological findings and potential treatments of this condition and learning points to prompt clinicians to consider this diagnosis in those with a personal history of cancer.", "journal": "BMJ case reports", "date": "2022-08-20", "authors": ["ChristopherGunn", "MazyarFani"], "doi": "10.1136/bcr-2022-250654\n10.1186/1470-7330-14-21\n10.1007/s00330-009-1577-1\n10.1007/s12094-011-0625-x\n10.3892/mco.2018.1635\n10.1016/j.jpainsymman.2003.12.018\n10.1089/jpm.2014.0387\n10.4103/IJPC.IJPC_205_19\n10.1080/15360288.2017.1301617\n10.1016/S0304-3959(99)00039-1\n10.1111/papr.12643\n10.1159/000360581\n10.2214/ajr.174.2.1740401\n10.1111/j.1440-1673.1990.tb02831.x\n10.11604/pamj.2020.36.231.21137\n10.1016/j.gore.2021.100814\n10.1016/j.spinee.2015.08.001\n10.1136/bcr-2017-223916\n10.1102/1470-7330.2013.0011\n10.1159/000227588\n10.1016/j.ygyno.2006.02.011\n10.3233/BD-140384\n10.1007/s002560050141"}
{"title": "Rapid tissue prototyping with micro-organospheres.", "abstract": "In\u00a0vitro tissue models hold great promise for modeling diseases and drug responses. Here, we used emulsion microfluidics to form micro-organospheres (MOSs), which are droplet-encapsulated miniature three-dimensional (3D) tissue models that can be established rapidly from patient tissues or cells. MOSs retain key biological features and responses to chemo-, targeted, and radiation therapies compared with organoids. The small size and large surface-to-volume ratio of MOSs enable various applications including quantitative assessment of nutrient dependence, pathogen-host interaction for anti-viral drug screening, and a rapid potency assay for chimeric antigen receptor (CAR)-T therapy. An automated MOS imaging pipeline combined with machine learning overcomes plating variation, distinguishes tumorspheres from stroma, differentiates cytostatic versus cytotoxic drug effects, and captures resistant clones and heterogeneity in drug response. This pipeline is capable of robust assessments of drug response at individual-tumorsphere resolution and provides a rapid and high-throughput therapeutic profiling platform for precision medicine.", "journal": "Stem cell reports", "date": "2022-08-20", "authors": ["ZhaohuiWang", "MatteoBoretto", "RosemaryMillen", "NaveenNatesh", "Elena SReckzeh", "CarolynHsu", "MarcosNegrete", "HaipeiYao", "WilliamQuayle", "Brook EHeaton", "Alfred THarding", "ShreeBose", "ElseDriehuis", "JoepBeumer", "Grecia ORivera", "Ravian Lvan Ineveld", "DonaldGex", "JessicaDeVilla", "DaisongWang", "JensPuschhof", "Maarten HGeurts", "AthenaYeung", "CaitHamele", "AmberSmith", "EricBankaitis", "KunXiang", "ShengliDing", "DanielNelson", "DanielDelubac", "AnneRios", "RalphAbi-Hachem", "DavidJang", "Bradley JGoldstein", "CarolynGlass", "Nicholas SHeaton", "DavidHsu", "HansClevers", "XilingShen"], "doi": "10.1016/j.stemcr.2022.07.016\n10.1016/j.copbio.2015.05.003\n10.1038/s41556-020-0472-5\n10.1038/s41556-018-0143-y\n10.1038/s41556-019-0360-z\n10.1016/j.medj.2021.08.005\n10.1038/s41551-020-0565-2\n10.1038/s41596-019-0232-9\n10.1038/nm.3388\n10.1016/j.cell.2018.07.009\n10.1016/j.stem.2022.04.006\n10.1158/2159-8290.CD-18-1522\n10.3390/jcm8111880\n10.1038/nature14415\n10.1002/advs.202102418\n10.1038/s41591-019-0584-2\n10.1038/ng.3127\n10.1038/s41586-020-2575-3\n10.1016/j.jtbi.2011.03.026\n10.1016/j.xcrm.2020.100161\n10.1063/1.4995479\n10.1158/1078-0432.CCR-20-5026\n10.48550/arXiv.1912.08193\n10.1016/j.ccell.2021.12.004\n10.1002/advs.201903739\n10.1038/nprot.2013.046\n10.1158/2326-6066.CIR-18-0428\n10.1158/1078-0432.CCR-20-0073\n10.1016/j.cell.2018.11.021\n10.1126/scitranslmed.aay2574\n10.1016/j.cell.2017.11.010\n10.15252/embj.2018100300\n10.1053/j.gastro.2011.07.050\n10.1016/j.stemcr.2021.04.009\n10.15252/embj.2018100928\n10.1016/j.isci.2020.101372\n10.1158/2159-8290.CD-18-0349\n10.1016/j.celrep.2020.107670\n10.1016/j.cell.2015.03.053\n10.1126/science.aao2774\n10.1039/d0bm01085e\n10.1016/j.stem.2019.10.010"}
{"title": "DAFLNet: Dual Asymmetric Feature Learning Network for COVID-19 Disease Diagnosis in X-Rays.", "abstract": "COVID-19 has become the largest public health event worldwide since its outbreak, and early detection is a prerequisite for effective treatment. Chest X-ray images have become an important basis for screening and monitoring the disease, and deep learning has shown great potential for this task. Many studies have proposed deep learning methods for automated diagnosis of COVID-19. Although these methods have achieved excellent performance in terms of detection, most have been evaluated using limited datasets and typically use a single deep learning network to extract features. To this end, the dual asymmetric feature learning network (DAFLNet) is proposed, which is divided into two modules, DAFFM and WDFM. DAFFM mainly comprises the backbone networks EfficientNetV2 and DenseNet for feature fusion. WDFM is mainly for weighted decision-level fusion and features a new pretrained network selection algorithm (PNSA) for determination of the optimal weights. Experiments on a large dataset were conducted using two schemes, DAFLNet-1 and DAFLNet-2, and both schemes outperformed eight state-of-the-art classification techniques in terms of classification performance. DAFLNet-1 achieved an average accuracy of up to 98.56% for the triple classification of COVID-19, pneumonia, and healthy images.", "journal": "Computational and mathematical methods in medicine", "date": "2022-08-20", "authors": ["JingyaoLiu", "JiashiZhao", "LiyuanZhang", "YuMiao", "WeiHe", "WeiliShi", "YanfangLi", "BaiJi", "KeZhang", "ZhengangJiang"], "doi": "10.1155/2022/3836498\n10.1016/j.cmpb.2020.105532\n10.1148/radiol.2020200642\n10.1016/j.inffus.2020.10.004\n10.22266/ijies2016.1231.24\n10.3390/diagnostics10060358\n10.1016/j.chaos.2020.110190\n10.1016/j.crad.2020.03.004\n10.1016/j.imu.2020.100360\n10.1016/j.compeleceng.2020.106765\n10.1007/s12559-020-09776-8\n10.1016/j.ins.2020.09.041\n10.1016/j.compbiomed.2020.103792\n10.1016/j.bspc.2019.04.031\n10.1016/j.inffus.2020.11.005\n10.1016/j.compbiomed.2020.103805\n10.1016/j.bspc.2020.102257\n10.1016/j.bspc.2022.103677\n10.1007/978-3-030-01234-2_1\n10.1186/s13040-021-00244-z\n10.1016/j.eswa.2022.116540\n10.1007/s11548-020-02283-z\n10.2196/19569\n10.1007/978-3-319-46493-0_38"}
{"title": "MFL-Net: An Efficient Lightweight Multi-Scale Feature Learning CNN for COVID-19 Diagnosis From CT Images.", "abstract": "Timely and accurate diagnosis of coronavirus disease 2019 (COVID-19) is crucial in curbing its spread. Slow testing results of reverse transcription-polymerase chain reaction (RT-PCR) and a shortage of test kits have led to consider chest computed tomography (CT) as an alternative screening and diagnostic tool. Many deep learning methods, especially convolutional neural networks (CNNs), have been developed to detect COVID-19 cases from chest CT scans. Most of these models demand a vast number of parameters which often suffer from overfitting in the presence of limited training data. Moreover, the linearly stacked single-branched architecture based models hamper the extraction of multi-scale features, reducing the detection performance. In this paper, to handle these issues, we propose an extremely lightweight CNN with multi-scale feature learning blocks called as MFL-Net. The MFL-Net comprises a sequence of MFL blocks that combines multiple convolutional layers with 3 \u00d73 filters and residual connections effectively, thereby extracting multi-scale features at different levels and preserving them throughout the block. The model has only 0.78M parameters and requires low computational cost and memory space compared to many ImageNet pretrained CNN architectures. Comprehensive experiments are carried out using two publicly available COVID-19 CT imaging datasets. The results demonstrate that the proposed model achieves higher performance than pretrained CNN models and state-of-the-art methods on both datasets with limited training data despite having an extremely lightweight architecture. The proposed method proves to be an effective aid for the healthcare system in the accurate and timely diagnosis of COVID-19.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-08-19", "authors": ["Amogh ManojJoshi", "Deepak RanjanNayak"], "doi": "10.1109/JBHI.2022.3196489"}
{"title": "Semi-supervised COVID-19 CT image segmentation using deep generative models.", "abstract": "A recurring problem in image segmentation is a lack of labelled data. This problem is especially acute in the segmentation of lung computed tomography (CT) of patients with Coronavirus Disease 2019 (COVID-19). The reason for this is simple: the disease has not been prevalent long enough to generate a great number of labels. Semi-supervised learning promises a way to learn from data that is unlabelled and has seen tremendous advancements in recent years. However, due to the complexity of its label space, those advancements cannot be applied to image segmentation. That being said, it is this same complexity that makes it extremely expensive to obtain pixel-level labels, making semi-supervised learning all the more appealing. This study seeks to bridge this gap by proposing a novel model that utilizes the image segmentation abilities of deep convolution networks and the semi-supervised learning abilities of generative models for chest CT images of patients with the COVID-19.\nWe propose a novel generative model called the shared variational autoencoder (SVAE). The SVAE utilizes a five-layer deep hierarchy of latent variables and deep convolutional mappings between them, resulting in a generative model that is well suited for lung CT images. Then, we add a novel component to the final layer of the SVAE which forces the model to reconstruct the input image using a segmentation that must match the ground truth segmentation whenever it is present. We name this final model StitchNet.\nWe compare StitchNet to other image segmentation models on a high-quality dataset of CT images from COVID-19 patients. We show that our model has comparable performance to the other segmentation models. We also explore the potential limitations and advantages in our proposed algorithm and propose some potential future research directions for this challenging issue.", "journal": "BMC bioinformatics", "date": "2022-08-17", "authors": ["JudahZammit", "Daryl L XFung", "QianLiu", "Carson Kai-SangLeung", "PingzhaoHu"], "doi": "10.1186/s12859-022-04878-6\n10.1016/j.cell.2020.04.045\n10.1109/TPAMI.2016.2644615\n10.1109/TPAMI.2019.2960224\n10.1016/j.patcog.2020.107269\n10.1186/s12967-021-02992-2"}
{"title": "A Novel Convolutional Neural Network Model as an Alternative Approach to Bowel Preparation Evaluation Before Colonoscopy in the COVID-19 Era: A Multicenter, Single-Blinded, Randomized Study.", "abstract": "Adequate bowel preparation is key to a successful colonoscopy, which is necessary for detecting adenomas and preventing colorectal cancer. We developed an artificial intelligence (AI) platform using a convolutional neural network (CNN) model (AI-CNN model) to evaluate the quality of bowel preparation before colonoscopy.\nThis was a colonoscopist-blinded, randomized study. Enrolled patients were randomized into an experimental group, in which our AI-CNN model was used to evaluate the quality of bowel preparation (AI-CNN group), or a control group, which performed self-evaluation per routine practice (control group). The primary outcome was the consistency (homogeneity) between the results of the 2 methods. The secondary outcomes included the quality of bowel preparation according to the Boston Bowel Preparation Scale (BBPS), polyp detection rate, and adenoma detection rate.\nA total of 1,434 patients were enrolled (AI-CNN, n = 730; control, n = 704). No significant difference was observed between the evaluation results (\"pass\" or \"not pass\") of the groups in the adequacy of bowel preparation as represented by BBPS scores. The mean BBPS scores, polyp detection rate, and adenoma detection rate were similar between the groups. These results indicated that the AI-CNN model and routine practice were generally consistent in the evaluation of bowel preparation quality. However, the mean BBPS score of patients with \"pass\" results were significantly higher in the AI-CNN group than in the control group, indicating that the AI-CNN model may further improve the quality of bowel preparation in patients exhibiting adequate bowel preparation.\nThe novel AI-CNN model, which demonstrated comparable outcomes to the routine practice, may serve as an alternative approach for evaluating bowel preparation quality before colonoscopy.", "journal": "The American journal of gastroenterology", "date": "2022-08-17", "authors": ["Yang-BorLu", "Si-CunLu", "Yung-NingHuang", "Shun-TianCai", "Puo-HsienLe", "Fang-YuHsu", "Yan-XingHu", "Hui-ShanHsieh", "Wei-TingChen", "Gui-LiXia", "Hong-ZhiXu", "WeiGong"], "doi": "10.14309/ajg.0000000000001900"}
{"title": "A regularization-driven Mean Teacher model based on semi-supervised learning for medical image segmentation.", "abstract": "", "journal": "Physics in medicine and biology", "date": "2022-08-16", "authors": ["QingWang", "XiangLi", "MingzhiChen", "LingnaChen", "JunxiChen"], "doi": "10.1088/1361-6560/ac89c8"}
{"title": "CovMnet-Deep Learning Model for classifying Coronavirus (COVID-19).", "abstract": "Diagnosing COVID-19, current pandemic disease using Chest X-ray images is widely used to evaluate the lung disorders. As the spread of the disease is enormous many medical camps are being conducted to screen the patients and Chest X-ray is a simple imaging modality to detect presence of lung disorders. Manual lung disorder detection using Chest X-ray by radiologist is a tedious process and may lead to inter and intra-rate errors. Various deep convolution neural network techniques were tested for detecting COVID-19 abnormalities in lungs using Chest X-ray images. This paper proposes deep learning model to classify COVID-19 and normal chest X-ray images. Experiments are carried out for deep feature extraction, fine-tuning of convolutional neural networks (CNN) hyper parameters, and end-to-end training of four variants of the CNN model. The proposed CovMnet provide better classification accuracy of 97.4% for COVID-19 /normal than those reported in the previous studies. The proposed CovMnet model has potential to aid radiologist to monitor COVID-19 disease and proves to be an efficient non-invasive COVID-19 diagnostic tool for lung disorders.", "journal": "Health and technology", "date": "2022-08-16", "authors": ["MalathyJawahar", "Jani AnbarasiL", "VinayakumarRavi", "JPrassanna", "S GracelineJasmine", "RManikandan", "RamesSekaran", "SuthendranKannan"], "doi": "10.1007/s12553-022-00688-1\n10.1016/j.chemolab.2020.104054\n10.1007/s10044-021-00984-y\n10.1007/s13246-020-00865-4\n10.1016/j.ins.2020.09.041\n10.1016/j.chaos.2020.109949\n10.1016/j.chaos.2020.110242\n10.4018/IJSSCI.2020070102\n10.4249/scholarpedia.1717\n10.1113/jphysiol.1970.sp009022\n10.1007/BF00344251\n10.1007/s00521-018-3761-1\n10.1007/s10096-020-03901-z"}
{"title": "Multiclass Classification for Detection of COVID-19 Infection in Chest X-Rays Using CNN.", "abstract": "Coronavirus took the world by surprise and caused a lot of trouble in all the important fields in life. The complexity of dealing with coronavirus lies in the fact that it is highly infectious and is a novel virus which is hard to detect with exact precision. The typical detection method for COVID-19 infection is the RT-PCR but it is a rather expensive method which is also invasive and has a high margin of error. Radiographies are a good alternative for COVID-19 detection given the experience of the radiologist and his learning capabilities. To make an accurate detection from chest X-Rays, deep learning technologies can be involved to analyze the radiographs, learn distinctive patterns of coronavirus' presence, find these patterns in the tested radiograph, and determine whether the sample is actually COVID-19 positive or negative. In this study, we propose a model based on deep learning technology using Convolutional Neural Networks and training it on a dataset containing a total of over 35,000 chest X-Ray images, nearly 16,000 for COVID-19 positive images, 15,000 for normal images, and 5,000 for pneumonia-positive images. The model's performance was assessed in terms of accuracy, precision, recall, and ", "journal": "Computational intelligence and neuroscience", "date": "2022-08-16", "authors": ["Rawan SaqerAlharbi", "Hadeel AysanAlsaadi", "SManimurugan", "TAnitha", "MiniluDejene"], "doi": "10.1155/2022/3289809\n10.1016/j.chaos.2020.110495\n10.1016/j.bspc.2022.103561\n10.1007/s13755-020-00135-3\n10.3390/ijerph18063056\n10.1007/978-3-642-15825-4_10\n10.3390/s2203121\n10.1007/s10489-020-01978-9\n10.1016/j.neucom.2021.03.034\n10.1016/j.bspc.2021.102920"}
{"title": "DCNN-FuzzyWOA: Artificial Intelligence Solution for Automatic Detection of COVID-19 Using X-Ray Images.", "abstract": "Artificial intelligence (AI) techniques have been considered effective technologies in diagnosing and breaking the transmission chain of COVID-19 disease. Recent research uses the deep convolution neural network (DCNN) as the discoverer or classifier of COVID-19 X-ray images. The most challenging part of neural networks is the subject of their training. Descent-based (GDB) algorithms have long been used to train fullymconnected layer (FCL) at DCNN. Despite the ability of GDBs to run and converge quickly in some applications, their disadvantage is the manual adjustment of many parameters. Therefore, it is not easy to parallelize them with graphics processing units (GPUs). Therefore, in this paper, the whale optimization algorithm (WOA) evolved by a fuzzy system called FuzzyWOA is proposed for DCNN training. With accurate and appropriate tuning of WOA's control parameters, the fuzzy system defines the boundary between the exploration and extraction phases in the search space. It causes the development and upgrade of WOA. To evaluate the performance and capability of the proposed DCNN-FuzzyWOA model, a publicly available database called COVID-Xray-5k is used. DCNN-PSO, DCNN-GA, and LeNet-5 benchmark models are used for fair comparisons. Comparative parameters include accuracy, processing time, standard deviation (STD), curves of ROC and precision-recall, and F1-Score. The results showed that the FuzzyWOA training algorithm with 20 epochs was able to achieve 100% accuracy, at a processing time of 880.44\u2009s with an F1-Score equal to 100%. Structurally, the i-6c-2s-12c-2s model achieved better results than the i-8c-2s-16c-2s model. However, the results of using FuzzyWOA for both models have been very encouraging compared to particle swarm optimization, genetic algorithm, and LeNet-5 methods.", "journal": "Computational intelligence and neuroscience", "date": "2022-08-16", "authors": ["AbbasSaffari", "MohammadKhishe", "MokhtarMohammadi", "AdilHussein Mohammed", "ShimaRashidi"], "doi": "10.1155/2022/5677961\n10.1016/j.imu.2020.100427\n10.1148/radiol.2020200642\n10.1155/2020/8889023\n10.1016/j.dsx.2020.04.012\n10.1109/ACCESS.2020.2989273\n10.1016/j.scs.2020.102589\n10.1016/j.eswa.2020.113338\n10.1007/s40430-017-0927-1\n10.1080/0952813x.2021.1960639\n10.1007/978-3-319-25751-8\n10.3390/ijerph18063056\n10.3390/healthcare9050522\n10.1145/3243316\n10.1109/TCYB.2020.2983860\n10.24425/aoa.2020.135281\n10.1016/j.oceaneng.2019.04.013\n10.24425/aoa.2019.126360\n10.1007/s10470-018-1366-3\n10.1007/s11277-019-06520-w\n10.1007/s10470-022-02014-1\n10.1016/j.advengsoft.2016.01.008\n10.21203/rs.3.rs-122787/v1\n10.1155/2020/8856801\n10.1007/s10462-020-09825-6\n10.1016/j.media.2020.101794\n10.1609/aaai.v33i01.3301590\n10.1109/oceans.2018.8604847\n10.1109/CEC48606.2020.9185541"}
{"title": "Innovations in thoracic imaging: CT, radiomics, AI and x-ray velocimetry.", "abstract": "In recent years, pulmonary imaging has seen enormous progress, with the introduction, validation and implementation of new hardware and software. There is a general trend from mere visual evaluation of radiological images to quantification of abnormalities and biomarkers, and assessment of 'non visual' markers that contribute to establishing diagnosis or prognosis. Important catalysts to these developments in thoracic imaging include new indications (like computed tomography [CT] lung cancer screening) and the COVID-19 pandemic. This review focuses on developments in CT, radiomics, artificial intelligence (AI) and x-ray velocimetry for imaging of the lungs. Recent developments in CT include the potential for ultra-low-dose CT imaging for lung nodules, and the advent of a new generation of CT systems based on photon-counting detector technology. Radiomics has demonstrated potential towards predictive and prognostic tasks particularly in lung cancer, previously not achievable by visual inspection by radiologists, exploiting high dimensional patterns (mostly texture related) on medical imaging data. Deep learning technology has revolutionized the field of AI and as a result, performance of AI algorithms is approaching human performance for an increasing number of specific tasks. X-ray velocimetry integrates x-ray (fluoroscopic) imaging with unique image processing to produce quantitative four dimensional measurement of lung tissue motion, and accurate calculations of lung ventilation.", "journal": "Respirology (Carlton, Vic.)", "date": "2022-08-16", "authors": ["RozemarijnVliegenthart", "AndreasFouras", "ColinJacobs", "NickolasPapanikolaou"], "doi": "10.1111/resp.14344\n10.1097/RLI.0000000000000822\n10.1148/radiol.210551\n10.1007/s00247-021-05146-0\n10.1109/CVPR.2017.369"}
{"title": "Classification of lungs infected COVID-19 images based on inception-ResNet.", "abstract": "Nowadays, COVID-19 is spreading rapidly worldwide, and seriously threatening lives . From the perspective of security and economy, the effective control of COVID-19 has a profound impact on the entire society. An effective strategy is to diagnose earlier to prevent the spread of the disease and prompt treatment of severe cases to improve the chance of survival.\nThe method of this paper is as follows: Firstly, the collected data set is processed by chest film image processing, and the bone removal process is carried out in the rib subtraction module. Then, the set preprocessing method performed histogram equalization, sharpening, and other preprocessing operations on the chest film. Finally, shallow and high-level feature mapping through the backbone network extracts the processed chest radiographs. We implement the self-attention mechanism in Inception-Resnet, perform the standard classification, and identify chest radiograph diseases through the classifier to realize the auxiliary COVID-19 diagnosis process at the medical level, all in an effort to further enhance the classification performance of the convolutional neural network. Numerous computer simulations demonstrate that the Inception-Resnet convolutional neural network performs CT image categorization and enhancement with greater efficiency and flexibility than conventional segmentation techniques.\nThe experimental COVID-19 CT dataset obtained in this paper is the new data for CT scans and medical imaging of normal, early COVID-19 patients and severe COVID-19 patients from Jinyintan hospital. The experiment plots the relationship between model accuracy, model loss and epoch, using ACC, TPR, SPE, F1 score and G-mean to measure the image maps of patients with and without the disease. Statistical measurement values are obtained by Inception-Resnet are 88.23%, 83.45%, 89.72%, 95.53% and 88.74%. The experimental results show that Inception-Resnet plays a more effective role than other image classification methods in evaluation indicators, and the method has higher robustness, accuracy and intuitiveness.\nWith CT images in the clinical diagnosis of COVID-19 images being widely used and the number of applied samples continuously increasing, the method in this paper is expected to become an additional diagnostic tool that can effectively improve the diagnostic accuracy of clinical COVID-19 images.", "journal": "Computer methods and programs in biomedicine", "date": "2022-08-15", "authors": ["YunfengChen", "YalanLin", "XiaodieXu", "JinzhenDing", "ChuzhaoLi", "YimingZeng", "WeiliLiu", "WeifangXie", "JianlongHuang"], "doi": "10.1016/j.cmpb.2022.107053\n10.1371/journal.pone.0180830\n10.1016/j.ejphar.2020.173644\n10.1155/2021/6658058"}
{"title": "COVID-19 diagnosis using chest CT scans and deep convolutional neural networks evolved by IP-based sine-cosine algorithm.", "abstract": "The prevalence of the COVID-19 virus and its variants has influenced all aspects of our life, and therefore, the precise diagnosis of this disease is vital. If a polymerase chain reaction test for a subject is negative, but he/she cannot easily breathe, taking a computed tomography (CT) image from his/her lung is urgently recommended. This study aims to optimize a deep convolution neural network (DCNN) structure to increase the COVID-19 diagnosis accuracy in lung CT images. This paper employs the sine-cosine algorithm (SCA) to optimize the structure of DCNN to take raw CT images and determine their status. Three improvements based on regular SCA are proposed to enhance both the accuracy and speed of the results. First, a new encoding approach is proposed based on the internet protocol (IP) address. Then, an enfeebled layer is proposed to generate a variable-length DCNN. The suggested model is examined over the COVID-CT and SARS-CoV-2 datasets. The proposed method is compared to a standard DCNN and seven variable-length models in terms of five known metrics, including sensitivity, accuracy, specificity, F1-score, precision, and receiver operative curve (ROC) and precision-recall curves. The results demonstrate that the proposed DCNN-IPSCA\u00a0surpasses other benchmarks, achieving final accuracy of (98.32%\u00a0and 98.01%), the sensitivity of (97.22% and 96.23%), and specificity of (96.77% and 96.44%) on the SARS-CoV-2 and COVID-CT datasets, respectively. Also, the proposed DCNN-IPSCA performs much better than the standard DCNN, with GPU and CPU training times, which are 387.69 and 63.10 times faster, respectively.", "journal": "Medical & biological engineering & computing", "date": "2022-08-13", "authors": ["BinfengXu", "DiegoMart\u00edn", "MohammadKhishe", "RezaBoostani"], "doi": "10.1007/s11517-022-02637-6\n10.1016/j.chaos.2020.109944\n10.1016/j.future.2019.05.009\n10.1016/j.aej.2021.06.024\n10.3390/math9091002\n10.1007/s10489-020-01888-w\n10.1016/j.bspc.2021.103326\n10.1177/0846537120913033\n10.1038/s41597-021-00900-3\n10.1002/jmv.25726\n10.1016/S0140-6736(13)61492-0\n10.1016/j.neucom.2016.12.038\n10.1016/j.bspc.2021.102764\n10.1162/106365602320169811\n10.1016/j.knosys.2015.12.022\n10.1145/1040132.1040133\n10.1016/j.imavis.2006.02.026\n10.1016/j.eswa.2016.04.005\n10.1016/j.eswa.2021.115732\n10.1007/s11047-019-09735-9\n10.1109/TEVC.2013.2281531"}
{"title": "Quantitative chest computed tomography combined with plasma cytokines predict outcomes in COVID-19 patients.", "abstract": "Despite extraordinary international efforts to dampen the spread and understand the mechanisms behind SARS-CoV-2 infections, accessible predictive biomarkers directly applicable in the clinic are yet to be discovered. Recent studies have revealed that diverse types of assays bear limited predictive power for COVID-19 outcomes. Here, we harness the predictive power of chest computed tomography (CT) in combination with plasma cytokines using a machine learning and k-fold cross-validation approach for predicting death during hospitalization and maximum severity degree in COVID-19 patients. Patients (n = 152) from the Mount Sinai Health System in New York with plasma cytokine assessment and a chest CT within five days from admission were included. Demographics, clinical, and laboratory variables, including plasma cytokines (IL-6, IL-8, and TNF-\u03b1), were collected from the electronic medical record. We found that CT quantitative alone was better at predicting severity (AUC 0.81) than death (AUC 0.70), while cytokine measurements alone better-predicted death (AUC 0.70) compared to severity (AUC 0.66). When combined, chest CT and plasma cytokines were good predictors of death (AUC 0.78) and maximum severity (AUC 0.82). Finally, we provide a simple scoring system (nomogram) using plasma IL-6, IL-8, TNF-\u03b1, ground-glass opacities (GGO) to aerated lung ratio and age as new metrics that may be used to monitor patients upon hospitalization and help physicians make critical decisions and considerations for patients at high risk of death for COVID-19.", "journal": "Heliyon", "date": "2022-08-13", "authors": ["GuillermoCarbonell", "Diane MarieDel Valle", "EdgarGonzalez-Kozlova", "BrettMarinelli", "EmmaKlein", "MariaEl Homsi", "DanielStocker", "MichaelChung", "AdamBernheim", "Nicole WSimons", "JianiXiang", "SharonNirenberg", "PatriciaKovatch", "SaraLewis", "MiriamMerad", "SachaGnjatic", "BachirTaouli"], "doi": "10.1016/j.heliyon.2022.e10166"}
{"title": "The risk profile of patients with COVID-19 as predictors of lung lesions severity and mortality-Development and validation of a prediction model.", "abstract": "We developed and validated a prediction model based on individuals' risk profiles to predict the severity of lung involvement and death in patients hospitalized with coronavirus disease 2019 (COVID-19) infection.\nIn this retrospective study, we studied hospitalized COVID-19 patients with data on chest CT scans performed during hospital stay (February 2020-April 2021) in a training dataset (TD) (\nIn the TD and the eVD, respectively, the mean [standard deviation (\nIn hospitalized patients with COVID-19, the severity of lung involvement is a strong predictor of death. Age, CRP levels, and duration of hospitalizations are the most important predictors of severe lung involvement. A simple prediction model based on available clinical and imaging data provides a validated tool that predicts the severity of lung involvement and death probability among hospitalized patients with COVID-19.", "journal": "Frontiers in microbiology", "date": "2022-08-13", "authors": ["EzatRahimi", "MinaShahisavandi", "Albert CidRoyo", "MohammadAzizi", "SaidEl Bouhaddani", "NasehSigari", "MiriamSturkenboom", "FaribaAhmadizar"], "doi": "10.3389/fmicb.2022.893750\n10.1007/s10140-022-02034-4\n10.3122/jabfm.2021.S1.200429\n10.1016/j.jamda.2020.05.045\n10.1371/journal.pone.0239235\n10.1023/A:1010933404324\n10.1016/j.jclinepi.2014.06.018\n10.1016/j.jiac.2022.02.025\n10.1007/s10238-020-00648-x\n10.3892/etm.2022.11315\n10.1016/j.hlc.2021.10.007\n10.1177/0284185121994695\n10.1016/j.tranpol.2021.07.004\n10.1016/j.acra.2022.02.019\n10.1016/j.cmi.2020.07.016\n10.1007/s10654-020-00698-1\n10.1016/j.jinf.2020.04.008\n10.1016/j.pmedr.2020.101298\n10.1016/j.neurol.2020.04.009\n10.1002/dmrr.3519\n10.3389/fimmu.2018.01302\n10.3389/fpubh.2021.695231\n10.2196/25852\n10.1080/13685538.2020.1774748\n10.1080/10408363.2020.1770685\n10.1007/s00330-020-06865-y\n10.1016/j.jaut.2020.102433\n10.1016/j.ajem.2020.09.017\n10.1093/ije/dyab012\n10.7326/M20-2973\n10.1136/thoraxjnl-2020-215518\n10.1016/j.puhe.2021.01.001\n10.1186/s12879-020-05154-9\n10.1007/s00330-021-08049-8\n10.1136/bmj.m1328\n10.2174/1573405617666210916120355\n10.1177/20503121211050755\n10.1142/11877"}
{"title": "Reinforcement Learning Based Diagnosis and Prediction for COVID-19 by Optimizing a Mixed Cost Function From CT Images.", "abstract": "A novel coronavirus disease (COVID-19) is a pandemic disease has caused 4 million deaths and more than 200 million infections worldwide (as of August 4, 2021). Rapid and accurate diagnosis of COVID-19 infection is critical to controlling the spread of the epidemic. In order to quickly and efficiently detect COVID-19 and reduce the threat of COVID-19 to human survival, we have firstly proposed a detection framework based on reinforcement learning for COVID-19 diagnosis, which constructs a mixed loss function that can integrate the advantages of multiple loss functions. This paper uses the accuracy of the validation set as the reward value, and obtains the initial model for the next epoch by searching the model corresponding to the maximum reward value in each epoch. We also have proposed a prediction framework that integrates multiple detection frameworks using parameter sharing to predict the progression of patients' disease without additional training. This paper also constructed a higher-quality version of the CT image dataset containing 247 cases screened by professional physicians, and obtained more excellent results on this dataset. Meanwhile, we used the other two COVID-19 datasets as external verifications, and still achieved a high accuracy rate without additional training. Finally, the experimental results show that our classification accuracy can reach 98.31%, and the precision, sensitivity, specificity, and AUC (Area Under Curve) are 98.82%, 97.99%, 98.67%, and 0.989, respectively. The accuracy of external verification can reach 93.34% and 91.05%. What's more, the accuracy of our prediction framework is 91.54%. A large number of experiments demonstrate that our proposed method is effective and robust for COVID-19 detection and prediction.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-08-12", "authors": ["SiyingChen", "MinghuiLiu", "PanDeng", "JialiDeng", "YiYuan", "XuanCheng", "TianshuXie", "LiboXie", "WeiZhang", "HaigangGong", "XiaominWang", "LifengXu", "HongPu", "MingLiu"], "doi": "10.1109/JBHI.2022.3197666"}
{"title": "Detection of COVID-19 from chest X-ray images: Boosting the performance with convolutional neural network and transfer learning.", "abstract": "Coronavirus disease (COVID-19) is a pandemic that has caused thousands of casualties and impacts all over the world. Most countries are facing a shortage of COVID-19 test kits in hospitals due to the daily increase in the number of cases. Early detection of COVID-19 can protect people from severe infection. Unfortunately, COVID-19 can be misdiagnosed as pneumonia or other illness and can lead to patient death. Therefore, in order to avoid the spread of COVID-19 among the population, it is necessary to implement an automated early diagnostic system as a rapid alternative diagnostic system. Several researchers have done very well in detecting COVID-19; however, most of them have lower accuracy and overfitting issues that make early screening of COVID-19 difficult. Transfer learning is the most successful technique to solve this problem with higher accuracy. In this paper, we studied the feasibility of applying transfer learning and added our own classifier to automatically classify COVID-19 because transfer learning is very suitable for medical imaging due to the limited availability of data. In this work, we proposed a CNN model based on deep transfer learning technique using six different pre-trained architectures, including VGG16, DenseNet201, MobileNetV2, ResNet50, Xception, and EfficientNetB0. A total of 3886 chest X-rays (1200 cases of COVID-19, 1341 healthy and 1345 cases of viral pneumonia) were used to study the effectiveness of the proposed CNN model. A comparative analysis of the proposed CNN models using three classes of chest X-ray datasets was carried out in order to find the most suitable model. Experimental results show that the proposed CNN model based on VGG16 was able to accurately diagnose COVID-19 patients with 97.84% accuracy, 97.90% precision, 97.89% sensitivity, and 97.89% of ", "journal": "Expert systems", "date": "2022-08-11", "authors": ["SohaibAsif", "YiWenhui", "KamranAmjad", "HouJin", "YiTao", "SiJinhai"], "doi": "10.1111/exsy.13099\n10.1080/07391102.2020.1767212\n10.20944/preprints202003.0300.v1"}
{"title": "A modified DeepLabV3+ based semantic segmentation of chest computed tomography images for COVID-19 lung infections.", "abstract": "Coronavirus disease (COVID-19) affects the lives of billions of people worldwide and has destructive impacts on daily life routines, the global economy, and public health. Early diagnosis and quantification of COVID-19 infection have a vital role in improving treatment outcomes and interrupting transmission. For this purpose, advances in medical imaging techniques like computed tomography (CT) scans offer great potential as an alternative to RT-PCR assay. CT scans enable a better understanding of infection morphology and tracking of lesion boundaries. Since manual analysis of CT can be extremely tedious and time-consuming, robust automated image segmentation is necessary for clinical diagnosis and decision support. This paper proposes an efficient segmentation framework based on the modified DeepLabV3+ using lower atrous rates in the Atrous Spatial Pyramid Pooling (ASPP) module. The lower atrous rates make receptive small to capture intricate morphological details. The encoder part of the framework utilizes a pre-trained residual network based on dilated convolutions for optimum resolution of feature maps. In order to evaluate the robustness of the modified model, a comprehensive comparison with other state-of-the-art segmentation methods was also performed. The experiments were carried out using a fivefold cross-validation technique on a publicly available database containing 100 single-slice CT scans from >40 patients with COVID-19. The modified DeepLabV3+ achieved good segmentation performance using around 43.9\u00a0M parameters. The lower atrous rates in the ASPP module improved segmentation performance. After fivefold cross-validation, the framework achieved an overall Dice similarity coefficient score of 0.881. The results demonstrate that several minor modifications to the DeepLabV3+ pipeline can provide robust solutions for improving segmentation performance and hardware implementation.", "journal": "International journal of imaging systems and technology", "date": "2022-08-10", "authors": ["HasanPolat"], "doi": "10.1002/ima.22772\n10.1002/ima.22566\n10.1002/ima.22525\n10.1016/j.measurement.2020.108288\n10.1016/j.mehy.2020.109761\n10.1148/radiol.2020200642\n10.1016/j.aej.2020.10.046\n10.1016/j.media.2017.07.005\n10.1016/j.tmaid.2020.101623\n10.1016/j.jrid.2020.04.001\n10.1111/exsy.12742\n10.1049/iet-cvi.2018.5129\n10.1049/iet-its.2018.5144\n10.1016/j.specom.2017.02.009\n10.1016/j.eswa.2021.115465\n10.1016/j.compbiomed.2020.104037\n10.1016/j.clinimag.2021.01.019\n10.1109/ICDMW.2018.00176\n10.30897/ijegeo.737993\n10.1016/j.media.2020.101794\n10.1186/s12880-020-00529-5\n10.1016/j.imu.2021.100681\n10.1109/CVPR.2016.90\n10.1007/s11042-020-09634-7\n10.1109/ACCESS.2016.2624938\n10.1016/j.compbiomed.2021.105134\n10.1016/j.compbiomed.2022.105383\n10.1007/s10278-021-00434-5\n10.1007/978-3-319-24574-4_28\n10.1109/TPAMI.2016.2572683\n10.1155/2021/9999368\n10.1145/3453892.3461322\n10.1016/j.asoc.2020.106897\n10.1016/j.cmpb.2021.106004\n10.1016/j.patcog.2022.108538\n10.31590/ejosat.819409\n10.3390/s20113183\n10.1016/j.patrec.2020.07.029\n10.1109/WACV.2018.00163\n10.3390/su13031224\n10.3390/diagnostics11091712\n10.1007/978-3-030-01234-2_49\n10.1007/978-3-319-10578-9_23\n10.1016/j.eswa.2020.114417\n10.1002/mp.14676\n10.48550/arXiv.1412.6980\n10.1007/s10462-020-09854-1\n10.5281/ZENODO.3757476\n10.1109/ACCESS.2021.3067047"}
{"title": "Deep Learning-Based Time-to-Death Prediction Model for COVID-19 Patients Using Clinical Data and Chest Radiographs.", "abstract": "Accurate estimation of mortality and time to death at admission for COVID-19 patients is important and several deep learning models have been created for this task. However, there are currently no prognostic models which use end-to-end deep learning to predict time to event for admitted COVID-19 patients using chest radiographs and clinical data. We retrospectively implemented a new artificial intelligence model combining DeepSurv (a multiple-perceptron implementation of the Cox proportional hazards model) and a convolutional neural network (CNN) using 1356 COVID-19 inpatients. For comparison, we also prepared DeepSurv only\u00a0with clinical data, DeepSurv only with images (CNNSurv), and Cox proportional hazards models. Clinical data and chest radiographs at admission were used to estimate patient outcome (death or discharge) and duration to the outcome. The Harrel's concordance index (c-index) of the DeepSurv with CNN model was 0.82 (0.75-0.88) and this was significantly higher than the DeepSurv only\u00a0with clinical data model (c-index\u2009=\u20090.77 (0.69-0.84), p\u2009=\u20090.011),\u00a0CNNSurv (c-index\u00a0= 0.70 (0.63-0.79), p = 0.001), and the Cox proportional hazards model (c-index\u2009=\u20090.71 (0.63-0.79), p\u2009=\u20090.001). These results suggest that the time-to-event prognosis model became more accurate when chest radiographs and clinical data were used together.", "journal": "Journal of digital imaging", "date": "2022-08-09", "authors": ["ToshimasaMatsumoto", "Shannon LeighWalston", "MichaelWalston", "DaijiroKabata", "YukioMiki", "MasatsuguShiba", "DaijuUeda"], "doi": "10.1007/s10278-022-00691-y\n10.7861/clinmed.2020-0214\n10.1186/s13613-020-00650-2\n10.1093/cid/ciaa414\n10.1016/S2213-8587(21)00089-9\n10.1001/jama.2018.11100\n10.1038/nature14539\n10.1186/s12874-018-0482-1\n10.1016/j.amjmed.2004.03.020\n10.1007/s11547-020-01232-9\n10.1007/s10140-020-01808-y\n10.1148/radiol.2020201754\n10.1148/radiol.2020200823\n10.1007/s00330-020-06827-4\n10.1007/s10278-013-9622-7\n10.1136/bmj.h5527\n10.1136/bmj.n2400\n10.1001/jamainternmed.2021.6203\n10.1056/NEJMoa2103417\n10.1016/j.jiph.2021.09.023\n10.1371/journal.pone.0241955\n10.1038/s41586-020-2521-4\n10.1023/A:1010933404324\n10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4\n10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2\n10.1111/j.0006-341X.2005.030814.x\n10.1136/bmj.m1328\n10.2196/25535\n10.2196/24973\n10.1016/j.media.2021.102096\n10.1148/ryai.2020200098\n10.1038/s41598-022-07890-1\n10.1038/s41598-021-93543-8\n10.1038/s41598-019-43372-7\n10.1016/S2589-7500(21)00039-X\n10.1016/j.lfs.2020.117788\n10.1186/s13054-019-2663-7\n10.1001/jamanetworkopen.2020.25881\n10.1001/jamanetworkopen.2020.5842\n10.1002/acm2.12995\n10.1007/BF00344251\n10.1148/radiol.2017171183\n10.1056/NEJMc2104626"}
{"title": "Development and Validation of Machine Models Using Natural Language Processing to Classify Substances Involved in Overdose Deaths.", "abstract": "Overdose is one of the leading causes of death in the US; however, surveillance data lag considerably from medical examiner determination of the death to reporting in national surveillance reports.\nTo automate the classification of deaths related to substances in medical examiner data using natural language processing (NLP) and machine learning (ML).\nDiagnostic study comparing different natural language processing and machine learning algorithms to identify substances related to overdose in 10 health jurisdictions in the US from January 1, 2020, to December 31, 2020. Unstructured text from 35\u202f433 medical examiner and coroners' death records was examined.\nText from each case was manually classified to a substance that was related to the death. Three feature representation methods were used and compared: text frequency-inverse document frequency (TF-IDF), global vectors for word representations (GloVe), and concept unique identifier (CUI) embeddings. Several ML algorithms were trained and best models were selected based on F-scores. The best models were tested on a hold-out test set and results were reported with 95% CIs.\nText data from death certificates were classified as any opioid, fentanyl, alcohol, cocaine, methamphetamine, heroin, prescription opioid, and an aggregate of other substances. Diagnostic metrics and 95% CIs were calculated for each combination of feature extraction method and machine learning classifier.\nOf 35\u202f433 death records analyzed (decedent median age, 58 years [IQR, 41-72 years]; 24\u202f449 [69%] were male), the most common substances related to deaths included any opioid (5739 [16%]), fentanyl (4758 [13%]), alcohol (2866 [8%]), cocaine (2247 [6%]), methamphetamine (1876 [5%]), heroin (1613 [5%]), prescription opioids (1197 [3%]), and any benzodiazepine (1076 [3%]). The CUI embeddings had similar or better diagnostic metrics compared with word embeddings and TF-IDF for all substances except alcohol. ML classifiers had perfect or near perfect performance in classifying deaths related to any opioids, heroin, fentanyl, prescription opioids, methamphetamine, cocaine, and alcohol. Classification of benzodiazepines was suboptimal using all 3 feature extraction methods.\nIn this diagnostic study, NLP/ML algorithms demonstrated excellent diagnostic performance at classifying substances related to overdoses. These algorithms should be integrated into workflows to decrease the lag time in reporting overdose surveillance data.", "journal": "JAMA network open", "date": "2022-08-09", "authors": ["DavidGoodman-Meza", "Chelsea LShover", "Jesus AMedina", "Amber BTang", "StevenShoptaw", "Alex A TBui"], "doi": "10.1001/jamanetworkopen.2022.25593\n10.2105/AJPH.2021.306256\n10.2105/AJPH.2017.304187\n10.15585/mmwr.mm7006a4\n10.1016/j.drugalcdep.2020.108314\n10.1371/journal.pone.0223318\n10.1016/j.drugalcdep.2021.109048\n10.1097/ADM.0000000000000775\n10.1136/amiajnl-2011-000464\n10.1016/j.jbi.2019.103185\n10.1002/pds.4772\n10.1002/pds.4810\n10.35111/wk4f-qt80\n10.1016/j.jbi.2018.09.008\n10.18653/v1/W19-5034\n10.1093/nar/gkh061\n10.15585/mmwr.mm7050e3\n10.48550/arXiv.1810.04805\n10.48550/arXiv.1904.03323"}
{"title": "Deep Learning-Aided Automated Pneumonia Detection and Classification Using CXR Scans.", "abstract": "The COVID-19 pandemic has caused a worldwide catastrophe and widespread devastation that reeled almost all countries. The pandemic has mounted pressure on the existing healthcare system and caused panic and desperation. The gold testing standard for COVID-19 detection, reverse transcription-polymerase chain reaction (RT-PCR), has shown its limitations with 70% accuracy, contributing to the incorrect diagnosis that exaggerated the complexities and increased the fatalities. The new variations further pose unseen challenges in terms of their diagnosis and subsequent treatment. The COVID-19 virus heavily impacts the lungs and fills the air sacs with fluid causing pneumonia. Thus, chest X-ray inspection is a viable option if the inspection detects COVID-19-induced pneumonia, hence confirming the exposure of COVID-19. Artificial intelligence and machine learning techniques are capable of examining chest X-rays in order to detect patterns that can confirm the presence of COVID-19-induced pneumonia. This research used CNN and deep learning techniques to detect COVID-19-induced pneumonia from chest X-rays. Transfer learning with fine-tuning ensures that the proposed work successfully classifies COVID-19-induced pneumonia, regular pneumonia, and normal conditions. Xception, Visual Geometry Group 16, and Visual Geometry Group 19 are used to realize transfer learning. The experimental results were promising in terms of precision, recall, F1 score, specificity, false omission rate, false negative rate, false positive rate, and false discovery rate with a COVID-19-induced pneumonia detection accuracy of 98%. Experimental results also revealed that the proposed work has not only correctly identified COVID-19 exposure but also made a distinction between COVID-19-induced pneumonia and regular pneumonia, as the latter is a very common disease, while COVID-19 is more lethal. These results mitigated the concern and overlap in the diagnosis of COVID-19-induced pneumonia and regular pneumonia. With further integrations, it can be employed as a potential standard model in differentiating the various lung-related infections, including COVID-19.", "journal": "Computational intelligence and neuroscience", "date": "2022-08-09", "authors": ["Deepak KumarJain", "TarishiSingh", "PraneetSaurabh", "DhananjayBisen", "NeerajSahu", "JayantMishra", "HabiburRahman"], "doi": "10.1155/2022/7474304\n10.1109/ICDABI51230.2020.9325626\n10.1001/jama.2020.1585\n10.1016/s0140-6736(20)30211-710.1016/s0140-6736(20)30211-7\n10.1056/NEJMoa2001316\n10.1016/S0140-6736(20)30183-5\n10.1093/clinchem/hvaa029\n10.1148/radiol.2020200230\n10.2214/AJR.20.23034\n10.1007/s10489-020-01826-w\n10.1109/ISMSIT.2019.8932878\n10.1109/NSSMIC.2018.8824292\n10.1109/ICNSC.2018.8361312\n10.1007/s11517-019-01965-4\n10.1109/TMI.2019.2894349\n10.1148/radiol.2019181960\n10.3390/app10020559\n10.1016/j.compbiomed.2020.103869\n10.1145/3195588.3195597\n10.1148/radiol.2020200905\n10.1371/journal.pmed.1002686\n10.1109/ACCESS.2020.3010287\n10.3390/app9194130\n10.1109/TMI.2020.2994459\n10.17632/9xkhgts2s6.1\n10.1007/s40009-020-00979-z\n10.1007/s11042-022-12775-6"}
{"title": "A Novel Multi-Stage Residual Feature Fusion Network for Detection of COVID-19 in Chest X-Ray Images.", "abstract": "To suppress the spread of COVID-19, accurate diagnosis at an early stage is crucial, chest screening with radiography imaging plays an important role in addition to the real-time reverse transcriptase polymerase chain reaction (RT-PCR) swab test. Due to the limited data, existing models suffer from incapable feature extraction and poor network convergence and optimization. Accordingly, a multi-stage residual network, MSRCovXNet, is proposed for effective detection of COVID-19 from chest x-ray (CXR) images. As a shallow yet effective classifier with the ResNet-18 as the feature extractor, MSRCovXNet is optimized by fusing two proposed feature enhancement modules (FEM), i.e., low-level and high-level feature maps (LLFMs and HLFMs), which contain respectively more local information and rich semantic information, respectively. For effective fusion of these two features, a single-stage FEM (MSFEM) and a multi-stage FEM (MSFEM) are proposed to enhance the semantic feature representation of the LLFMs and the local feature representation of the HLFMs, respectively. Without ensembling other deep learning models, our MSRCovXNet has a precision of 98.9% and a recall of 94% in detection of COVID-19, which outperforms several state-of-the-art models. When evaluated on the COVIDGR dataset, an average accuracy of 82.2% is achieved, leading other methods by at least 1.2%.", "journal": "IEEE transactions on molecular, biological, and multi-scale communications", "date": "2022-08-09", "authors": ["ZhenyuFang", "JinchangRen", "CalumMacLellan", "HuihuiLi", "HuiminZhao", "AmirHussain", "GiancarloFortino"], "doi": "10.1109/TMBMC.2021.3099367"}
{"title": "Automated analysis of limited echocardiograms: Feasibility and relationship to outcomes in COVID-19.", "abstract": "As automated echocardiographic analysis is increasingly utilized, continued evaluation within hospital settings is important to further understand its potential value. The importance of cardiac involvement in patients hospitalized with COVID-19 provides an opportunity to evaluate the feasibility and clinical relevance of automated analysis applied to limited echocardiograms.\nIn this multisite US cohort, the feasibility of automated AI analysis was evaluated on 558 limited echocardiograms in patients hospitalized with COVID-19. Reliability of automated assessment of left ventricular (LV) volumes, ejection fraction (EF), and LV longitudinal strain (LS) was assessed against clinically obtained measures and echocardiographic findings. Automated measures were evaluated against patient outcomes using ROC analysis, survival modeling, and logistic regression for the outcomes of 30-day mortality and in-hospital sequelae.\nFeasibility of automated analysis for both LVEF and LS was 87.5% (488/558 patients). AI analysis was performed with biplane method in 300 (61.5%) and single plane apical 4- or 2-chamber analysis in 136 (27.9%) and 52 (10.7%) studies, respectively. Clinical LVEF was assessed using visual estimation in 192 (39.3%), biplane in 163 (33.4%), and single plane or linear methods in 104 (21.2%) of the 488 studies; 29 (5.9%) studies did not have clinically reported LVEF. LV LS was clinically reported in 80 (16.4%). Consistency between automated and clinical values demonstrated Pearson's R, root mean square error (RMSE) and intraclass correlation coefficient (ICC) of 0.61, 11.3% and 0.72, respectively, for LVEF; 0.73, 3.9% and 0.74, respectively for LS; 0.76, 24.4ml and 0.87, respectively, for end-diastolic volume; and 0.82, 12.8 ml, and 0.91, respectively, for end-systolic volume. Abnormal automated measures of LVEF and LS were associated with LV wall motion abnormalities, left atrial enlargement, and right ventricular dysfunction. Automated analysis was associated with outcomes, including survival.\nAutomated analysis was highly feasible on limited echocardiograms using abbreviated protocols, consistent with equivalent clinically obtained metrics, and associated with echocardiographic abnormalities and patient outcomes.", "journal": "Frontiers in cardiovascular medicine", "date": "2022-08-09", "authors": ["Patricia APellikka", "Jordan BStrom", "Gabriel MPajares-Hurtado", "Martin GKeane", "BenjaminKhazan", "SalimaQamruddin", "AustinTutor", "FahadGul", "EricPeterson", "RituThamman", "ShivaniWatson", "DeepaMandale", "Christopher GScott", "TasneemNaqvi", "Gary MWoodward", "WilliamHawkes"], "doi": "10.3389/fcvm.2022.937068\n10.1016/S2589-7500(20)30160-6\n10.1038/s41746-017-0013-1\n10.1109/ACCESS.2020.3010326\n10.1007/978-3-030-01045-4_9\n10.1109/TMI.2017.2690836\n10.1016/j.jcmg.2020.08.034\n10.1109/TUFFC.2020.3003403\n10.1007/978-3-030-39343-4_43\n10.1016/j.echo.2020.09.011\n10.1016/j.jcmg.2019.02.024\n10.1161/CIRCULATIONAHA.118.034338\n10.1038/s41746-019-0216-8\n10.1038/s41586-020-2145-8\n10.1530/ERP-18-0056\n10.1038/s41551-020-00667-9\n10.1016/j.jcmg.2021.04.018\n10.1136/heartjnl-2020-318256\n10.1016/j.echo.2021.05.010\n10.1016/j.jacc.2020.06.007\n10.1007/s11739-020-02604-9\n10.1016/j.jacc.2020.08.069\n10.1016/j.echo.2021.03.010\n10.1016/j.mayocp.2021.01.006\n10.1016/j.echo.2021.10.015\n10.1007/s10554-020-01968-5\n10.1016/j.jcmg.2020.07.026\n10.1016/j.echo.2014.10.003\n10.1001/jamacardio.2021.6059\n10.1161/CIRCIMAGING.119.009303\n10.1016/j.echo.2020.01.008\n10.1093/ehjci/jeaa072\n10.1016/j.mayocp.2020.12.015\n10.1016/j.jacc.2012.09.035\n10.1136/heartjnl-2019-316297\n10.1016/j.jcmg.2017.11.017\n10.1016/j.echo.2019.08.012\n10.1016/S0735-1097(21)04505-8\n10.1016/j.jacc.2020.08.066\n10.1016/j.jcmg.2020.04.014\n10.1001/jamacardio.2020.3538\n10.1136/bmj.m1966\n10.1016/j.echo.2020.04.017\n10.1016/j.jacc.2015.07.052\n10.1016/j.echo.2016.03.002\n10.4330/wjc.v7.i12.948\n10.1001/jamacardio.2019.2952\n10.2147/VHRM.S206747\n10.1016/j.jcmg.2016.06.012"}
{"title": "Primary SARS-CoV-2 Pneumonia Screening in Adults: Analysis of the Correlation Between High-Resolution Computed Tomography Pulmonary Patterns and Initial Oxygen Saturation Levels.", "abstract": "Chest High-Resolution Computed Tomography (HRCT) is mandatory for patients with confirmed Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) infection and a high Respiratory Rate (RR) because sublobar consolidation is the likely pathological pattern in addition to Ground Glass Opacities (GGOs).\nThe present study determined the correlation between the percentage extent of typical pulmonary lesions on HRCT, as a representation of severity, and the RR and peripheral oxygen saturation level (SpO\nThe present retrospective study was conducted in 332 adult patients who presented with dyspnea and hypoxemia and were admitted to Prince Mohammed bin Abdulaziz Hospital, Riyadh, Saudi Arabia between May 15, 2020 and December 15, 2020. All the patients underwent chest HRCT. Of the total, 198 patients with primary noncomplicated SARS-CoV-2 pneumonia were finally selected based on the typical chest HRCT patterns. The main CT patterns, GGO and sublobar consolidation, were individually quantified as a percentage of the total pulmonary involvement through algebraic summation of the percentage of the 19 pulmonary segments affected. Additionally, the statistical correlation strength between the total percentage pulmonary involvement and the age, initial RR, and percentage SpO\nThe mean \u00b1 Standard Deviation (SD) age of the 198 patients was 48.9 \u00b1 11.4 years. GGO magnitude alone exhibited a significant weak positive correlation with patients' age (r = 0.2; p = 0.04). Sublobar consolidation extent exhibited a relatively stronger positive correlation with RR than GGO magnitude (r = 0.23; p = 0.002). A relatively stronger negative correlation was observed between the GGO extent and SpO\nThe correlation between the magnitudes of typical pulmonary lesion patterns, particularly GGO, which exhibited an incremental correlation pattern on chest HRCT, and the SpO", "journal": "Current medical imaging", "date": "2022-08-06", "authors": ["BatilAlonazi", "Mohamed AMostafa", "Ahmed MFarghaly", "Salah AZindani", "Jehad AAl-Watban", "FerasAltaimi", "Abdulrahim SAlmotairy", "Moram AFagiry", "Mustafa ZMahmoud"], "doi": "10.2174/1573405618666220802095119"}
{"title": "Deep Learning Based COVID-19 Detection Using Medical Images: Is Insufficient Data Handled Well?", "abstract": "Deep learning is a prominent method for automatic detection of COVID-19 disease using a medical dataset. This paper aims to give a perspective on the data insufficiency issue that exists in COVID-19 detection associated with deep learning. The extensive study of the available datasets comprising CT and X-ray images is presented in this paper, which can be very much useful in the context of a deep learning framework for COVID-19 detection. Moreover, various data handling techniques that are very essential in deep learning models are discussed in detail. Advanced data handling techniques and approaches to modify deep learning models are suggested to handle the data insufficiency problem in deep learning based on COVID-19 detection.", "journal": "Current medical imaging", "date": "2022-08-06", "authors": ["CarenBabu", "RahulManohar O", "D AbrahamChandy"], "doi": "10.2174/1573405618666220803123626"}
{"title": "Performance of a Chest Radiograph AI Diagnostic Tool for COVID-19: A Prospective Observational Study.", "abstract": "To conduct a prospective observational study across 12 U.S. hospitals to evaluate real-time performance of an interpretable artificial intelligence (AI) model to detect COVID-19 on chest radiographs.\nA total of 95\u2009363 chest radiographs were included in model training, external validation, and real-time validation. The model was deployed as a clinical decision support system, and performance was prospectively evaluated. There were 5335 total real-time predictions and a COVID-19 prevalence of 4.8% (258 of 5335). Model performance was assessed with use of receiver operating characteristic analysis, precision-recall curves, and F1 score. Logistic regression was used to evaluate the association of race and sex with AI model diagnostic accuracy. To compare model accuracy with the performance of board-certified radiologists, a third dataset of 1638 images was read independently by two radiologists.\nParticipants positive for COVID-19 had higher COVID-19 diagnostic scores than participants negative for COVID-19 (median, 0.1 [IQR, 0.0-0.8] vs 0.0 [IQR, 0.0-0.1], respectively; \nAI-based tools have not yet reached full diagnostic potential for COVID-19 and underperform compared with radiologist prediction.", "journal": "Radiology. Artificial intelligence", "date": "2022-08-05", "authors": ["JuSun", "LePeng", "TaihuiLi", "DyahAdila", "ZachZaiman", "Genevieve BMelton-Meaux", "Nicholas EIngraham", "EricMurray", "DanielBoley", "SeanSwitzer", "John LBurns", "KunHuang", "TadashiAllen", "Scott DSteenburg", "Judy WawiraGichoya", "ErichKummerfeld", "Christopher JTignanelli"], "doi": "10.1148/ryai.210217\n10.1101/2020.09.13.20193565v2"}
{"title": "Diagnostic performance of artificial intelligence algorithms for detection of pulmonary involvement by COVID-19 based on portable radiography.", "abstract": "To evaluate the diagnostic performance of different artificial intelligence (AI) algorithms for the identification of pulmonary involvement by SARS-CoV-2 based on portable chest radiography (RX).\nProspective observational study that included patients admitted for suspected COVID-19 infection in a university hospital between July and November 2020. The reference standard of pulmonary involvement by SARS-CoV-2 comprised a positive PCR test and low-tract respiratory symptoms.\n493 patients were included, 140 (28%) with positive PCR and 32 (7%) with SARS-CoV-2 pneumonia. The AI-B algorithm had the best diagnostic performance (areas under the ROC curve AI-B 0.73, vs. AI-A 0.51, vs. AI-C 0.57). Using a detection threshold greater than 55%, AI-B had greater diagnostic performance than the specialist [(area under the curve of 0.68 (95% CI 0.64-0.72), vs. 0.54 (95% CI 0.49-0.59)].\nAI algorithms based on portable RX enabled a diagnostic performance comparable to human assessment for the detection of SARS-CoV-2 lung involvement.\nTo evaluate the diagnostic performance of different artificial intelligence (AI) algorithms for the identification of pulmonary involvement by SARS-CoV-2 based on portable chest radiography (RX).\nProspective observational study that included patients admitted for suspected COVID-19 infection in a university hospital between July and November 2020. The reference standard of pulmonary involvement by SARS-CoV-2 comprised a positive PCR test and low-tract respiratory symptoms.\n493 patients were included, 140 (28%) with positive PCR and 32 (7%) with SARS-CoV-2 pneumonia. The AI-B algorithm had the best diagnostic performance (areas under the ROC curve AI-B 0.73, vs. AI-A 0.51, vs. AI-C 0.57). Using a detection threshold greater than 55%, AI-B had greater diagnostic performance than the specialist [(area under the curve of 0.68 (95% CI 0.64-0.72), vs. 0.54 (95% CI 0.49-0.59)].\nAI algorithms based on portable RX enabled a diagnostic performance comparable to human assessment for the detection of SARS-CoV-2 lung involvement.", "journal": "Medicina clinica", "date": "2022-08-03", "authors": ["Ricardo LuisCobe\u00f1as", "Mar\u00edade Vedia", "JuanFlorez", "DanielaJaramillo", "LucianaFerrari", "RicardoRe"], "doi": "10.1016/j.medcli.2022.04.016\n10.2196/19104\n10.7717/peerj-cs.551\n10.1186/s41747-020-00195-w\n10.1148/radiol.2020201874\n10.1016/j.jiph.2020.06.028"}
{"title": "Development and verification of radiomics framework for computed tomography image segmentation.", "abstract": "Radiomics has been considered an imaging marker for capturing quantitative image information (QII). The introduction of radiomics to image segmentation is desirable but challenging.\nThis study aims to develop and validate a radiomics-based framework for image segmentation (RFIS).\nRFIS is designed using features extracted from volume (svfeatures) created by sliding window (swvolume). The 53 svfeatures are extracted from 11 phantom series. Outliers in the svfeature datasets are detected by isolation forest (iForest) and specified as the mean value. The percentage coefficient of variation (%COV) is calculated to evaluate the reproducibility of svfeatures. RFIS is constructed and applied to the gross target volume (GTV) segmentation from the peritumoral region (GTV with a 10 mm margin) to assess its feasibility. The 127 lung cancer images are enrolled. The test-retest method, correlation matrix, and Mann-Whitney U test (p < 0.05) are used to select non-redundant svfeatures of statistical significance from the reproducible svfeatures. The synthetic minority over-sampling technique is utilized to balance the minority group in the training sets. The support vector machine is employed for RFIS construction, which is tuned in the training set using 10-fold stratified cross-validation and then evaluated in the test sets. The swvolumes with the consistent classification results are grouped and merged. Mode filtering is performed to remove very small subvolumes and create relatively large regions of completely uniform character. In addition, RFIS performance is evaluated by the area under the receiver operating characteristic (ROC) curve (AUC), accuracy, sensitivity, specificity, and Dice similarity coefficient (DSC).\n30249 phantom and 145008 patient image swvolumes were analyzed. Forty-nine (92.45% of 53) svfeatures represented excellent reproducibility(%COV<15). Forty-five features (91.84% of 49) included five categories that passed test-retest analysis. Thirteen svfeatures (28.89% of 45) svfeatures were selected for RFIS construction. RFIS showed an average (95% confidence interval) sensitivity of 0.848 (95% CI:0.844-0.883), a specificity of 0.821 (95% CI: 0.818-0.825), an accuracy of 83.48% (95% CI: 83.27%-83.70%), and an AUC of 0.906 (95% CI: 0.904-0.908) with cross-validation. The sensitivity, specificity, accuracy, and AUC were equal to 0.762 (95% CI: 0.754-0.770), 0.840 (95% CI: 0.837-0.844), 82.29% (95% CI: 81.90%-82.60%), and 0.877 (95% CI: 0.873-0.881) in the test set, respectively. GTV was segmented by grouping and merging swvolume with identical classification results. The mean DSC after mode filtering was 0.707 \u00b1 0.093 in the training sets and 0.688 \u00b1 0.072 in the test sets.\nReproducible svfeatures can capture the differences in QII among swvolumes. RFIS can be applied to swvolume classification, which achieves image segmentation by grouping and merging the swvolume with similar QII.", "journal": "Medical physics", "date": "2022-08-03", "authors": ["JiabingGu", "BaoshengLi", "HuazhongShu", "JianZhu", "QingtaoQiu", "TongBai"], "doi": "10.1002/mp.15904\n10.1007/s00066-017-1175-0\n10.1038/ncomms5006\n10.1186/1748-717X-7-32\n10.1016/j.radonc.2014.08.028\n10.1016/j.radonc.2019.03.004\n10.3390/cancers12061682\n10.1016/j.bspc.2021.102522\n10.1038/nrclinonc.2017.141\n10.1002/mp.15539\n10.1002/mp.15178\n10.1016/j.radonc.2020.10.016\n10.1002/mrm.22572\n10.1148/radiol.13122697\n10.1002/mp.15392\n10.1016/j.ebiom.2019.05.023\n10.1088/1361-6560/ac2ea7\n10.1097/RLI.0000000000000180\n10.1007/s10278-013-9622-7\n10.7937/K9/TCIA.2017.zuzrml5b\n10.1088/0031-9155/60/14/5471\n10.1007/s00330-018-5343-0\n10.1007/s11307-016-0973-6\n10.1038/s41598-020-60868-9\n10.1145/2133360.2133363\n10.1007/s10278-014-9716-x\n10.3389/fonc.2021.692973\n10.3389/fonc.2016.00071\n10.1613/jair.953\n10.1190/1.2431821\n10.1109/PROC.1979.11327\n10.3348/kjr.2018.0070\n10.1016/j.mri.2003.09.001\n10.1016/j.radonc.2019.08.008\n10.1002/mp.12123\n10.1007/s00330-017-4859-z\n10.1088/0031-9155/61/13/r150\n10.2967/jnumed.113.129858\n10.3390/cancers13081814\n10.1016/j.radonc.2017.11.025\n10.1002/mp.15582"}
{"title": "Deep Learning-Based Networks for Detecting Anomalies in Chest X-Rays.", "abstract": "X-ray images aid medical professionals in the diagnosis and detection of pathologies. They are critical, for example, in the diagnosis of pneumonia, the detection of masses, and, more recently, the detection of COVID-19-related conditions. The chest X-ray is one of the first imaging tests performed when pathology is suspected because it is one of the most accessible radiological examinations. Deep learning-based neural networks, particularly convolutional neural networks, have exploded in popularity in recent years and have become indispensable tools for image classification. Transfer learning approaches, in particular, have enabled the use of previously trained networks' knowledge, eliminating the need for large data sets and lowering the high computational costs associated with this type of network. This research focuses on using deep learning-based neural networks to detect anomalies in chest X-rays. Different convolutional network-based approaches are investigated using the ChestX-ray14 database, which contains over 100,000 X-ray images with labels relating to 14 different pathologies, and different classification objectives are evaluated. Starting with the pretrained networks VGG19, ResNet50, and Inceptionv3, networks based on transfer learning are implemented, with different schemes for the classification stage and data augmentation. Similarly, an ad hoc architecture is proposed and evaluated without transfer learning for the classification objective with more examples. The results show that transfer learning produces acceptable results in most of the tested cases, indicating that it is a viable first step for using deep networks when there are not enough labeled images, which is a common problem when working with medical images. The ad hoc network, on the other hand, demonstrated good generalization with data augmentation and an acceptable accuracy value. The findings suggest that using convolutional neural networks with and without transfer learning to design classifiers for detecting pathologies in chest X-rays is a good idea.", "journal": "BioMed research international", "date": "2022-08-03", "authors": ["MalekBadr", "ShahaAl-Otaibi", "NazikAlturki", "TanvirAbir"], "doi": "10.1155/2022/7833516\n10.1201/b10866-37\n10.1109/CVPR.2017.369\n10.1109/ICSCCC.2018.8703316\n10.1016/B978-0-12-816718-2.00008-7\n10.1155/2022/1959371\n10.1007/978-981-15-4112-4_7\n10.3390/jcm11072054\n10.23919/MIPRO48935.2020.9245376\n10.1155/2022/4569879\n10.14569/IJACSA.2021.0121026\n10.1109/ELNANO.2018.8477564\n10.1155/2021/8148772\n10.1155/2022/3294954\n10.1007/s00607-021-00992-0\n10.1155/2021/5759184\n10.1155/2021/6799202\n10.24191/mjoc.v4i1.6095\n10.1007/s11548-020-02305-w\n10.1155/2021/1220374\n10.1117/12.2293971\n10.1007/s10916-021-01745-4"}
{"title": "Detecting COVID-19 patients via MLES-Net deep learning models from X-Ray images.", "abstract": "Corona Virus Disease 2019 (COVID-19) first appeared in December 2019, and spread rapidly around the world. COVID-19 is a pneumonia caused by novel coronavirus infection in 2019. COVID-19 is highly infectious and transmissible. By 7 May 2021, the total number of cumulative number of deaths is 3,259,033. In order to diagnose the infected person in time to prevent the spread of the virus, the diagnosis method for COVID-19 is extremely important. To solve the above problems, this paper introduces a Multi-Level Enhanced Sensation module (MLES), and proposes a new convolutional neural network model, MLES-Net, based on this module.\nAttention has the ability to automatically focus on the key points in various information, and Attention can realize parallelism, which can replace some recurrent neural networks to a certain extent and improve the efficiency of the model. We used the correlation between global and local features to generate the attention mask. First, the feature map was divided into multiple groups, and the initial attention mask was obtained by the dot product of each feature group and the feature after the global pooling. Then the attention masks were normalized. At the same time, there were two scaling and translating parameters in each group so that the normalize operation could be restored. Then, the final attention mask was obtained through the sigmoid function, and the feature of each location in the original feature group was scaled. Meanwhile, we use different classifiers on the network models with different network layers.\nThe network uses three classifiers, FC module (fully connected layer), GAP module (global average pooling layer) and GAPFC module (global average pooling layer and fully connected layer), to improve recognition efficiency. GAPFC as a classifier can obtain the best comprehensive effect by comparing the number of parameters, the amount of calculation and the detection accuracy. The experimental results show that the MLES-Net56-GAPFC achieves the best overall accuracy rate (95.27%) and the best recognition rate for COVID-19 category (100%).\nMLES-Net56-GAPFC has good classification ability for the characteristics of high similarity between categories of COVID-19 X-Ray images and low intra-category variability. Considering the factors such as accuracy rate, number of network model parameters and calculation amount, we believe that the MLES-Net56-GAPFC network model has better practicability.", "journal": "BMC medical imaging", "date": "2022-07-31", "authors": ["WeiWang", "YongbinJiang", "XinWang", "PengZhang", "JiLi"], "doi": "10.1186/s12880-022-00861-y\n10.1016/j.physio.2020.03.003\n10.1109/5.726791\n10.1109/TIP.2017.2710620\n10.2991/ijcis.d.191209.001\n10.1186/s12880-019-0399-0\n10.1109/TUFFC.2020.3005512\n10.1109/ACCESS.2020.3001973\n10.7150/ijms.46684\n10.1109/TMI.2020.2995508\n10.1007/s42979-020-00401-x\n10.1007/s42979-020-00335-4\n10.1007/s42979-020-00300-1\n10.1109/ACCESS.2021.3058537\n10.1007/s42979-020-00216-w\n10.1007/s42979-020-00383-w\n10.2991/ijcis.d.201123.001\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2020.103869\n10.1109/ACCESS.2020.3003810\n10.1371/journal.pone.0235187\n10.1016/j.imu.2020.100412\n10.1049/ipr2.12474\n10.1016/j.imu.2020.100505"}
{"title": "Unsupervised machine learning demonstrates the prognostic value of TAPSE/PASP ratio among hospitalized patients with COVID-19.", "abstract": "The ratio of tricuspid annular plane systolic excursion (TAPSE) to pulmonary artery systolic pressure (PASP) is a validated index of right ventricular-pulmonary arterial (RV-PA) coupling with prognostic value. We determined the predictive value of TAPSE/PASP ratio and adverse clinical outcomes in hospitalized patients with COVID-19.\nTwo hundred and twenty-nine consecutive hospitalized racially/ethnically diverse adults (\u226518 years of age) admitted with COVID-19 between March and June 2020 with clinically indicated transthoracic echocardiograms (TTE) that included adequate tricuspid regurgitation (TR) velocities for calculation of PASP were studied. The exposure of interest was impaired RV-PA coupling as assessed by TAPSE/PASP ratio. The primary outcome was in-hospital mortality. Secondary endpoints comprised of ICU admission, incident acute respiratory distress syndrome (ARDS), and systolic heart failure.\nOne hundred and seventy-six patients had both technically adequate TAPSE measurements and measurable TR velocities for analysis. After adjustment for age, sex, BMI, race/ethnicity, diabetes mellitus, and smoking status, log(TAPSE/PASP) had a significantly inverse association with ICU admission (p = 0.015) and death (p = 0.038). ROC analysis showed the optimal cutoff for TAPSE/PASP for death was 0.51\u00a0mm\u00a0mmHg\nImpaired RV-PA coupling, assessed noninvasively via the TAPSE/PASP ratio, was predictive of need for ICU level care and in-hospital mortality in hospitalized patients with COVID-19 suggesting utility of TAPSE/PASP in identification of poor clinical outcomes in this population both by traditional statistical and unsupervised machine learning based methods.", "journal": "Echocardiography (Mount Kisco, N.Y.)", "date": "2022-07-31", "authors": ["VivekJani", "KaranKapoor", "JosephMeyer", "JimLu", "ErinGoerlich", "Thomas SMetkus", "Jose AMadrazo", "ErinMichos", "KatherineWu", "NicoleBavaro", "ShelbyKutty", "Allison GHays", "MonicaMukherjee"], "doi": "10.1111/echo.15432"}
{"title": "A comparison of Covid-19 early detection between convolutional neural networks and radiologists.", "abstract": "The role of chest radiography in COVID-19 disease has changed since the beginning of the pandemic from a diagnostic tool when microbiological resources were scarce to a different one focused on detecting and monitoring COVID-19 lung involvement. Using chest radiographs, early detection of the disease is still helpful in resource-poor environments. However, the sensitivity of a chest radiograph for diagnosing COVID-19 is modest, even for expert radiologists. In this paper, the performance of a deep learning algorithm on the first clinical encounter is evaluated and compared with a group of radiologists with different years of experience.\nThe algorithm uses an ensemble of four deep convolutional networks, Ensemble4Covid, trained to detect COVID-19 on frontal chest radiographs. The algorithm was tested using images from the first clinical encounter of positive and negative cases. Its performance was compared with five radiologists on a smaller test subset of patients. The algorithm's performance was also validated using the public dataset COVIDx.\nCompared to the consensus of five radiologists, the Ensemble4Covid model achieved an AUC of 0.85, whereas the radiologists achieved an AUC of 0.71. Compared with other state-of-the-art models, the performance of a single model of our ensemble achieved nonsignificant differences in the public dataset COVIDx.\nThe results show that the use of images from the first clinical encounter significantly drops the detection performance of COVID-19. The performance of our Ensemble4Covid under these challenging conditions is considerably higher compared to a consensus of five radiologists. Artificial intelligence can be used for the fast diagnosis of COVID-19.", "journal": "Insights into imaging", "date": "2022-07-29", "authors": ["AlbertoAlbiol", "FranciscoAlbiol", "RobertoParedes", "Juana Mar\u00edaPlasencia-Mart\u00ednez", "AnaBlanco Barrio", "Jos\u00e9 M Garc\u00edaSantos", "SalvadorTortajada", "Victoria MGonz\u00e1lez Monta\u00f1o", "Clara ERodr\u00edguez Godoy", "SarayFern\u00e1ndez G\u00f3mez", "ElenaOliver-Garcia", "Mar\u00edade la Iglesia Vay\u00e1", "Francisca LM\u00e1rquez P\u00e9rez", "Juan IRayo Madrid"], "doi": "10.1186/s13244-022-01250-3\n10.1001/JAMA.2020.21694\n10.1007/S00330-020-07347-X\n10.1007/S00330-020-06967-7\n10.1148/RADIOL.2020201160/ASSET/IMAGES/LARGE/RADIOL.2020201160.FIG6.JPEG\n10.1148/RADIOL.2020202944/ASSET/IMAGES/LARGE/RADIOL.2020202944.TBL4.JPEG\n10.1148/RADIOL.2020203511/ASSET/IMAGES/LARGE/RADIOL.2020203511.FIG6C.JPEG\n10.1148/RADIOL.2021204522/ASSET/IMAGES/LARGE/RADIOL.2021204522.FIG8C.JPEG\n10.1109/TMI.2020.2993291\n10.1007/s00330-020-07354-y\n10.1007/s00330-020-07270-1\n10.1148/RADIOL.2020201874\n10.1016/J.MAYOCP.2020.07.024\n10.1109/TKDE.2009.191\n10.1037/H0031619\n10.1214/ss/1177013815\n10.1002/1097-0142\n10.1148/RADIOL.2020201365/ASSET/IMAGES/LARGE/RADIOL.2020201365.TBL2.JPEG\n10.1016/J.JACR.2019.05.019\n10.1186/S41747-020-00203-Z/FIGURES/3\n10.1148/RADIOL.2020204226"}
{"title": "Automatic scoring of COVID-19 severity in X-ray imaging based on a novel deep learning workflow.", "abstract": "In this study, we propose a two-stage workflow used for the segmentation and scoring of lung diseases. The workflow inherits quantification, qualification, and visual assessment of lung diseases on X-ray images estimated by radiologists and clinicians. It requires the fulfillment of two core stages devoted to lung and disease segmentation as well as an additional post-processing stage devoted to scoring. The latter integrated block is utilized, mainly, for the estimation of segment scores and computes the overall severity score of a patient. The models of the proposed workflow were trained and tested on four publicly available X-ray datasets of COVID-19 patients and two X-ray datasets of patients with no pulmonary pathology. Based on a combined dataset consisting of 580 COVID-19 patients and 784 patients with no disorders, our best-performing algorithm is based on a combination of DeepLabV3\u2009+\u2009, for lung segmentation, and MA-Net, for disease segmentation. The proposed algorithms' mean absolute error (MAE) of 0.30 is significantly reduced in comparison to established COVID-19 algorithms; BS-net and COVID-Net-S, possessing MAEs of 2.52 and 1.83 respectively. Moreover, the proposed two-stage workflow was not only more accurate but also computationally efficient, it was approximately 11 times faster than the mentioned methods. In summary, we proposed an accurate, time-efficient, and versatile approach for segmentation and scoring of lung diseases illustrated for COVID-19 and with broader future applications for pneumonia, tuberculosis, pneumothorax, amongst others.", "journal": "Scientific reports", "date": "2022-07-28", "authors": ["Viacheslav VDanilov", "DianaLitmanovich", "AlexProutski", "AlexanderKirpich", "DatoNefaridze", "AlexKarpovsky", "YuriyGankin"], "doi": "10.1038/s41598-022-15013-z\n10.2139/ssrn.3685938\n10.1093/cid/ciaa1012\n10.1016/j.jaci.2020.04.006\n10.1016/j.cmi.2020.04.012\n10.1148/ryct.2020200034\n10.1148/radiol.2020200527\n10.1016/j.chest.2020.04.003\n10.1007/s11547-020-01202-1\n10.1007/s10489-020-01829-7\n10.1016/j.imu.2021.100835\n10.1016/j.compbiomed.2020.103869\n10.1007/s13755-020-00116-6\n10.1016/j.eswa.2020.114054\n10.1007/s42600-020-00091-7\n10.1109/ACCESS.2020.3025372\n10.17632/8gf9vpkhgy.1\n10.17632/36fjrg9s69.1\n10.1016/j.media.2021.102046\n10.1038/s41598-021-88538-4\n10.1177/0885066603251897\n10.1371/journal.pone.0093885\n10.1186/s12931-019-1201-0\n10.1038/s41572-018-0051-2\n10.1371/journal.pone.0197418\n10.1186/s12880-015-0103-y\n10.1136/thoraxjnl-2017-211280\n10.3389/fphys.2021.672823\n10.1186/s12890-020-01286-5\n10.1148/radiol.2020201160\n10.1038/s41598-020-79470-0\n10.1016/j.ijid.2020.05.021\n10.1007/s00330-020-07270-1\n10.11613/BM.2012.031\n10.1109/TIP.2010.2044963\n10.1016/j.ijmedinf.2014.10.004\n10.1016/j.afjem.2020.09.009"}
{"title": "Mortality Prediction Analysis among COVID-19 Inpatients Using Clinical Variables and Deep Learning Chest Radiography Imaging Features.", "abstract": "The emergence of the COVID-19 pandemic over a relatively brief interval illustrates the need for rapid data-driven approaches to facilitate clinical decision making. We examined a machine learning process to predict inpatient mortality among COVID-19 patients using clinical and chest radiographic data. Modeling was performed with a de-identified dataset of encounters prior to widespread vaccine availability. Non-imaging predictors included demographics, pre-admission clinical history, and past medical history variables. Imaging features were extracted from chest radiographs by applying a deep convolutional neural network with transfer learning. A multi-layer perceptron combining 64 deep learning features from chest radiographs with 98 patient clinical features was trained to predict mortality. The Local Interpretable Model-Agnostic Explanations (LIME) method was used to explain model predictions. Non-imaging data alone predicted mortality with an ROC-AUC of 0.87 \u00b1 0.03 (mean \u00b1 SD), while the addition of imaging data improved prediction slightly (ROC-AUC: 0.91 \u00b1 0.02). The application of LIME to the combined imaging and clinical model found HbA1c values to contribute the most to model prediction (17.1 \u00b1 1.7%), while imaging contributed 8.8 \u00b1 2.8%. Age, gender, and BMI contributed 8.7%, 8.2%, and 7.1%, respectively. Our findings demonstrate a viable explainable AI approach to quantify the contributions of imaging and clinical data to COVID mortality predictions.", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2022-07-28", "authors": ["Xuan VNguyen", "EnginDikici", "SemaCandemir", "Robyn LBall", "Luciano MPrevedello"], "doi": "10.3390/tomography8040151\n10.1038/s41586-020-2008-3\n10.1148/radiol.2020200490\n10.1002/path.5549\n10.1148/radiol.2020200642\n10.1109/RBME.2020.2987975\n10.1016/j.bbe.2020.08.008\n10.1038/s41598-020-76550-z\n10.1038/s41591-020-0931-3\n10.1109/TMI.2020.2993291\n10.1148/radiol.2020204226\n10.1186/s40537-016-0043-6\n10.1007/s10278-013-9622-7\n10.1007/978-3-319-24574-4_28\n10.2214/ajr.174.1.1740071\n10.1016/j.media.2005.02.002\n10.1371/journal.pone.0190069\n10.1109/ACCESS.2021.3086020\n10.1109/ACCESS.2020.2976199\n10.1016/S2589-7500(21)00039-X\n10.1007/s00330-022-08588-8\n10.1183/13993003.02113-2020\n10.7717/peerj.10337\n10.1186/s12911-021-01742-0\n10.7717/peerj-cs.889\n10.3389/fdgth.2021.681608\n10.3390/diagnostics11081383\n10.1002/dmrr.3476"}
{"title": "Federated Learning Approach with Pre-Trained Deep Learning Models for COVID-19 Detection from Unsegmented CT images.", "abstract": "(1) Background: Coronavirus disease 2019 (COVID-19) is an infectious disease caused by SARS-CoV-2. Reverse transcription polymerase chain reaction (RT-PCR) remains the current gold standard for detecting SARS-CoV-2 infections in nasopharyngeal swabs. In Romania, the first reported patient to have contracted COVID-19 was officially declared on 26 February 2020. (2) Methods: This study proposes a federated learning approach with pre-trained deep learning models for COVID-19 detection. Three clients were locally deployed with their own dataset. The goal of the clients was to collaborate in order to obtain a global model without sharing samples from the dataset. The algorithm we developed was connected to our internal picture archiving and communication system and, after running backwards, it encountered chest CT changes suggestive for COVID-19 in a patient investigated in our medical imaging department on the 28 January 2020. (4) Conclusions: Based on our results, we recommend using an automated AI-assisted software in order to detect COVID-19 based on the lung imaging changes as an adjuvant diagnostic method to the current gold standard (RT-PCR) in order to greatly enhance the management of these patients and also limit the spread of the disease, not only to the general population but also to healthcare professionals.", "journal": "Life (Basel, Switzerland)", "date": "2022-07-28", "authors": ["Lucian MihaiFlorescu", "Costin TeodorStreba", "Mircea-Sebastian\u015eerb\u0103nescu", "M\u0103d\u0103linM\u0103muleanu", "Dan NicolaeFlorescu", "Rossy Vl\u0103du\u0163Teic\u0103", "Raluca ElenaNica", "Ioana AndreeaGheonea"], "doi": "10.3390/life12070958\n10.3389/fmicb.2020.631736\n10.1002/jmv.25766\n10.1056/NEJMoa2001017\n10.3390/life12010077\n10.1148/ryct.2020200034\n10.1016/j.jmoldx.2021.04.009\n10.47162/RJME.61.2.21\n10.1007/s00330-021-07937-3\n10.1111/exsy.12759\n10.1038/nature14539\n10.1016/j.patcog.2021.108081\n10.1016/j.asoc.2020.106912\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103869\n10.3390/diagnostics10060358\n10.1016/j.imu.2020.100360\n10.1109/ACCESS.2020.3010287\n10.1007/s00264-020-04609-7\n10.1016/j.cmpb.2020.105608\n10.1016/j.cmpb.2020.105581\n10.1016/j.eswa.2020.114054\n10.1016/j.compbiomed.2020.103795\n10.1007/s10489-020-01826-w\n10.2196/19569\n10.1183/13993003.00775-2020\n10.1016/j.asoc.2021.107330\n10.1109/JSEN.2021.3076767\n10.7910/DVN/6ACUZJ\n10.17632/3y55vgckg6.2\n10.1148/radiol.11092149\n10.12968/hmed.2020.0077\n10.5114/pjr.2021.103237\n10.1016/j.ijid.2014.12.007\n10.1016/j.crad.2016.06.110\n10.1148/radiographics.21.2.g01mr17403\n10.17632/ygvgkdbmvt.1\n10.7937/TCIA.2020.NNC2-0461\n10.1073/pnas.79.8.2554\n10.1109/EMBC.2017.8037515\n10.1007/s10278-021-00508-4\n10.21037/jtd.2017.03.157\n10.1167/tvst.9.2.14\n10.1364/AO.29.004790\n10.1016/j.jbi.2014.05.006\n10.1016/j.jacr.2022.03.015\n10.1109/TCOMM.2020.2990686\n10.11919/j.issn.1002-0829.215010\n10.1007/s11263-019-01228-7"}
{"title": "Online Behaviours during the COVID-19 Pandemic and Their Associations with Psychological Factors: An International Exploratory Study.", "abstract": "This cross-sectional study aimed to explore specific online behaviours and their association with a range of underlying psychological and other behavioural factors during the COVID-19 pandemic. Eight countries (Italy, Spain, the United Kingdom, Lithuania, Portugal, Japan, Hungary, and Brazil) participated in an international investigation involving 2223 participants (", "journal": "International journal of environmental research and public health", "date": "2022-07-28", "authors": ["JuliusBurkauskas", "Naomi AFineberg", "KonstantinosIoannidis", "Samuel RChamberlain", "HenriettaBowden-Jones", "IngaGriskova-Bulanova", "AistePranckeviciene", "Artemisa RDores", "Irene PCarvalho", "FernandoBarbosa", "PierluigiSimonato", "IlariaDe Luca", "RosinMooney", "Maria \u00c1ngelesG\u00f3mez-Mart\u00ednez", "ZsoltDemetrovics", "Krisztina Edina\u00c1bel", "AttilaSzabo", "HironobuFujiwara", "MamiShibata", "Alejandra RMelero-Ventola", "Eva MArroyo-Anll\u00f3", "Ricardo MSantos-Labrador", "KeiKobayashi", "FrancescoDi Carlo", "CristinaMonteiro", "GiovanniMartinotti", "OrnellaCorazza"], "doi": "10.3390/ijerph19148823\n10.1016/j.euroneuro.2018.08.004\n10.5114/biolsport.2020.96857\n10.1016/j.cobeha.2022.101179\n10.1016/j.comppsych.2020.152180\n10.1111/ajad.13066\n10.1037/pas0000870\n10.3389/fpsyt.2020.580977\n10.3389/fpsyt.2021.648501\n10.1371/journal.pone.0213060\n10.3109/00952990.2013.803111\n10.1037/ppm0000264\n10.1016/j.neubiorev.2021.03.005\n10.1007/s11920-021-01271-7\n10.1016/j.jpsychires.2020.11.004\n10.1016/j.paid.2020.110457\n10.1007/s42399-020-00309-w\n10.1007/s12144-022-02824-6\n10.1111/aphw.12051\n10.1007/s12671-020-01505-4\n10.4103/indianjpsychiatry.indianjpsychiatry_409_21\n10.1101/2020.05.10.20095844\n10.3389/fpsyt.2020.577135\n10.3389/fpsyt.2021.634464\n10.3389/fpsyg.2021.685137\n10.3389/fpsyt.2020.565769\n10.1186/s43045-022-00180-6\n10.1016/j.abrep.2020.100311\n10.55319/js.v1i1.9\n10.1089/cyber.2020.0645\n10.1038/d41586-019-02776-1\n10.1556/2006.2020.00016\n10.1556/2006.2020.00040\n10.1007/s11469-020-00358-1\n10.1556/2006.2020.00015\n10.1016/j.heliyon.2020.e05135\n10.1177/0004867412461693\n10.1556/JBA.2.2013.016\n10.1016/j.abrep.2015.03.002\n10.3389/fpsyg.2021.689058\n10.1017/S1352465813000556\n10.1002/cpp.702\n10.1016/j.eclinm.2022.101305\n10.3389/fpubh.2020.00392\n10.1016/j.neuropsychologia.2018.11.015\n10.1016/j.etdah.2021.100010\n10.1186/s12889-021-11398-0\n10.1007/s12671-021-01674-w\n10.1016/j.chb.2017.11.020\n10.1556/2006.8.2019.34\n10.3390/children7090148\n10.1016/j.bodyim.2020.02.010\n10.1097/CIN.0000000000000458\n10.1016/j.addbeh.2018.02.017\n10.1037/adb0000160\n10.1007/s10508-006-9064-0\n10.1556/2006.7.2018.134\n10.1016/j.sexol.2015.09.006\n10.1037/a0035774\n10.3389/fpsyt.2021.623508\n10.1371/journal.pone.0260386\n10.1007/s10508-021-02077-7\n10.3390/jpm11040288\n10.3390/healthcare10050948\n10.1016/j.comppsych.2021.152279\n10.1016/j.ijsu.2020.04.018\n10.15585/mmwr.mm6932a1\n10.1080/10550887.2021.1895962"}
{"title": "Bag of Tricks for Improving Deep Learning Performance on Multimodal Image Classification.", "abstract": "A comprehensive medical image-based diagnosis is usually performed across various image modalities before passing a final decision; hence, designing a deep learning model that can use any medical image modality to diagnose a particular disease is of great interest. The available methods are multi-staged, with many computational bottlenecks in between. This paper presents an improved end-to-end method of multimodal image classification using deep learning models. We present top research methods developed over the years to improve models trained from scratch and transfer learning approaches. We show that when fully trained, a model can first implicitly discriminate the imaging modality and then diagnose the relevant disease. Our developed models were applied to COVID-19 classification from chest X-ray, CT scan, and lung ultrasound image modalities. The model that achieved the highest accuracy correctly maps all input images to their respective modality, then classifies the disease achieving overall 91.07% accuracy.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2022-07-26", "authors": ["Steve AAdeshina", "Adeyinka PAdedigba"], "doi": "10.3390/bioengineering9070312\n10.1111/exd.13777\n10.1109/ACCESS.2020.3016780\n10.3390/diagnostics10080565\n10.1007/s40747-021-00321-0\n10.31083/j.fbl2707198\n10.1101/2020.04.24.20078584\n10.1109/ACCESS.2020.3010287\n10.48550/arXiv.1907.08610\n10.1016/j.ibmed.2021.100034\n10.3390/bioengineering9040161"}
{"title": "A Novel Deep Learning and Ensemble Learning Mechanism for Delta-Type COVID-19 Detection.", "abstract": "Recently, the novel coronavirus disease 2019 (COVID-19) has posed many challenges to the research community by presenting grievous severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) that results in a huge number of mortalities and high morbidities worldwide. Furthermore, the symptoms-based variations in virus type add new challenges for the research and practitioners to combat. COVID-19-infected patients comprise trenchant radiographic visual features, including dry cough, fever, dyspnea, fatigue, etc. Chest X-ray is considered a simple and non-invasive clinical adjutant that performs a key role in the identification of these ocular responses related to COVID-19 infection. Nevertheless, the defined availability of proficient radiologists to understand the X-ray images and the elusive aspects of disease radiographic replies to remnant the biggest bottlenecks in manual diagnosis. To address these issues, the proposed research study presents a hybrid deep learning model for the accurate diagnosing of Delta-type COVID-19 infection using X-ray images. This hybrid model comprises visual geometry group 16 (VGG16) and a support vector machine (SVM), where the VGG16 is accustomed to the identification process, while the SVM is used for the severity-based analysis of the infected people. An overall accuracy rate of 97.37% is recorded for the assumed model. Other performance metrics such as the area under the curve (AUC), precision, F-score, misclassification rate, and confusion matrix are used for validation and analysis purposes. Finally, the applicability of the presumed model is assimilated with other relevant techniques. The high identification rates shine the applicability of the formulated hybrid model in the targeted research domain.", "journal": "Frontiers in public health", "date": "2022-07-26", "authors": ["Habib UllahKhan", "SulaimanKhan", "ShahNazir"], "doi": "10.3389/fpubh.2022.875971\n10.1016/j.compbiomed.2020.103805\n10.1016/j.eswa.2020.114054\n10.1109/INMIC50486.2020.9318212\n10.1007/s10489-020-01902-1\n10.1109/MITP.2020.3036820\n10.1016/j.eswa.2020.113909\n10.1056/NEJMoa2001191\n10.1016/j.ijid.2020.01.009\n10.1056/NEJMc2001468\n10.1016/j.compeleceng.2020.106906\n10.32604/cmc.2021.013878\n10.1007/s10044-021-00970-4\n10.1038/s41598-020-76550-z\n10.1093/jamia/ocaa280\n10.3390/sym12040651\n10.1016/j.engappai.2019.03.021\n10.1016/j.advengsoft.2017.05.014\n10.1016/j.engappai.2020.103541\n10.1038/s41598-020-71294-2\n10.22581/muet1982.2101.14\n10.1177/0020294020964826"}
{"title": "A Deep Learning and Handcrafted Based Computationally Intelligent Technique for Effective COVID-19 Detection from X-ray/CT-scan Imaging.", "abstract": "The world has witnessed dramatic changes because of the advent of COVID19 in the last few days of 2019. During the last more than two years, COVID-19 has badly affected the world in diverse ways. It has not only affected human health and mortality rate but also the economic condition on a global scale. There is an urgent need today to cope with this pandemic and its diverse effects. Medical imaging has revolutionized the treatment of various diseases during the last four decades. Automated detection and classification systems have proven to be of great assistance to the doctors and scientific community for the treatment of various diseases. In this paper, a novel framework for an efficient COVID-19 classification system is proposed which uses the hybrid feature extraction approach. After preprocessing image data, two types of features i.e., deep learning and handcrafted, are extracted. For Deep learning features, two pre-trained models namely ResNet101 and DenseNet201 are used. Handcrafted features are extracted using Weber Local Descriptor (WLD). The Excitation component of WLD is utilized and features are reduced using DCT. Features are extracted from both models, handcrafted features are fused, and significant features are selected using entropy. Experiments have proven the effectiveness of the proposed model. A comprehensive set of experiments have been performed and results are compared with the existing well-known methods. The proposed technique has performed better in terms of accuracy and time.", "journal": "Journal of grid computing", "date": "2022-07-26", "authors": ["MohammedHabib", "MuhammadRamzan", "Sajid AliKhan"], "doi": "10.1007/s10723-022-09615-0\n10.1109/ACCESS.2020.2999468\n10.1007/s11063-018-09976-2\n10.1109/ACCESS.2017.2789324\n10.1007/s10723-020-09506-2\n10.1007/s10723-021-09594-8\n10.1007/s10723-021-09564-0\n10.1016/j.compbiomed.2018.03.016\n10.1007/s10723-020-09513-3\n10.1007/s10723-021-09590-y\n10.1016/j.diii.2020.03.014\n10.1016/j.chaos.2020.109944\n10.1016/j.compbiomed.2021.104453\n10.1016/j.chemolab.2020.104054\n10.1016/j.bspc.2021.102987\n10.1016/j.bbe.2021.05.013\n10.1016/j.compbiomed.2021.104306\n10.1016/j.eswa.2021.115650\n10.1016/j.bspc.2021.102602\n10.1016/j.bspc.2021.102588\n10.1016/j.clinimag.2021.07.004\n10.1016/j.iot.2021.100377\n10.1016/j.asoc.2021.107184\n10.1016/j.eswa.2021.114883\n10.1007/s10489-021-02393-4\n10.1007/s10489-020-01867-1\n10.1007/s11042-021-11192-5\n10.1023/B:VLSI.0000028532.53893.82\n10.1109/TPAMI.2009.155\n10.1109/T-C.1974.223784\n10.1109/TNNLS.2020.2966319\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1007/s13246-020-00952-6\n10.1109/ACCESS.2020.2994762"}
{"title": "Artificial intelligence enabled non-invasive T-ray imaging technique for early detection of coronavirus infected patients.", "abstract": "A new artificial intelligence (AI) supported T-Ray imaging system designed and implemented for non-invasive and non-ionizing screening for coronavirus-affected patients. The new system has the potential to replace the standard conventional X-Ray based imaging modality of virus detection. This research article reports the development of solid state room temperature terahertz source for thermograph study. Exposure time and radiation energy are optimized through several real-time experiments. During its incubation period, Coronavirus stays within the cell of the upper respiratory tract and its presence often causes an increased level of blood supply to the virus-affected cells/inter-cellular region that results in a localized increase of water content in those cells & tissues in comparison to its neighbouring normal cells. Under THz-radiation exposure, the incident energy gets absorbed more in virus-affected cells/inter-cellular region and gets heated; thus, the sharp temperature gradient is observed in the corresponding thermograph study. Additionally, structural changes in virus-affected zones make a significant contribution in getting better contrast in thermographs. Considering the effectiveness of the Artificial Intelligence (AI) analysis tool in various medical diagnoses, the authors have employed an explainable AI-assisted methodology to correctly identify and mark the affected pulmonary region for the developed imaging technique and thus validate the model. This AI-enabled non-ionizing THz-thermography method is expected to address the voids in early COVID diagnosis, at the onset of infection.", "journal": "Informatics in medicine unlocked", "date": "2022-07-26", "authors": ["SwarnavaBiswas", "SaikatAdhikari", "RiddhiChawla", "NiladriMaiti", "DineshBhatia", "PranjalPhukan", "MoumitaMukherjee"], "doi": "10.1016/j.imu.2022.101025\n10.17762/ijnpme.v7i03.66\n10.1016/j.aci.2018.08.003"}
{"title": "An efficient deep learning-based framework for tuberculosis detection using chest X-ray images.", "abstract": "Early diagnosis of tuberculosis (TB) is an essential and challenging task to prevent disease, decrease mortality risk, and stop transmission to other people. The chest X-ray (CXR) is the top choice for lung disease screening in clinics because it is cost-effective and easily accessible in most countries. However, manual screening of CXR images is a heavy burden for radiologists, resulting in a high rate of inter-observer variances. Hence, proposing a cost-effective and accurate computer aided diagnosis (CAD) system for TB diagnosis is challenging for researchers. In this research, we proposed an efficient and straightforward deep learning network called TBXNet, which can accurately classify a large number of TB CXR images. The network is based on five dual convolutions blocks with varying filter sizes of 32, 64, 128, 256 and 512, respectively. The dual convolution blocks are fused with a pre-trained layer in the fusion layer of the network. In addition, the pre-trained layer is utilized for transferring pre-trained knowledge into the fusion layer. The proposed TBXNet has achieved an accuracy of 98.98%, and 99.17% on Dataset A and Dataset B, respectively. Furthermore, the generalizability of the proposed work is validated against Dataset C, which is based on normal, tuberculous, pneumonia, and COVID-19 CXR images. The TBXNet has obtained the highest results in Precision (95.67%), Recall (95.10%), F1-score (95.38%), and Accuracy (95.10%), which is comparatively better than all other state-of-the-art methods.", "journal": "Tuberculosis (Edinburgh, Scotland)", "date": "2022-07-26", "authors": ["AhmedIqbal", "MuhammadUsman", "ZohairAhmed"], "doi": "10.1016/j.tube.2022.102234"}
{"title": "Multi-population generalizability of a deep learning-based chest radiograph severity score for COVID-19.", "abstract": "To tune and test the generalizability of a deep learning-based model for assessment of COVID-19 lung disease severity on chest radiographs (CXRs) from different patient populations. A published convolutional Siamese neural network-based model previously trained on hospitalized patients with COVID-19 was tuned using 250 outpatient CXRs. This model produces a quantitative measure of COVID-19 lung disease severity (pulmonary x-ray severity (PXS) score). The model was evaluated on CXRs from 4 test sets, including 3 from the United States (patients hospitalized at an academic medical center (N = 154), patients hospitalized at a community hospital (N = 113), and outpatients (N = 108)) and 1 from Brazil (patients at an academic medical center emergency department (N = 303)). Radiologists from both countries independently assigned reference standard CXR severity scores, which were correlated with the PXS scores as a measure of model performance (Pearson R). The Uniform Manifold Approximation and Projection (UMAP) technique was used to visualize the neural network results. Tuning the deep learning model with outpatient data showed high model performance in 2 United States hospitalized patient datasets (R = 0.88 and R = 0.90, compared to baseline R = 0.86). Model performance was similar, though slightly lower, when tested on the United States outpatient and Brazil emergency department datasets (R = 0.86 and R = 0.85, respectively). UMAP showed that the model learned disease severity information that generalized across test sets. A deep learning model that extracts a COVID-19 severity score on CXRs showed generalizable performance across multiple populations from 2 continents, including outpatients and hospitalized patients.", "journal": "Medicine", "date": "2022-07-23", "authors": ["Matthew DLi", "Nishanth TArun", "MehakAggarwal", "SharutGupta", "PraveerSingh", "Brent PLittle", "Dexter PMendoza", "Gustavo C ACorradi", "Marcelo STakahashi", "Suely FFerraciolli", "Marc DSucci", "MinLang", "Bernardo CBizzo", "IttaiDayan", "Felipe CKitamura", "JayashreeKalpathy-Cramer"], "doi": "10.1097/MD.0000000000029587"}
{"title": "COVIDx-US: An Open-Access Benchmark Dataset of Ultrasound Imaging Data for AI-Driven COVID-19 Analytics.", "abstract": "The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. Apart from the global health crises, the pandemic has also caused significant economic and financial difficulties and socio-physiological implications. Effective screening, triage, treatment planning, and prognostication of outcome play a key role in controlling the pandemic. Recent studies have highlighted the role of point-of-care ultrasound imaging for COVID-19 screening and prognosis, particularly given that it is non-invasive, globally available, and easy-to-sanitize. COVIDx-US Dataset: Motivated by these attributes and the promise of artificial intelligence tools to aid clinicians, we introduce COVIDx-US, an open-access benchmark dataset of COVID-19 related ultrasound imaging data. The COVIDx-US dataset was curated from multiple data sources and its current version, i.e., v1.5., consists of 173 ultrasound videos and 21,570 processed images across 147 patients with COVID-19 infection, non-COVID-19 infection, other lung diseases/conditions, as well as normal control cases.\nThe COVIDx-US dataset was released as part of a large open-source initiative, the COVID-Net initiative, and will be continuously growing, as more data sources become available. To the best of the authors' knowledge, COVIDx-US is the first and largest open-access fully-curated benchmark lung ultrasound imaging dataset that contains a standardized and unified lung ultrasound score per video file, providing better interpretation while enabling other research avenues such as severity assessment. In addition, the dataset is reproducible, easy-to-use, and easy-to-scale thanks to the well-documented modular design.", "journal": "Frontiers in bioscience (Landmark edition)", "date": "2022-07-23", "authors": ["AshkanEbadi", "PengchengXi", "AlexanderMacLean", "AdrianFlorea", "St\u00e9phaneTremblay", "SonnyKohli", "AlexanderWong"], "doi": "10.31083/j.fbl2707198"}
{"title": "Ftl-CoV19: A Transfer Learning Approach to Detect COVID-19.", "abstract": "COVID-19 is an infectious and contagious disease caused by the new coronavirus. The total number of cases is over 19 million and continues to grow. A common symptom noticed among COVID-19 patients is lung infection that results in breathlessness, and the lack of essential resources such as testing, oxygen, and ventilators enhances its severity. Chest X-ray can be used to design and develop a COVID-19 detection mechanism for a quicker diagnosis using AI and machine learning techniques. Due to this silver lining, various new COVID-19 detection techniques and prediction models have been introduced in recent times based on chest radiography images. However, due to a high level of unpredictability and the absence of essential data, standard models have showcased low efficiency and also suffer from overheads and complexities. This paper proposes a model fine tuning transfer learning-coronavirus 19 (Ftl-CoV19) for COVID-19 detection through chest X-rays, which embraces the ideas of transfer learning in pretrained VGG16 model with including combination of convolution, max pooling, and dense layer at different stages of model. Ftl-CoV19 reported promising experimental results; it observed training and validation accuracy of 98.82% and 99.27% with precision of 100%, recall of 98%, and F1 score of 99%. These results outperformed other conventional state of arts such as CNN, ResNet50, InceptionV3, and Xception.", "journal": "Computational intelligence and neuroscience", "date": "2022-07-23", "authors": ["TarishiSingh", "PraneetSaurabh", "DhananjayBisen", "LalitKane", "MayankPathak", "G RSinha"], "doi": "10.1155/2022/1953992\n10.15557/pimr.2020.0024\n10.1016/j.genrep.2020.100756\n10.1109/TAI.2021.3062771\n10.1109/tmi.2020.2995508\n10.1109/ACCESS.2020.2997311\n10.1109/RBME.2020.2987975\n10.1109/CANDO-EPE51100.2020.9337794\n10.1109/TCYB.2019.2950779\n10.1016/j.numecd.2020.07.031\n10.1155/2020/9756518\n10.1101/2020.10.13.20212035\n10.1101/2020.10.13.20212035\n10.1109/mpuls.2020.3008354\n10.1109/access.2020.3009328\n10.1109/tmi.2020.2993291\n10.1109/tmi.2020.2995965\n10.1109/ACCESS.2018.2814605\n10.1016/j.imu.2020.100360\n10.1109/ACCESS.2019.2946000\n10.17632/9xkhgts2s6.1\n10.1007/s10489-020-01826-w\n10.48550/arXiv.1409.1556\n10.3390/s19163556\n10.1007/s11042-020-10038-w"}
{"title": "Automated diagnosis and prognosis of COVID-19 pneumonia from initial ER chest X-rays using deep learning.", "abstract": "Airspace disease as seen on chest X-rays is an important point in triage for patients initially presenting to the emergency department with suspected COVID-19 infection. The purpose of this study is to evaluate a previously trained interpretable deep learning algorithm for the diagnosis and prognosis of COVID-19 pneumonia from chest X-rays obtained in the ED.\nThis retrospective study included 2456 (50% RT-PCR positive for COVID-19) adult patients who received both a chest X-ray and SARS-CoV-2 RT-PCR test from January 2020 to March of 2021 in the emergency department at a single U.S.\nA total of 2000 patients were included as an additional training cohort and 456 patients in the randomized internal holdout testing cohort for a previously trained Siemens AI-Radiology Companion deep learning convolutional neural network algorithm. Three cardiothoracic fellowship-trained radiologists systematically evaluated each chest X-ray and generated an airspace disease area-based severity score which was compared against the same score produced by artificial intelligence. The interobserver agreement, diagnostic accuracy, and predictive capability for inpatient outcomes were assessed. Principal statistical tests used in this study include both univariate and multivariate logistic regression.\nOverall ICC was 0.820 (95% CI 0.790-0.840). The diagnostic AUC for SARS-CoV-2 RT-PCR positivity was 0.890 (95% CI 0.861-0.920) for the neural network and 0.936 (95% CI 0.918-0.960) for radiologists. Airspace opacities score by AI alone predicted ICU admission (AUC\u2009=\u20090.870) and mortality (0.829) in all patients. Addition of age and BMI into a multivariate log model improved mortality prediction (AUC\u2009=\u20090.906).\nThe deep learning algorithm provides an accurate and interpretable assessment of the disease burden in COVID-19 pneumonia on chest radiographs. The reported severity scores correlate with expert assessment and accurately predicts important clinical outcomes. The algorithm contributes additional prognostic information not currently incorporated into patient management.", "journal": "BMC infectious diseases", "date": "2022-07-22", "authors": ["Jordan HChamberlin", "GilbertoAquino", "SophiaNance", "AndrewWortham", "NathanLeaphart", "NamrataPaladugu", "SeanBrady", "HenryBaird", "MatthewFiegel", "LoganFitzpatrick", "MadisonKocher", "FlorinGhesu", "AwaisMansoor", "PhilippHoelzer", "MathisZimmermann", "W EnnisJames", "D JamesonDennis", "Brian AHouston", "Ismail MKabakus", "DhirajBaruah", "U JosephSchoepf", "Jeremy RBurt"], "doi": "10.1186/s12879-022-07617-7\n10.1136/bmj.m2426\n10.1007/s11547-020-01232-9\n10.1016/j.ijid.2020.05.021\n10.1186/s41747-020-00195-w\n10.1148/radiol.2021219021\n10.1148/ryct.2020200028\n10.1186/s43055-020-00296-x\n10.1148/ryct.2020200337\n10.1148/radiol.2021219022\n10.1136/bmj.m1328\n10.1016/j.jiph.2020.06.028\n10.1109/RBME.2020.2987975\n10.1038/s41598-021-93719-2\n10.1038/s42256-021-00307-0\n10.1148/radiol.2020202944\n10.1148/ryai.2020200079\n10.1371/journal.pone.0236621\n10.1148/ryct.2020200280\n10.1148/ryai.2020200029\n10.1001/jamanetworkopen.2021.41096\n10.1109/TPAMI.2018.2858826\n10.1371/journal.pmed.1002707\n10.1016/j.chest.2020.04.003\n10.1148/ryai.2020190043\n10.1016/j.compbiomed.2021.104665\n10.18280/ts.370313"}
{"title": "Human versus Artificial Intelligence-Based Echocardiographic Analysis as a Predictor of Outcomes: An Analysis from the World Alliance Societies of Echocardiography COVID Study.", "abstract": "Transthoracic echocardiography is the leading cardiac imaging modality for patients admitted with COVID-19, a condition of high short-term mortality. The aim of this study was to test the hypothesis that artificial intelligence (AI)-based analysis of echocardiographic images could predict mortality more accurately than conventional analysis by a human expert.\nPatients admitted to 13 hospitals for acute COVID-19 who underwent transthoracic echocardiography were included. Left ventricular ejection fraction (LVEF) and left ventricular longitudinal strain (LVLS) were obtained manually by multiple expert readers and by automated AI software. The ability of the manual and AI analyses to predict all-cause mortality was compared.\nIn total, 870 patients were enrolled. The mortality rate was 27.4% after a mean follow-up period of 230\u00a0\u00b1\u00a0115\u00a0days. AI analysis had lower variability than manual analysis for both LVEF (P\u00a0=\u00a0.003) and LVLS (P\u00a0=\u00a0.005). AI-derived LVEF and LVLS were predictors of mortality in univariable and multivariable regression analysis (odds ratio, 0.974 [95% CI, 0.956-0.991; P\u00a0=\u00a0.003] for LVEF; odds ratio, 1.060 [95% CI, 1.019-1.105; P\u00a0=\u00a0.004] for LVLS), but LVEF and LVLS obtained by manual analysis were not. Direct comparison of the predictive value of AI versus manual measurements of LVEF and LVLS showed that AI was significantly better (P\u00a0=\u00a0.005 and P\u00a0=\u00a0.003, respectively). In addition, AI-derived LVEF and LVLS had more significant and stronger correlations to other objective biomarkers of acute disease than manual reads.\nAI-based analysis of LVEF and LVLS had similar feasibility as manual analysis, minimized variability, and consequently increased the statistical power to predict mortality. AI-based, but not manual, analyses were a significant predictor of in-hospital and follow-up mortality.", "journal": "Journal of the American Society of Echocardiography : official publication of the American Society of Echocardiography", "date": "2022-07-22", "authors": ["Federico MAsch", "TineDescamps", "RizwanSarwar", "IlyaKaragodin", "Cristiane CarvalhoSingulane", "MingxingXie", "Edwin STucay", "Ana CTude Rodrigues", "Zuilma YVasquez-Ortiz", "Mark JMonaghan", "Bayardo AOrdonez Salazar", "LaurieSoulat-Dufour", "AzinAlizadehasl", "AtoosaMostafavi", "AntonellaMoreo", "RodolfoCitro", "AkhilNarang", "ChunWu", "KarimaAddetia", "RossUpton", "Gary MWoodward", "Roberto MLang", "NoneNone"], "doi": "10.1016/j.echo.2022.07.004"}
{"title": "Resting-state functional connectome predicts individual differences in depression during COVID-19 pandemic.", "abstract": "Stressful life events are significant risk factors for depression, and increases in depressive symptoms have been observed during the COVID-19 pandemic. The aim of this study is to explore the neural makers for individuals' depression during COVID-19, using connectome-based predictive modeling (CPM). Then we tested whether these neural markers could be used to identify groups at high/low risk for depression with a longitudinal dataset. The results suggested that the high-risk group demonstrated a higher level and increment of depression during the pandemic, as compared to the low-risk group. Furthermore, a support vector machine (SVM) algorithm was used to discriminate major depression disorder patients and healthy controls, using neural features defined by CPM. The results confirmed the CPM's ability for capturing the depression-related patterns with individuals' resting-state functional connectivity signature. The exploration for the anatomy of these functional connectivity features emphasized the role of an emotion-regulation circuit and an interoception circuit in the neuropathology of depression. In summary, the present study augments current understanding of potential pathological mechanisms underlying depression during an acute and unpredictable life-threatening event and suggests that resting-state functional connectivity may provide potential effective neural markers for identifying susceptible populations. (PsycInfo Database Record (c) 2022 APA, all rights reserved).", "journal": "The American psychologist", "date": "2022-07-22", "authors": ["YuMao", "QunlinChen", "DongtaoWei", "WenjingYang", "JiangzhouSun", "YaxuYu", "KaixiangZhuang", "XiaoqinWang", "LiHe", "TingyongFeng", "XuLei", "QinghuaHe", "HongChen", "ShukaiDuan", "JiangQiu"], "doi": "10.1037/amp0001031"}
{"title": "Two-stage hybrid network for segmentation of COVID-19 pneumonia lesions in CT images: a multicenter study.", "abstract": "COVID-19 has been spreading continuously since its outbreak, and the detection of its manifestations in the lung via chest computed tomography (CT) imaging is essential to investigate the diagnosis and prognosis of COVID-19 as an indispensable step. Automatic and accurate segmentation of infected lesions is highly required for fast and accurate diagnosis and further assessment of COVID-19 pneumonia. However, the two-dimensional methods generally neglect the intraslice context, while the three-dimensional methods usually have high GPU memory consumption and calculation cost. To address these limitations, we propose a two-stage hybrid UNet to automatically segment infected regions, which is evaluated on the multicenter data obtained from seven hospitals. Moreover, we train a 3D-ResNet for COVID-19 pneumonia screening. In segmentation tasks, the Dice coefficient reaches 97.23% for lung segmentation and 84.58% for lesion segmentation. In classification tasks, our model can identify COVID-19 pneumonia with an area under the receiver-operating characteristic curve value of 0.92, an accuracy of 92.44%, a sensitivity of 93.94%, and a specificity of 92.45%. In comparison with other state-of-the-art methods, the proposed approach could be implemented as an efficient assisting tool for radiologists in COVID-19 diagnosis from CT images.", "journal": "Medical & biological engineering & computing", "date": "2022-07-21", "authors": ["YaxinShang", "ZechenWei", "HuiHui", "XiaohuLi", "LiangLi", "YongqiangYu", "LigongLu", "LiLi", "HongjunLi", "QiYang", "MeiyunWang", "MeixiaoZhan", "WeiWang", "GuanghaoZhang", "XiangjunWu", "LiWang", "JieLiu", "JieTian", "YunfeiZha"], "doi": "10.1007/s11517-022-02619-8\n10.1016/j.micinf.2020.02.002\n10.1080/20477724.2020.1725339\n10.1148/radiol.2020200230\n10.1007/s11517-020-02299-2\n10.1109/ACCESS.2020.3001973\n10.1109/JBHI.2020.3034296\n10.1016/j.jare.2020.03.005\n10.1148/ryct.2020200075\n10.1016/j.compbiomed.2020.104037\n10.1109/TMI.2020.3001810\n10.1109/TMI.2018.2845918\n10.1007/978-3-030-00889-5_1\n10.1109/TMI.2019.2959609\n10.1007/s00521-005-0467-y\n10.1021/acs.jmedchem.9b02147\n10.1016/j.patrec.2019.03.022\n10.1016/j.measurement.2018.11.006\n10.1080/07038992.1998.10874685\n10.1109/TPAMI.2016.2572683\n10.1109/TPAMI.2016.2644615"}
{"title": "Development and external validation of a deep learning-based computed tomography classification system for COVID-19.", "abstract": "We aimed to develop and externally validate a novel machine learning model that can classify CT image findings as positive or negative for SARS-CoV-2 reverse transcription polymerase chain reaction (RT-PCR).\nWe used 2,928 images from a wide variety of case-control type data sources for the development and internal validation of the machine learning model. A total of 633 COVID-19 cases and 2,295 non-COVID-19 cases were included in the study. We randomly divided cases into training and tuning sets at a ratio of 8:2. For external validation, we used 893 images from 740 consecutive patients at 11 acute care hospitals suspected of having COVID-19 at the time of diagnosis. The dataset included 343 COVID-19 patients. The reference standard was RT-PCR.\nIn external validation, the sensitivity and specificity of the model were 0.869 and 0.432, at the low-level cutoff, 0.724 and 0.721, at the high-level cutoff. Area under the receiver operating characteristic was 0.76.\nOur machine learning model exhibited a high sensitivity in external validation datasets and may assist physicians to rule out COVID-19 diagnosis in a timely manner at emergency departments. Further studies are warranted to improve model specificity.", "journal": "Annals of clinical epidemiology", "date": "2022-07-08", "authors": ["YukiKataoka", "TomohisaBaba", "TatsuyoshiIkenoue", "YoshinoriMatsuoka", "JunichiMatsumoto", "JunjiKumasawa", "KentaroTochitani", "HirakuFunakoshi", "TomohiroHosoda", "AikoKugimiya", "MichinoriShirano", "FumikoHamabe", "SachiyoIwata", "YoshiroKitamura", "TsubasaGoto", "ShingoHamaguchi", "TakafumiHaraguchi", "ShungoYamamoto", "HiromitsuSumikawa", "KojiNishida", "HarukaNishida", "KoichiAriyoshi", "HiroakiSugiura", "HidenoriNakagawa", "TomohiroAsaoka", "NaofumiYoshida", "RentaroOda", "TakashiKoyama", "YuiIwai", "YoshihiroMiyashita", "KoyaOkazaki", "KiminobuTanizawa", "TomohiroHanda", "ShojiKido", "ShingoFukuma", "NoriyukiTomiyama", "ToyohiroHirai", "TakashiOgura"], "doi": "10.37737/ace.22014\n10.1148/radiol.2020200642\n10.4103/ijri.IJRI_739_20\n10.1016/j.cell.2020.04.045\n10.1038/s41598-021-93832-2\n10.1016/j.imu.2020.100427\n10.1148/radiol.2020200905\n10.1183/13993003.00775-2020\n10.1038/s42256-021-00307-0\n10.1016/j.media.2021.102125\n10.7326/M14-0697\n10.7937/TCIA.2020.GQRY-NC81\n10.1007/s10278-013-9622-7\n10.1056/NEJMoa1102873\n10.7937/tcia.2020.6c7y-gq39\n10.7937/K9/TCIA.2015.PF0M9REI\n10.1371/journal.pone.0258760.\n10.21203/rs.3.rs-979599/v1\n10.1111/bcp.12531\n10.1148/ryct.2020200152\n10.1148/rg.2020200159\n10.1164/rccm.201501-0017OC\n10.1016/j.chest.2020.04.003\n10.1056/NEJMoa2107934\n10.1056/NEJMoa2118542\n10.1186/s40249-021-00910-8\n10.21037/atm-21-5571\n10.1016/j.bjid.2021.101541\n10.2214/AJR.21.25640"}
{"title": "Value and prognostic impact of a deep learning segmentation model of COVID-19 lung lesions on low-dose chest CT.", "abstract": "1) To develop a deep learning (DL) pipeline allowing quantification of COVID-19 pulmonary lesions on low-dose computed tomography (LDCT). 2) To assess the prognostic value of DL-driven lesion quantification.\nThis monocentric retrospective study included training and test datasets taken from 144 and 30 patients, respectively. The reference was the manual segmentation of 3 labels: normal lung, ground-glass opacity(GGO) and consolidation(Cons). Model performance was evaluated with technical metrics, disease volume and extent. Intra- and interobserver agreement were recorded. The prognostic value of DL-driven disease extent was assessed in 1621 distinct patients using C-statistics. The end point was a combined outcome defined as death, hospitalization>10 days, intensive care unit hospitalization or oxygen therapy.\nThe Dice coefficients for lesion (GGO+Cons) segmentations were 0.75\u00b10.08, exceeding the values for human interobserver (0.70\u00b10.08; 0.70\u00b10.10) and intraobserver measures (0.72\u00b10.09). DL-driven lesion quantification had a stronger correlation with the reference than inter- or intraobserver measures. After stepwise selection and adjustment for clinical characteristics, quantification significantly increased the prognostic accuracy of the model (0.82 vs. 0.90; \nA DL-driven model can provide reproducible and accurate segmentation of COVID-19 lesions on LDCT. Automatic lesion quantification has independent prognostic value for the identification of high-risk patients.", "journal": "Research in diagnostic and interventional imaging", "date": "2022-03-01", "authors": ["AxelBartoli", "JorisFournel", "ArnaudMaurin", "BaptisteMarchi", "PaulHabert", "MaximeCastelli", "Jean-YvesGaubert", "SebastienCortaredona", "Jean-ChristopheLagier", "MatthieuMillion", "DidierRaoult", "BadihGhattas", "AlexisJacquier"], "doi": "10.1016/j.redii.2022.100003\n10.1056/NEJMoa2001017\n10.1056/NEJMp2000929\n10.1186/s40779-020-00246-8\n10.1148/radiol.2020200230\n10.1148/radiol.2020200432\n10.1016/j.diii.2020.06.001\n10.1016/j.diii.2020.03.014\n10.1097/RLI.0000000000000672\n10.1371/journal.pone.0230548\n10.1016/j.media.2017.07.005\n10.1183/09031936.00071812\n10.1016/j.compbiomed.2018.10.033\n10.1016/j.tmaid.2020.101738\n10.1016/j.tmaid.2020.101663\n10.1016/j.tmaid.2020.101632\n10.1148/ryct.2020200047\n10.1016/j.mri.2012.05.001\n10.1148/radiol.2462070712\n10.1148/radiol.2017171920\n10.1109/TMI.2020.3000314\n10.1016/j.patcog.2021.108109\n10.1016/j.diii.2020.03.010\n10.1148/radiol.2020200905\n10.1101/2020.04.17.20069187\n10.1016/j.diii.2020.05.003\n10.21203/rs.3.rs-571332/v1\n10.1016/j.media.2021.101992\n10.1109/TMI.2020.2996645\n10.1016/j.cmpb.2021.106004\n10.1007/s11547-020-01195-x\n10.7150/thno.45985\n10.1016/j.compbiomed.2017.11.013"}
{"title": "FocusCovid: automated COVID-19 detection using deep learning with chest X-ray images.", "abstract": "COVID-19 is an acronym for coronavirus disease 2019. Initially, it was called 2019-nCoV, and later International Committee on Taxonomy of Viruses (ICTV) termed it SARS-CoV-2. On 30th January 2020, the World Health Organization (WHO) declared it a pandemic. With an increasing number of COVID-19 cases, the available medical infrastructure is essential to detect the suspected cases. Medical imaging techniques such as Computed Tomography (CT), chest radiography can play an important role in the early screening and detection of COVID-19 cases. It is important to identify and separate the cases to stop the further spread of the virus. Artificial Intelligence can play an important role in COVID-19 detection and decreases the workload on collapsing medical infrastructure. In this paper, a deep convolutional neural network-based architecture is proposed for the COVID-19 detection using chest radiographs. The dataset used to train and test the model is available on different public repositories. Despite having the high accuracy of the model, the decision on COVID-19 should be made in consultation with the trained medical clinician.", "journal": "Evolving systems", "date": "2022-01-01", "authors": ["TarunAgrawal", "PrakashChoudhary"], "doi": "10.1007/s12530-021-09385-2\n10.1007/s10489-020-01826-w\n10.1148/radiol.2020200642\n10.1007/s40846-020-00529-4\n10.1007/s13246-020-00865-4\n10.1007/s11548-019-01917-1\n10.1613/jair.953\n10.1109/TBME.2012.2226583\n10.1118/1.3561504\n10.1109/ACCESS.2020.3010287\n10.1118/1.3013555\n10.1016/j.earlhumdev.2020.105026\n10.1056/NEJMoa2002032\n10.1016/j.eswa.2017.11.028\n10.1016/S0140-6736(20)30183-5\n10.1016/j.imu.2020.100412\n10.1148/radiol.2020200527\n10.1148/radiol.2017162326\n10.1038/nature14539\n10.1016/j.media.2017.07.005\n10.1016/j.asoc.2020.106580\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.109944\n10.1016/S1473-3099(20)30086-4\n10.1016/j.chaos.2020.110122\n10.1007/s00264-019-04466-z\n10.1038/s41598-019-56847-4\n10.1016/S2213-2600(20)30076-X"}
{"title": "Development of smart camera systems based on artificial intelligence network for social distance detection to fight against COVID-19.", "abstract": "In this work, an artificial intelligence network-based smart camera system prototype, which tracks social distance using a bird's-eye perspective, has been developed. \"MobileNet SSD-v3\", \"Faster-R-CNN Inception-v2\", \"Faster-R-CNN ResNet-50\" models have been utilized to identify people in video sequences. The final prototype based on the Faster R-CNN model is an integrated embedded system that detects social distance with the camera. The software developed using the \"Nvidia Jetson Nano\" development kit and Raspberry Pi camera module calculates all necessary actions in itself, detects social distance violations, makes audible and light warnings, and reports the results to the server. It is predicted that the developed smart camera prototype can be integrated into public spaces within the \"sustainable smart cities,\" the scope that the world is on the verge of a change.", "journal": "Applied soft computing", "date": "2021-10-01", "authors": ["OnurKaraman", "AdiAlhudhaif", "KemalPolat"], "doi": "10.1016/j.asoc.2021.107610\n10.1016/j.ijantimicag.2020.105951\n10.1007/s11427-020-1637-5\n10.1001/jama.2020.1585\n10.1056/NEJMoa2001191\n10.3906/sag-2004-172\n10.1542/peds.2020-0702\n10.3345/cep.2020.00493\n10.1016/S0140-6736(20)30313-5\n10.1136/bmj.m1066\n10.1016/S0140-6736(20)30679-6\n10.1093/jtm/taaa039\n10.1093/jtm/taaa020\n10.1136/bmj.m3223\n10.1016/j.neucom.2018.01.092\n10.1016/j.scitotenv.2020.138858\n10.3390/make1030044\n10.1016/j.procs.2018.10.335\n10.1145/2184319.2184337\n10.1016/j.sysarc.2020.101896\n10.1093/eurpub/ckn107"}
{"title": "Machine learning for medical imaging-based COVID-19 detection and diagnosis.", "abstract": "The novel coronavirus disease 2019 (COVID-19) is considered to be a significant health challenge worldwide because of its rapid human-to-human transmission, leading to a rise in the number of infected people and deaths. The detection of COVID-19 at the earliest stage is therefore of paramount importance for controlling the pandemic spread and reducing the mortality rate. The real-time reverse transcription-polymerase chain reaction, the primary method of diagnosis for coronavirus infection, has a relatively high false negative rate while detecting early stage disease. Meanwhile, the manifestations of COVID-19, as seen through medical imaging methods such as computed tomography (CT), radiograph (X-ray), and ultrasound imaging, show individual characteristics that differ from those of healthy cases or other types of pneumonia. Machine learning (ML) applications for COVID-19 diagnosis, detection, and the assessment of disease severity based on medical imaging have gained considerable attention. Herein, we review the recent progress of ML in COVID-19 detection with a particular focus on ML models using CT and X-ray images published in high-ranking journals, including a discussion of the predominant features of medical imaging in patients with COVID-19. Deep Learning algorithms, particularly convolutional neural networks, have been utilized widely for image segmentation and classification to identify patients with COVID-19 and many ML modules have achieved remarkable predictive results using datasets with limited sample sizes.", "journal": "International journal of intelligent systems", "date": "2021-09-01", "authors": ["RokayaRehouma", "MichaelBuchert", "Yi-Ping PhoebeChen"], "doi": "10.1002/int.22504"}
{"title": "Learning to learn by yourself: Unsupervised meta-learning with self-knowledge distillation for COVID-19 diagnosis from pneumonia cases.", "abstract": "The goal of diagnosing the coronavirus disease 2019 (COVID-19) from suspected pneumonia cases, that is, recognizing COVID-19 from chest X-ray or computed tomography (CT) images, is to improve diagnostic accuracy, leading to faster intervention. The most important and challenging problem here is to design an effective and robust diagnosis model. To this end, there are three challenges to overcome: (1) The lack of training samples limits the success of existing deep-learning-based methods. (2) Many public COVID-19 data sets contain only a few images without fine-grained labels. (3) Due to the explosive growth of suspected cases, it is ", "journal": "International journal of intelligent systems", "date": "2021-08-01", "authors": ["WenboZheng", "LanYan", "ChaoGou", "Zhi-ChengZhang", "Jun JZhang", "MingHu", "Fei-YueWang"], "doi": "10.1002/int.22449\n10.1038/s41591-020-0874-8\n10.1038/s41586-020-2284-y\n10.1056/NEJMoa2008975\n10.1038/s41586-020-2355-0\n10.1038/s41586-020-2332-7\n10.1056/NEJMoa2007621\n10.1038/s41581-020-0280-y\n10.1038/s41586-020-2293-x\n10.1056/NEJMoa2012410\n10.1109/RBME.2020.2987975\n10.1038/s42256-020-0180-7\n10.1038/s41591-020-0931-3\n10.1038/s41586-020-2196-x\n10.1038/s41467-020-17280-8\n10.1016/j.cell.2020.06.035\n10.4049/jimmunol.1601542\n10.4049/jimmunol.177.4.2138\n10.3390/v11010041\n10.3390/v11020174\n10.1038/s41591-020-0895-3\n10.1038/s41591-020-0897-1\n10.1109/TNNLS.2020.2979745\n10.1109/TNNLS.2019.2957187\n10.1056/NEJMoa2002032\n10.1109/tnnls.2016.2518700\n10.1145/3386252\n10.1002/int.22230\n10.1002/int.21567\n10.3760/cma.j.cn131368-20200409-00266\n10.2214/AJR.15.14445\n10.1001/jama.2019.21118\n10.1183/13993003.01217-2020\n10.1016/j.jaci.2020.05.003\n10.1016/j.cell.2020.06.010\n10.1186/s40164-021-00202-9\n10.1001/jamainternmed.2020.2033\n10.1109/JIOT.2021.3056185\n10.1109/JAS.2020.1003393\n10.1109/TII.2021.3059023\n10.1109/TNNLS.2021.3054306\n10.1109/JBHI.2021.3058293\n10.1109/TII.2021.3057683\n10.1109/TII.2021.3057524\n10.1109/TNNLS.2021.3054746\n10.1016/S2589-7500(19)30108-6\n10.1109/TNNLS.2018.2851924"}
{"title": "Deep learning assisted COVID-19 detection using full CT-scans.", "abstract": "The ongoing pandemic of COVID-19 has shown the limitations of our current medical institutions. There is a need for research in automated diagnosis for speeding up the process while maintaining accuracy and reducing the computational requirements. This work proposes an automated diagnosis of COVID-19 infection from CT scans of the patients using deep learning technique. The proposed model, ", "journal": "Internet of things (Amsterdam, Netherlands)", "date": "2021-06-01", "authors": ["Varan SinghRohila", "NitinGupta", "AmitKaul", "Deepak KumarSharma"], "doi": "10.1016/j.iot.2021.100377\n10.1148/ryct.2020200028"}
{"title": "CovFrameNet: An Enhanced Deep Learning Framework for COVID-19 Detection.", "abstract": "The novel coronavirus, also known as COVID-19, is a pandemic that has weighed heavily on the socio-economic affairs of the world. Research into the production of relevant vaccines is progressively being advanced with the development of the Pfizer and BioNTech, AstraZeneca, Moderna, Sputnik V, Janssen, Sinopharm, Valneva, Novavax and Sanofi Pasteur vaccines. There is, however, a need for a computational intelligence solution approach to mediate the process of facilitating quick detection of the disease. Different computational intelligence methods, which comprise natural language processing, knowledge engineering, and deep learning, have been proposed in the literature to tackle the spread of coronavirus disease. More so, the application of deep learning models have demonstrated an impressive performance compared to other methods. This paper aims to advance the application of deep learning and image pre-processing techniques to characterise and detect novel coronavirus infection. Furthermore, the study proposes a framework named CovFrameNet., which consist of a pipelined image pre-processing method and a deep learning model for feature extraction, classification, and performance measurement. The novelty of this study lies in the design of a CNN architecture that incorporates an enhanced image pre-processing mechanism. The National Institutes of Health (NIH) Chest X-Ray dataset and COVID-19 Radiography database were used to evaluate and validate the effectiveness of the proposed deep learning model. Results obtained revealed that the proposed model achieved an accuracy of 0.1, recall/precision of 0.85, F-measure of 0.9, and specificity of 1.0. Thus, the study's outcome showed that a CNN-based method with image pre-processing capability could be adopted for the pre-screening of suspected COVID-19 cases, and the confirmation of RT-PCR-based detected cases of COVID-19.", "journal": "IEEE access : practical innovations, open solutions", "date": "2021-05-25", "authors": ["Olaide NathanielOyelade", "Absalom El-ShamirEzugwu", "HarunaChiroma"], "doi": "10.1109/ACCESS.2021.3083516\n10.1136/bmj.m1328\n10.1007/s10096-020-03901-z\n10.1080/07391102.2020.1788642\n10.1007/s10489-020-02076-6\n10.1101/2020.05.18.20105577\n10.1016/j.imu.2020.100395\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105581\n10.1007/s10489-020-01829-7\n10.1016/j.imu.2020.100405\n10.1016/j.chaos.2020.110338\n10.1152/physiolgenomics.00029.2020\n10.1016/j.patcog.2020.107700\n10.1007/s40009-020-01009-8\n10.1007/s10489-020-01904-z\n10.1007/s10489-020-02010-w\n10.1007/s13246-020-00934-8\n10.1148/radiol.2020200432\n10.1109/CVPR.2017.369\n10.1186/s40537-019-0197-0\n10.1109/RTEICT.2016.7808140\n10.1088/1742-6596/762/1/012035\n10.3390/sym10110648\n10.3389/fmed.2020.00427\n10.1016/j.chaos.2020.109944\n10.1186/s12890-020-01286-5\n10.22059/jitm.2020.79187\n10.32604/cmc.2021.014265\n10.1007/s00500-020-05424-3\n10.32604/cmc.2021.012955\n10.1109/JIOT.2021.3050775\n10.1109/ACCESS.2020.2995597\n10.32604/cmc.2021.012874"}
{"title": "CovSegNet: A Multi Encoder-Decoder Architecture for Improved Lesion Segmentation of COVID-19 Chest CT Scans.", "abstract": "Automatic lung lesion segmentation of chest computer tomography (CT) scans is considered a pivotal stage toward accurate diagnosis and severity measurement of COVID-19. Traditional U-shaped encoder-decoder architecture and its variants suffer from diminutions of contextual information in pooling/upsampling operations with increased semantic gaps among encoded and decoded feature maps as well as instigate vanishing gradient problems for its sequential gradient propagation that result in suboptimal performance. Moreover, operating with 3-D CT volume poses further limitations due to the exponential increase of computational complexity making the optimization difficult. In this article, an automated COVID-19 lesion segmentation scheme is proposed utilizing a highly efficient neural network architecture, namely CovSegNet, to overcome these limitations. Additionally, a two-phase training scheme is introduced where a deeper 2-D network is employed for generating region-of-interest (ROI)-enhanced CT volume followed by a shallower 3-D network for further enhancement with more contextual information without increasing computational burden. Along with the traditional vertical expansion of Unet, we have introduced horizontal expansion with multistage encoder-decoder modules for achieving optimum performance. Additionally, multiscale feature maps are integrated into the scale transition process to overcome the loss of contextual information. Moreover, a multiscale fusion module is introduced with a pyramid fusion scheme to reduce the semantic gaps between subsequent encoder/decoder modules while facilitating the parallel optimization for efficient gradient propagation. Outstanding performances have been achieved in three publicly available datasets that largely outperform other state-of-the-art approaches. The proposed scheme can be easily extended for achieving optimum segmentation performances in a wide variety of applications. ", "journal": "IEEE transactions on artificial intelligence", "date": "2021-03-15", "authors": ["TanvirMahmud", "Md AwsafurRahman", "Shaikh AnowarulFattah", "Sun-YuanKung"], "doi": "10.1109/TAI.2021.3064913"}
{"title": "COVID-19: Automatic Detection of the Novel Coronavirus Disease From CT Images Using an Optimized Convolutional Neural Network.", "abstract": "It is widely known that a quick disclosure of the COVID-19 can help to reduce its spread dramatically. Transcriptase polymerase chain reaction could be a more useful, rapid, and trustworthy technique for the evaluation and classification of the COVID-19 disease. Currently, a computerized method for classifying computed tomography (CT) images of chests can be crucial for speeding up the detection while the COVID-19 epidemic is rapidly spreading. In this article, the authors have proposed an optimized convolutional neural network model (ADECO-CNN) to divide infected and not infected patients. Furthermore, the ADECO-CNN approach is compared with pretrained convolutional neural network (CNN)-based VGG19, GoogleNet, and ResNet models. Extensive analysis proved that the ADECO-CNN-optimized CNN model can classify CT images with 99.99% accuracy, 99.96% sensitivity, 99.92% precision, and 99.97% specificity.", "journal": "IEEE transactions on industrial informatics", "date": "2021-02-05", "authors": ["AnielloCastiglione", "PandiVijayakumar", "MicheleNappi", "SaimaSadiq", "MuhammadUmer"], "doi": "10.1109/TII.2021.3057524\n10.1016/j.micinf.2020.01.003\n10.1186/s40779-020-0233-6\n10.1109/TMI.2020.3040950\n10.1007/s10916-017-0861-x\n10.1002/mp.14609\n10.1007/s00330-021-07715-1\n10.1101/2020.04.24.20078584\n10.5555/3015812.3015979\n10.1109/TCBB.2021.3065361"}
{"title": "COVID-19 Chest CT Image Segmentation Network by Multi-Scale Fusion and Enhancement Operations.", "abstract": "A novel coronavirus disease 2019 (COVID-19) was detected and has spread rapidly across various countries around the world since the end of the year 2019. Computed Tomography (CT) images have been used as a crucial alternative to the time-consuming RT-PCR test. However, pure manual segmentation of CT images faces a serious challenge with the increase of suspected cases, resulting in urgent requirements for accurate and automatic segmentation of COVID-19 infections. Unfortunately, since the imaging characteristics of the COVID-19 infection are diverse and similar to the backgrounds, existing medical image segmentation methods cannot achieve satisfactory performance. In this article, we try to establish a new deep convolutional neural network tailored for segmenting the chest CT images with COVID-19 infections. We first maintain a large and new chest CT image dataset consisting of 165,667 annotated chest CT images from 861 patients with confirmed COVID-19. Inspired by the observation that the boundary of the infected lung can be enhanced by adjusting the global intensity, in the proposed deep CNN, we introduce a feature variation block which adaptively adjusts the global properties of the features for segmenting COVID-19 infection. The proposed FV block can enhance the capability of feature representation effectively and adaptively for diverse cases. We fuse features at different scales by proposing Progressive Atrous Spatial Pyramid Pooling to handle the sophisticated infection areas with diverse appearance and shapes. The proposed method achieves state-of-the-art performance. Dice similarity coefficients are 0.987 and 0.726 for lung and COVID-19 segmentation, respectively. We conducted experiments on the data collected in China and Germany and show that the proposed deep CNN can produce impressive performance effectively. The proposed network enhances the segmentation ability of the COVID-19 infection, makes the connection with other techniques and contributes to the development of remedying COVID-19 infection.", "journal": "IEEE transactions on big data", "date": "2021-02-02", "authors": ["QingsenYan", "BoWang", "DongGong", "ChuanLuo", "WeiZhao", "JianhuShen", "JingyangAi", "QinfengShi", "YanningZhang", "ShuoJin", "LiangZhang", "ZhengYou"], "doi": "10.1109/TBDATA.2021.3056564\n10.1109/JBHI.2020.3042069"}
{"title": "Use of Conventional Chest Imaging and Artificial Intelligence in COVID-19 Infection. A Review of the Literature.", "abstract": "The coronavirus disease caused by SARS-Cov-2 is a pandemic with millions of confirmed cases around the world and a high death toll. Currently, the real-time polymerase chain reaction (RT-PCR) is the standard diagnostic method for determining COVID-19 infection. Various failures in the detection of the disease by means of laboratory samples have raised certain doubts about the characterisation of the infection and the spread of contacts. In clinical practice, chest radiography (RT) and chest computed tomography (CT) are extremely helpful and have been widely used in the detection and diagnosis of COVID-19. RT is the most common and widely available diagnostic imaging technique, however, its reading by less qualified personnel, in many cases with work overload, causes a high number of errors to be committed. Chest CT can be used for triage, diagnosis, assessment of severity, progression, and response to treatment. Currently, artificial intelligence (AI) algorithms have shown promise in image classification, showing that they can reduce diagnostic errors by at least matching the diagnostic performance of radiologists. This review shows how AI applied to thoracic radiology speeds up and improves diagnosis, allowing to optimise the workflow of radiologists. It can provide an objective evaluation and achieve a reduction in subjectivity and variability. AI can also help to optimise the resources and increase the efficiency in the management of COVID-19 infection.\nLa enfermedad causada por el coronavirus SARS-CoV-2 es una pandemia con millones de casos confirmados en todo el mundo, y un alto n\u00famero de fallecimientos. Actualmente, la reacci\u00f3n en cadena de la polimerasa en tiempo real (RT-PCR) es el m\u00e9todo de diagn\u00f3stico est\u00e1ndar para determinar la infecci\u00f3n por COVID-19. Diversos fracasos en la detecci\u00f3n de la enfermedad por medio de muestras de laboratorio han planteado ciertas dudas sobre la caracterizaci\u00f3n de la infecci\u00f3n y la propagaci\u00f3n a los contactos.En la pr\u00e1ctica cl\u00ednica, la radiograf\u00eda de t\u00f3rax (RT) y la tomograf\u00eda computarizada (TC) de t\u00f3rax son extremadamente \u00fatiles y se han utilizado extensamente en la detecci\u00f3n y el diagn\u00f3stico de la COVID-19. La RT es la t\u00e9cnica de diagn\u00f3stico por imagen m\u00e1s com\u00fan, y la que est\u00e1 m\u00e1s ampliamente disponible, sin embargo, su lectura por personal menos cualificado, en muchos casos con sobrecarga de trabajo, hace que se cometa un gran n\u00famero de errores. La TC de t\u00f3rax se puede utilizar para el triaje, el diagn\u00f3stico, la evaluaci\u00f3n de la gravedad, la progresi\u00f3n y la respuesta al tratamiento. Actualmente, los algoritmos de inteligencia artificial (IA) han resultado prometedores en la clasificaci\u00f3n de im\u00e1genes, mostrando que pueden reducir los errores de diagn\u00f3stico, como m\u00ednimo igualando el rendimiento diagn\u00f3stico de los radi\u00f3logos.Esta revisi\u00f3n muestra c\u00f3mo la IA aplicada a la RT acelera y mejora el diagn\u00f3stico, lo que permite optimizar el flujo de trabajo de los radi\u00f3logos. Puede proporcionar una evaluaci\u00f3n objetiva y lograr una reducci\u00f3n de la subjetividad y la variabilidad. La IA tambi\u00e9n puede ayudar a optimizar los recursos y aumentar la eficiencia en la gesti\u00f3n de la infecci\u00f3n por COVID-19.", "journal": "Open respiratory archives", "date": "2021-01-08", "authors": ["Mar\u00eda DoloresCorbacho Abelaira", "FernandoCorbacho Abelaira", "AlbertoRuano-Ravina", "AlbertoFern\u00e1ndez-Villar"], "doi": "10.1016/j.opresp.2020.100078\n10.1128/jcm/58/5/JCM.00297-20.atom\n10.1101/2020.03.19.20039354\n10.1101/2020.03.20.20039834\n10.1101/2020.02.25.20021568\n10.1101/2020.03.12.20027185\n10.1101/2020.02.14.20023028\n10.1101/2020.02.23.20026930\n10.1007/s00330-020-06817-6"}
{"title": "Requirement of artificial intelligence technology awareness for thoracic surgeons.", "abstract": "We have recently witnessed incredible interest in computer-based, internet web-dependent mechanisms and artificial intelligence (AI)-dependent technique emergence in our day-to-day lives. In the recent era of COVID-19 pandemic, this nonhuman, machine-based technology has gained a lot of momentum.\nThe supercomputers and robotics with AI technology have shown the potential to equal or even surpass human experts' accuracy in some tasks in the future. Artificial intelligence (AI) is prompting massive data interweaving with elements from many digital sources such as medical imaging sorting, electronic health records, and transforming healthcare delivery. But in thoracic surgical and our counterpart pulmonary medical field, AI's main applications are still for interpretation of thoracic imaging, lung histopathological slide evaluation, physiological data interpretation, and biosignal testing only. The query arises whether AI-enabled technology-based or autonomous robots could ever do or provide better thoracic surgical procedures than current surgeons but it seems like an impossibility now.\nThis review article aims to provide information pertinent to the use of AI to thoracic surgical specialists. In this review article, we described AI and related terminologies, current utilisation, challenges, potential, and current need for awareness of this technology.", "journal": "The cardiothoracic surgeon", "date": "2021-01-01", "authors": ["AnshumanDarbari", "KrishanKumar", "ShubhankarDarbari", "Prashant LPatil"], "doi": "10.1186/s43057-021-00053-4\n10.1056/NEJMp1606402\n10.1136/svn-2017-000101\n10.1609/aimag.v27i4.1904\n10.1001/jamanetworkopen.2019.17221\n10.1038/s41591-018-0300-7\n10.1038/s41746-018-0048-y\n10.1186/s12916-019-1426-2\n10.1007/s13244-018-0645-y\n10.1016/j.gie.2020.06.040\n10.1136/thoraxjnl-2020-214556\n10.21037/tlcr-20-591\n10.1183/16000617.0181-2020\n10.1093/icvts/ivw085\n10.1183/16000617.0010-2020\n10.21037/vats.2018.11.01\n10.1002/rcs.1968\n10.21037/acs.2019.03.03\n10.1177/1553350618771417\n10.1126/scirobotics.aan6080\n10.1007/s11684-020-0784-7\n10.48111/2021.01.10\n10.1159/000492428\n10.1016/j.jss.2020.03.046\n10.1001/jamasurg.2019.2821\n10.1001/jama.2019.16842"}
{"title": "[Role of Chest Radiographs and CT Scans and the Application of Artificial Intelligence in Coronavirus Disease 2019].", "abstract": "Coronavirus disease (COVID-19) has threatened public health as a global pandemic. Chest CT and radiography are crucial in managing COVID-19 in addition to reverse transcription-polymerase chain reaction, which is the gold standard for COVID-19 diagnosis. This is a review of the current status of the use of chest CT and radiography in COVID-19 diagnosis and management and an introduction of early representative studies on the application of artificial intelligence to chest CT and radiography. The authors also share their experiences to provide insights into the future value of artificial intelligence.\n\ucf54\ub85c\ub098\ubc14\uc774\ub7ec\uc2a4\uac10\uc5fc\uc99d-19 (coronavirus disease 2019; \uc774\ud558 COVID-19)\ub294 \uc804 \uc138\uacc4\uc801 \ub300\uc720\ud589 \uc9c8\ud658\uc73c\ub85c \uc778\ub958 \ubcf4\uac74\uc744 \uc704\ud611\ud558\uace0 \uc788\ub2e4. \ud749\ubd80 CT \ubc0f \ud749\ubd80X\uc120\uc0ac\uc9c4\uc740 COVID-19\uc758 \ud45c\uc900 \uc9c4\ub2e8\uac80\uc0ac\uc778 \uc5ed\uc804\uc0ac \uc911\ud569\ud6a8\uc18c \uc5f0\uc1c4\ubc18\uc751\uc5d0 \ub354\ud558\uc5ec COVID-19 \uc9c4\ub2e8 \ubc0f \uc911\uc99d\ub3c4 \ud3c9\uac00\uc5d0\uc11c \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud558\uace0 \uc788\ub2e4. \ubcf8 \uc885\uc124\uc5d0\uc11c\ub294 \ud749\ubd80 CT \ubc0f \ud749\ubd80X\uc120\uc0ac\uc9c4\uc758 COVID-19 \ud3d0\ub834\uc5d0 \ub300\ud55c \ud604\uc7ac \uc5ed\ud560\uc5d0 \ub300\ud558\uc5ec \uc0b4\ud3b4\ubcf4\uace0 \uc778\uacf5\uc9c0\ub2a5\uc744 \uc801\uc6a9\ud55c \ub300\ud45c\uc801 \ucd08\uae30 \uc5f0\uad6c\ub4e4\uacfc \uc800\uc790\ub4e4\uc758 \uacbd\ud5d8\uc744 \uc18c\uac1c\ud568\uc73c\ub85c\uc368 \ud5a5\ud6c4 \ud65c\uc6a9\uac00\uce58\uc5d0 \ub300\ud574 \uc0b4\ud3b4\ubcf4\uace0\uc790 \ud55c\ub2e4.", "journal": "Taehan Yongsang Uihakhoe chi", "date": "2020-11-01", "authors": ["Seung-JinYoo", "Jin MoGoo", "Soon HoYoon"], "doi": "10.3348/jksr.2020.0138\n10.1007/s00330-020-07013-2\n10.1148/radiol.2020201754\n10.1007/s00330-020-07035-w\n10.1148/radiol.2020200905\n10.21203/rs.3.rs-48290/v1"}
{"title": "A Movement Detection System Using Continuous-Wave Doppler Radar Sensor and Convolutional Neural Network to Detect Cough and Other Gestures.", "abstract": "The 2019 coronavirus disease (COVID-19) pandemic has contaminated millions of people, resulting in high fatality rates. Recently emerging artificial intelligence technologies like the convolutional neural network (CNN) are strengthening the power of imaging tools and can help medical specialists. CNN combined with other sensors creates a new solution to fight COVID-19 transmission. This paper presents a novel method to detect coughs (an important symptom of COVID-19) using a K-band continuous-wave Doppler radar with most popular CNNs architectures: AlexNet, VGG-19, and GoogLeNet. The proposed method has cough detection test accuracy of 88.0% using AlexNet CNN with people 1 m away from the microwave radar sensor, test accuracy of 80.0% with people 3 m away from the radar sensor, and test accuracy of 86.5% with a single mixed dataset with people 1 m and 3 m away from the radar sensor. The K-band radar sensor is inexpensive, completely camera-free and collects no personally-identifying information, allaying privacy concerns while still providing in-depth public health data on individual, local, and national levels. Additionally, the measurements are conducted without human contact, making the process proposed in this work safe for the investigation of contagious diseases such as COVID-19. The proposed cough detection system using microwave radar sensor has environmental robustness and dark/light-independence, unlike traditional cameras. The proposed microwave radar sensor can be used alone or in group with other sensors in a fusion sensor system to create a robust system to detect cough and other movements, mainly if using CNNs.", "journal": "IEEE sensors journal", "date": "2020-10-05", "authors": ["Euclides LourencoChuma", "YuzoIano"], "doi": "10.1109/JSEN.2020.3028494\n10.1109/RBME.2020.2987975"}
{"title": "Leveraging Data Science to Combat COVID-19: A Comprehensive Review.", "abstract": "COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools-including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization-that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.", "journal": "IEEE transactions on artificial intelligence", "date": "2020-09-02", "authors": ["SiddiqueLatif", "MuhammadUsman", "SanaullahManzoor", "WaleedIqbal", "JunaidQadir", "GarethTyson", "IgnacioCastro", "AdeelRazi", "Maged N KamelBoulos", "AdrianWeller", "JonCrowcroft"], "doi": "10.1109/TAI.2020.3020521\n10.1126/sciadv.abc0764"}
